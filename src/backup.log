Starting training
batch:          0 | loss: 5.30233 | failed:    0
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:         10 | loss: 5.31060 | failed:    0
batch:         20 | loss: 5.29681 | failed:    0
batch:         30 | loss: 5.29152 | failed:    0
batch:         40 | loss: 5.30063 | failed:    0
batch:         50 | loss: 5.30021 | failed:    0
batch:         60 | loss: 5.29780 | failed:    0
batch:         70 | loss: 5.28270 | failed:    0
batch:         80 | loss: 5.29574 | failed:    0
batch:         90 | loss: 5.29630 | failed:    0
batch:        100 | loss: 5.29559 | failed:    0
batch:        110 | loss: 5.29551 | failed:    0
batch:        120 | loss: 5.30014 | failed:    0
batch:        130 | loss: 5.29893 | failed:    0
batch:        140 | loss: 5.29037 | failed:    0
batch:        150 | loss: 5.29271 | failed:    0
batch:        160 | loss: 5.29661 | failed:    0
batch:        170 | loss: 5.30046 | failed:    0
batch:        180 | loss: 5.26825 | failed:    0
batch:        190 | loss: 5.29646 | failed:    0
batch:        200 | loss: 5.31317 | failed:    0
batch:        210 | loss: 5.29953 | failed:    0
batch:        220 | loss: 5.27338 | failed:    0
batch:        230 | loss: 5.22806 | failed:    0
batch:        240 | loss: 5.30420 | failed:    0
batch:        250 | loss: 5.29545 | failed:    0
batch:        260 | loss: 5.26177 | failed:    0
batch:        270 | loss: 5.28916 | failed:    0
batch:        280 | loss: 5.24060 | failed:    0
batch:        290 | loss: 5.24820 | failed:    0
batch:        300 | loss: 5.29088 | failed:    0
batch:        310 | loss: 5.31105 | failed:    0
batch:        320 | loss: 5.28767 | failed:    0
batch:        330 | loss: 5.29052 | failed:    0
batch:        340 | loss: 5.29168 | failed:    0
batch:        350 | loss: 5.30006 | failed:    0
batch:        360 | loss: 5.24118 | failed:    0
batch:        370 | loss: 5.27494 | failed:    0
batch:        380 | loss: 5.26737 | failed:    0
batch:        390 | loss: 5.29378 | failed:    0
batch:        400 | loss: 5.29353 | failed:    0
batch:        410 | loss: 5.29777 | failed:    0
batch:        420 | loss: 5.28465 | failed:    0
batch:        430 | loss: 5.28536 | failed:    0
batch:        440 | loss: 5.25630 | failed:    0
batch:        450 | loss: 5.27624 | failed:    0
batch:        460 | loss: 5.23307 | failed:    0
batch:        470 | loss: 5.26192 | failed:    0
batch:        480 | loss: 5.27150 | failed:    0
batch:        490 | loss: 5.28930 | failed:    0
batch:        500 | loss: 5.24903 | failed:    0
batch:        510 | loss: 5.30027 | failed:    0
batch:        520 | loss: 5.24650 | failed:    0
batch:        530 | loss: 5.22302 | failed:    0
batch:        540 | loss: 5.29665 | failed:    0
batch:        550 | loss: 5.27668 | failed:    0
batch:        560 | loss: 5.02923 | failed:    0
batch:        570 | loss: 5.34718 | failed:    0
batch:        580 | loss: 5.25555 | failed:    0
batch:        590 | loss: 5.28623 | failed:    0
batch:        600 | loss: 5.28763 | failed:    0
batch:        610 | loss: 5.26033 | failed:    0
batch:        620 | loss: 5.28926 | failed:    0
batch:        630 | loss: 5.29739 | failed:    0
batch:        640 | loss: 5.24878 | failed:    0
batch:        650 | loss: 5.27152 | failed:    0
batch:        660 | loss: 5.27308 | failed:    0
batch:        670 | loss: 5.21004 | failed:    0
batch:        680 | loss: 5.26020 | failed:    0
batch:        690 | loss: 5.26080 | failed:    0
batch:        700 | loss: 5.27371 | failed:    0
batch:        710 | loss: 5.27720 | failed:    0
batch:        720 | loss: 5.28433 | failed:    0
batch:        730 | loss: 5.30733 | failed:    0
batch:        740 | loss: 5.29323 | failed:    0
batch:        750 | loss: 5.28169 | failed:    0
batch:        760 | loss: 5.24747 | failed:    0
batch:        770 | loss: 5.29040 | failed:    0
batch:        780 | loss: 5.26593 | failed:    0
batch:        790 | loss: 5.26082 | failed:    0
batch:        800 | loss: 5.24104 | failed:    0
batch:        810 | loss: 5.30991 | failed:    0
batch:        820 | loss: 5.27111 | failed:    0
batch:        830 | loss: 5.22291 | failed:    0
batch:        840 | loss: 5.29556 | failed:    0
batch:        850 | loss: 5.27237 | failed:    0
batch:        860 | loss: 5.29498 | failed:    0
batch:        870 | loss: 5.26907 | failed:    0
batch:        880 | loss: 5.25219 | failed:    0
batch:        890 | loss: 5.25827 | failed:    0
batch:        900 | loss: 5.28241 | failed:    0
batch:        910 | loss: 5.27073 | failed:    0
batch:        920 | loss: 5.24089 | failed:    0
batch:        930 | loss: 5.23265 | failed:    0
batch:        940 | loss: 5.30056 | failed:    0
batch:        950 | loss: 5.23286 | failed:    0
batch:        960 | loss: 5.22391 | failed:    0
batch:        970 | loss: 5.24946 | failed:    0
batch:        980 | loss: 5.21258 | failed:    0
batch:        990 | loss: 5.02207 | failed:    0
batch:       1000 | loss: 5.30719 | failed:    0
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       1010 | loss: 5.26083 | failed:    0
batch:       1020 | loss: 5.28209 | failed:    0
batch:       1030 | loss: 5.28319 | failed:    0
batch:       1040 | loss: 5.30856 | failed:    0
batch:       1050 | loss: 5.29158 | failed:    0
batch:       1060 | loss: 5.27183 | failed:    0
batch:       1070 | loss: 5.27098 | failed:    0
batch:       1080 | loss: 5.25748 | failed:    0
batch:       1090 | loss: 5.27449 | failed:    0
batch:       1100 | loss: 5.27959 | failed:    0
batch:       1110 | loss: 5.27649 | failed:    0
batch:       1120 | loss: 5.24252 | failed:    0
batch:       1130 | loss: 5.24205 | failed:    0
batch:       1140 | loss: 5.17102 | failed:    0
batch:       1150 | loss: 5.25318 | failed:    0
batch:       1160 | loss: 5.27569 | failed:    0
batch:       1170 | loss: 5.22348 | failed:    0
batch:       1180 | loss: 5.29804 | failed:    0
batch:       1190 | loss: 5.27724 | failed:    0
batch:       1200 | loss: 5.23808 | failed:    0
batch:       1210 | loss: 5.26730 | failed:    0
batch:       1220 | loss: 5.11720 | failed:    0
batch:       1230 | loss: 5.24993 | failed:    0
batch:       1240 | loss: 5.27710 | failed:    0
batch:       1250 | loss: 5.26146 | failed:    0
batch:       1260 | loss: 5.26605 | failed:    0
batch:       1270 | loss: 5.20726 | failed:    0
batch:       1280 | loss: 5.25911 | failed:    0
batch:       1290 | loss: 5.24836 | failed:    0
batch:       1300 | loss: 5.24759 | failed:    0
batch:       1310 | loss: 5.28790 | failed:    0
batch:       1320 | loss: 5.25893 | failed:    0
batch:       1330 | loss: 5.30275 | failed:    0
batch:       1340 | loss: 5.25301 | failed:    0
batch:       1350 | loss: 5.28002 | failed:    0
batch:       1360 | loss: 5.26051 | failed:    0
batch:       1370 | loss: 5.23407 | failed:    0
batch:       1380 | loss: 5.26011 | failed:    0
batch:       1390 | loss: 5.22974 | failed:    0
batch:       1400 | loss: 5.25938 | failed:    0
batch:       1410 | loss: 5.25471 | failed:    0
batch:       1420 | loss: 5.29460 | failed:    0
batch:       1430 | loss: 5.22209 | failed:    0
batch:       1440 | loss: 5.24869 | failed:    0
batch:       1450 | loss: 5.29852 | failed:    0
batch:       1460 | loss: 5.26863 | failed:    0
batch:       1470 | loss: 5.25794 | failed:    0
batch:       1480 | loss: 5.24262 | failed:    0
batch:       1490 | loss: 5.30681 | failed:    0
batch:       1500 | loss: 5.30326 | failed:    0
batch:       1510 | loss: 5.28587 | failed:    0
batch:       1520 | loss: 5.28122 | failed:    0
batch:       1530 | loss: 5.28045 | failed:    0
batch:       1540 | loss: 5.25046 | failed:    0
batch:       1550 | loss: 5.26760 | failed:    0
batch:       1560 | loss: 5.29560 | failed:    0
batch:       1570 | loss: 5.24351 | failed:    0
batch:       1580 | loss: 5.23125 | failed:    0
batch:       1590 | loss: 5.29367 | failed:    0
batch:       1600 | loss: 5.23367 | failed:    0
batch:       1610 | loss: 5.15168 | failed:    0
batch:       1620 | loss: 5.22094 | failed:    0
batch:       1630 | loss: 5.27296 | failed:    0
batch:       1640 | loss: 5.27136 | failed:    0
batch:       1650 | loss: 5.24144 | failed:    0
batch:       1660 | loss: 5.22735 | failed:    0
batch:       1670 | loss: 5.27987 | failed:    0
batch:       1680 | loss: 5.21096 | failed:    0
batch:       1690 | loss: 5.24261 | failed:    0
batch:       1700 | loss: 5.23407 | failed:    0
batch:       1710 | loss: 5.21111 | failed:    0
batch:       1720 | loss: 5.33718 | failed:    0
batch:       1730 | loss: 5.31079 | failed:    0
batch:       1740 | loss: 5.29431 | failed:    0
batch:       1750 | loss: 5.28880 | failed:    0
batch:       1760 | loss: 5.26839 | failed:    0
batch:       1770 | loss: 5.27046 | failed:    0
batch:       1780 | loss: 5.13258 | failed:    0
batch:       1790 | loss: 5.25562 | failed:    0
batch:       1800 | loss: 5.21920 | failed:    0
batch:       1810 | loss: 5.23856 | failed:    0
batch:       1820 | loss: 5.23870 | failed:    0
batch:       1830 | loss: 5.25387 | failed:    0
batch:       1840 | loss: 5.26824 | failed:    0
batch:       1850 | loss: 5.24187 | failed:    0
batch:       1860 | loss: 5.26789 | failed:    0
batch:       1870 | loss: 5.28689 | failed:    0
batch:       1880 | loss: 5.26420 | failed:    0
batch:       1890 | loss: 5.23132 | failed:    0
batch:       1900 | loss: 5.33067 | failed:    0
batch:       1910 | loss: 5.25713 | failed:    0
batch:       1920 | loss: 5.27191 | failed:    0
batch:       1930 | loss: 5.25014 | failed:    0
batch:       1940 | loss: 5.22530 | failed:    0
batch:       1950 | loss: 5.29027 | failed:    0
batch:       1960 | loss: 5.24165 | failed:    0
batch:       1970 | loss: 5.19120 | failed:    0
batch:       1980 | loss: 5.26999 | failed:    0
batch:       1990 | loss: 5.25741 | failed:    0
batch:       2000 | loss: 5.26543 | failed:    0
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       2010 | loss: 5.24268 | failed:    0
batch:       2020 | loss: 5.14329 | failed:    0
batch:       2030 | loss: 5.24525 | failed:    0
batch:       2040 | loss: 5.22453 | failed:    0
batch:       2050 | loss: 5.22159 | failed:    0
batch:       2060 | loss: 5.28066 | failed:    0
batch:       2070 | loss: 5.21299 | failed:    0
batch:       2080 | loss: 5.34311 | failed:    0
batch:       2090 | loss: 5.26875 | failed:    0
batch:       2100 | loss: 5.19704 | failed:    0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       2110 | loss: 5.29368 | failed:    0
batch:       2120 | loss: 5.27503 | failed:    0
batch:       2130 | loss: 5.25655 | failed:    1
batch:       2140 | loss: 5.27253 | failed:    1
batch:       2150 | loss: 5.25919 | failed:    1
batch:       2160 | loss: 5.23700 | failed:    1
batch:       2170 | loss: 5.26423 | failed:    1
batch:       2180 | loss: 5.19804 | failed:    1
batch:       2190 | loss: 5.07320 | failed:    1
batch:       2200 | loss: 4.87373 | failed:    1
batch:       2210 | loss: 5.28713 | failed:    1
batch:       2220 | loss: 5.15481 | failed:    1
batch:       2230 | loss: 5.21554 | failed:    1
batch:       2240 | loss: 5.28194 | failed:    1
batch:       2250 | loss: 5.26830 | failed:    1
batch:       2260 | loss: 5.20611 | failed:    1
batch:       2270 | loss: 5.23275 | failed:    1
batch:       2280 | loss: 5.22828 | failed:    1
batch:       2290 | loss: 5.27502 | failed:    1
batch:       2300 | loss: 5.27718 | failed:    1
batch:       2310 | loss: 5.25449 | failed:    1
batch:       2320 | loss: 5.25411 | failed:    1
batch:       2330 | loss: 5.26744 | failed:    1
batch:       2340 | loss: 5.25336 | failed:    1
batch:       2350 | loss: 5.30698 | failed:    1
batch:       2360 | loss: 5.28645 | failed:    1
batch:       2370 | loss: 5.28128 | failed:    1
batch:       2380 | loss: 5.26737 | failed:    1
batch:       2390 | loss: 5.25585 | failed:    1
batch:       2400 | loss: 5.26432 | failed:    1
batch:       2410 | loss: 5.26576 | failed:    1
batch:       2420 | loss: 5.24273 | failed:    1
batch:       2430 | loss: 5.25824 | failed:    1
batch:       2440 | loss: 5.23593 | failed:    1
batch:       2450 | loss: 5.20134 | failed:    1
batch:       2460 | loss: 5.24092 | failed:    1
batch:       2470 | loss: 5.30503 | failed:    1
batch:       2480 | loss: 5.19863 | failed:    1
batch:       2490 | loss: 5.24842 | failed:    1
batch:       2500 | loss: 5.22918 | failed:    1
batch:       2510 | loss: 5.22841 | failed:    1
batch:       2520 | loss: 5.17355 | failed:    1
batch:       2530 | loss: 5.26031 | failed:    1
batch:       2540 | loss: 5.20217 | failed:    1
batch:       2550 | loss: 5.24084 | failed:    1
batch:       2560 | loss: 5.13961 | failed:    1
batch:       2570 | loss: 5.28718 | failed:    1
batch:       2580 | loss: 5.24481 | failed:    1
batch:       2590 | loss: 5.24188 | failed:    1
batch:       2600 | loss: 5.20161 | failed:    1
batch:       2610 | loss: 5.20411 | failed:    1
batch:       2620 | loss: 5.26024 | failed:    1
batch:       2630 | loss: 5.26139 | failed:    1
batch:       2640 | loss: 5.22167 | failed:    1
batch:       2650 | loss: 5.22051 | failed:    1
batch:       2660 | loss: 5.19387 | failed:    1
batch:       2670 | loss: 5.17569 | failed:    1
batch:       2680 | loss: 5.25450 | failed:    1
batch:       2690 | loss: 5.23285 | failed:    1
batch:       2700 | loss: 5.28687 | failed:    1
batch:       2710 | loss: 5.29139 | failed:    1
batch:       2720 | loss: 5.22118 | failed:    1
batch:       2730 | loss: 5.24621 | failed:    1
batch:       2740 | loss: 5.30533 | failed:    1
batch:       2750 | loss: 5.28256 | failed:    1
batch:       2760 | loss: 5.21551 | failed:    1
batch:       2770 | loss: 5.19753 | failed:    1
batch:       2780 | loss: 5.24125 | failed:    1
batch:       2790 | loss: 5.21819 | failed:    1
batch:       2800 | loss: 5.27092 | failed:    1
batch:       2810 | loss: 5.25496 | failed:    1
batch:       2820 | loss: 5.29755 | failed:    1
batch:       2830 | loss: 5.25397 | failed:    1
batch:       2840 | loss: 5.23032 | failed:    1
batch:       2850 | loss: 5.20754 | failed:    1
batch:       2860 | loss: 5.26694 | failed:    1
batch:       2870 | loss: 5.26383 | failed:    1
batch:       2880 | loss: 5.23780 | failed:    1
batch:       2890 | loss: 5.24826 | failed:    1
batch:       2900 | loss: 5.25706 | failed:    1
batch:       2910 | loss: 5.27084 | failed:    1
batch:       2920 | loss: 5.18435 | failed:    1
batch:       2930 | loss: 5.27874 | failed:    1
batch:       2940 | loss: 5.27174 | failed:    1
batch:       2950 | loss: 5.25572 | failed:    1
batch:       2960 | loss: 5.22634 | failed:    1
batch:       2970 | loss: 5.21266 | failed:    1
batch:       2980 | loss: 5.20869 | failed:    1
batch:       2990 | loss: 5.07124 | failed:    1
batch:       3000 | loss: 5.23547 | failed:    1
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       3010 | loss: 5.22049 | failed:    1
batch:       3020 | loss: 5.63964 | failed:    1
batch:       3030 | loss: 5.19630 | failed:    1
batch:       3040 | loss: 5.27614 | failed:    1
batch:       3050 | loss: 5.26840 | failed:    1
batch:       3060 | loss: 5.22349 | failed:    1
batch:       3070 | loss: 5.19101 | failed:    1
batch:       3080 | loss: 5.29640 | failed:    1
batch:       3090 | loss: 5.26813 | failed:    1
batch:       3100 | loss: 5.25820 | failed:    1
batch:       3110 | loss: 5.07395 | failed:    1
batch:       3120 | loss: 5.25673 | failed:    1
batch:       3130 | loss: 5.14975 | failed:    1
batch:       3140 | loss: 5.22297 | failed:    1
batch:       3150 | loss: 5.19889 | failed:    1
batch:       3160 | loss: 5.23635 | failed:    1
batch:       3170 | loss: 5.27242 | failed:    1
batch:       3180 | loss: 5.24794 | failed:    1
batch:       3190 | loss: 5.23847 | failed:    1
batch:       3200 | loss: 5.25932 | failed:    1
batch:       3210 | loss: 5.26557 | failed:    1
batch:       3220 | loss: 5.24840 | failed:    1
batch:       3230 | loss: 5.21944 | failed:    1
batch:       3240 | loss: 5.24323 | failed:    1
batch:       3250 | loss: 5.23129 | failed:    1
batch:       3260 | loss: 5.16957 | failed:    1
batch:       3270 | loss: 5.19895 | failed:    1
batch:       3280 | loss: 5.24684 | failed:    1
batch:       3290 | loss: 5.20970 | failed:    1
batch:       3300 | loss: 5.24047 | failed:    1
batch:       3310 | loss: 5.24503 | failed:    1
batch:       3320 | loss: 5.19104 | failed:    1
batch:       3330 | loss: 5.11498 | failed:    1
batch:       3340 | loss: 5.12204 | failed:    1
batch:       3350 | loss: 5.16883 | failed:    1
batch:       3360 | loss: 5.18322 | failed:    1
batch:       3370 | loss: 5.04183 | failed:    1
batch:       3380 | loss: 5.17225 | failed:    1
batch:       3390 | loss: 5.29246 | failed:    1
batch:       3400 | loss: 5.23326 | failed:    1
batch:       3410 | loss: 5.16232 | failed:    1
batch:       3420 | loss: 5.22536 | failed:    1
batch:       3430 | loss: 5.22160 | failed:    1
batch:       3440 | loss: 5.13866 | failed:    1
batch:       3450 | loss: 5.12438 | failed:    1
batch:       3460 | loss: 5.27075 | failed:    1
batch:       3470 | loss: 5.29160 | failed:    1
batch:       3480 | loss: 5.23603 | failed:    1
batch:       3490 | loss: 5.25805 | failed:    1
batch:       3500 | loss: 5.19413 | failed:    1
batch:       3510 | loss: 5.21957 | failed:    1
batch:       3520 | loss: 5.10938 | failed:    1
batch:       3530 | loss: 5.20306 | failed:    1
batch:       3540 | loss: 5.23282 | failed:    1
batch:       3550 | loss: 5.32497 | failed:    1
batch:       3560 | loss: 5.17680 | failed:    1
batch:       3570 | loss: 5.20593 | failed:    1
batch:       3580 | loss: 5.17873 | failed:    1
batch:       3590 | loss: 5.20707 | failed:    1
batch:       3600 | loss: 5.26409 | failed:    1
batch:       3610 | loss: 5.22088 | failed:    1
batch:       3620 | loss: 5.25061 | failed:    1
batch:       3630 | loss: 5.41388 | failed:    1
batch:       3640 | loss: 5.28101 | failed:    1
batch:       3650 | loss: 5.23608 | failed:    1
batch:       3660 | loss: 5.31132 | failed:    1
batch:       3670 | loss: 5.28955 | failed:    1
batch:       3680 | loss: 5.18936 | failed:    1
batch:       3690 | loss: 5.27207 | failed:    1
batch:       3700 | loss: 5.21083 | failed:    1
batch:       3710 | loss: 5.19361 | failed:    1
batch:       3720 | loss: 5.23439 | failed:    1
batch:       3730 | loss: 5.15581 | failed:    1
batch:       3740 | loss: 5.15169 | failed:    1
batch:       3750 | loss: 5.23552 | failed:    1
batch:       3760 | loss: 5.25607 | failed:    1
batch:       3770 | loss: 5.23506 | failed:    1
batch:       3780 | loss: 5.29929 | failed:    1
batch:       3790 | loss: 5.23985 | failed:    1
batch:       3800 | loss: 5.19915 | failed:    1
batch:       3810 | loss: 5.26666 | failed:    1
batch:       3820 | loss: 5.24470 | failed:    1
batch:       3830 | loss: 5.18893 | failed:    1
batch:       3840 | loss: 5.17179 | failed:    1
batch:       3850 | loss: 5.16171 | failed:    1
batch:       3860 | loss: 5.21989 | failed:    1
batch:       3870 | loss: 5.31489 | failed:    1
batch:       3880 | loss: 5.27041 | failed:    1
batch:       3890 | loss: 5.23123 | failed:    1
batch:       3900 | loss: 5.20768 | failed:    1
batch:       3910 | loss: 5.26910 | failed:    1
batch:       3920 | loss: 5.16522 | failed:    1
batch:       3930 | loss: 5.19641 | failed:    1
batch:       3940 | loss: 5.20481 | failed:    1
batch:       3950 | loss: 5.21392 | failed:    1
batch:       3960 | loss: 5.23668 | failed:    1
batch:       3970 | loss: 5.25006 | failed:    1
batch:       3980 | loss: 5.22628 | failed:    1
batch:       3990 | loss: 5.24100 | failed:    1
batch:       4000 | loss: 5.23938 | failed:    1
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       4010 | loss: 5.21181 | failed:    1
batch:       4020 | loss: 5.29303 | failed:    1
batch:       4030 | loss: 5.26100 | failed:    1
batch:       4040 | loss: 5.19462 | failed:    1
batch:       4050 | loss: 5.22246 | failed:    1
batch:       4060 | loss: 5.16794 | failed:    1
batch:       4070 | loss: 5.32389 | failed:    1
batch:       4080 | loss: 5.13886 | failed:    1
batch:       4090 | loss: 5.29156 | failed:    1
batch:       4100 | loss: 5.25502 | failed:    1
batch:       4110 | loss: 5.25554 | failed:    1
batch:       4120 | loss: 5.18481 | failed:    1
batch:       4130 | loss: 5.24464 | failed:    1
batch:       4140 | loss: 5.20435 | failed:    1
batch:       4150 | loss: 5.20197 | failed:    1
batch:       4160 | loss: 5.27747 | failed:    1
batch:       4170 | loss: 5.19152 | failed:    1
batch:       4180 | loss: 5.27360 | failed:    1
batch:       4190 | loss: 5.21543 | failed:    1
batch:       4200 | loss: 5.16880 | failed:    1
batch:       4210 | loss: 5.22072 | failed:    1
batch:       4220 | loss: 5.23562 | failed:    1
batch:       4230 | loss: 5.29038 | failed:    1
batch:       4240 | loss: 5.24003 | failed:    1
batch:       4250 | loss: 5.17405 | failed:    1
batch:       4260 | loss: 5.25545 | failed:    1
batch:       4270 | loss: 5.23057 | failed:    1
batch:       4280 | loss: 5.23890 | failed:    1
batch:       4290 | loss: 5.20432 | failed:    1
batch:       4300 | loss: 5.20309 | failed:    1
batch:       4310 | loss: 5.24669 | failed:    1
batch:       4320 | loss: 5.24001 | failed:    1
batch:       4330 | loss: 5.26931 | failed:    1
batch:       4340 | loss: 5.20947 | failed:    1
batch:       4350 | loss: 5.27332 | failed:    1
batch:       4360 | loss: 5.26453 | failed:    1
batch:       4370 | loss: 5.28177 | failed:    1
batch:       4380 | loss: 5.25469 | failed:    1
batch:       4390 | loss: 5.23747 | failed:    1
batch:       4400 | loss: 5.20277 | failed:    1
batch:       4410 | loss: 5.19905 | failed:    1
batch:       4420 | loss: 5.30297 | failed:    1
batch:       4430 | loss: 5.22245 | failed:    1
batch:       4440 | loss: 5.25230 | failed:    1
batch:       4450 | loss: 5.24402 | failed:    1
batch:       4460 | loss: 5.25460 | failed:    1
batch:       4470 | loss: 5.18827 | failed:    1
batch:       4480 | loss: 5.23315 | failed:    1
batch:       4490 | loss: 5.24413 | failed:    1
batch:       4500 | loss: 5.24293 | failed:    1
batch:       4510 | loss: 5.20563 | failed:    1
batch:       4520 | loss: 5.25405 | failed:    1
batch:       4530 | loss: 5.20931 | failed:    1
batch:       4540 | loss: 5.17924 | failed:    1
batch:       4550 | loss: 5.07059 | failed:    1
batch:       4560 | loss: 5.15933 | failed:    1
batch:       4570 | loss: 5.21045 | failed:    1
batch:       4580 | loss: 5.18205 | failed:    1
batch:       4590 | loss: 5.05526 | failed:    1
batch:       4600 | loss: 5.19361 | failed:    1
batch:       4610 | loss: 5.35330 | failed:    1
batch:       4620 | loss: 5.27744 | failed:    1
batch:       4630 | loss: 5.29029 | failed:    1
batch:       4640 | loss: 5.18808 | failed:    1
batch:       4650 | loss: 5.20783 | failed:    1
batch:       4660 | loss: 5.28151 | failed:    1
batch:       4670 | loss: 5.25674 | failed:    1
batch:       4680 | loss: 5.26728 | failed:    1
batch:       4690 | loss: 5.24489 | failed:    1
batch:       4700 | loss: 5.26165 | failed:    1
batch:       4710 | loss: 5.28013 | failed:    1
batch:       4720 | loss: 5.26285 | failed:    1
batch:       4730 | loss: 5.21301 | failed:    1
batch:       4740 | loss: 5.20309 | failed:    1
batch:       4750 | loss: 5.29115 | failed:    1
batch:       4760 | loss: 5.23006 | failed:    1
batch:       4770 | loss: 5.26261 | failed:    1
batch:       4780 | loss: 5.22446 | failed:    1
batch:       4790 | loss: 5.24787 | failed:    1
batch:       4800 | loss: 5.12411 | failed:    1
batch:       4810 | loss: 5.11279 | failed:    1
batch:       4820 | loss: 5.05676 | failed:    1
batch:       4830 | loss: 5.31821 | failed:    1
batch:       4840 | loss: 5.25134 | failed:    1
batch:       4850 | loss: 5.25110 | failed:    1
batch:       4860 | loss: 5.27177 | failed:    1
batch:       4870 | loss: 5.28358 | failed:    1
batch:       4880 | loss: 5.21035 | failed:    1
batch:       4890 | loss: 5.23488 | failed:    1
batch:       4900 | loss: 5.24851 | failed:    1
batch:       4910 | loss: 5.22398 | failed:    1
batch:       4920 | loss: 5.25158 | failed:    1
batch:       4930 | loss: 5.27431 | failed:    1
batch:       4940 | loss: 5.21116 | failed:    1
batch:       4950 | loss: 5.26448 | failed:    1
batch:       4960 | loss: 5.27011 | failed:    1
batch:       4970 | loss: 5.15676 | failed:    1
batch:       4980 | loss: 5.34407 | failed:    1
batch:       4990 | loss: 5.24231 | failed:    1
batch:       5000 | loss: 5.23976 | failed:    1
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       5010 | loss: 5.21908 | failed:    1
batch:       5020 | loss: 5.26199 | failed:    1
batch:       5030 | loss: 5.19008 | failed:    1
batch:       5040 | loss: 5.19971 | failed:    1
batch:       5050 | loss: 5.29082 | failed:    1
batch:       5060 | loss: 5.26083 | failed:    1
batch:       5070 | loss: 5.27069 | failed:    1
batch:       5080 | loss: 5.27121 | failed:    1
batch:       5090 | loss: 5.24253 | failed:    1
batch:       5100 | loss: 5.30620 | failed:    1
batch:       5110 | loss: 5.24978 | failed:    1
batch:       5120 | loss: 5.24438 | failed:    1
batch:       5130 | loss: 5.18268 | failed:    1
batch:       5140 | loss: 5.23043 | failed:    1
batch:       5150 | loss: 5.19695 | failed:    1
batch:       5160 | loss: 5.22922 | failed:    1
batch:       5170 | loss: 5.23477 | failed:    1
batch:       5180 | loss: 5.27202 | failed:    1
batch:       5190 | loss: 5.28055 | failed:    1
batch:       5200 | loss: 5.17720 | failed:    1
batch:       5210 | loss: 5.19268 | failed:    1
batch:       5220 | loss: 5.14790 | failed:    1
batch:       5230 | loss: 5.19150 | failed:    1
batch:       5240 | loss: 5.17964 | failed:    1
batch:       5250 | loss: 5.24734 | failed:    1
batch:       5260 | loss: 5.23864 | failed:    1
batch:       5270 | loss: 5.16076 | failed:    1
batch:       5280 | loss: 5.19620 | failed:    1
batch:       5290 | loss: 5.26984 | failed:    1
batch:       5300 | loss: 5.18226 | failed:    1
batch:       5310 | loss: 5.15305 | failed:    1
batch:       5320 | loss: 5.19564 | failed:    1
batch:       5330 | loss: 5.23198 | failed:    1
batch:       5340 | loss: 5.28850 | failed:    1
batch:       5350 | loss: 5.23774 | failed:    1
batch:       5360 | loss: 5.21562 | failed:    1
batch:       5370 | loss: 5.27901 | failed:    1
batch:       5380 | loss: 5.29784 | failed:    1
batch:       5390 | loss: 5.27469 | failed:    1
batch:       5400 | loss: 5.20829 | failed:    1
batch:       5410 | loss: 5.15432 | failed:    1
batch:       5420 | loss: 5.05633 | failed:    1
batch:       5430 | loss: 5.12307 | failed:    1
batch:       5440 | loss: 5.23289 | failed:    1
batch:       5450 | loss: 5.30496 | failed:    1
batch:       5460 | loss: 5.21956 | failed:    1
batch:       5470 | loss: 5.23614 | failed:    1
batch:       5480 | loss: 5.19714 | failed:    1
batch:       5490 | loss: 5.24593 | failed:    1
batch:       5500 | loss: 5.20616 | failed:    1
batch:       5510 | loss: 5.20606 | failed:    1
batch:       5520 | loss: 5.21624 | failed:    1
batch:       5530 | loss: 5.28481 | failed:    1
batch:       5540 | loss: 5.19912 | failed:    1
batch:       5550 | loss: 5.22775 | failed:    1
batch:       5560 | loss: 5.30053 | failed:    1
batch:       5570 | loss: 5.24579 | failed:    1
batch:       5580 | loss: 5.23830 | failed:    1
batch:       5590 | loss: 5.22507 | failed:    1
batch:       5600 | loss: 5.26179 | failed:    1
batch:       5610 | loss: 5.25843 | failed:    1
batch:       5620 | loss: 5.26987 | failed:    1
batch:       5630 | loss: 5.21626 | failed:    1
batch:       5640 | loss: 5.13798 | failed:    1
batch:       5650 | loss: 5.20906 | failed:    1
batch:       5660 | loss: 5.20384 | failed:    1
batch:       5670 | loss: 5.16833 | failed:    1
batch:       5680 | loss: 5.26534 | failed:    1
batch:       5690 | loss: 5.23871 | failed:    1
batch:       5700 | loss: 5.21997 | failed:    1
batch:       5710 | loss: 5.18425 | failed:    1
batch:       5720 | loss: 5.28979 | failed:    1
batch:       5730 | loss: 5.25272 | failed:    1
batch:       5740 | loss: 5.20005 | failed:    1
batch:       5750 | loss: 5.16861 | failed:    1
batch:       5760 | loss: 5.25165 | failed:    1
batch:       5770 | loss: 5.25109 | failed:    1
batch:       5780 | loss: 5.25637 | failed:    1
batch:       5790 | loss: 5.19815 | failed:    1
batch:       5800 | loss: 5.25220 | failed:    1
batch:       5810 | loss: 5.19015 | failed:    1
batch:       5820 | loss: 5.23541 | failed:    1
batch:       5830 | loss: 5.27126 | failed:    1
batch:       5840 | loss: 5.23268 | failed:    1
batch:       5850 | loss: 5.21647 | failed:    1
batch:       5860 | loss: 5.14115 | failed:    1
batch:       5870 | loss: 5.24621 | failed:    1
batch:       5880 | loss: 5.15766 | failed:    1
batch:       5890 | loss: 5.29224 | failed:    1
batch:       5900 | loss: 5.27394 | failed:    1
batch:       5910 | loss: 5.26965 | failed:    1
batch:       5920 | loss: 5.25563 | failed:    1
batch:       5930 | loss: 5.24169 | failed:    1
batch:       5940 | loss: 5.16254 | failed:    1
batch:       5950 | loss: 5.23792 | failed:    1
batch:       5960 | loss: 5.20986 | failed:    1
batch:       5970 | loss: 5.25188 | failed:    1
batch:       5980 | loss: 5.11210 | failed:    1
batch:       5990 | loss: 5.25817 | failed:    1
batch:       6000 | loss: 5.20467 | failed:    1
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       6010 | loss: 5.14077 | failed:    1
batch:       6020 | loss: 5.21945 | failed:    1
batch:       6030 | loss: 5.22124 | failed:    1
batch:       6040 | loss: 5.16171 | failed:    1
batch:       6050 | loss: 5.15472 | failed:    1
batch:       6060 | loss: 5.23861 | failed:    1
batch:       6070 | loss: 5.27923 | failed:    1
batch:       6080 | loss: 5.21689 | failed:    1
batch:       6090 | loss: 5.23133 | failed:    1
batch:       6100 | loss: 5.18593 | failed:    1
batch:       6110 | loss: 5.21794 | failed:    1
batch:       6120 | loss: 5.26174 | failed:    1
batch:       6130 | loss: 5.10621 | failed:    1
batch:       6140 | loss: 5.13686 | failed:    1
batch:       6150 | loss: 5.22870 | failed:    1
batch:       6160 | loss: 5.21502 | failed:    1
batch:       6170 | loss: 5.20745 | failed:    1
batch:       6180 | loss: 4.91480 | failed:    1
batch:       6190 | loss: 5.23930 | failed:    1
batch:       6200 | loss: 5.19882 | failed:    1
batch:       6210 | loss: 5.25684 | failed:    1
batch:       6220 | loss: 5.19150 | failed:    1
batch:       6230 | loss: 5.24666 | failed:    1
batch:       6240 | loss: 5.23839 | failed:    1
batch:       6250 | loss: 5.25915 | failed:    1
batch:       6260 | loss: 5.15306 | failed:    1
batch:       6270 | loss: 5.26432 | failed:    1
batch:       6280 | loss: 5.19811 | failed:    1
batch:       6290 | loss: 5.24423 | failed:    1
batch:       6300 | loss: 5.26062 | failed:    1
batch:       6310 | loss: 5.33773 | failed:    1
batch:       6320 | loss: 5.28068 | failed:    1
batch:       6330 | loss: 5.25734 | failed:    1
batch:       6340 | loss: 5.27582 | failed:    1
batch:       6350 | loss: 5.24183 | failed:    1
batch:       6360 | loss: 5.25617 | failed:    1
batch:       6370 | loss: 5.22124 | failed:    1
batch:       6380 | loss: 5.08749 | failed:    1
batch:       6390 | loss: 5.21907 | failed:    1
batch:       6400 | loss: 5.14584 | failed:    1
batch:       6410 | loss: 5.22220 | failed:    1
batch:       6420 | loss: 5.21572 | failed:    1
batch:       6430 | loss: 5.17456 | failed:    1
batch:       6440 | loss: 5.15393 | failed:    1
batch:       6450 | loss: 5.18900 | failed:    1
batch:       6460 | loss: 5.21054 | failed:    1
batch:       6470 | loss: 5.27213 | failed:    1
batch:       6480 | loss: 5.28474 | failed:    1
batch:       6490 | loss: 5.24554 | failed:    1
batch:       6500 | loss: 5.26297 | failed:    1
batch:       6510 | loss: 5.24435 | failed:    1
batch:       6520 | loss: 5.23352 | failed:    1
batch:       6530 | loss: 4.97982 | failed:    1
batch:       6540 | loss: 4.85284 | failed:    1
batch:       6550 | loss: 4.70701 | failed:    1
batch:       6560 | loss: 5.23649 | failed:    1
batch:       6570 | loss: 5.19483 | failed:    1
batch:       6580 | loss: 5.22666 | failed:    1
batch:       6590 | loss: 5.23351 | failed:    1
batch:       6600 | loss: 5.23156 | failed:    1
batch:       6610 | loss: 5.26743 | failed:    1
batch:       6620 | loss: 5.19938 | failed:    1
batch:       6630 | loss: 5.19887 | failed:    1
batch:       6640 | loss: 5.25983 | failed:    1
batch:       6650 | loss: 5.23608 | failed:    1
batch:       6660 | loss: 5.21362 | failed:    1
batch:       6670 | loss: 5.25078 | failed:    1
batch:       6680 | loss: 5.25687 | failed:    1
batch:       6690 | loss: 5.24657 | failed:    1
batch:       6700 | loss: 5.23063 | failed:    1
batch:       6710 | loss: 5.12031 | failed:    1
batch:       6720 | loss: 5.17499 | failed:    1
batch:       6730 | loss: 5.15215 | failed:    1
batch:       6740 | loss: 5.31541 | failed:    1
batch:       6750 | loss: 5.27726 | failed:    1
batch:       6760 | loss: 5.18437 | failed:    1
batch:       6770 | loss: 5.28337 | failed:    1
batch:       6780 | loss: 5.27493 | failed:    1
batch:       6790 | loss: 5.25549 | failed:    1
batch:       6800 | loss: 5.21669 | failed:    1
batch:       6810 | loss: 5.19710 | failed:    1
batch:       6820 | loss: 5.21980 | failed:    1
batch:       6830 | loss: 5.21120 | failed:    1
batch:       6840 | loss: 5.15257 | failed:    1
batch:       6850 | loss: 5.19815 | failed:    1
batch:       6860 | loss: 5.22956 | failed:    1
batch:       6870 | loss: 5.21900 | failed:    1
batch:       6880 | loss: 5.14433 | failed:    1
batch:       6890 | loss: 5.14699 | failed:    1
batch:       6900 | loss: 5.31976 | failed:    1
batch:       6910 | loss: 5.27888 | failed:    1
batch:       6920 | loss: 5.27038 | failed:    1
batch:       6930 | loss: 5.27080 | failed:    1
batch:       6940 | loss: 5.24767 | failed:    1
batch:       6950 | loss: 5.23351 | failed:    1
batch:       6960 | loss: 5.18865 | failed:    1
batch:       6970 | loss: 5.29885 | failed:    1
batch:       6980 | loss: 5.28765 | failed:    1
batch:       6990 | loss: 5.18429 | failed:    1
batch:       7000 | loss: 5.25878 | failed:    1
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       7010 | loss: 5.30514 | failed:    1
batch:       7020 | loss: 5.24985 | failed:    1
batch:       7030 | loss: 5.24236 | failed:    1
batch:       7040 | loss: 5.17356 | failed:    1
batch:       7050 | loss: 5.28250 | failed:    1
batch:       7060 | loss: 5.28386 | failed:    1
batch:       7070 | loss: 5.16650 | failed:    1
batch:       7080 | loss: 5.17507 | failed:    1
batch:       7090 | loss: 5.18812 | failed:    1
batch:       7100 | loss: 5.21749 | failed:    1
batch:       7110 | loss: 5.23948 | failed:    1
batch:       7120 | loss: 5.29812 | failed:    1
batch:       7130 | loss: 5.23463 | failed:    1
batch:       7140 | loss: 5.29790 | failed:    1
batch:       7150 | loss: 5.26009 | failed:    1
batch:       7160 | loss: 5.16441 | failed:    1
batch:       7170 | loss: 5.23739 | failed:    1
batch:       7180 | loss: 5.13731 | failed:    1
batch:       7190 | loss: 5.17981 | failed:    1
batch:       7200 | loss: 5.22687 | failed:    1
batch:       7210 | loss: 5.16608 | failed:    1
batch:       7220 | loss: 5.26013 | failed:    1
batch:       7230 | loss: 5.17272 | failed:    1
batch:       7240 | loss: 5.29117 | failed:    1
batch:       7250 | loss: 5.27843 | failed:    1
batch:       7260 | loss: 5.16069 | failed:    1
batch:       7270 | loss: 5.30695 | failed:    1
batch:       7280 | loss: 5.25479 | failed:    1
batch:       7290 | loss: 5.16897 | failed:    1
batch:       7300 | loss: 5.21121 | failed:    1
batch:       7310 | loss: 5.20551 | failed:    1
batch:       7320 | loss: 5.25647 | failed:    1
batch:       7330 | loss: 5.22044 | failed:    1
batch:       7340 | loss: 5.27591 | failed:    1
batch:       7350 | loss: 5.23606 | failed:    1
batch:       7360 | loss: 5.25120 | failed:    1
batch:       7370 | loss: 5.26597 | failed:    1
batch:       7380 | loss: 5.25351 | failed:    1
batch:       7390 | loss: 5.28481 | failed:    1
batch:       7400 | loss: 5.24018 | failed:    1
batch:       7410 | loss: 5.23083 | failed:    1
batch:       7420 | loss: 5.17473 | failed:    1
batch:       7430 | loss: 5.16119 | failed:    1
batch:       7440 | loss: 5.19601 | failed:    1
batch:       7450 | loss: 5.04152 | failed:    1
batch:       7460 | loss: 4.84140 | failed:    1
batch:       7470 | loss: 5.03170 | failed:    1
batch:       7480 | loss: 5.37577 | failed:    1
batch:       7490 | loss: 5.20008 | failed:    1
batch:       7500 | loss: 5.27127 | failed:    1
batch:       7510 | loss: 5.23903 | failed:    1
batch:       7520 | loss: 5.21929 | failed:    1
batch:       7530 | loss: 5.24134 | failed:    1
batch:       7540 | loss: 5.21709 | failed:    1
batch:       7550 | loss: 5.18265 | failed:    1
batch:       7560 | loss: 5.16334 | failed:    1
batch:       7570 | loss: 5.11940 | failed:    1
batch:       7580 | loss: 5.15767 | failed:    1
batch:       7590 | loss: 5.24103 | failed:    1
batch:       7600 | loss: 5.19588 | failed:    1
batch:       7610 | loss: 5.15722 | failed:    1
batch:       7620 | loss: 5.10042 | failed:    1
batch:       7630 | loss: 5.17081 | failed:    1
batch:       7640 | loss: 5.27069 | failed:    1
batch:       7650 | loss: 5.22698 | failed:    1
batch:       7660 | loss: 5.11326 | failed:    1
batch:       7670 | loss: 5.16007 | failed:    1
batch:       7680 | loss: 5.13653 | failed:    1
batch:       7690 | loss: 5.24454 | failed:    1
batch:       7700 | loss: 5.24750 | failed:    1
batch:       7710 | loss: 5.26935 | failed:    1
batch:       7720 | loss: 5.26884 | failed:    1
batch:       7730 | loss: 5.22135 | failed:    1
batch:       7740 | loss: 5.21687 | failed:    1
batch:       7750 | loss: 5.23191 | failed:    1
batch:       7760 | loss: 5.26423 | failed:    1
batch:       7770 | loss: 5.22583 | failed:    1
batch:       7780 | loss: 5.22621 | failed:    1
batch:       7790 | loss: 5.16077 | failed:    1
batch:       7800 | loss: 5.27488 | failed:    1
batch:       7810 | loss: 5.16522 | failed:    1
batch:       7820 | loss: 5.22640 | failed:    1
batch:       7830 | loss: 5.20093 | failed:    1
batch:       7840 | loss: 5.21228 | failed:    1
batch:       7850 | loss: 5.19809 | failed:    1
batch:       7860 | loss: 5.15437 | failed:    1
batch:       7870 | loss: 5.18043 | failed:    1
batch:       7880 | loss: 5.20979 | failed:    1
batch:       7890 | loss: 5.26318 | failed:    1
batch:       7900 | loss: 5.23936 | failed:    1
batch:       7910 | loss: 5.18355 | failed:    1
batch:       7920 | loss: 5.19228 | failed:    1
batch:       7930 | loss: 5.22834 | failed:    1
batch:       7940 | loss: 5.09891 | failed:    1
batch:       7950 | loss: 5.27624 | failed:    1
batch:       7960 | loss: 5.08961 | failed:    1
batch:       7970 | loss: 5.22207 | failed:    1
batch:       7980 | loss: 5.20820 | failed:    1
batch:       7990 | loss: 5.29799 | failed:    1
batch:       8000 | loss: 5.26851 | failed:    1
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       8010 | loss: 5.15697 | failed:    1
batch:       8020 | loss: 5.22281 | failed:    1
batch:       8030 | loss: 5.17326 | failed:    1
batch:       8040 | loss: 5.20245 | failed:    1
batch:       8050 | loss: 5.24305 | failed:    1
batch:       8060 | loss: 5.25630 | failed:    1
batch:       8070 | loss: 5.26447 | failed:    1
batch:       8080 | loss: 5.19637 | failed:    1
batch:       8090 | loss: 5.21910 | failed:    1
batch:       8100 | loss: 5.20815 | failed:    1
batch:       8110 | loss: 5.25596 | failed:    1
batch:       8120 | loss: 5.25043 | failed:    1
batch:       8130 | loss: 5.16332 | failed:    1
batch:       8140 | loss: 5.22098 | failed:    1
batch:       8150 | loss: 5.28192 | failed:    1
batch:       8160 | loss: 5.13479 | failed:    1
batch:       8170 | loss: 5.22843 | failed:    1
batch:       8180 | loss: 5.10682 | failed:    1
batch:       8190 | loss: 5.26858 | failed:    1
batch:       8200 | loss: 5.18088 | failed:    1
batch:       8210 | loss: 5.24731 | failed:    1
batch:       8220 | loss: 5.21463 | failed:    1
batch:       8230 | loss: 5.24869 | failed:    1
batch:       8240 | loss: 5.26744 | failed:    1
batch:       8250 | loss: 5.21519 | failed:    1
batch:       8260 | loss: 5.17693 | failed:    1
batch:       8270 | loss: 5.20326 | failed:    1
batch:       8280 | loss: 5.07071 | failed:    1
batch:       8290 | loss: 5.23178 | failed:    1
batch:       8300 | loss: 5.25190 | failed:    1
batch:       8310 | loss: 5.22155 | failed:    1
batch:       8320 | loss: 5.18522 | failed:    1
batch:       8330 | loss: 5.24752 | failed:    1
batch:       8340 | loss: 5.25714 | failed:    1
batch:       8350 | loss: 5.19909 | failed:    1
batch:       8360 | loss: 5.14497 | failed:    1
batch:       8370 | loss: 5.16045 | failed:    1
batch:       8380 | loss: 5.21717 | failed:    1
batch:       8390 | loss: 5.17814 | failed:    1
batch:       8400 | loss: 5.23693 | failed:    1
batch:       8410 | loss: 5.11486 | failed:    1
batch:       8420 | loss: 5.30450 | failed:    1
batch:       8430 | loss: 5.07922 | failed:    1
batch:       8440 | loss: 5.21279 | failed:    1
batch:       8450 | loss: 5.23071 | failed:    1
batch:       8460 | loss: 5.16360 | failed:    1
batch:       8470 | loss: 5.15621 | failed:    1
batch:       8480 | loss: 5.20018 | failed:    1
batch:       8490 | loss: 5.13924 | failed:    1
batch:       8500 | loss: 5.24503 | failed:    1
batch:       8510 | loss: 5.23991 | failed:    1
batch:       8520 | loss: 5.21504 | failed:    1
batch:       8530 | loss: 5.18395 | failed:    1
batch:       8540 | loss: 5.22477 | failed:    1
batch:       8550 | loss: 5.26584 | failed:    1
batch:       8560 | loss: 5.14340 | failed:    1
batch:       8570 | loss: 5.25256 | failed:    1
batch:       8580 | loss: 5.19921 | failed:    1
batch:       8590 | loss: 5.22058 | failed:    1
batch:       8600 | loss: 5.22852 | failed:    1
batch:       8610 | loss: 5.22090 | failed:    1
batch:       8620 | loss: 5.20958 | failed:    1
batch:       8630 | loss: 5.07340 | failed:    1
batch:       8640 | loss: 5.26403 | failed:    1
batch:       8650 | loss: 5.19734 | failed:    1
batch:       8660 | loss: 5.26647 | failed:    1
batch:       8670 | loss: 5.23533 | failed:    1
batch:       8680 | loss: 5.26854 | failed:    1
batch:       8690 | loss: 5.31576 | failed:    1
batch:       8700 | loss: 5.26611 | failed:    1
batch:       8710 | loss: 5.23926 | failed:    1
batch:       8720 | loss: 5.11761 | failed:    1
batch:       8730 | loss: 5.24370 | failed:    1
batch:       8740 | loss: 5.27286 | failed:    1
batch:       8750 | loss: 5.23744 | failed:    1
batch:       8760 | loss: 5.27201 | failed:    1
batch:       8770 | loss: 5.19468 | failed:    1
batch:       8780 | loss: 5.24276 | failed:    1
batch:       8790 | loss: 5.19367 | failed:    1
batch:       8800 | loss: 5.26936 | failed:    1
batch:       8810 | loss: 5.22066 | failed:    1
batch:       8820 | loss: 5.15651 | failed:    1
batch:       8830 | loss: 5.19913 | failed:    1
batch:       8840 | loss: 5.20376 | failed:    1
batch:       8850 | loss: 5.16014 | failed:    1
batch:       8860 | loss: 5.26581 | failed:    1
batch:       8870 | loss: 5.23088 | failed:    1
batch:       8880 | loss: 5.12168 | failed:    1
batch:       8890 | loss: 5.16161 | failed:    1
batch:       8900 | loss: 5.20350 | failed:    1
batch:       8910 | loss: 5.23963 | failed:    1
batch:       8920 | loss: 5.08877 | failed:    1
batch:       8930 | loss: 5.17930 | failed:    1
batch:       8940 | loss: 5.15615 | failed:    1
batch:       8950 | loss: 5.21381 | failed:    1
batch:       8960 | loss: 5.23132 | failed:    1
batch:       8970 | loss: 5.26784 | failed:    1
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       8980 | loss: 5.24480 | failed:    1
batch:       8990 | loss: 5.24906 | failed:    1
batch:       9000 | loss: 5.17957 | failed:    2
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:       9010 | loss: 5.25472 | failed:    2
batch:       9020 | loss: 5.19821 | failed:    2
batch:       9030 | loss: 5.27593 | failed:    2
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9040 | loss: 5.15974 | failed:    2
batch:       9050 | loss: 5.22884 | failed:    2
batch:       9060 | loss: 5.21873 | failed:    3
batch:       9070 | loss: 5.12899 | failed:    3
batch:       9080 | loss: 5.22997 | failed:    3
batch:       9090 | loss: 5.22276 | failed:    3
batch:       9100 | loss: 5.19061 | failed:    3
batch:       9110 | loss: 5.19476 | failed:    3
batch:       9120 | loss: 5.14034 | failed:    3
batch:       9130 | loss: 5.26316 | failed:    3
batch:       9140 | loss: 5.25093 | failed:    3
batch:       9150 | loss: 5.19366 | failed:    3
batch:       9160 | loss: 5.19554 | failed:    3
batch:       9170 | loss: 5.15550 | failed:    3
batch:       9180 | loss: 5.14640 | failed:    3
batch:       9190 | loss: 5.15938 | failed:    3
batch:       9200 | loss: 5.20267 | failed:    3
batch:       9210 | loss: 5.23462 | failed:    3
batch:       9220 | loss: 5.13548 | failed:    3
batch:       9230 | loss: 5.12983 | failed:    3
batch:       9240 | loss: 5.13118 | failed:    3
batch:       9250 | loss: 5.31446 | failed:    3
batch:       9260 | loss: 5.20994 | failed:    3
batch:       9270 | loss: 5.11528 | failed:    3
batch:       9280 | loss: 5.21717 | failed:    3
batch:       9290 | loss: 5.23536 | failed:    3
batch:       9300 | loss: 5.24718 | failed:    3
batch:       9310 | loss: 5.19110 | failed:    3
batch:       9320 | loss: 5.12809 | failed:    3
batch:       9330 | loss: 5.27105 | failed:    3
batch:       9340 | loss: 5.21071 | failed:    3
batch:       9350 | loss: 5.14919 | failed:    3
batch:       9360 | loss: 5.20260 | failed:    3
batch:       9370 | loss: 5.08781 | failed:    3
batch:       9380 | loss: 5.14177 | failed:    3
batch:       9390 | loss: 5.21299 | failed:    3
batch:       9400 | loss: 5.18784 | failed:    3
batch:       9410 | loss: 5.13541 | failed:    3
batch:       9420 | loss: 5.14401 | failed:    3
batch:       9430 | loss: 5.20302 | failed:    3
batch:       9440 | loss: 5.09773 | failed:    3
batch:       9450 | loss: 5.24658 | failed:    3
batch:       9460 | loss: 5.24006 | failed:    3
batch:       9470 | loss: 5.13298 | failed:    3
batch:       9480 | loss: 5.23891 | failed:    3
batch:       9490 | loss: 5.19193 | failed:    3
batch:       9500 | loss: 5.24925 | failed:    3
batch:       9510 | loss: 5.30756 | failed:    3
batch:       9520 | loss: 5.21474 | failed:    3
batch:       9530 | loss: 5.22729 | failed:    3
batch:       9540 | loss: 5.14152 | failed:    3
batch:       9550 | loss: 5.22080 | failed:    3
batch:       9560 | loss: 5.25168 | failed:    3
batch:       9570 | loss: 5.13572 | failed:    3
batch:       9580 | loss: 5.26008 | failed:    3
batch:       9590 | loss: 5.18712 | failed:    3
batch:       9600 | loss: 5.30055 | failed:    3
batch:       9610 | loss: 5.18851 | failed:    3
batch:       9620 | loss: 5.24370 | failed:    3
batch:       9630 | loss: 5.18961 | failed:    3
batch:       9640 | loss: 5.20138 | failed:    3
batch:       9650 | loss: 5.21853 | failed:    3
batch:       9660 | loss: 5.13737 | failed:    3
batch:       9670 | loss: 5.26700 | failed:    3
batch:       9680 | loss: 5.21679 | failed:    3
batch:       9690 | loss: 5.20430 | failed:    3
batch:       9700 | loss: 5.25752 | failed:    3
batch:       9710 | loss: 5.23958 | failed:    3
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9720 | loss: 5.16198 | failed:    3
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9730 | loss: 5.27966 | failed:    3
batch:       9750 | loss: 5.24277 | failed:   16
batch:       9760 | loss: 5.17465 | failed:   16
batch:       9770 | loss: 5.27039 | failed:   16
batch:       9780 | loss: 5.26017 | failed:   16
batch:       9790 | loss: 5.16136 | failed:   16
batch:       9800 | loss: 5.21877 | failed:   16
batch:       9810 | loss: 5.19750 | failed:   16
batch:       9820 | loss: 5.19747 | failed:   16
batch:       9830 | loss: 5.21184 | failed:   16
batch:       9840 | loss: 5.23543 | failed:   16
batch:       9850 | loss: 5.28295 | failed:   16
batch:       9860 | loss: 5.23809 | failed:   16
batch:       9870 | loss: 5.24616 | failed:   16
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9880 | loss: 5.16383 | failed:   16
batch:       9890 | loss: 5.46742 | failed:   16
batch:       9900 | loss: 5.18891 | failed:   20
batch:       9910 | loss: 5.20675 | failed:   20
batch:       9920 | loss: 5.19110 | failed:   20
batch:       9930 | loss: 5.17508 | failed:   20
batch:       9940 | loss: 5.22921 | failed:   20
batch:       9950 | loss: 5.20281 | failed:   20
batch:       9960 | loss: 5.07278 | failed:   20
batch:       9970 | loss: 5.19362 | failed:   20
batch:       9980 | loss: 4.98775 | failed:   20
batch:       9990 | loss: 5.29776 | failed:   20
batch:      10000 | loss: 5.17452 | failed:   20
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      10010 | loss: 5.03041 | failed:   20
batch:      10020 | loss: 5.25650 | failed:   20
batch:      10030 | loss: 5.26712 | failed:   20
batch:      10040 | loss: 5.24667 | failed:   20
batch:      10050 | loss: 5.18040 | failed:   20
batch:      10060 | loss: 5.16947 | failed:   20
batch:      10070 | loss: 5.09720 | failed:   20
batch:      10080 | loss: 5.21712 | failed:   20
batch:      10090 | loss: 5.25593 | failed:   20
batch:      10100 | loss: 5.19795 | failed:   20
batch:      10110 | loss: 5.30392 | failed:   20
batch:      10120 | loss: 5.19806 | failed:   20
batch:      10130 | loss: 5.04320 | failed:   20
batch:      10140 | loss: 5.26595 | failed:   20
batch:      10150 | loss: 5.26949 | failed:   20
batch:      10160 | loss: 5.19851 | failed:   20
batch:      10170 | loss: 5.21653 | failed:   20
batch:      10180 | loss: 5.17252 | failed:   20
batch:      10190 | loss: 5.20119 | failed:   20
batch:      10200 | loss: 5.17429 | failed:   20
batch:      10210 | loss: 5.32547 | failed:   20
batch:      10220 | loss: 5.26238 | failed:   20
batch:      10230 | loss: 5.21266 | failed:   20
batch:      10240 | loss: 5.23008 | failed:   20
batch:      10250 | loss: 5.11784 | failed:   20
batch:      10260 | loss: 5.17409 | failed:   20
batch:      10270 | loss: 5.23012 | failed:   20
batch:      10280 | loss: 5.18652 | failed:   20
batch:      10290 | loss: 5.24845 | failed:   20
batch:      10300 | loss: 5.24883 | failed:   20
batch:      10310 | loss: 5.17788 | failed:   20
batch:      10320 | loss: 5.21931 | failed:   20
batch:      10330 | loss: 5.24673 | failed:   20
batch:      10340 | loss: 5.26440 | failed:   20
batch:      10350 | loss: 5.19067 | failed:   20
batch:      10360 | loss: 5.28635 | failed:   20
batch:      10370 | loss: 5.25300 | failed:   20
batch:      10380 | loss: 5.25856 | failed:   20
batch:      10390 | loss: 5.23689 | failed:   20
batch:      10400 | loss: 5.14075 | failed:   20
batch:      10410 | loss: 5.24740 | failed:   20
batch:      10420 | loss: 5.25961 | failed:   20
batch:      10430 | loss: 5.25204 | failed:   20
batch:      10440 | loss: 5.16054 | failed:   20
batch:      10450 | loss: 5.20711 | failed:   20
batch:      10460 | loss: 5.18730 | failed:   20
batch:      10470 | loss: 5.19323 | failed:   20
batch:      10480 | loss: 5.24793 | failed:   20
batch:      10490 | loss: 5.23924 | failed:   20
batch:      10500 | loss: 5.15975 | failed:   20
batch:      10510 | loss: 5.22428 | failed:   20
batch:      10520 | loss: 5.20615 | failed:   20
batch:      10530 | loss: 5.17640 | failed:   20
batch:      10540 | loss: 5.23763 | failed:   20
batch:      10550 | loss: 5.27008 | failed:   20
batch:      10560 | loss: 5.23627 | failed:   20
batch:      10570 | loss: 5.21674 | failed:   20
batch:      10580 | loss: 5.16248 | failed:   20
batch:      10590 | loss: 5.17818 | failed:   20
batch:      10600 | loss: 5.22717 | failed:   20
batch:      10610 | loss: 5.28269 | failed:   20
batch:      10620 | loss: 5.22137 | failed:   20
batch:      10630 | loss: 5.16490 | failed:   20
batch:      10640 | loss: 5.23332 | failed:   20
batch:      10650 | loss: 5.18068 | failed:   20
batch:      10660 | loss: 5.16338 | failed:   20
batch:      10670 | loss: 5.26852 | failed:   20
batch:      10680 | loss: 5.26989 | failed:   20
batch:      10690 | loss: 5.26478 | failed:   20
batch:      10700 | loss: 5.19374 | failed:   20
batch:      10710 | loss: 5.21676 | failed:   20
batch:      10720 | loss: 5.28744 | failed:   20
batch:      10730 | loss: 5.21449 | failed:   20
batch:      10740 | loss: 5.18268 | failed:   20
batch:      10750 | loss: 5.22745 | failed:   20
batch:      10760 | loss: 5.20618 | failed:   20
batch:      10770 | loss: 5.25489 | failed:   20
batch:      10780 | loss: 5.21201 | failed:   20
batch:      10790 | loss: 5.04220 | failed:   20
batch:      10800 | loss: 5.12432 | failed:   20
batch:      10810 | loss: 5.16130 | failed:   20
batch:      10820 | loss: 5.08540 | failed:   20
batch:      10830 | loss: 5.15611 | failed:   20
batch:      10840 | loss: 5.21889 | failed:   20
batch:      10850 | loss: 5.14502 | failed:   20
batch:      10860 | loss: 5.11898 | failed:   20
batch:      10870 | loss: 5.23845 | failed:   20
batch:      10880 | loss: 5.15324 | failed:   20
batch:      10890 | loss: 5.16595 | failed:   20
batch:      10900 | loss: 5.18831 | failed:   20
batch:      10910 | loss: 5.19220 | failed:   20
batch:      10920 | loss: 5.17031 | failed:   20
batch:      10930 | loss: 5.19338 | failed:   20
batch:      10940 | loss: 5.21489 | failed:   20
batch:      10950 | loss: 5.26931 | failed:   20
batch:      10960 | loss: 5.22717 | failed:   20
batch:      10970 | loss: 5.15391 | failed:   20
batch:      10980 | loss: 5.21565 | failed:   20
batch:      10990 | loss: 5.25633 | failed:   20
batch:      11000 | loss: 5.29365 | failed:   20
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      11010 | loss: 5.15363 | failed:   20
batch:      11020 | loss: 5.19763 | failed:   20
batch:      11030 | loss: 5.16157 | failed:   20
batch:      11040 | loss: 5.28319 | failed:   20
batch:      11050 | loss: 5.24494 | failed:   20
batch:      11060 | loss: 5.12878 | failed:   20
batch:      11070 | loss: 5.25969 | failed:   20
batch:      11080 | loss: 5.26036 | failed:   20
batch:      11090 | loss: 5.23989 | failed:   20
batch:      11100 | loss: 5.23277 | failed:   20
batch:      11110 | loss: 5.27266 | failed:   20
batch:      11120 | loss: 5.22348 | failed:   20
batch:      11130 | loss: 5.19997 | failed:   20
batch:      11140 | loss: 5.17144 | failed:   20
batch:      11150 | loss: 5.04898 | failed:   20
batch:      11160 | loss: 5.22453 | failed:   20
batch:      11170 | loss: 5.14941 | failed:   20
batch:      11180 | loss: 5.14720 | failed:   20
batch:      11190 | loss: 5.12169 | failed:   20
batch:      11200 | loss: 5.21181 | failed:   20
batch:      11210 | loss: 5.23612 | failed:   20
batch:      11220 | loss: 5.14159 | failed:   20
batch:      11230 | loss: 5.19962 | failed:   20
batch:      11240 | loss: 5.21451 | failed:   20
batch:      11250 | loss: 5.15413 | failed:   20
batch:      11260 | loss: 5.13175 | failed:   20
batch:      11270 | loss: 5.26635 | failed:   20
batch:      11280 | loss: 5.16075 | failed:   20
batch:      11290 | loss: 5.17362 | failed:   20
batch:      11300 | loss: 5.24036 | failed:   20
batch:      11310 | loss: 5.17662 | failed:   20
batch:      11320 | loss: 5.14899 | failed:   20
batch:      11330 | loss: 5.15638 | failed:   20
batch:      11340 | loss: 5.20820 | failed:   20
batch:      11350 | loss: 5.08406 | failed:   20
batch:      11360 | loss: 5.11456 | failed:   20
batch:      11370 | loss: 5.14141 | failed:   20
batch:      11380 | loss: 5.15984 | failed:   20
batch:      11390 | loss: 5.21902 | failed:   20
batch:      11400 | loss: 5.24400 | failed:   20
batch:      11410 | loss: 5.21354 | failed:   20
batch:      11420 | loss: 5.12513 | failed:   20
batch:      11430 | loss: 5.21933 | failed:   20
batch:      11440 | loss: 5.17236 | failed:   20
batch:      11450 | loss: 5.17542 | failed:   20
batch:      11460 | loss: 5.10609 | failed:   20
batch:      11470 | loss: 5.11422 | failed:   20
batch:      11480 | loss: 5.14907 | failed:   20
batch:      11490 | loss: 5.10734 | failed:   20
batch:      11500 | loss: 5.28691 | failed:   20
batch:      11510 | loss: 5.15714 | failed:   20
batch:      11520 | loss: 5.31422 | failed:   20
batch:      11530 | loss: 5.21888 | failed:   20
batch:      11540 | loss: 5.21289 | failed:   20
batch:      11550 | loss: 5.22707 | failed:   20
batch:      11560 | loss: 5.16469 | failed:   20
batch:      11570 | loss: 5.16206 | failed:   20
batch:      11580 | loss: 5.22075 | failed:   20
batch:      11590 | loss: 5.18265 | failed:   20
batch:      11600 | loss: 5.21331 | failed:   20
batch:      11610 | loss: 5.17916 | failed:   20
batch:      11620 | loss: 5.17061 | failed:   20
batch:      11630 | loss: 5.21477 | failed:   20
batch:      11640 | loss: 5.28227 | failed:   20
batch:      11650 | loss: 5.27364 | failed:   20
batch:      11660 | loss: 5.06664 | failed:   20
batch:      11670 | loss: 5.12759 | failed:   20
batch:      11680 | loss: 5.20894 | failed:   20
batch:      11690 | loss: 5.11728 | failed:   20
batch:      11700 | loss: 5.22132 | failed:   20
batch:      11710 | loss: 5.12497 | failed:   20
batch:      11720 | loss: 5.23723 | failed:   20
batch:      11730 | loss: 5.08198 | failed:   20
batch:      11740 | loss: 5.15084 | failed:   20
batch:      11750 | loss: 5.16622 | failed:   20
batch:      11760 | loss: 5.13474 | failed:   20
batch:      11770 | loss: 5.25721 | failed:   20
batch:      11780 | loss: 5.18663 | failed:   20
batch:      11790 | loss: 5.17966 | failed:   20
batch:      11800 | loss: 5.07435 | failed:   20
batch:      11810 | loss: 5.01979 | failed:   20
batch:      11820 | loss: 5.26980 | failed:   20
batch:      11830 | loss: 5.19028 | failed:   20
batch:      11840 | loss: 5.15649 | failed:   20
batch:      11850 | loss: 5.20479 | failed:   20
batch:      11860 | loss: 5.21403 | failed:   20
batch:      11870 | loss: 5.15537 | failed:   20
batch:      11880 | loss: 5.16374 | failed:   20
batch:      11890 | loss: 5.18043 | failed:   20
batch:      11900 | loss: 5.24515 | failed:   20
batch:      11910 | loss: 5.26479 | failed:   20
batch:      11920 | loss: 5.19624 | failed:   20
batch:      11930 | loss: 5.13774 | failed:   20
batch:      11940 | loss: 5.17730 | failed:   20
batch:      11950 | loss: 5.25814 | failed:   20
batch:      11960 | loss: 5.27310 | failed:   20
batch:      11970 | loss: 5.24285 | failed:   20
batch:      11980 | loss: 5.20597 | failed:   20
batch:      11990 | loss: 5.21511 | failed:   20
batch:      12000 | loss: 5.25018 | failed:   20
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      12010 | loss: 5.27224 | failed:   20
batch:      12020 | loss: 5.25042 | failed:   20
batch:      12030 | loss: 5.26544 | failed:   20
batch:      12040 | loss: 5.20263 | failed:   20
batch:      12050 | loss: 5.25804 | failed:   20
batch:      12060 | loss: 5.26016 | failed:   20
batch:      12070 | loss: 5.14848 | failed:   20
batch:      12080 | loss: 5.24472 | failed:   20
batch:      12090 | loss: 5.10106 | failed:   20
batch:      12100 | loss: 5.23990 | failed:   20
batch:      12110 | loss: 5.10759 | failed:   20
batch:      12120 | loss: 5.24221 | failed:   20
batch:      12130 | loss: 5.17726 | failed:   20
batch:      12140 | loss: 5.17018 | failed:   20
batch:      12150 | loss: 5.16340 | failed:   20
batch:      12160 | loss: 5.19740 | failed:   20
batch:      12170 | loss: 5.21579 | failed:   20
batch:      12180 | loss: 5.27567 | failed:   20
batch:      12190 | loss: 5.04711 | failed:   20
batch:      12200 | loss: 5.04401 | failed:   20
batch:      12210 | loss: 5.23925 | failed:   20
batch:      12220 | loss: 5.21874 | failed:   20
batch:      12230 | loss: 5.20132 | failed:   20
batch:      12240 | loss: 5.18228 | failed:   20
batch:      12250 | loss: 5.16297 | failed:   20
batch:      12260 | loss: 5.23824 | failed:   20
batch:      12270 | loss: 5.13921 | failed:   20
batch:      12280 | loss: 5.20735 | failed:   20
batch:      12290 | loss: 5.32725 | failed:   20
batch:      12300 | loss: 5.22325 | failed:   20
batch:      12310 | loss: 5.22817 | failed:   20
batch:      12320 | loss: 5.24200 | failed:   20
batch:      12330 | loss: 5.25250 | failed:   20
batch:      12340 | loss: 5.17601 | failed:   20
batch:      12350 | loss: 5.23009 | failed:   20
batch:      12360 | loss: 5.15129 | failed:   20
batch:      12370 | loss: 5.16214 | failed:   20
batch:      12380 | loss: 5.31452 | failed:   20
batch:      12390 | loss: 5.16112 | failed:   20
batch:      12400 | loss: 5.29720 | failed:   20
batch:      12410 | loss: 5.23694 | failed:   20
batch:      12420 | loss: 5.16917 | failed:   20
batch:      12430 | loss: 5.21012 | failed:   20
batch:      12440 | loss: 5.29787 | failed:   20
batch:      12450 | loss: 5.14934 | failed:   20
batch:      12460 | loss: 5.14030 | failed:   20
batch:      12470 | loss: 5.19372 | failed:   20
batch:      12480 | loss: 5.13287 | failed:   20
batch:      12490 | loss: 5.19572 | failed:   20
batch:      12500 | loss: 5.16760 | failed:   20
batch:      12510 | loss: 5.05706 | failed:   20
batch:      12520 | loss: 5.20964 | failed:   20
batch:      12530 | loss: 5.27944 | failed:   20
batch:      12540 | loss: 5.21934 | failed:   20
batch:      12550 | loss: 5.28551 | failed:   20
batch:      12560 | loss: 5.25957 | failed:   20
batch:      12570 | loss: 5.20141 | failed:   20
batch:      12580 | loss: 5.20796 | failed:   20
batch:      12590 | loss: 5.17951 | failed:   20
batch:      12600 | loss: 5.16735 | failed:   20
batch:      12610 | loss: 5.20298 | failed:   20
batch:      12620 | loss: 5.20016 | failed:   20
batch:      12630 | loss: 5.13295 | failed:   20
batch:      12640 | loss: 5.11313 | failed:   20
batch:      12650 | loss: 5.20737 | failed:   20
batch:      12660 | loss: 5.19825 | failed:   20
batch:      12670 | loss: 5.18367 | failed:   20
batch:      12680 | loss: 5.11084 | failed:   20
batch:      12690 | loss: 5.14377 | failed:   20
batch:      12700 | loss: 5.36705 | failed:   20
batch:      12710 | loss: 5.29085 | failed:   20
batch:      12720 | loss: 5.17972 | failed:   20
batch:      12730 | loss: 5.22165 | failed:   20
batch:      12740 | loss: 5.25526 | failed:   20
batch:      12750 | loss: 5.20948 | failed:   20
batch:      12760 | loss: 5.13542 | failed:   20
batch:      12770 | loss: 5.06880 | failed:   20
batch:      12780 | loss: 4.92641 | failed:   20
batch:      12790 | loss: 5.08108 | failed:   20
batch:      12800 | loss: 5.11096 | failed:   20
batch:      12810 | loss: 5.13577 | failed:   20
batch:      12820 | loss: 5.29060 | failed:   20
batch:      12830 | loss: 5.18769 | failed:   20
batch:      12840 | loss: 5.19176 | failed:   20
batch:      12850 | loss: 5.25048 | failed:   20
batch:      12860 | loss: 5.10855 | failed:   20
batch:      12870 | loss: 5.14116 | failed:   20
batch:      12880 | loss: 5.20994 | failed:   20
batch:      12890 | loss: 5.15571 | failed:   20
batch:      12900 | loss: 5.05127 | failed:   20
batch:      12910 | loss: 5.01990 | failed:   20
batch:      12920 | loss: 5.26265 | failed:   20
batch:      12930 | loss: 5.20213 | failed:   20
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      12940 | loss: 5.29339 | failed:   20
batch:      12950 | loss: 5.16578 | failed:   20
batch:      12960 | loss: 5.25078 | failed:   21
batch:      12970 | loss: 5.16081 | failed:   21
batch:      12980 | loss: 5.18529 | failed:   21
batch:      12990 | loss: 5.20789 | failed:   21
batch:      13000 | loss: 5.19007 | failed:   21
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      13010 | loss: 5.10261 | failed:   21
batch:      13020 | loss: 5.18500 | failed:   21
batch:      13030 | loss: 5.35154 | failed:   21
batch:      13040 | loss: 5.31401 | failed:   21
batch:      13050 | loss: 5.22720 | failed:   21
batch:      13060 | loss: 5.24460 | failed:   21
batch:      13070 | loss: 5.26587 | failed:   21
batch:      13080 | loss: 5.24018 | failed:   21
batch:      13090 | loss: 5.21693 | failed:   21
batch:      13100 | loss: 5.29514 | failed:   21
batch:      13110 | loss: 5.20909 | failed:   21
batch:      13120 | loss: 5.18945 | failed:   21
batch:      13130 | loss: 5.18795 | failed:   21
batch:      13140 | loss: 5.18358 | failed:   21
batch:      13150 | loss: 5.15199 | failed:   21
batch:      13160 | loss: 5.15270 | failed:   21
batch:      13170 | loss: 5.21368 | failed:   21
batch:      13180 | loss: 5.21108 | failed:   21
batch:      13190 | loss: 5.16412 | failed:   21
batch:      13200 | loss: 5.14356 | failed:   21
batch:      13210 | loss: 5.17870 | failed:   21
batch:      13220 | loss: 5.21186 | failed:   21
batch:      13230 | loss: 5.19007 | failed:   21
batch:      13240 | loss: 5.07190 | failed:   21
batch:      13250 | loss: 5.25125 | failed:   21
batch:      13260 | loss: 5.22829 | failed:   21
batch:      13270 | loss: 5.16771 | failed:   21
batch:      13280 | loss: 5.24097 | failed:   21
batch:      13290 | loss: 5.24617 | failed:   21
batch:      13300 | loss: 5.20883 | failed:   21
batch:      13310 | loss: 5.15268 | failed:   21
batch:      13320 | loss: 5.22912 | failed:   21
batch:      13330 | loss: 5.19818 | failed:   21
batch:      13340 | loss: 5.11180 | failed:   21
batch:      13350 | loss: 5.13517 | failed:   21
batch:      13360 | loss: 5.21764 | failed:   21
batch:      13370 | loss: 5.17586 | failed:   21
batch:      13380 | loss: 5.20817 | failed:   21
batch:      13390 | loss: 5.27521 | failed:   21
batch:      13400 | loss: 5.16431 | failed:   21
batch:      13410 | loss: 5.19525 | failed:   21
batch:      13420 | loss: 5.24149 | failed:   21
batch:      13430 | loss: 5.10225 | failed:   21
batch:      13440 | loss: 5.06236 | failed:   21
batch:      13450 | loss: 4.95581 | failed:   21
batch:      13460 | loss: 5.03375 | failed:   21
batch:      13470 | loss: 5.08718 | failed:   21
batch:      13480 | loss: 5.36969 | failed:   21
batch:      13490 | loss: 4.97664 | failed:   21
batch:      13500 | loss: 5.27607 | failed:   21
batch:      13510 | loss: 5.20536 | failed:   21
batch:      13520 | loss: 5.23368 | failed:   21
batch:      13530 | loss: 5.24388 | failed:   21
batch:      13540 | loss: 5.06155 | failed:   21
batch:      13550 | loss: 5.11232 | failed:   21
batch:      13560 | loss: 5.23976 | failed:   21
batch:      13570 | loss: 5.14264 | failed:   21
batch:      13580 | loss: 5.22345 | failed:   21
batch:      13590 | loss: 5.22686 | failed:   21
batch:      13600 | loss: 5.17417 | failed:   21
batch:      13610 | loss: 5.19549 | failed:   21
batch:      13620 | loss: 5.15344 | failed:   21
batch:      13630 | loss: 5.07236 | failed:   21
batch:      13640 | loss: 5.16932 | failed:   21
batch:      13650 | loss: 5.21668 | failed:   21
batch:      13660 | loss: 5.18240 | failed:   21
batch:      13670 | loss: 5.07769 | failed:   21
batch:      13680 | loss: 5.10999 | failed:   21
batch:      13690 | loss: 5.16779 | failed:   21
batch:      13700 | loss: 5.21994 | failed:   21
batch:      13710 | loss: 5.03024 | failed:   21
batch:      13720 | loss: 5.25204 | failed:   21
batch:      13730 | loss: 5.21327 | failed:   21
batch:      13740 | loss: 5.23401 | failed:   21
batch:      13750 | loss: 5.21593 | failed:   21
batch:      13760 | loss: 5.23749 | failed:   21
batch:      13770 | loss: 5.18871 | failed:   21
batch:      13780 | loss: 5.22915 | failed:   21
batch:      13790 | loss: 5.02833 | failed:   21
batch:      13800 | loss: 5.16338 | failed:   21
batch:      13810 | loss: 5.24161 | failed:   21
batch:      13820 | loss: 5.19528 | failed:   21
batch:      13830 | loss: 5.13018 | failed:   21
batch:      13840 | loss: 5.19772 | failed:   21
batch:      13850 | loss: 5.30609 | failed:   21
batch:      13860 | loss: 5.21961 | failed:   21
batch:      13870 | loss: 5.12406 | failed:   21
batch:      13880 | loss: 5.30309 | failed:   21
batch:      13890 | loss: 5.10602 | failed:   21
batch:      13900 | loss: 5.16045 | failed:   21
batch:      13910 | loss: 4.93437 | failed:   21
batch:      13920 | loss: 5.27921 | failed:   21
batch:      13930 | loss: 5.25017 | failed:   21
batch:      13940 | loss: 5.22372 | failed:   21
batch:      13950 | loss: 5.18677 | failed:   21
batch:      13960 | loss: 5.14607 | failed:   21
batch:      13970 | loss: 5.19255 | failed:   21
batch:      13980 | loss: 5.11155 | failed:   21
batch:      13990 | loss: 5.17102 | failed:   21
batch:      14000 | loss: 5.22210 | failed:   21
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      14010 | loss: 5.13826 | failed:   21
batch:      14020 | loss: 5.21484 | failed:   21
batch:      14030 | loss: 5.19856 | failed:   21
batch:      14040 | loss: 5.10638 | failed:   21
batch:      14050 | loss: 5.15640 | failed:   21
batch:      14060 | loss: 5.17661 | failed:   21
batch:      14070 | loss: 5.14803 | failed:   21
batch:      14080 | loss: 5.13496 | failed:   21
batch:      14090 | loss: 5.16045 | failed:   21
batch:      14100 | loss: 5.29503 | failed:   21
batch:      14110 | loss: 5.16564 | failed:   21
batch:      14120 | loss: 5.21875 | failed:   21
batch:      14130 | loss: 5.17259 | failed:   21
batch:      14140 | loss: 5.16939 | failed:   21
batch:      14150 | loss: 5.28969 | failed:   21
batch:      14160 | loss: 5.19973 | failed:   21
batch:      14170 | loss: 5.13514 | failed:   21
batch:      14180 | loss: 5.26480 | failed:   21
batch:      14190 | loss: 5.12793 | failed:   21
batch:      14200 | loss: 5.11148 | failed:   21
batch:      14210 | loss: 5.19002 | failed:   21
batch:      14220 | loss: 5.24846 | failed:   21
batch:      14230 | loss: 5.24732 | failed:   21
batch:      14240 | loss: 5.20395 | failed:   21
batch:      14250 | loss: 5.13405 | failed:   21
batch:      14260 | loss: 5.13976 | failed:   21
batch:      14270 | loss: 5.28868 | failed:   21
batch:      14280 | loss: 5.27008 | failed:   21
batch:      14290 | loss: 5.21503 | failed:   21
batch:      14300 | loss: 5.15813 | failed:   21
batch:      14310 | loss: 5.22532 | failed:   21
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      14320 | loss: 5.22545 | failed:   21
batch:      14330 | loss: 5.15493 | failed:   21
batch:      14350 | loss: 5.26387 | failed:   25
batch:      14360 | loss: 5.33216 | failed:   25
batch:      14370 | loss: 5.21095 | failed:   25
batch:      14380 | loss: 5.23290 | failed:   25
batch:      14390 | loss: 5.21134 | failed:   25
batch:      14400 | loss: 5.15102 | failed:   25
batch:      14410 | loss: 5.00138 | failed:   25
batch:      14420 | loss: 5.23979 | failed:   25
batch:      14430 | loss: 5.23028 | failed:   25
batch:      14440 | loss: 5.26931 | failed:   25
batch:      14450 | loss: 5.22336 | failed:   25
batch:      14460 | loss: 5.22486 | failed:   25
batch:      14470 | loss: 5.15829 | failed:   25
batch:      14480 | loss: 5.19850 | failed:   25
batch:      14490 | loss: 5.22513 | failed:   25
batch:      14500 | loss: 5.20964 | failed:   25
batch:      14510 | loss: 5.18557 | failed:   25
batch:      14520 | loss: 5.27828 | failed:   25
batch:      14530 | loss: 5.18846 | failed:   25
batch:      14540 | loss: 5.13546 | failed:   25
batch:      14550 | loss: 5.20639 | failed:   25
batch:      14560 | loss: 5.21459 | failed:   25
batch:      14570 | loss: 5.18337 | failed:   25
batch:      14580 | loss: 5.26156 | failed:   25
batch:      14590 | loss: 5.22341 | failed:   25
batch:      14600 | loss: 5.10890 | failed:   25
batch:      14610 | loss: 5.25975 | failed:   25
batch:      14620 | loss: 5.21176 | failed:   25
batch:      14630 | loss: 5.23816 | failed:   25
batch:      14640 | loss: 5.23184 | failed:   25
batch:      14650 | loss: 5.15433 | failed:   25
batch:      14660 | loss: 5.09686 | failed:   25
batch:      14670 | loss: 5.13466 | failed:   25
batch:      14680 | loss: 5.32905 | failed:   25
batch:      14690 | loss: 5.09385 | failed:   25
batch:      14700 | loss: 5.18971 | failed:   25
batch:      14710 | loss: 5.21776 | failed:   25
batch:      14720 | loss: 5.14084 | failed:   25
batch:      14730 | loss: 5.25108 | failed:   25
batch:      14740 | loss: 5.27213 | failed:   25
batch:      14750 | loss: 5.26217 | failed:   25
batch:      14760 | loss: 5.23672 | failed:   25
batch:      14770 | loss: 5.27678 | failed:   25
batch:      14780 | loss: 5.19100 | failed:   25
batch:      14790 | loss: 5.19579 | failed:   25
batch:      14800 | loss: 5.09071 | failed:   25
batch:      14810 | loss: 5.19350 | failed:   25
batch:      14820 | loss: 5.13592 | failed:   25
batch:      14830 | loss: 5.22070 | failed:   25
batch:      14840 | loss: 5.12508 | failed:   25
batch:      14850 | loss: 5.29608 | failed:   25
batch:      14860 | loss: 5.18371 | failed:   25
batch:      14870 | loss: 5.22510 | failed:   25
batch:      14880 | loss: 5.19728 | failed:   25
batch:      14890 | loss: 5.21027 | failed:   25
batch:      14900 | loss: 5.26442 | failed:   25
batch:      14910 | loss: 5.22170 | failed:   25
batch:      14920 | loss: 5.26494 | failed:   25
batch:      14930 | loss: 5.27337 | failed:   25
batch:      14940 | loss: 5.16498 | failed:   25
batch:      14950 | loss: 5.28866 | failed:   25
batch:      14960 | loss: 5.30020 | failed:   25
batch:      14970 | loss: 5.24077 | failed:   25
batch:      14980 | loss: 5.19583 | failed:   25
batch:      14990 | loss: 5.22162 | failed:   25
batch:      15000 | loss: 5.20130 | failed:   25
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      15010 | loss: 5.19725 | failed:   25
batch:      15020 | loss: 5.24470 | failed:   25
batch:      15030 | loss: 5.14655 | failed:   25
batch:      15040 | loss: 5.18465 | failed:   25
batch:      15050 | loss: 5.24042 | failed:   25
batch:      15060 | loss: 5.29849 | failed:   25
batch:      15070 | loss: 5.22503 | failed:   25
batch:      15080 | loss: 5.07648 | failed:   25
batch:      15090 | loss: 5.27105 | failed:   25
batch:      15100 | loss: 5.13278 | failed:   25
batch:      15110 | loss: 5.17343 | failed:   25
batch:      15120 | loss: 5.15201 | failed:   25
batch:      15130 | loss: 5.33685 | failed:   25
batch:      15140 | loss: 5.02338 | failed:   25
batch:      15150 | loss: 5.05188 | failed:   25
batch:      15160 | loss: 5.21241 | failed:   25
batch:      15170 | loss: 5.17602 | failed:   25
batch:      15180 | loss: 5.08434 | failed:   25
batch:      15190 | loss: 5.16294 | failed:   25
batch:      15200 | loss: 5.10139 | failed:   25
batch:      15210 | loss: 5.21026 | failed:   25
batch:      15220 | loss: 5.23273 | failed:   25
batch:      15230 | loss: 5.14208 | failed:   25
batch:      15240 | loss: 5.25923 | failed:   25
batch:      15250 | loss: 5.24907 | failed:   25
batch:      15260 | loss: 5.19142 | failed:   25
batch:      15270 | loss: 5.22460 | failed:   25
batch:      15280 | loss: 5.23810 | failed:   25
batch:      15290 | loss: 5.19588 | failed:   25
batch:      15300 | loss: 5.19215 | failed:   25
batch:      15310 | loss: 5.31155 | failed:   25
batch:      15320 | loss: 5.24695 | failed:   25
batch:      15330 | loss: 5.18089 | failed:   25
batch:      15340 | loss: 5.22976 | failed:   25
batch:      15350 | loss: 5.25226 | failed:   25
batch:      15360 | loss: 5.23967 | failed:   25
batch:      15370 | loss: 5.23996 | failed:   25
batch:      15380 | loss: 5.24277 | failed:   25
batch:      15390 | loss: 5.20960 | failed:   25
batch:      15400 | loss: 5.26834 | failed:   25
batch:      15410 | loss: 5.21738 | failed:   25
batch:      15420 | loss: 5.20323 | failed:   25
batch:      15430 | loss: 5.21492 | failed:   25
batch:      15440 | loss: 5.12965 | failed:   25
batch:      15450 | loss: 4.98864 | failed:   25
batch:      15460 | loss: 5.26231 | failed:   25
batch:      15470 | loss: 5.17154 | failed:   25
batch:      15480 | loss: 5.14876 | failed:   25
batch:      15490 | loss: 5.18207 | failed:   25
batch:      15500 | loss: 5.19641 | failed:   25
batch:      15510 | loss: 5.14130 | failed:   25
batch:      15520 | loss: 5.12622 | failed:   25
batch:      15530 | loss: 5.19895 | failed:   25
batch:      15540 | loss: 5.25912 | failed:   25
batch:      15550 | loss: 5.20692 | failed:   25
batch:      15560 | loss: 5.15288 | failed:   25
batch:      15570 | loss: 5.25955 | failed:   25
batch:      15580 | loss: 5.23204 | failed:   25
batch:      15590 | loss: 5.30048 | failed:   25
batch:      15600 | loss: 5.24575 | failed:   25
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      15610 | loss: 5.16984 | failed:   25
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      15620 | loss: 5.24513 | failed:   25
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      15650 | loss: 5.20575 | failed:   41
batch:      15660 | loss: 5.24825 | failed:   41
batch:      15670 | loss: 5.21905 | failed:   41
batch:      15680 | loss: 5.24916 | failed:   41
batch:      15690 | loss: 5.19548 | failed:   41
batch:      15700 | loss: 5.09294 | failed:   41
batch:      15710 | loss: 5.17983 | failed:   41
batch:      15720 | loss: 5.19263 | failed:   41
batch:      15730 | loss: 5.18884 | failed:   41
batch:      15740 | loss: 5.29093 | failed:   41
batch:      15750 | loss: 5.26256 | failed:   41
batch:      15760 | loss: 5.29017 | failed:   41
batch:      15770 | loss: 5.18195 | failed:   41
batch:      15780 | loss: 5.26429 | failed:   41
batch:      15790 | loss: 5.21670 | failed:   41
batch:      15800 | loss: 5.19207 | failed:   41
batch:      15810 | loss: 5.26172 | failed:   41
batch:      15820 | loss: 5.20457 | failed:   41
batch:      15830 | loss: 5.22773 | failed:   41
batch:      15840 | loss: 5.25209 | failed:   41
batch:      15850 | loss: 5.23846 | failed:   41
batch:      15860 | loss: 5.22953 | failed:   41
batch:      15870 | loss: 5.19916 | failed:   41
batch:      15880 | loss: 5.16949 | failed:   41
batch:      15890 | loss: 5.07759 | failed:   41
batch:      15900 | loss: 5.25304 | failed:   41
batch:      15910 | loss: 5.22650 | failed:   41
batch:      15920 | loss: 5.16535 | failed:   41
batch:      15930 | loss: 5.16471 | failed:   41
batch:      15940 | loss: 5.18974 | failed:   41
batch:      15950 | loss: 5.24095 | failed:   41
batch:      15960 | loss: 5.29281 | failed:   41
batch:      15970 | loss: 5.20483 | failed:   41
batch:      15980 | loss: 5.26926 | failed:   41
batch:      15990 | loss: 5.26144 | failed:   41
batch:      16000 | loss: 5.16822 | failed:   41
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      16010 | loss: 5.15056 | failed:   41
batch:      16020 | loss: 5.18445 | failed:   41
batch:      16030 | loss: 5.24774 | failed:   41
batch:      16040 | loss: 5.20836 | failed:   41
batch:      16050 | loss: 5.23747 | failed:   41
batch:      16060 | loss: 5.13468 | failed:   41
batch:      16070 | loss: 5.19280 | failed:   41
batch:      16080 | loss: 5.17727 | failed:   41
batch:      16090 | loss: 5.24227 | failed:   41
batch:      16100 | loss: 5.18154 | failed:   41
batch:      16110 | loss: 5.21027 | failed:   41
batch:      16120 | loss: 5.25413 | failed:   41
batch:      16130 | loss: 5.23444 | failed:   41
batch:      16140 | loss: 5.28635 | failed:   41
batch:      16150 | loss: 5.23693 | failed:   41
batch:      16160 | loss: 5.17459 | failed:   41
batch:      16170 | loss: 5.23554 | failed:   41
batch:      16180 | loss: 5.16171 | failed:   41
batch:      16190 | loss: 5.20487 | failed:   41
batch:      16200 | loss: 5.06695 | failed:   41
batch:      16210 | loss: 5.19072 | failed:   41
batch:      16220 | loss: 5.05170 | failed:   41
batch:      16230 | loss: 5.22123 | failed:   41
batch:      16240 | loss: 5.19125 | failed:   41
batch:      16250 | loss: 5.18668 | failed:   41
batch:      16260 | loss: 5.23258 | failed:   41
batch:      16270 | loss: 5.20438 | failed:   41
batch:      16280 | loss: 5.19940 | failed:   41
batch:      16290 | loss: 5.18955 | failed:   41
batch:      16300 | loss: 5.12890 | failed:   41
batch:      16310 | loss: 5.13176 | failed:   41
batch:      16320 | loss: 5.14756 | failed:   41
batch:      16330 | loss: 5.28074 | failed:   41
batch:      16340 | loss: 5.22025 | failed:   41
batch:      16350 | loss: 5.23297 | failed:   41
batch:      16360 | loss: 5.28523 | failed:   41
batch:      16370 | loss: 5.24252 | failed:   41
batch:      16380 | loss: 5.27312 | failed:   41
batch:      16390 | loss: 5.20351 | failed:   41
batch:      16400 | loss: 5.14539 | failed:   41
batch:      16410 | loss: 5.16275 | failed:   41
batch:      16420 | loss: 5.19102 | failed:   41
batch:      16430 | loss: 5.25239 | failed:   41
batch:      16440 | loss: 5.10198 | failed:   41
batch:      16450 | loss: 5.21556 | failed:   41
batch:      16460 | loss: 5.16266 | failed:   41
batch:      16470 | loss: 5.22231 | failed:   41
batch:      16480 | loss: 5.21878 | failed:   41
batch:      16490 | loss: 5.18463 | failed:   41
batch:      16500 | loss: 5.13339 | failed:   41
batch:      16510 | loss: 5.26240 | failed:   41
batch:      16520 | loss: 5.17339 | failed:   41
batch:      16530 | loss: 5.20023 | failed:   41
batch:      16540 | loss: 5.27259 | failed:   41
batch:      16550 | loss: 5.30913 | failed:   41
batch:      16560 | loss: 5.25242 | failed:   41
batch:      16570 | loss: 5.15443 | failed:   41
batch:      16580 | loss: 5.09744 | failed:   41
batch:      16590 | loss: 5.20496 | failed:   41
batch:      16600 | loss: 5.22222 | failed:   41
batch:      16610 | loss: 5.04734 | failed:   41
batch:      16620 | loss: 5.17701 | failed:   41
batch:      16630 | loss: 5.18779 | failed:   41
batch:      16640 | loss: 5.14446 | failed:   41
batch:      16650 | loss: 5.15403 | failed:   41
batch:      16660 | loss: 5.20101 | failed:   41
batch:      16670 | loss: 5.19035 | failed:   41
batch:      16680 | loss: 5.23773 | failed:   41
batch:      16690 | loss: 5.23234 | failed:   41
batch:      16700 | loss: 5.13588 | failed:   41
batch:      16710 | loss: 5.19628 | failed:   41
batch:      16720 | loss: 5.18971 | failed:   41
batch:      16730 | loss: 5.21028 | failed:   41
batch:      16740 | loss: 5.10662 | failed:   41
batch:      16750 | loss: 5.16112 | failed:   41
batch:      16760 | loss: 5.19384 | failed:   41
batch:      16770 | loss: 5.09579 | failed:   41
batch:      16780 | loss: 5.10832 | failed:   41
batch:      16790 | loss: 5.19528 | failed:   41
batch:      16800 | loss: 5.16485 | failed:   41
batch:      16810 | loss: 5.24824 | failed:   41
batch:      16820 | loss: 5.14683 | failed:   41
batch:      16830 | loss: 5.26427 | failed:   41
batch:      16840 | loss: 5.18373 | failed:   41
batch:      16850 | loss: 5.12405 | failed:   41
batch:      16860 | loss: 5.15046 | failed:   41
batch:      16870 | loss: 5.16948 | failed:   41
batch:      16880 | loss: 5.18476 | failed:   41
batch:      16890 | loss: 5.35469 | failed:   41
batch:      16900 | loss: 5.11188 | failed:   41
batch:      16910 | loss: 5.27182 | failed:   41
batch:      16920 | loss: 5.23563 | failed:   41
batch:      16930 | loss: 5.20950 | failed:   41
batch:      16940 | loss: 5.15946 | failed:   41
batch:      16950 | loss: 5.27160 | failed:   41
batch:      16960 | loss: 5.23916 | failed:   41
batch:      16970 | loss: 5.25039 | failed:   41
batch:      16980 | loss: 5.24916 | failed:   41
batch:      16990 | loss: 5.20157 | failed:   41
batch:      17000 | loss: 5.21758 | failed:   41
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      17010 | loss: 5.23838 | failed:   41
batch:      17020 | loss: 5.22991 | failed:   41
batch:      17030 | loss: 5.15907 | failed:   41
batch:      17040 | loss: 5.05455 | failed:   41
batch:      17050 | loss: 5.11765 | failed:   41
batch:      17060 | loss: 5.16573 | failed:   41
batch:      17070 | loss: 5.24337 | failed:   41
batch:      17080 | loss: 5.26743 | failed:   41
batch:      17090 | loss: 5.23329 | failed:   41
batch:      17100 | loss: 5.25152 | failed:   41
batch:      17110 | loss: 5.29228 | failed:   41
batch:      17120 | loss: 5.20816 | failed:   41
batch:      17130 | loss: 5.13174 | failed:   41
batch:      17140 | loss: 5.18320 | failed:   41
batch:      17150 | loss: 5.12048 | failed:   41
batch:      17160 | loss: 5.01436 | failed:   41
batch:      17170 | loss: 5.21988 | failed:   41
batch:      17180 | loss: 5.20973 | failed:   41
batch:      17190 | loss: 5.24299 | failed:   41
batch:      17200 | loss: 5.15988 | failed:   41
batch:      17210 | loss: 5.23916 | failed:   41
batch:      17220 | loss: 5.08097 | failed:   41
batch:      17230 | loss: 5.22023 | failed:   41
batch:      17240 | loss: 5.09704 | failed:   41
batch:      17250 | loss: 5.25904 | failed:   41
batch:      17260 | loss: 5.13548 | failed:   41
batch:      17270 | loss: 5.15432 | failed:   41
batch:      17280 | loss: 5.13807 | failed:   41
batch:      17290 | loss: 5.12663 | failed:   41
batch:      17300 | loss: 5.15528 | failed:   41
batch:      17310 | loss: 5.16985 | failed:   41
batch:      17320 | loss: 5.12047 | failed:   41
batch:      17330 | loss: 5.01012 | failed:   41
batch:      17340 | loss: 5.22336 | failed:   41
batch:      17350 | loss: 5.29484 | failed:   41
batch:      17360 | loss: 5.12587 | failed:   41
batch:      17370 | loss: 5.22406 | failed:   41
batch:      17380 | loss: 5.23076 | failed:   41
batch:      17390 | loss: 5.18206 | failed:   41
batch:      17400 | loss: 5.23478 | failed:   41
batch:      17410 | loss: 5.22098 | failed:   41
batch:      17420 | loss: 5.18318 | failed:   41
batch:      17430 | loss: 5.20412 | failed:   41
batch:      17440 | loss: 5.22656 | failed:   41
batch:      17450 | loss: 5.25314 | failed:   41
batch:      17460 | loss: 5.21418 | failed:   41
batch:      17470 | loss: 5.23212 | failed:   41
batch:      17480 | loss: 5.16397 | failed:   41
batch:      17490 | loss: 5.14078 | failed:   41
batch:      17500 | loss: 5.05358 | failed:   41
batch:      17510 | loss: 5.17557 | failed:   41
batch:      17520 | loss: 5.16239 | failed:   41
batch:      17530 | loss: 5.20607 | failed:   41
batch:      17540 | loss: 5.25379 | failed:   41
batch:      17550 | loss: 5.29156 | failed:   41
batch:      17560 | loss: 5.25319 | failed:   41
batch:      17570 | loss: 5.21603 | failed:   41
batch:      17580 | loss: 5.20673 | failed:   41
batch:      17590 | loss: 5.11057 | failed:   41
batch:      17600 | loss: 5.17839 | failed:   41
batch:      17610 | loss: 5.19928 | failed:   41
batch:      17620 | loss: 5.12334 | failed:   41
batch:      17630 | loss: 5.03648 | failed:   41
batch:      17640 | loss: 5.26339 | failed:   41
batch:      17650 | loss: 5.22780 | failed:   41
batch:      17660 | loss: 5.23322 | failed:   41
batch:      17670 | loss: 5.22957 | failed:   41
batch:      17680 | loss: 5.12534 | failed:   41
batch:      17690 | loss: 5.24345 | failed:   41
batch:      17700 | loss: 5.22276 | failed:   41
batch:      17710 | loss: 5.23775 | failed:   41
batch:      17720 | loss: 5.24318 | failed:   41
batch:      17730 | loss: 5.17636 | failed:   41
batch:      17740 | loss: 5.18013 | failed:   41
batch:      17750 | loss: 5.14822 | failed:   41
batch:      17760 | loss: 5.24760 | failed:   41
batch:      17770 | loss: 5.23349 | failed:   41
batch:      17780 | loss: 5.16374 | failed:   41
batch:      17790 | loss: 5.20784 | failed:   41
batch:      17800 | loss: 5.18298 | failed:   41
batch:      17810 | loss: 5.23405 | failed:   41
batch:      17820 | loss: 5.12989 | failed:   41
batch:      17830 | loss: 5.06460 | failed:   41
batch:      17840 | loss: 5.10499 | failed:   41
batch:      17850 | loss: 5.19805 | failed:   41
batch:      17860 | loss: 5.16021 | failed:   41
batch:      17870 | loss: 5.20913 | failed:   41
batch:      17880 | loss: 4.96624 | failed:   41
batch:      17890 | loss: 5.06403 | failed:   41
batch:      17900 | loss: 5.15890 | failed:   41
batch:      17910 | loss: 5.19663 | failed:   41
batch:      17920 | loss: 5.20104 | failed:   41
batch:      17930 | loss: 5.12583 | failed:   41
batch:      17940 | loss: 5.13636 | failed:   41
batch:      17950 | loss: 5.23445 | failed:   41
batch:      17960 | loss: 5.20366 | failed:   41
batch:      17970 | loss: 5.11674 | failed:   41
batch:      17980 | loss: 5.25792 | failed:   41
batch:      17990 | loss: 5.25272 | failed:   41
batch:      18000 | loss: 5.17975 | failed:   41
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      18010 | loss: 5.17978 | failed:   41
batch:      18020 | loss: 5.20197 | failed:   41
batch:      18030 | loss: 5.12092 | failed:   41
batch:      18040 | loss: 5.22296 | failed:   41
batch:      18050 | loss: 5.21585 | failed:   41
batch:      18060 | loss: 5.24856 | failed:   41
batch:      18070 | loss: 5.20464 | failed:   41
batch:      18080 | loss: 5.08472 | failed:   41
batch:      18090 | loss: 5.30363 | failed:   41
batch:      18100 | loss: 5.28574 | failed:   41
batch:      18110 | loss: 5.18499 | failed:   41
batch:      18120 | loss: 5.25249 | failed:   41
batch:      18130 | loss: 5.18888 | failed:   41
batch:      18140 | loss: 5.12285 | failed:   41
batch:      18150 | loss: 5.24762 | failed:   41
batch:      18160 | loss: 5.18131 | failed:   41
batch:      18170 | loss: 5.25763 | failed:   41
batch:      18180 | loss: 5.24863 | failed:   41
batch:      18190 | loss: 5.14601 | failed:   41
batch:      18200 | loss: 5.25480 | failed:   41
batch:      18210 | loss: 5.26411 | failed:   41
batch:      18220 | loss: 5.09411 | failed:   41
batch:      18230 | loss: 5.15903 | failed:   41
batch:      18240 | loss: 5.13721 | failed:   41
batch:      18250 | loss: 5.17981 | failed:   41
batch:      18260 | loss: 5.10643 | failed:   41
batch:      18270 | loss: 5.18088 | failed:   41
batch:      18280 | loss: 5.21033 | failed:   41
batch:      18290 | loss: 5.30619 | failed:   41
batch:      18300 | loss: 5.02242 | failed:   41
batch:      18310 | loss: 5.29540 | failed:   41
batch:      18320 | loss: 5.11920 | failed:   41
batch:      18330 | loss: 5.17814 | failed:   41
batch:      18340 | loss: 5.20770 | failed:   41
batch:      18350 | loss: 5.16962 | failed:   41
batch:      18360 | loss: 5.07529 | failed:   41
batch:      18370 | loss: 5.12069 | failed:   41
batch:      18380 | loss: 5.28667 | failed:   41
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      18390 | loss: 5.17629 | failed:   41
batch:      18400 | loss: 5.22620 | failed:   41
batch:      18410 | loss: 5.20192 | failed:   43
batch:      18420 | loss: 5.25260 | failed:   43
batch:      18430 | loss: 5.28901 | failed:   43
batch:      18440 | loss: 5.23607 | failed:   43
batch:      18450 | loss: 5.20660 | failed:   43
batch:      18460 | loss: 5.13647 | failed:   43
batch:      18470 | loss: 5.18753 | failed:   43
batch:      18480 | loss: 5.17322 | failed:   43
batch:      18490 | loss: 5.21147 | failed:   43
batch:      18500 | loss: 5.27313 | failed:   43
batch:      18510 | loss: 5.21793 | failed:   43
batch:      18520 | loss: 5.21167 | failed:   43
batch:      18530 | loss: 5.13494 | failed:   43
batch:      18540 | loss: 5.15576 | failed:   43
batch:      18550 | loss: 5.16218 | failed:   43
batch:      18560 | loss: 5.15293 | failed:   43
batch:      18570 | loss: 5.17400 | failed:   43
batch:      18580 | loss: 5.20804 | failed:   43
batch:      18590 | loss: 5.19664 | failed:   43
batch:      18600 | loss: 5.24428 | failed:   43
batch:      18610 | loss: 5.21425 | failed:   43
batch:      18620 | loss: 5.23868 | failed:   43
batch:      18630 | loss: 5.21505 | failed:   43
batch:      18640 | loss: 5.17973 | failed:   43
batch:      18650 | loss: 5.19908 | failed:   43
batch:      18660 | loss: 5.14436 | failed:   43
batch:      18670 | loss: 5.13070 | failed:   43
batch:      18680 | loss: 5.29842 | failed:   43
batch:      18690 | loss: 5.22379 | failed:   43
batch:      18700 | loss: 5.16511 | failed:   43
batch:      18710 | loss: 5.27453 | failed:   43
batch:      18720 | loss: 5.26313 | failed:   43
batch:      18730 | loss: 5.17375 | failed:   43
batch:      18740 | loss: 5.21562 | failed:   43
batch:      18750 | loss: 5.24442 | failed:   43
batch:      18760 | loss: 5.28176 | failed:   43
batch:      18770 | loss: 5.26403 | failed:   43
batch:      18780 | loss: 5.19314 | failed:   43
batch:      18790 | loss: 5.21098 | failed:   43
batch:      18800 | loss: 5.08879 | failed:   43
batch:      18810 | loss: 5.12045 | failed:   43
batch:      18820 | loss: 5.25075 | failed:   43
batch:      18830 | loss: 5.13212 | failed:   43
batch:      18840 | loss: 5.15052 | failed:   43
batch:      18850 | loss: 5.31286 | failed:   43
batch:      18860 | loss: 5.24403 | failed:   43
batch:      18870 | loss: 5.11608 | failed:   43
batch:      18880 | loss: 5.19712 | failed:   43
batch:      18890 | loss: 5.18491 | failed:   43
batch:      18900 | loss: 5.15131 | failed:   43
batch:      18910 | loss: 5.14816 | failed:   43
batch:      18920 | loss: 5.16141 | failed:   43
batch:      18930 | loss: 5.17596 | failed:   43
batch:      18940 | loss: 5.21388 | failed:   43
batch:      18950 | loss: 5.04299 | failed:   43
batch:      18960 | loss: 5.18938 | failed:   43
batch:      18970 | loss: 5.03923 | failed:   43
batch:      18980 | loss: 5.26609 | failed:   43
batch:      18990 | loss: 5.21923 | failed:   43
batch:      19000 | loss: 5.22905 | failed:   43
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      19010 | loss: 5.21303 | failed:   43
batch:      19020 | loss: 5.18933 | failed:   43
batch:      19030 | loss: 5.23343 | failed:   43
batch:      19040 | loss: 5.21302 | failed:   43
batch:      19050 | loss: 5.18284 | failed:   43
batch:      19060 | loss: 5.13454 | failed:   43
batch:      19070 | loss: 5.24963 | failed:   43
batch:      19080 | loss: 5.16234 | failed:   43
batch:      19090 | loss: 5.19322 | failed:   43
batch:      19100 | loss: 5.21534 | failed:   43
batch:      19110 | loss: 5.17613 | failed:   43
batch:      19120 | loss: 5.20163 | failed:   43
batch:      19130 | loss: 5.10753 | failed:   43
batch:      19140 | loss: 5.18050 | failed:   43
batch:      19150 | loss: 5.10519 | failed:   43
batch:      19160 | loss: 5.19290 | failed:   43
batch:      19170 | loss: 5.08105 | failed:   43
batch:      19180 | loss: 5.21858 | failed:   43
batch:      19190 | loss: 5.13976 | failed:   43
batch:      19200 | loss: 5.21676 | failed:   43
batch:      19210 | loss: 5.01163 | failed:   43
batch:      19220 | loss: 5.16183 | failed:   43
batch:      19230 | loss: 5.25404 | failed:   43
batch:      19240 | loss: 5.22768 | failed:   43
batch:      19250 | loss: 4.87448 | failed:   43
batch:      19260 | loss: 5.19915 | failed:   43
batch:      19270 | loss: 5.17149 | failed:   43
batch:      19280 | loss: 5.08510 | failed:   43
batch:      19290 | loss: 5.18749 | failed:   43
batch:      19300 | loss: 5.05950 | failed:   43
batch:      19310 | loss: 5.13751 | failed:   43
batch:      19320 | loss: 5.19595 | failed:   43
batch:      19330 | loss: 5.15385 | failed:   43
batch:      19340 | loss: 5.08478 | failed:   43
batch:      19350 | loss: 5.13883 | failed:   43
batch:      19360 | loss: 5.23499 | failed:   43
batch:      19370 | loss: 5.24162 | failed:   43
batch:      19380 | loss: 5.21411 | failed:   43
batch:      19390 | loss: 5.10115 | failed:   43
batch:      19400 | loss: 5.23818 | failed:   43
batch:      19410 | loss: 5.18821 | failed:   43
batch:      19420 | loss: 5.24235 | failed:   43
batch:      19430 | loss: 5.18028 | failed:   43
batch:      19440 | loss: 5.20565 | failed:   43
batch:      19450 | loss: 5.18362 | failed:   43
batch:      19460 | loss: 5.08128 | failed:   43
batch:      19470 | loss: 5.16953 | failed:   43
batch:      19480 | loss: 5.28862 | failed:   43
batch:      19490 | loss: 5.28153 | failed:   43
batch:      19500 | loss: 5.26762 | failed:   43
batch:      19510 | loss: 5.22437 | failed:   43
batch:      19520 | loss: 5.26725 | failed:   43
batch:      19530 | loss: 5.25623 | failed:   43
batch:      19540 | loss: 5.24485 | failed:   43
batch:      19550 | loss: 5.25592 | failed:   43
batch:      19560 | loss: 5.20347 | failed:   43
batch:      19570 | loss: 5.26868 | failed:   43
batch:      19580 | loss: 5.22327 | failed:   43
batch:      19590 | loss: 5.24907 | failed:   43
batch:      19600 | loss: 5.22737 | failed:   43
batch:      19610 | loss: 5.14329 | failed:   43
batch:      19620 | loss: 5.08905 | failed:   43
batch:      19630 | loss: 5.21730 | failed:   43
batch:      19640 | loss: 5.10444 | failed:   43
batch:      19650 | loss: 5.23807 | failed:   43
batch:      19660 | loss: 5.19817 | failed:   43
batch:      19670 | loss: 5.06237 | failed:   43
batch:      19680 | loss: 5.18244 | failed:   43
batch:      19690 | loss: 5.23926 | failed:   43
batch:      19700 | loss: 5.12775 | failed:   43
batch:      19710 | loss: 5.10420 | failed:   43
batch:      19720 | loss: 5.12017 | failed:   43
batch:      19730 | loss: 5.19597 | failed:   43
batch:      19740 | loss: 5.19581 | failed:   43
batch:      19750 | loss: 5.19926 | failed:   43
batch:      19760 | loss: 5.31261 | failed:   43
batch:      19770 | loss: 5.13502 | failed:   43
batch:      19780 | loss: 5.15305 | failed:   43
batch:      19790 | loss: 5.27287 | failed:   43
batch:      19800 | loss: 5.28130 | failed:   43
batch:      19810 | loss: 5.24887 | failed:   43
batch:      19820 | loss: 5.18128 | failed:   43
batch:      19830 | loss: 5.14810 | failed:   43
batch:      19840 | loss: 5.20107 | failed:   43
batch:      19850 | loss: 5.18662 | failed:   43
batch:      19860 | loss: 5.19214 | failed:   43
batch:      19870 | loss: 4.99708 | failed:   43
batch:      19880 | loss: 5.19972 | failed:   43
batch:      19890 | loss: 5.18391 | failed:   43
batch:      19900 | loss: 5.09589 | failed:   43
batch:      19910 | loss: 5.15486 | failed:   43
batch:      19920 | loss: 5.20408 | failed:   43
batch:      19930 | loss: 5.18716 | failed:   43
batch:      19940 | loss: 5.20381 | failed:   43
batch:      19950 | loss: 5.10941 | failed:   43
batch:      19960 | loss: 5.20369 | failed:   43
batch:      19970 | loss: 5.19880 | failed:   43
batch:      19980 | loss: 5.02950 | failed:   43
batch:      19990 | loss: 5.13704 | failed:   43
batch:      20000 | loss: 5.22838 | failed:   43
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      20010 | loss: 5.18981 | failed:   43
batch:      20020 | loss: 5.20705 | failed:   43
batch:      20030 | loss: 5.25237 | failed:   43
batch:      20040 | loss: 5.28913 | failed:   43
batch:      20050 | loss: 5.18263 | failed:   43
batch:      20060 | loss: 5.28039 | failed:   43
batch:      20070 | loss: 5.24081 | failed:   43
batch:      20080 | loss: 5.16882 | failed:   43
batch:      20090 | loss: 5.15596 | failed:   43
batch:      20100 | loss: 5.12391 | failed:   43
batch:      20110 | loss: 5.17072 | failed:   43
batch:      20120 | loss: 5.18315 | failed:   43
batch:      20130 | loss: 5.10116 | failed:   43
batch:      20140 | loss: 5.23786 | failed:   43
batch:      20150 | loss: 5.22512 | failed:   43
batch:      20160 | loss: 5.24440 | failed:   43
batch:      20170 | loss: 5.20476 | failed:   43
batch:      20180 | loss: 5.20773 | failed:   43
batch:      20190 | loss: 5.21643 | failed:   43
batch:      20200 | loss: 5.23990 | failed:   43
batch:      20210 | loss: 5.15214 | failed:   43
batch:      20220 | loss: 5.02693 | failed:   43
batch:      20230 | loss: 5.20783 | failed:   43
batch:      20240 | loss: 5.08853 | failed:   43
batch:      20250 | loss: 5.24420 | failed:   43
batch:      20260 | loss: 5.27803 | failed:   43
batch:      20270 | loss: 5.20612 | failed:   43
batch:      20280 | loss: 5.21839 | failed:   43
batch:      20290 | loss: 5.29270 | failed:   43
batch:      20300 | loss: 5.20791 | failed:   43
batch:      20310 | loss: 5.30196 | failed:   43
batch:      20320 | loss: 5.10710 | failed:   43
batch:      20330 | loss: 5.22787 | failed:   43
batch:      20340 | loss: 5.23020 | failed:   43
batch:      20350 | loss: 5.11983 | failed:   43
batch:      20360 | loss: 5.08938 | failed:   43
batch:      20370 | loss: 5.22568 | failed:   43
batch:      20380 | loss: 5.21383 | failed:   43
batch:      20390 | loss: 5.18218 | failed:   43
batch:      20400 | loss: 5.19948 | failed:   43
batch:      20410 | loss: 5.20555 | failed:   43
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      20420 | loss: 5.05961 | failed:   43
batch:      20430 | loss: 5.11947 | failed:   43
batch:      20440 | loss: 5.16868 | failed:   45
batch:      20450 | loss: 5.12943 | failed:   45
batch:      20460 | loss: 5.14478 | failed:   45
batch:      20470 | loss: 5.16483 | failed:   45
batch:      20480 | loss: 5.17512 | failed:   45
batch:      20490 | loss: 5.13813 | failed:   45
batch:      20500 | loss: 5.12399 | failed:   45
batch:      20510 | loss: 5.29121 | failed:   45
batch:      20520 | loss: 5.26475 | failed:   45
batch:      20530 | loss: 5.23617 | failed:   45
batch:      20540 | loss: 5.20441 | failed:   45
batch:      20550 | loss: 5.17083 | failed:   45
batch:      20560 | loss: 5.17341 | failed:   45
batch:      20570 | loss: 5.24453 | failed:   45
batch:      20580 | loss: 5.22487 | failed:   45
batch:      20590 | loss: 5.20766 | failed:   45
batch:      20600 | loss: 5.35151 | failed:   45
batch:      20610 | loss: 5.19835 | failed:   45
batch:      20620 | loss: 5.24632 | failed:   45
batch:      20630 | loss: 5.25900 | failed:   45
batch:      20640 | loss: 5.18041 | failed:   45
batch:      20650 | loss: 5.23780 | failed:   45
batch:      20660 | loss: 5.03409 | failed:   45
batch:      20670 | loss: 5.27376 | failed:   45
batch:      20680 | loss: 5.27629 | failed:   45
batch:      20690 | loss: 5.08002 | failed:   45
batch:      20700 | loss: 5.26220 | failed:   45
batch:      20710 | loss: 5.21889 | failed:   45
batch:      20720 | loss: 5.23891 | failed:   45
batch:      20730 | loss: 5.18880 | failed:   45
batch:      20740 | loss: 5.21747 | failed:   45
batch:      20750 | loss: 5.20488 | failed:   45
batch:      20760 | loss: 5.10809 | failed:   45
batch:      20770 | loss: 5.15945 | failed:   45
batch:      20780 | loss: 5.18844 | failed:   45
batch:      20790 | loss: 5.16153 | failed:   45
batch:      20800 | loss: 5.19864 | failed:   45
batch:      20810 | loss: 5.15029 | failed:   45
batch:      20820 | loss: 5.16781 | failed:   45
batch:      20830 | loss: 5.14923 | failed:   45
batch:      20840 | loss: 5.13362 | failed:   45
batch:      20850 | loss: 5.28394 | failed:   45
batch:      20860 | loss: 5.27484 | failed:   45
batch:      20870 | loss: 5.23413 | failed:   45
batch:      20880 | loss: 5.24158 | failed:   45
batch:      20890 | loss: 5.24595 | failed:   45
batch:      20900 | loss: 5.11003 | failed:   45
batch:      20910 | loss: 5.21614 | failed:   45
batch:      20920 | loss: 5.19158 | failed:   45
batch:      20930 | loss: 5.21417 | failed:   45
batch:      20940 | loss: 5.27940 | failed:   45
batch:      20950 | loss: 5.18742 | failed:   45
batch:      20960 | loss: 5.00770 | failed:   45
batch:      20970 | loss: 5.10377 | failed:   45
batch:      20980 | loss: 5.09397 | failed:   45
batch:      20990 | loss: 5.19187 | failed:   45
batch:      21000 | loss: 5.16767 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      21010 | loss: 5.11524 | failed:   45
batch:      21020 | loss: 5.15164 | failed:   45
batch:      21030 | loss: 5.19705 | failed:   45
batch:      21040 | loss: 5.17928 | failed:   45
batch:      21050 | loss: 5.24556 | failed:   45
batch:      21060 | loss: 5.14527 | failed:   45
batch:      21070 | loss: 5.16701 | failed:   45
batch:      21080 | loss: 5.14253 | failed:   45
batch:      21090 | loss: 5.16577 | failed:   45
batch:      21100 | loss: 5.06835 | failed:   45
batch:      21110 | loss: 5.15937 | failed:   45
batch:      21120 | loss: 5.25518 | failed:   45
batch:      21130 | loss: 5.13255 | failed:   45
batch:      21140 | loss: 5.16625 | failed:   45
batch:      21150 | loss: 5.19150 | failed:   45
batch:      21160 | loss: 4.89653 | failed:   45
batch:      21170 | loss: 5.19738 | failed:   45
batch:      21180 | loss: 5.13681 | failed:   45
batch:      21190 | loss: 5.12322 | failed:   45
batch:      21200 | loss: 4.87550 | failed:   45
batch:      21210 | loss: 4.97037 | failed:   45
batch:      21220 | loss: 4.89421 | failed:   45
batch:      21230 | loss: 5.34908 | failed:   45
batch:      21240 | loss: 5.23205 | failed:   45
batch:      21250 | loss: 5.23829 | failed:   45
batch:      21260 | loss: 5.21343 | failed:   45
batch:      21270 | loss: 5.19379 | failed:   45
batch:      21280 | loss: 5.06244 | failed:   45
batch:      21290 | loss: 5.10876 | failed:   45
batch:      21300 | loss: 5.19273 | failed:   45
batch:      21310 | loss: 5.02175 | failed:   45
batch:      21320 | loss: 5.17503 | failed:   45
batch:      21330 | loss: 5.21309 | failed:   45
batch:      21340 | loss: 5.24844 | failed:   45
batch:      21350 | loss: 5.16927 | failed:   45
batch:      21360 | loss: 5.18948 | failed:   45
batch:      21370 | loss: 5.09577 | failed:   45
batch:      21380 | loss: 5.28497 | failed:   45
batch:      21390 | loss: 5.15224 | failed:   45
batch:      21400 | loss: 5.20082 | failed:   45
batch:      21410 | loss: 5.18791 | failed:   45
batch:      21420 | loss: 5.17993 | failed:   45
batch:      21430 | loss: 5.20652 | failed:   45
batch:      21440 | loss: 5.15690 | failed:   45
batch:      21450 | loss: 5.21423 | failed:   45
batch:      21460 | loss: 5.16100 | failed:   45
batch:      21470 | loss: 5.26943 | failed:   45
batch:      21480 | loss: 5.14889 | failed:   45
batch:      21490 | loss: 5.17486 | failed:   45
batch:      21500 | loss: 5.18730 | failed:   45
batch:      21510 | loss: 5.17479 | failed:   45
batch:      21520 | loss: 5.18959 | failed:   45
batch:      21530 | loss: 5.23195 | failed:   45
batch:      21540 | loss: 5.15133 | failed:   45
batch:      21550 | loss: 5.13804 | failed:   45
batch:      21560 | loss: 5.18939 | failed:   45
batch:      21570 | loss: 5.20669 | failed:   45
batch:      21580 | loss: 5.17603 | failed:   45
batch:      21590 | loss: 5.09000 | failed:   45
batch:      21600 | loss: 5.09764 | failed:   45
batch:      21610 | loss: 5.19469 | failed:   45
batch:      21620 | loss: 5.15215 | failed:   45
batch:      21630 | loss: 5.14431 | failed:   45
batch:      21640 | loss: 5.30509 | failed:   45
batch:      21650 | loss: 5.33290 | failed:   45
batch:      21660 | loss: 5.26051 | failed:   45
batch:      21670 | loss: 5.17500 | failed:   45
batch:      21680 | loss: 5.22194 | failed:   45
batch:      21690 | loss: 5.25619 | failed:   45
batch:      21700 | loss: 5.18323 | failed:   45
batch:      21710 | loss: 5.22870 | failed:   45
batch:      21720 | loss: 5.15341 | failed:   45
batch:      21730 | loss: 5.16502 | failed:   45
batch:      21740 | loss: 5.20270 | failed:   45
batch:      21750 | loss: 5.12379 | failed:   45
batch:      21760 | loss: 5.01347 | failed:   45
batch:      21770 | loss: 5.20262 | failed:   45
batch:      21780 | loss: 5.36766 | failed:   45
batch:      21790 | loss: 5.15454 | failed:   45
batch:      21800 | loss: 5.22360 | failed:   45
batch:      21810 | loss: 5.17028 | failed:   45
batch:      21820 | loss: 5.20756 | failed:   45
batch:      21830 | loss: 5.21754 | failed:   45
batch:      21840 | loss: 5.21749 | failed:   45
batch:      21850 | loss: 5.18693 | failed:   45
batch:      21860 | loss: 5.23346 | failed:   45
batch:      21870 | loss: 5.19797 | failed:   45
batch:      21880 | loss: 5.09027 | failed:   45
batch:      21890 | loss: 5.26786 | failed:   45
batch:      21900 | loss: 5.15217 | failed:   45
batch:      21910 | loss: 5.23551 | failed:   45
batch:      21920 | loss: 5.26019 | failed:   45
batch:      21930 | loss: 5.17169 | failed:   45
batch:      21940 | loss: 5.05447 | failed:   45
batch:      21950 | loss: 5.25150 | failed:   45
batch:      21960 | loss: 5.22780 | failed:   45
batch:      21970 | loss: 5.23633 | failed:   45
batch:      21980 | loss: 5.20153 | failed:   45
batch:      21990 | loss: 4.93593 | failed:   45
batch:      22000 | loss: 5.09322 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      22010 | loss: 5.05215 | failed:   45
batch:      22020 | loss: 5.05626 | failed:   45
batch:      22030 | loss: 5.09423 | failed:   45
batch:      22040 | loss: 5.04865 | failed:   45
batch:      22050 | loss: 5.04340 | failed:   45
batch:      22060 | loss: 5.13508 | failed:   45
batch:      22070 | loss: 5.08273 | failed:   45
batch:      22080 | loss: 5.22727 | failed:   45
batch:      22090 | loss: 5.17067 | failed:   45
batch:      22100 | loss: 5.22423 | failed:   45
batch:      22110 | loss: 5.25117 | failed:   45
batch:      22120 | loss: 5.15811 | failed:   45
batch:      22130 | loss: 5.26363 | failed:   45
batch:      22140 | loss: 5.17615 | failed:   45
batch:      22150 | loss: 5.20390 | failed:   45
batch:      22160 | loss: 5.06981 | failed:   45
batch:      22170 | loss: 5.31994 | failed:   45
batch:      22180 | loss: 5.29706 | failed:   45
batch:      22190 | loss: 5.13704 | failed:   45
batch:      22200 | loss: 5.20964 | failed:   45
batch:      22210 | loss: 5.10062 | failed:   45
batch:      22220 | loss: 5.09516 | failed:   45
batch:      22230 | loss: 5.21157 | failed:   45
batch:      22240 | loss: 5.27201 | failed:   45
batch:      22250 | loss: 5.13457 | failed:   45
batch:      22260 | loss: 5.22462 | failed:   45
batch:      22270 | loss: 5.27870 | failed:   45
batch:      22280 | loss: 4.96442 | failed:   45
batch:      22290 | loss: 5.26975 | failed:   45
batch:      22300 | loss: 5.23105 | failed:   45
batch:      22310 | loss: 5.10380 | failed:   45
batch:      22320 | loss: 5.23738 | failed:   45
batch:      22330 | loss: 5.06592 | failed:   45
batch:      22340 | loss: 5.08696 | failed:   45
batch:      22350 | loss: 5.18710 | failed:   45
batch:      22360 | loss: 5.21178 | failed:   45
batch:      22370 | loss: 5.19846 | failed:   45
batch:      22380 | loss: 5.15053 | failed:   45
batch:      22390 | loss: 5.08093 | failed:   45
batch:      22400 | loss: 5.24278 | failed:   45
batch:      22410 | loss: 5.21077 | failed:   45
batch:      22420 | loss: 4.95413 | failed:   45
batch:      22430 | loss: 5.23281 | failed:   45
batch:      22440 | loss: 5.18803 | failed:   45
batch:      22450 | loss: 5.24212 | failed:   45
batch:      22460 | loss: 5.12894 | failed:   45
batch:      22470 | loss: 5.22094 | failed:   45
batch:      22480 | loss: 5.21735 | failed:   45
batch:      22490 | loss: 5.32044 | failed:   45
batch:      22500 | loss: 5.04422 | failed:   45
batch:      22510 | loss: 5.19668 | failed:   45
batch:      22520 | loss: 5.19964 | failed:   45
batch:      22530 | loss: 5.17501 | failed:   45
batch:      22540 | loss: 4.84511 | failed:   45
batch:      22550 | loss: 4.95383 | failed:   45
batch:      22560 | loss: 5.30899 | failed:   45
batch:      22570 | loss: 5.27361 | failed:   45
batch:      22580 | loss: 5.23209 | failed:   45
batch:      22590 | loss: 5.18831 | failed:   45
batch:      22600 | loss: 5.17212 | failed:   45
batch:      22610 | loss: 5.12777 | failed:   45
batch:      22620 | loss: 5.20978 | failed:   45
batch:      22630 | loss: 5.00548 | failed:   45
batch:      22640 | loss: 5.17772 | failed:   45
batch:      22650 | loss: 5.14514 | failed:   45
batch:      22660 | loss: 5.14277 | failed:   45
batch:      22670 | loss: 5.14636 | failed:   45
batch:      22680 | loss: 5.12067 | failed:   45
batch:      22690 | loss: 5.15572 | failed:   45
batch:      22700 | loss: 5.11792 | failed:   45
batch:      22710 | loss: 5.00211 | failed:   45
batch:      22720 | loss: 5.09861 | failed:   45
batch:      22730 | loss: 5.11033 | failed:   45
batch:      22740 | loss: 5.06183 | failed:   45
batch:      22750 | loss: 5.30801 | failed:   45
batch:      22760 | loss: 5.22449 | failed:   45
batch:      22770 | loss: 5.03102 | failed:   45
batch:      22780 | loss: 5.22269 | failed:   45
batch:      22790 | loss: 5.24511 | failed:   45
batch:      22800 | loss: 5.14994 | failed:   45
batch:      22810 | loss: 5.14892 | failed:   45
batch:      22820 | loss: 5.17355 | failed:   45
batch:      22830 | loss: 5.18418 | failed:   45
batch:      22840 | loss: 5.23909 | failed:   45
batch:      22850 | loss: 5.15489 | failed:   45
batch:      22860 | loss: 5.23343 | failed:   45
batch:      22870 | loss: 5.15364 | failed:   45
batch:      22880 | loss: 5.18622 | failed:   45
batch:      22890 | loss: 5.22650 | failed:   45
batch:      22900 | loss: 5.10317 | failed:   45
batch:      22910 | loss: 5.14072 | failed:   45
batch:      22920 | loss: 5.13028 | failed:   45
batch:      22930 | loss: 5.13770 | failed:   45
batch:      22940 | loss: 5.25557 | failed:   45
batch:      22950 | loss: 5.21323 | failed:   45
batch:      22960 | loss: 5.23173 | failed:   45
batch:      22970 | loss: 5.20819 | failed:   45
batch:      22980 | loss: 5.24583 | failed:   45
batch:      22990 | loss: 5.23142 | failed:   45
batch:      23000 | loss: 5.15722 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      23010 | loss: 5.06184 | failed:   45
batch:      23020 | loss: 5.24626 | failed:   45
batch:      23030 | loss: 5.23773 | failed:   45
batch:      23040 | loss: 5.26265 | failed:   45
batch:      23050 | loss: 5.15251 | failed:   45
batch:      23060 | loss: 5.16931 | failed:   45
batch:      23070 | loss: 5.18481 | failed:   45
batch:      23080 | loss: 5.26355 | failed:   45
batch:      23090 | loss: 5.03165 | failed:   45
batch:      23100 | loss: 5.15504 | failed:   45
batch:      23110 | loss: 5.23844 | failed:   45
batch:      23120 | loss: 5.23334 | failed:   45
batch:      23130 | loss: 5.18750 | failed:   45
batch:      23140 | loss: 5.24365 | failed:   45
batch:      23150 | loss: 5.19239 | failed:   45
batch:      23160 | loss: 5.11653 | failed:   45
batch:      23170 | loss: 5.17079 | failed:   45
batch:      23180 | loss: 5.23166 | failed:   45
batch:      23190 | loss: 5.29558 | failed:   45
batch:      23200 | loss: 5.20611 | failed:   45
batch:      23210 | loss: 5.03918 | failed:   45
batch:      23220 | loss: 5.16910 | failed:   45
batch:      23230 | loss: 5.20136 | failed:   45
batch:      23240 | loss: 5.16049 | failed:   45
batch:      23250 | loss: 5.16508 | failed:   45
batch:      23260 | loss: 5.11324 | failed:   45
batch:      23270 | loss: 5.12421 | failed:   45
batch:      23280 | loss: 5.15801 | failed:   45
batch:      23290 | loss: 5.20597 | failed:   45
batch:      23300 | loss: 5.09368 | failed:   45
batch:      23310 | loss: 5.08244 | failed:   45
batch:      23320 | loss: 5.09166 | failed:   45
batch:      23330 | loss: 5.17904 | failed:   45
batch:      23340 | loss: 5.26361 | failed:   45
batch:      23350 | loss: 5.16937 | failed:   45
batch:      23360 | loss: 5.14235 | failed:   45
batch:      23370 | loss: 5.29331 | failed:   45
batch:      23380 | loss: 5.24166 | failed:   45
batch:      23390 | loss: 5.20522 | failed:   45
batch:      23400 | loss: 5.17954 | failed:   45
batch:      23410 | loss: 5.43944 | failed:   45
batch:      23420 | loss: 5.32309 | failed:   45
batch:      23430 | loss: 5.08947 | failed:   45
batch:      23440 | loss: 5.20768 | failed:   45
batch:      23450 | loss: 5.18836 | failed:   45
batch:      23460 | loss: 5.01420 | failed:   45
batch:      23470 | loss: 5.24292 | failed:   45
batch:      23480 | loss: 5.08794 | failed:   45
batch:      23490 | loss: 5.11867 | failed:   45
batch:      23500 | loss: 5.19906 | failed:   45
batch:      23510 | loss: 5.15597 | failed:   45
batch:      23520 | loss: 5.20923 | failed:   45
batch:      23530 | loss: 5.01039 | failed:   45
batch:      23540 | loss: 5.20693 | failed:   45
batch:      23550 | loss: 5.21622 | failed:   45
batch:      23560 | loss: 5.09649 | failed:   45
batch:      23570 | loss: 5.09083 | failed:   45
batch:      23580 | loss: 4.78508 | failed:   45
batch:      23590 | loss: 5.19948 | failed:   45
batch:      23600 | loss: 5.19041 | failed:   45
batch:      23610 | loss: 5.16443 | failed:   45
batch:      23620 | loss: 5.25669 | failed:   45
batch:      23630 | loss: 5.18399 | failed:   45
batch:      23640 | loss: 5.22789 | failed:   45
batch:      23650 | loss: 5.02858 | failed:   45
batch:      23660 | loss: 5.16093 | failed:   45
batch:      23670 | loss: 5.27257 | failed:   45
batch:      23680 | loss: 5.01168 | failed:   45
batch:      23690 | loss: 5.00681 | failed:   45
batch:      23700 | loss: 5.06244 | failed:   45
batch:      23710 | loss: 5.09085 | failed:   45
batch:      23720 | loss: 5.14619 | failed:   45
batch:      23730 | loss: 5.23002 | failed:   45
batch:      23740 | loss: 5.21923 | failed:   45
batch:      23750 | loss: 5.24355 | failed:   45
batch:      23760 | loss: 5.21794 | failed:   45
batch:      23770 | loss: 5.17263 | failed:   45
batch:      23780 | loss: 5.27261 | failed:   45
batch:      23790 | loss: 5.26459 | failed:   45
batch:      23800 | loss: 5.29328 | failed:   45
batch:      23810 | loss: 5.29121 | failed:   45
batch:      23820 | loss: 5.21039 | failed:   45
batch:      23830 | loss: 5.20912 | failed:   45
batch:      23840 | loss: 5.23135 | failed:   45
batch:      23850 | loss: 5.13044 | failed:   45
batch:      23860 | loss: 5.27302 | failed:   45
batch:      23870 | loss: 5.25779 | failed:   45
batch:      23880 | loss: 5.17699 | failed:   45
batch:      23890 | loss: 5.13168 | failed:   45
batch:      23900 | loss: 5.18277 | failed:   45
batch:      23910 | loss: 5.20840 | failed:   45
batch:      23920 | loss: 5.18830 | failed:   45
batch:      23930 | loss: 5.18995 | failed:   45
batch:      23940 | loss: 5.18473 | failed:   45
batch:      23950 | loss: 4.96279 | failed:   45
batch:      23960 | loss: 5.24633 | failed:   45
batch:      23970 | loss: 5.11871 | failed:   45
batch:      23980 | loss: 5.18010 | failed:   45
batch:      23990 | loss: 5.19761 | failed:   45
batch:      24000 | loss: 5.14109 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      24010 | loss: 5.21835 | failed:   45
batch:      24020 | loss: 5.21692 | failed:   45
batch:      24030 | loss: 5.08212 | failed:   45
batch:      24040 | loss: 5.19762 | failed:   45
batch:      24050 | loss: 5.18735 | failed:   45
batch:      24060 | loss: 5.20989 | failed:   45
batch:      24070 | loss: 5.20881 | failed:   45
batch:      24080 | loss: 5.05641 | failed:   45
batch:      24090 | loss: 5.27682 | failed:   45
batch:      24100 | loss: 5.23256 | failed:   45
batch:      24110 | loss: 5.16325 | failed:   45
batch:      24120 | loss: 5.14116 | failed:   45
batch:      24130 | loss: 5.14333 | failed:   45
batch:      24140 | loss: 5.17778 | failed:   45
batch:      24150 | loss: 5.10134 | failed:   45
batch:      24160 | loss: 5.04856 | failed:   45
batch:      24170 | loss: 5.26641 | failed:   45
batch:      24180 | loss: 5.25069 | failed:   45
batch:      24190 | loss: 5.15896 | failed:   45
batch:      24200 | loss: 5.19037 | failed:   45
batch:      24210 | loss: 5.11829 | failed:   45
batch:      24220 | loss: 5.10097 | failed:   45
batch:      24230 | loss: 5.06903 | failed:   45
batch:      24240 | loss: 5.11743 | failed:   45
batch:      24250 | loss: 5.10164 | failed:   45
batch:      24260 | loss: 5.20433 | failed:   45
batch:      24270 | loss: 5.28270 | failed:   45
batch:      24280 | loss: 5.14696 | failed:   45
batch:      24290 | loss: 5.17508 | failed:   45
batch:      24300 | loss: 5.22183 | failed:   45
batch:      24310 | loss: 5.18119 | failed:   45
batch:      24320 | loss: 5.17667 | failed:   45
batch:      24330 | loss: 5.16946 | failed:   45
batch:      24340 | loss: 5.22436 | failed:   45
batch:      24350 | loss: 5.23538 | failed:   45
batch:      24360 | loss: 5.22163 | failed:   45
batch:      24370 | loss: 5.18373 | failed:   45
batch:      24380 | loss: 5.11252 | failed:   45
batch:      24390 | loss: 5.19842 | failed:   45
batch:      24400 | loss: 5.20510 | failed:   45
batch:      24410 | loss: 5.17694 | failed:   45
batch:      24420 | loss: 5.20698 | failed:   45
batch:      24430 | loss: 5.24640 | failed:   45
batch:      24440 | loss: 5.19730 | failed:   45
batch:      24450 | loss: 5.18753 | failed:   45
batch:      24460 | loss: 5.27399 | failed:   45
batch:      24470 | loss: 5.25743 | failed:   45
batch:      24480 | loss: 5.18811 | failed:   45
batch:      24490 | loss: 5.02247 | failed:   45
batch:      24500 | loss: 5.20435 | failed:   45
batch:      24510 | loss: 5.11826 | failed:   45
batch:      24520 | loss: 5.17260 | failed:   45
batch:      24530 | loss: 5.17190 | failed:   45
batch:      24540 | loss: 5.07552 | failed:   45
batch:      24550 | loss: 5.22997 | failed:   45
batch:      24560 | loss: 5.24580 | failed:   45
batch:      24570 | loss: 5.20605 | failed:   45
batch:      24580 | loss: 5.19760 | failed:   45
batch:      24590 | loss: 5.21850 | failed:   45
batch:      24600 | loss: 5.22989 | failed:   45
batch:      24610 | loss: 5.23294 | failed:   45
batch:      24620 | loss: 5.22351 | failed:   45
batch:      24630 | loss: 5.21878 | failed:   45
batch:      24640 | loss: 5.19022 | failed:   45
batch:      24650 | loss: 5.25797 | failed:   45
batch:      24660 | loss: 5.35879 | failed:   45
batch:      24670 | loss: 5.12181 | failed:   45
batch:      24680 | loss: 5.12367 | failed:   45
batch:      24690 | loss: 5.05679 | failed:   45
batch:      24700 | loss: 5.01894 | failed:   45
batch:      24710 | loss: 5.19642 | failed:   45
batch:      24720 | loss: 5.22000 | failed:   45
batch:      24730 | loss: 5.21196 | failed:   45
batch:      24740 | loss: 5.25800 | failed:   45
batch:      24750 | loss: 5.17797 | failed:   45
batch:      24760 | loss: 5.26725 | failed:   45
batch:      24770 | loss: 5.21347 | failed:   45
batch:      24780 | loss: 5.17558 | failed:   45
batch:      24790 | loss: 5.22624 | failed:   45
batch:      24800 | loss: 5.20375 | failed:   45
batch:      24810 | loss: 5.17366 | failed:   45
batch:      24820 | loss: 5.09227 | failed:   45
batch:      24830 | loss: 5.22891 | failed:   45
batch:      24840 | loss: 5.17267 | failed:   45
batch:      24850 | loss: 5.20939 | failed:   45
batch:      24860 | loss: 5.21471 | failed:   45
batch:      24870 | loss: 5.20853 | failed:   45
batch:      24880 | loss: 5.23181 | failed:   45
batch:      24890 | loss: 5.21116 | failed:   45
batch:      24900 | loss: 5.09487 | failed:   45
batch:      24910 | loss: 5.19360 | failed:   45
batch:      24920 | loss: 5.07939 | failed:   45
batch:      24930 | loss: 5.17832 | failed:   45
batch:      24940 | loss: 5.11781 | failed:   45
batch:      24950 | loss: 5.10231 | failed:   45
batch:      24960 | loss: 5.21010 | failed:   45
batch:      24970 | loss: 5.23495 | failed:   45
batch:      24980 | loss: 5.25286 | failed:   45
batch:      24990 | loss: 5.12309 | failed:   45
batch:      25000 | loss: 5.10892 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      25010 | loss: 5.07766 | failed:   45
batch:      25020 | loss: 5.20563 | failed:   45
batch:      25030 | loss: 5.24442 | failed:   45
batch:      25040 | loss: 5.16222 | failed:   45
batch:      25050 | loss: 5.21256 | failed:   45
batch:      25060 | loss: 5.23388 | failed:   45
batch:      25070 | loss: 4.89226 | failed:   45
batch:      25080 | loss: 5.52074 | failed:   45
batch:      25090 | loss: 5.09753 | failed:   45
batch:      25100 | loss: 5.23538 | failed:   45
batch:      25110 | loss: 5.21523 | failed:   45
batch:      25120 | loss: 5.23459 | failed:   45
batch:      25130 | loss: 5.27639 | failed:   45
batch:      25140 | loss: 5.17251 | failed:   45
batch:      25150 | loss: 5.03696 | failed:   45
batch:      25160 | loss: 5.12143 | failed:   45
batch:      25170 | loss: 5.17729 | failed:   45
batch:      25180 | loss: 5.16979 | failed:   45
batch:      25190 | loss: 5.16321 | failed:   45
batch:      25200 | loss: 5.26880 | failed:   45
batch:      25210 | loss: 5.09546 | failed:   45
batch:      25220 | loss: 5.14698 | failed:   45
batch:      25230 | loss: 4.99096 | failed:   45
batch:      25240 | loss: 5.12099 | failed:   45
batch:      25250 | loss: 5.18446 | failed:   45
batch:      25260 | loss: 5.25099 | failed:   45
batch:      25270 | loss: 5.09420 | failed:   45
batch:      25280 | loss: 5.26570 | failed:   45
batch:      25290 | loss: 4.96992 | failed:   45
batch:      25300 | loss: 5.14479 | failed:   45
batch:      25310 | loss: 5.14622 | failed:   45
batch:      25320 | loss: 5.20173 | failed:   45
batch:      25330 | loss: 5.12939 | failed:   45
batch:      25340 | loss: 5.10589 | failed:   45
batch:      25350 | loss: 5.11903 | failed:   45
batch:      25360 | loss: 5.07086 | failed:   45
batch:      25370 | loss: 5.25115 | failed:   45
batch:      25380 | loss: 5.21918 | failed:   45
batch:      25390 | loss: 5.19871 | failed:   45
batch:      25400 | loss: 5.25321 | failed:   45
batch:      25410 | loss: 5.25861 | failed:   45
batch:      25420 | loss: 5.20577 | failed:   45
batch:      25430 | loss: 5.24476 | failed:   45
batch:      25440 | loss: 5.17907 | failed:   45
batch:      25450 | loss: 5.29651 | failed:   45
batch:      25460 | loss: 5.17782 | failed:   45
batch:      25470 | loss: 5.22673 | failed:   45
batch:      25480 | loss: 5.24146 | failed:   45
batch:      25490 | loss: 5.22098 | failed:   45
batch:      25500 | loss: 5.20029 | failed:   45
batch:      25510 | loss: 5.20120 | failed:   45
batch:      25520 | loss: 5.18022 | failed:   45
batch:      25530 | loss: 5.24141 | failed:   45
batch:      25540 | loss: 5.16634 | failed:   45
batch:      25550 | loss: 4.85826 | failed:   45
batch:      25560 | loss: 5.07042 | failed:   45
batch:      25570 | loss: 5.01522 | failed:   45
batch:      25580 | loss: 5.11137 | failed:   45
batch:      25590 | loss: 5.11844 | failed:   45
batch:      25600 | loss: 5.13491 | failed:   45
batch:      25610 | loss: 5.09250 | failed:   45
batch:      25620 | loss: 5.22000 | failed:   45
batch:      25630 | loss: 5.15593 | failed:   45
batch:      25640 | loss: 5.17020 | failed:   45
batch:      25650 | loss: 5.29263 | failed:   45
batch:      25660 | loss: 5.19253 | failed:   45
batch:      25670 | loss: 5.41898 | failed:   45
batch:      25680 | loss: 5.16523 | failed:   45
batch:      25690 | loss: 5.12581 | failed:   45
batch:      25700 | loss: 5.16803 | failed:   45
batch:      25710 | loss: 5.18642 | failed:   45
batch:      25720 | loss: 5.22944 | failed:   45
batch:      25730 | loss: 5.18279 | failed:   45
batch:      25740 | loss: 5.02360 | failed:   45
batch:      25750 | loss: 5.23646 | failed:   45
batch:      25760 | loss: 5.07707 | failed:   45
batch:      25770 | loss: 5.13901 | failed:   45
batch:      25780 | loss: 5.23465 | failed:   45
batch:      25790 | loss: 5.28541 | failed:   45
batch:      25800 | loss: 5.11838 | failed:   45
batch:      25810 | loss: 5.25370 | failed:   45
batch:      25820 | loss: 5.22332 | failed:   45
batch:      25830 | loss: 5.26075 | failed:   45
batch:      25840 | loss: 5.13068 | failed:   45
batch:      25850 | loss: 5.20119 | failed:   45
batch:      25860 | loss: 5.20230 | failed:   45
batch:      25870 | loss: 5.25448 | failed:   45
batch:      25880 | loss: 5.10247 | failed:   45
batch:      25890 | loss: 5.22151 | failed:   45
batch:      25900 | loss: 5.28092 | failed:   45
batch:      25910 | loss: 5.19428 | failed:   45
batch:      25920 | loss: 5.12777 | failed:   45
batch:      25930 | loss: 5.21648 | failed:   45
batch:      25940 | loss: 5.01266 | failed:   45
batch:      25950 | loss: 5.22175 | failed:   45
batch:      25960 | loss: 5.07492 | failed:   45
batch:      25970 | loss: 5.03178 | failed:   45
batch:      25980 | loss: 5.11022 | failed:   45
batch:      25990 | loss: 5.26959 | failed:   45
batch:      26000 | loss: 5.28563 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      26010 | loss: 5.19792 | failed:   45
batch:      26020 | loss: 5.18702 | failed:   45
batch:      26030 | loss: 5.23253 | failed:   45
batch:      26040 | loss: 5.19124 | failed:   45
batch:      26050 | loss: 5.22185 | failed:   45
batch:      26060 | loss: 5.22393 | failed:   45
batch:      26070 | loss: 5.19731 | failed:   45
batch:      26080 | loss: 5.25720 | failed:   45
batch:      26090 | loss: 4.99006 | failed:   45
batch:      26100 | loss: 5.14057 | failed:   45
batch:      26110 | loss: 5.14441 | failed:   45
batch:      26120 | loss: 5.04622 | failed:   45
batch:      26130 | loss: 5.11769 | failed:   45
batch:      26140 | loss: 4.94018 | failed:   45
batch:      26150 | loss: 5.16292 | failed:   45
batch:      26160 | loss: 5.15199 | failed:   45
batch:      26170 | loss: 5.14269 | failed:   45
batch:      26180 | loss: 5.11197 | failed:   45
batch:      26190 | loss: 5.11904 | failed:   45
batch:      26200 | loss: 5.12541 | failed:   45
batch:      26210 | loss: 5.05871 | failed:   45
batch:      26220 | loss: 5.26800 | failed:   45
batch:      26230 | loss: 5.20075 | failed:   45
batch:      26240 | loss: 5.16562 | failed:   45
batch:      26250 | loss: 5.06592 | failed:   45
batch:      26260 | loss: 5.21814 | failed:   45
batch:      26270 | loss: 5.28457 | failed:   45
batch:      26280 | loss: 5.19659 | failed:   45
batch:      26290 | loss: 5.22901 | failed:   45
batch:      26300 | loss: 5.17800 | failed:   45
batch:      26310 | loss: 5.23280 | failed:   45
batch:      26320 | loss: 5.24159 | failed:   45
batch:      26330 | loss: 5.25426 | failed:   45
batch:      26340 | loss: 5.15844 | failed:   45
batch:      26350 | loss: 5.21954 | failed:   45
batch:      26360 | loss: 5.20040 | failed:   45
batch:      26370 | loss: 5.04496 | failed:   45
batch:      26380 | loss: 5.12532 | failed:   45
batch:      26390 | loss: 4.94836 | failed:   45
batch:      26400 | loss: 5.07533 | failed:   45
batch:      26410 | loss: 5.17268 | failed:   45
batch:      26420 | loss: 5.21814 | failed:   45
batch:      26430 | loss: 5.20991 | failed:   45
batch:      26440 | loss: 5.15187 | failed:   45
batch:      26450 | loss: 5.08928 | failed:   45
batch:      26460 | loss: 5.11845 | failed:   45
batch:      26470 | loss: 5.08086 | failed:   45
batch:      26480 | loss: 5.08133 | failed:   45
batch:      26490 | loss: 4.97103 | failed:   45
batch:      26500 | loss: 5.22556 | failed:   45
batch:      26510 | loss: 5.20244 | failed:   45
batch:      26520 | loss: 5.21976 | failed:   45
batch:      26530 | loss: 5.21220 | failed:   45
batch:      26540 | loss: 5.22953 | failed:   45
batch:      26550 | loss: 5.08432 | failed:   45
batch:      26560 | loss: 5.18966 | failed:   45
batch:      26570 | loss: 5.26721 | failed:   45
batch:      26580 | loss: 5.26605 | failed:   45
batch:      26590 | loss: 5.18329 | failed:   45
batch:      26600 | loss: 5.28704 | failed:   45
batch:      26610 | loss: 5.14796 | failed:   45
batch:      26620 | loss: 5.06660 | failed:   45
batch:      26630 | loss: 5.09132 | failed:   45
batch:      26640 | loss: 5.25172 | failed:   45
batch:      26650 | loss: 5.18633 | failed:   45
batch:      26660 | loss: 5.12224 | failed:   45
batch:      26670 | loss: 5.11756 | failed:   45
batch:      26680 | loss: 5.11340 | failed:   45
batch:      26690 | loss: 4.46348 | failed:   45
batch:      26700 | loss: 5.25038 | failed:   45
batch:      26710 | loss: 5.12326 | failed:   45
batch:      26720 | loss: 5.15914 | failed:   45
batch:      26730 | loss: 5.14786 | failed:   45
batch:      26740 | loss: 5.10660 | failed:   45
batch:      26750 | loss: 5.13884 | failed:   45
batch:      26760 | loss: 5.18701 | failed:   45
batch:      26770 | loss: 5.20715 | failed:   45
batch:      26780 | loss: 5.13729 | failed:   45
batch:      26790 | loss: 5.21397 | failed:   45
batch:      26800 | loss: 5.14384 | failed:   45
batch:      26810 | loss: 4.97805 | failed:   45
batch:      26820 | loss: 5.06632 | failed:   45
batch:      26830 | loss: 5.21038 | failed:   45
batch:      26840 | loss: 5.17423 | failed:   45
batch:      26850 | loss: 5.17735 | failed:   45
batch:      26860 | loss: 5.18984 | failed:   45
batch:      26870 | loss: 5.20824 | failed:   45
batch:      26880 | loss: 5.22167 | failed:   45
batch:      26890 | loss: 5.19277 | failed:   45
batch:      26900 | loss: 5.16649 | failed:   45
batch:      26910 | loss: 5.12197 | failed:   45
batch:      26920 | loss: 5.11346 | failed:   45
batch:      26930 | loss: 5.09837 | failed:   45
batch:      26940 | loss: 5.09183 | failed:   45
batch:      26950 | loss: 5.20592 | failed:   45
batch:      26960 | loss: 5.23379 | failed:   45
batch:      26970 | loss: 5.21548 | failed:   45
batch:      26980 | loss: 5.17229 | failed:   45
batch:      26990 | loss: 5.22207 | failed:   45
batch:      27000 | loss: 5.24495 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      27010 | loss: 5.17901 | failed:   45
batch:      27020 | loss: 5.19991 | failed:   45
batch:      27030 | loss: 5.19942 | failed:   45
batch:      27040 | loss: 5.10359 | failed:   45
batch:      27050 | loss: 5.14802 | failed:   45
batch:      27060 | loss: 5.06115 | failed:   45
batch:      27070 | loss: 5.11697 | failed:   45
batch:      27080 | loss: 5.16152 | failed:   45
batch:      27090 | loss: 5.19176 | failed:   45
batch:      27100 | loss: 5.17751 | failed:   45
batch:      27110 | loss: 5.01567 | failed:   45
batch:      27120 | loss: 5.09471 | failed:   45
batch:      27130 | loss: 5.30239 | failed:   45
batch:      27140 | loss: 5.20414 | failed:   45
batch:      27150 | loss: 5.23023 | failed:   45
batch:      27160 | loss: 5.27363 | failed:   45
batch:      27170 | loss: 5.17825 | failed:   45
batch:      27180 | loss: 5.24519 | failed:   45
batch:      27190 | loss: 5.24634 | failed:   45
batch:      27200 | loss: 5.19702 | failed:   45
batch:      27210 | loss: 5.24106 | failed:   45
batch:      27220 | loss: 5.21874 | failed:   45
batch:      27230 | loss: 5.01892 | failed:   45
batch:      27240 | loss: 5.04479 | failed:   45
batch:      27250 | loss: 5.22967 | failed:   45
batch:      27260 | loss: 5.18766 | failed:   45
batch:      27270 | loss: 5.29480 | failed:   45
batch:      27280 | loss: 5.26670 | failed:   45
batch:      27290 | loss: 5.12219 | failed:   45
batch:      27300 | loss: 5.17271 | failed:   45
batch:      27310 | loss: 5.22887 | failed:   45
batch:      27320 | loss: 5.22078 | failed:   45
batch:      27330 | loss: 5.20303 | failed:   45
batch:      27340 | loss: 5.10860 | failed:   45
batch:      27350 | loss: 5.19350 | failed:   45
batch:      27360 | loss: 5.28075 | failed:   45
batch:      27370 | loss: 5.23399 | failed:   45
batch:      27380 | loss: 5.21511 | failed:   45
batch:      27390 | loss: 5.15005 | failed:   45
batch:      27400 | loss: 5.17458 | failed:   45
batch:      27410 | loss: 5.16407 | failed:   45
batch:      27420 | loss: 5.22173 | failed:   45
batch:      27430 | loss: 5.17872 | failed:   45
batch:      27440 | loss: 5.09050 | failed:   45
batch:      27450 | loss: 4.99986 | failed:   45
batch:      27460 | loss: 5.14842 | failed:   45
batch:      27470 | loss: 5.18543 | failed:   45
batch:      27480 | loss: 5.05118 | failed:   45
batch:      27490 | loss: 5.23318 | failed:   45
batch:      27500 | loss: 5.15145 | failed:   45
batch:      27510 | loss: 4.96960 | failed:   45
batch:      27520 | loss: 5.24292 | failed:   45
batch:      27530 | loss: 5.22258 | failed:   45
batch:      27540 | loss: 5.22935 | failed:   45
batch:      27550 | loss: 5.09798 | failed:   45
batch:      27560 | loss: 5.14828 | failed:   45
batch:      27570 | loss: 5.16380 | failed:   45
batch:      27580 | loss: 5.24278 | failed:   45
batch:      27590 | loss: 5.20539 | failed:   45
batch:      27600 | loss: 5.05738 | failed:   45
batch:      27610 | loss: 5.18551 | failed:   45
batch:      27620 | loss: 5.23265 | failed:   45
batch:      27630 | loss: 5.20063 | failed:   45
batch:      27640 | loss: 5.15194 | failed:   45
batch:      27650 | loss: 5.13295 | failed:   45
batch:      27660 | loss: 5.25985 | failed:   45
batch:      27670 | loss: 5.23564 | failed:   45
batch:      27680 | loss: 5.19885 | failed:   45
batch:      27690 | loss: 5.13753 | failed:   45
batch:      27700 | loss: 5.14358 | failed:   45
batch:      27710 | loss: 5.18684 | failed:   45
batch:      27720 | loss: 5.19234 | failed:   45
batch:      27730 | loss: 5.03505 | failed:   45
batch:      27740 | loss: 5.18829 | failed:   45
batch:      27750 | loss: 5.12795 | failed:   45
batch:      27760 | loss: 5.26747 | failed:   45
batch:      27770 | loss: 5.11878 | failed:   45
batch:      27780 | loss: 5.15518 | failed:   45
batch:      27790 | loss: 5.18201 | failed:   45
batch:      27800 | loss: 5.25722 | failed:   45
batch:      27810 | loss: 5.24586 | failed:   45
batch:      27820 | loss: 5.23778 | failed:   45
batch:      27830 | loss: 5.25430 | failed:   45
batch:      27840 | loss: 5.24663 | failed:   45
batch:      27850 | loss: 5.18856 | failed:   45
batch:      27860 | loss: 5.18086 | failed:   45
batch:      27870 | loss: 5.17784 | failed:   45
batch:      27880 | loss: 5.12220 | failed:   45
batch:      27890 | loss: 5.14542 | failed:   45
batch:      27900 | loss: 5.16931 | failed:   45
batch:      27910 | loss: 5.22970 | failed:   45
batch:      27920 | loss: 5.23211 | failed:   45
batch:      27930 | loss: 5.17863 | failed:   45
batch:      27940 | loss: 5.24301 | failed:   45
batch:      27950 | loss: 5.24471 | failed:   45
batch:      27960 | loss: 5.18343 | failed:   45
batch:      27970 | loss: 5.06566 | failed:   45
batch:      27980 | loss: 5.18767 | failed:   45
batch:      27990 | loss: 5.25960 | failed:   45
batch:      28000 | loss: 5.22569 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      28010 | loss: 5.17519 | failed:   45
batch:      28020 | loss: 5.23324 | failed:   45
batch:      28030 | loss: 5.17706 | failed:   45
batch:      28040 | loss: 4.89718 | failed:   45
batch:      28050 | loss: 5.24185 | failed:   45
batch:      28060 | loss: 4.85057 | failed:   45
batch:      28070 | loss: 5.23882 | failed:   45
batch:      28080 | loss: 5.22348 | failed:   45
batch:      28090 | loss: 5.09918 | failed:   45
batch:      28100 | loss: 5.20778 | failed:   45
batch:      28110 | loss: 5.22798 | failed:   45
batch:      28120 | loss: 5.20607 | failed:   45
batch:      28130 | loss: 5.24757 | failed:   45
batch:      28140 | loss: 5.23767 | failed:   45
batch:      28150 | loss: 5.11768 | failed:   45
batch:      28160 | loss: 5.21667 | failed:   45
batch:      28170 | loss: 5.20623 | failed:   45
batch:      28180 | loss: 5.14604 | failed:   45
batch:      28190 | loss: 5.16011 | failed:   45
batch:      28200 | loss: 4.93080 | failed:   45
batch:      28210 | loss: 5.21045 | failed:   45
batch:      28220 | loss: 5.12689 | failed:   45
batch:      28230 | loss: 5.18965 | failed:   45
batch:      28240 | loss: 5.08263 | failed:   45
batch:      28250 | loss: 5.26791 | failed:   45
batch:      28260 | loss: 5.10636 | failed:   45
batch:      28270 | loss: 5.08891 | failed:   45
batch:      28280 | loss: 5.38644 | failed:   45
batch:      28290 | loss: 5.28199 | failed:   45
batch:      28300 | loss: 5.24446 | failed:   45
batch:      28310 | loss: 5.08773 | failed:   45
batch:      28320 | loss: 5.20750 | failed:   45
batch:      28330 | loss: 5.27180 | failed:   45
batch:      28340 | loss: 5.25289 | failed:   45
batch:      28350 | loss: 5.11421 | failed:   45
batch:      28360 | loss: 5.18967 | failed:   45
batch:      28370 | loss: 5.05484 | failed:   45
batch:      28380 | loss: 5.10125 | failed:   45
batch:      28390 | loss: 5.13684 | failed:   45
batch:      28400 | loss: 5.24191 | failed:   45
batch:      28410 | loss: 5.15596 | failed:   45
batch:      28420 | loss: 5.10002 | failed:   45
batch:      28430 | loss: 5.33184 | failed:   45
batch:      28440 | loss: 5.25463 | failed:   45
batch:      28450 | loss: 5.07095 | failed:   45
batch:      28460 | loss: 5.06200 | failed:   45
batch:      28470 | loss: 5.09494 | failed:   45
batch:      28480 | loss: 5.16460 | failed:   45
batch:      28490 | loss: 5.14666 | failed:   45
batch:      28500 | loss: 4.94675 | failed:   45
batch:      28510 | loss: 5.16303 | failed:   45
batch:      28520 | loss: 5.16094 | failed:   45
batch:      28530 | loss: 5.14552 | failed:   45
batch:      28540 | loss: 5.06230 | failed:   45
batch:      28550 | loss: 5.04167 | failed:   45
batch:      28560 | loss: 5.23896 | failed:   45
batch:      28570 | loss: 5.24299 | failed:   45
batch:      28580 | loss: 5.22423 | failed:   45
batch:      28590 | loss: 5.12235 | failed:   45
batch:      28600 | loss: 4.95986 | failed:   45
batch:      28610 | loss: 4.99510 | failed:   45
batch:      28620 | loss: 5.29047 | failed:   45
batch:      28630 | loss: 5.26474 | failed:   45
batch:      28640 | loss: 5.15076 | failed:   45
batch:      28650 | loss: 5.02172 | failed:   45
batch:      28660 | loss: 5.21907 | failed:   45
batch:      28670 | loss: 5.22771 | failed:   45
batch:      28680 | loss: 5.20100 | failed:   45
batch:      28690 | loss: 5.19563 | failed:   45
batch:      28700 | loss: 5.15132 | failed:   45
batch:      28710 | loss: 5.21402 | failed:   45
batch:      28720 | loss: 5.21532 | failed:   45
batch:      28730 | loss: 5.16017 | failed:   45
batch:      28740 | loss: 5.19200 | failed:   45
batch:      28750 | loss: 5.09340 | failed:   45
batch:      28760 | loss: 5.16683 | failed:   45
batch:      28770 | loss: 5.06199 | failed:   45
batch:      28780 | loss: 5.18866 | failed:   45
batch:      28790 | loss: 5.16875 | failed:   45
batch:      28800 | loss: 5.15487 | failed:   45
batch:      28810 | loss: 5.26670 | failed:   45
batch:      28820 | loss: 5.21388 | failed:   45
batch:      28830 | loss: 5.21630 | failed:   45
batch:      28840 | loss: 5.08154 | failed:   45
batch:      28850 | loss: 5.10372 | failed:   45
batch:      28860 | loss: 5.24738 | failed:   45
batch:      28870 | loss: 4.96959 | failed:   45
batch:      28880 | loss: 5.18891 | failed:   45
batch:      28890 | loss: 5.18503 | failed:   45
batch:      28900 | loss: 5.10757 | failed:   45
batch:      28910 | loss: 5.26409 | failed:   45
batch:      28920 | loss: 5.12665 | failed:   45
batch:      28930 | loss: 5.10740 | failed:   45
batch:      28940 | loss: 5.12444 | failed:   45
batch:      28950 | loss: 5.13134 | failed:   45
batch:      28960 | loss: 5.18887 | failed:   45
batch:      28970 | loss: 5.28038 | failed:   45
batch:      28980 | loss: 5.16159 | failed:   45
batch:      28990 | loss: 5.00975 | failed:   45
batch:      29000 | loss: 5.21247 | failed:   45
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      29010 | loss: 5.16660 | failed:   45
batch:      29020 | loss: 5.20165 | failed:   45
batch:      29030 | loss: 5.19847 | failed:   45
batch:      29040 | loss: 5.11427 | failed:   45
batch:      29050 | loss: 5.13857 | failed:   45
batch:      29060 | loss: 5.29932 | failed:   45
batch:      29070 | loss: 5.23163 | failed:   45
batch:      29080 | loss: 5.14237 | failed:   45
batch:      29090 | loss: 5.17484 | failed:   45
batch:      29100 | loss: 5.14128 | failed:   45
batch:      29110 | loss: 5.03614 | failed:   45
batch:      29120 | loss: 5.09817 | failed:   45
batch:      29130 | loss: 4.98810 | failed:   45
batch:      29140 | loss: 5.12244 | failed:   45
batch:      29150 | loss: 5.18052 | failed:   45
batch:      29160 | loss: 5.05618 | failed:   45
batch:      29170 | loss: 5.20198 | failed:   45
batch:      29180 | loss: 4.99467 | failed:   45
batch:      29190 | loss: 5.31087 | failed:   45
batch:      29200 | loss: 5.26400 | failed:   45
batch:      29210 | loss: 5.30039 | failed:   45
batch:      29220 | loss: 5.21995 | failed:   45
batch:      29230 | loss: 5.18439 | failed:   45
batch:      29240 | loss: 5.15690 | failed:   45
batch:      29250 | loss: 5.23097 | failed:   45
batch:      29260 | loss: 5.16757 | failed:   45
batch:      29270 | loss: 5.10572 | failed:   45
batch:      29280 | loss: 5.25518 | failed:   45
batch:      29290 | loss: 5.21524 | failed:   45
batch:      29300 | loss: 5.18703 | failed:   45
batch:      29310 | loss: 5.26027 | failed:   45
batch:      29320 | loss: 5.21374 | failed:   45
batch:      29330 | loss: 5.18065 | failed:   45
batch:      29340 | loss: 5.35040 | failed:   45
batch:      29350 | loss: 5.17621 | failed:   45
batch:      29360 | loss: 5.23203 | failed:   45
batch:      29370 | loss: 5.24067 | failed:   45
batch:      29380 | loss: 5.07389 | failed:   45
batch:      29390 | loss: 5.06593 | failed:   45
batch:      29400 | loss: 5.19872 | failed:   45
batch:      29410 | loss: 5.22060 | failed:   45
batch:      29420 | loss: 5.16769 | failed:   45
batch:      29430 | loss: 5.17710 | failed:   45
batch:      29440 | loss: 5.23352 | failed:   45
batch:      29450 | loss: 5.19374 | failed:   45
batch:      29460 | loss: 5.12948 | failed:   45
batch:      29470 | loss: 5.19673 | failed:   45
batch:      29480 | loss: 5.18400 | failed:   45
batch:      29490 | loss: 5.17178 | failed:   45
batch:      29500 | loss: 5.19189 | failed:   45
batch:      29510 | loss: 5.18039 | failed:   45
batch:      29520 | loss: 5.15585 | failed:   45
batch:      29530 | loss: 5.26127 | failed:   45
batch:      29540 | loss: 5.12729 | failed:   45
batch:      29550 | loss: 5.17244 | failed:   45
batch:      29560 | loss: 5.11263 | failed:   45
batch:      29570 | loss: 5.11940 | failed:   45
batch:      29580 | loss: 5.21523 | failed:   45
batch:      29590 | loss: 5.13567 | failed:   45
batch:      29600 | loss: 5.16062 | failed:   45
batch:      29610 | loss: 5.24544 | failed:   45
batch:      29620 | loss: 5.13095 | failed:   45
batch:      29630 | loss: 5.11248 | failed:   45
batch:      29640 | loss: 5.27821 | failed:   45
batch:      29650 | loss: 5.05080 | failed:   45
batch:      29660 | loss: 5.14435 | failed:   45
batch:      29670 | loss: 5.24020 | failed:   45
batch:      29680 | loss: 5.15374 | failed:   45
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      29690 | loss: 5.18757 | failed:   45
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      29700 | loss: 5.23941 | failed:   45
batch:      29720 | loss: 5.11580 | failed:   48
batch:      29730 | loss: 5.17546 | failed:   48
batch:      29740 | loss: 5.13472 | failed:   48
batch:      29750 | loss: 5.13475 | failed:   48
batch:      29760 | loss: 5.16805 | failed:   48
batch:      29770 | loss: 5.10564 | failed:   48
batch:      29780 | loss: 5.15100 | failed:   48
batch:      29790 | loss: 5.23886 | failed:   48
batch:      29800 | loss: 5.17126 | failed:   48
batch:      29810 | loss: 5.19164 | failed:   48
batch:      29820 | loss: 5.17687 | failed:   48
batch:      29830 | loss: 5.32904 | failed:   48
batch:      29840 | loss: 5.20026 | failed:   48
batch:      29850 | loss: 5.08674 | failed:   48
batch:      29860 | loss: 5.16230 | failed:   48
batch:      29870 | loss: 4.92679 | failed:   48
batch:      29880 | loss: 5.07534 | failed:   48
batch:      29890 | loss: 5.22819 | failed:   48
batch:      29900 | loss: 5.18617 | failed:   48
batch:      29910 | loss: 5.17661 | failed:   48
batch:      29920 | loss: 5.27999 | failed:   48
batch:      29930 | loss: 5.19460 | failed:   48
batch:      29940 | loss: 4.90785 | failed:   48
batch:      29950 | loss: 5.12320 | failed:   48
batch:      29960 | loss: 5.23326 | failed:   48
batch:      29970 | loss: 5.16116 | failed:   48
batch:      29980 | loss: 5.08943 | failed:   48
batch:      29990 | loss: 5.11169 | failed:   48
batch:      30000 | loss: 5.07569 | failed:   48
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      30010 | loss: 5.07497 | failed:   48
batch:      30020 | loss: 5.28448 | failed:   48
batch:      30030 | loss: 5.02136 | failed:   48
batch:      30040 | loss: 5.26525 | failed:   48
batch:      30050 | loss: 5.17442 | failed:   48
batch:      30060 | loss: 5.16163 | failed:   48
batch:      30070 | loss: 5.11360 | failed:   48
batch:      30080 | loss: 5.15244 | failed:   48
batch:      30090 | loss: 5.16553 | failed:   48
batch:      30100 | loss: 5.14115 | failed:   48
batch:      30110 | loss: 5.31559 | failed:   48
batch:      30120 | loss: 5.15901 | failed:   48
batch:      30130 | loss: 5.16293 | failed:   48
batch:      30140 | loss: 5.14080 | failed:   48
batch:      30150 | loss: 5.20884 | failed:   48
batch:      30160 | loss: 5.22560 | failed:   48
batch:      30170 | loss: 5.18141 | failed:   48
batch:      30180 | loss: 4.94511 | failed:   48
batch:      30190 | loss: 4.87952 | failed:   48
batch:      30200 | loss: 5.12441 | failed:   48
batch:      30210 | loss: 5.21072 | failed:   48
batch:      30220 | loss: 5.21057 | failed:   48
batch:      30230 | loss: 5.19453 | failed:   48
batch:      30240 | loss: 5.18560 | failed:   48
batch:      30250 | loss: 5.23422 | failed:   48
batch:      30260 | loss: 5.14821 | failed:   48
batch:      30270 | loss: 5.14956 | failed:   48
batch:      30280 | loss: 5.07063 | failed:   48
batch:      30290 | loss: 5.09002 | failed:   48
batch:      30300 | loss: 5.16107 | failed:   48
batch:      30310 | loss: 5.00128 | failed:   48
batch:      30320 | loss: 5.19105 | failed:   48
batch:      30330 | loss: 5.19414 | failed:   48
batch:      30340 | loss: 5.03677 | failed:   48
batch:      30350 | loss: 4.89110 | failed:   48
batch:      30360 | loss: 5.17053 | failed:   48
batch:      30370 | loss: 5.21499 | failed:   48
batch:      30380 | loss: 5.22111 | failed:   48
batch:      30390 | loss: 5.26200 | failed:   48
batch:      30400 | loss: 5.18636 | failed:   48
batch:      30410 | loss: 5.12885 | failed:   48
batch:      30420 | loss: 5.11644 | failed:   48
batch:      30430 | loss: 5.22995 | failed:   48
batch:      30440 | loss: 5.13843 | failed:   48
batch:      30450 | loss: 5.17518 | failed:   48
batch:      30460 | loss: 5.27357 | failed:   48
batch:      30470 | loss: 5.12120 | failed:   48
batch:      30480 | loss: 5.13856 | failed:   48
batch:      30490 | loss: 5.19414 | failed:   48
batch:      30500 | loss: 5.24592 | failed:   48
batch:      30510 | loss: 5.06244 | failed:   48
batch:      30520 | loss: 5.11126 | failed:   48
batch:      30530 | loss: 4.97357 | failed:   48
batch:      30540 | loss: 5.15106 | failed:   48
batch:      30550 | loss: 5.06866 | failed:   48
batch:      30560 | loss: 5.25066 | failed:   48
batch:      30570 | loss: 5.26863 | failed:   48
batch:      30580 | loss: 5.17808 | failed:   48
batch:      30590 | loss: 5.12271 | failed:   48
batch:      30600 | loss: 5.23293 | failed:   48
batch:      30610 | loss: 5.09758 | failed:   48
batch:      30620 | loss: 5.18992 | failed:   48
batch:      30630 | loss: 5.20566 | failed:   48
batch:      30640 | loss: 5.29771 | failed:   48
batch:      30650 | loss: 5.30728 | failed:   48
batch:      30660 | loss: 5.25099 | failed:   48
batch:      30670 | loss: 5.19017 | failed:   48
batch:      30680 | loss: 5.17664 | failed:   48
batch:      30690 | loss: 5.17516 | failed:   48
batch:      30700 | loss: 5.22972 | failed:   48
batch:      30710 | loss: 5.16805 | failed:   48
batch:      30720 | loss: 5.22784 | failed:   48
batch:      30730 | loss: 5.21364 | failed:   48
batch:      30740 | loss: 5.22498 | failed:   48
batch:      30750 | loss: 5.14586 | failed:   48
batch:      30760 | loss: 5.14683 | failed:   48
batch:      30770 | loss: 5.01046 | failed:   48
batch:      30780 | loss: 5.14199 | failed:   48
batch:      30790 | loss: 5.15723 | failed:   48
batch:      30800 | loss: 5.17075 | failed:   48
batch:      30810 | loss: 5.12997 | failed:   48
batch:      30820 | loss: 5.16982 | failed:   48
batch:      30830 | loss: 5.14760 | failed:   48
batch:      30840 | loss: 5.11203 | failed:   48
batch:      30850 | loss: 5.11955 | failed:   48
batch:      30860 | loss: 5.13139 | failed:   48
batch:      30870 | loss: 5.11978 | failed:   48
batch:      30880 | loss: 5.24104 | failed:   48
batch:      30890 | loss: 5.15804 | failed:   48
batch:      30900 | loss: 5.20287 | failed:   48
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      30910 | loss: 5.03974 | failed:   48
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      30920 | loss: 5.21594 | failed:   48
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      30950 | loss: 5.26692 | failed:   60
batch:      30960 | loss: 5.31721 | failed:   60
batch:      30970 | loss: 5.18682 | failed:   60
batch:      30980 | loss: 5.20486 | failed:   60
batch:      30990 | loss: 5.16929 | failed:   60
batch:      31000 | loss: 5.11392 | failed:   60
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      31010 | loss: 5.12829 | failed:   60
batch:      31020 | loss: 5.14173 | failed:   60
batch:      31030 | loss: 5.30787 | failed:   60
batch:      31040 | loss: 5.18545 | failed:   60
batch:      31050 | loss: 5.23791 | failed:   60
batch:      31060 | loss: 5.12757 | failed:   60
batch:      31070 | loss: 5.18671 | failed:   60
batch:      31080 | loss: 5.21605 | failed:   60
batch:      31090 | loss: 5.23851 | failed:   60
batch:      31100 | loss: 5.22968 | failed:   60
batch:      31110 | loss: 5.11074 | failed:   60
batch:      31120 | loss: 5.21574 | failed:   60
batch:      31130 | loss: 5.13460 | failed:   60
batch:      31140 | loss: 5.16969 | failed:   60
batch:      31150 | loss: 5.08194 | failed:   60
batch:      31160 | loss: 5.25058 | failed:   60
batch:      31170 | loss: 5.24146 | failed:   60
batch:      31180 | loss: 5.21697 | failed:   60
batch:      31190 | loss: 5.12224 | failed:   60
batch:      31200 | loss: 5.10559 | failed:   60
batch:      31210 | loss: 5.25068 | failed:   60
batch:      31220 | loss: 5.19420 | failed:   60
batch:      31230 | loss: 5.12179 | failed:   60
batch:      31240 | loss: 5.14110 | failed:   60
batch:      31250 | loss: 5.17743 | failed:   60
batch:      31260 | loss: 5.26109 | failed:   60
batch:      31270 | loss: 5.06222 | failed:   60
batch:      31280 | loss: 5.19727 | failed:   60
batch:      31290 | loss: 5.15878 | failed:   60
batch:      31300 | loss: 5.09206 | failed:   60
batch:      31310 | loss: 5.25467 | failed:   60
batch:      31320 | loss: 5.12723 | failed:   60
batch:      31330 | loss: 5.16400 | failed:   60
batch:      31340 | loss: 5.15608 | failed:   60
batch:      31350 | loss: 5.23701 | failed:   60
batch:      31360 | loss: 5.25900 | failed:   60
batch:      31370 | loss: 5.20401 | failed:   60
batch:      31380 | loss: 5.20772 | failed:   60
batch:      31390 | loss: 4.99828 | failed:   60
batch:      31400 | loss: 4.97480 | failed:   60
batch:      31410 | loss: 5.15632 | failed:   60
batch:      31420 | loss: 5.06788 | failed:   60
batch:      31430 | loss: 5.39804 | failed:   60
batch:      31440 | loss: 5.11376 | failed:   60
batch:      31450 | loss: 5.10415 | failed:   60
batch:      31460 | loss: 5.16160 | failed:   60
batch:      31470 | loss: 5.23311 | failed:   60
batch:      31480 | loss: 5.19725 | failed:   60
batch:      31490 | loss: 5.03999 | failed:   60
batch:      31500 | loss: 5.13204 | failed:   60
batch:      31510 | loss: 5.00190 | failed:   60
batch:      31520 | loss: 5.26802 | failed:   60
batch:      31530 | loss: 5.11585 | failed:   60
batch:      31540 | loss: 5.21366 | failed:   60
batch:      31550 | loss: 4.95242 | failed:   60
batch:      31560 | loss: 5.13308 | failed:   60
batch:      31570 | loss: 5.29602 | failed:   60
batch:      31580 | loss: 5.17549 | failed:   60
batch:      31590 | loss: 5.11868 | failed:   60
batch:      31600 | loss: 5.19722 | failed:   60
batch:      31610 | loss: 5.32272 | failed:   60
batch:      31620 | loss: 5.24934 | failed:   60
batch:      31630 | loss: 5.19369 | failed:   60
batch:      31640 | loss: 5.19800 | failed:   60
batch:      31650 | loss: 5.16437 | failed:   60
batch:      31660 | loss: 5.12282 | failed:   60
batch:      31670 | loss: 5.04934 | failed:   60
batch:      31680 | loss: 5.10319 | failed:   60
batch:      31690 | loss: 5.18977 | failed:   60
batch:      31700 | loss: 4.98499 | failed:   60
batch:      31710 | loss: 5.19206 | failed:   60
batch:      31720 | loss: 5.18266 | failed:   60
batch:      31730 | loss: 5.11344 | failed:   60
batch:      31740 | loss: 5.13991 | failed:   60
batch:      31750 | loss: 5.09591 | failed:   60
batch:      31760 | loss: 5.12965 | failed:   60
batch:      31770 | loss: 5.09751 | failed:   60
batch:      31780 | loss: 5.24236 | failed:   60
batch:      31790 | loss: 5.13425 | failed:   60
batch:      31800 | loss: 5.22640 | failed:   60
batch:      31810 | loss: 5.25094 | failed:   60
batch:      31820 | loss: 5.22060 | failed:   60
batch:      31830 | loss: 4.67629 | failed:   60
batch:      31840 | loss: 5.18547 | failed:   60
batch:      31850 | loss: 4.97240 | failed:   60
batch:      31860 | loss: 5.19437 | failed:   60
batch:      31870 | loss: 5.17899 | failed:   60
batch:      31880 | loss: 5.13645 | failed:   60
batch:      31890 | loss: 5.03511 | failed:   60
batch:      31900 | loss: 5.17078 | failed:   60
batch:      31910 | loss: 5.27918 | failed:   60
batch:      31920 | loss: 5.19334 | failed:   60
batch:      31930 | loss: 5.22806 | failed:   60
batch:      31940 | loss: 5.20776 | failed:   60
batch:      31950 | loss: 5.21107 | failed:   60
batch:      31960 | loss: 5.01782 | failed:   60
batch:      31970 | loss: 4.89973 | failed:   60
batch:      31980 | loss: 5.19999 | failed:   60
batch:      31990 | loss: 5.22858 | failed:   60
batch:      32000 | loss: 5.17575 | failed:   60
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      32010 | loss: 5.04965 | failed:   60
batch:      32020 | loss: 5.17607 | failed:   60
batch:      32030 | loss: 5.16419 | failed:   60
batch:      32040 | loss: 5.06477 | failed:   60
batch:      32050 | loss: 5.07920 | failed:   60
batch:      32060 | loss: 5.09442 | failed:   60
batch:      32070 | loss: 5.21458 | failed:   60
batch:      32080 | loss: 5.16133 | failed:   60
batch:      32090 | loss: 5.08586 | failed:   60
batch:      32100 | loss: 5.10890 | failed:   60
batch:      32110 | loss: 5.15236 | failed:   60
batch:      32120 | loss: 5.11715 | failed:   60
batch:      32130 | loss: 5.11638 | failed:   60
batch:      32140 | loss: 5.16438 | failed:   60
batch:      32150 | loss: 5.19237 | failed:   60
batch:      32160 | loss: 5.14878 | failed:   60
batch:      32170 | loss: 5.06309 | failed:   60
batch:      32180 | loss: 5.13385 | failed:   60
batch:      32190 | loss: 5.32867 | failed:   60
batch:      32200 | loss: 5.25260 | failed:   60
batch:      32210 | loss: 5.19794 | failed:   60
batch:      32220 | loss: 5.13823 | failed:   60
batch:      32230 | loss: 5.18262 | failed:   60
batch:      32240 | loss: 5.11239 | failed:   60
batch:      32250 | loss: 5.16237 | failed:   60
batch:      32260 | loss: 5.20392 | failed:   60
batch:      32270 | loss: 5.20807 | failed:   60
batch:      32280 | loss: 4.83353 | failed:   60
batch:      32290 | loss: 5.21877 | failed:   60
batch:      32300 | loss: 5.18903 | failed:   60
batch:      32310 | loss: 5.15067 | failed:   60
batch:      32320 | loss: 5.17564 | failed:   60
batch:      32330 | loss: 5.12169 | failed:   60
batch:      32340 | loss: 4.98467 | failed:   60
batch:      32350 | loss: 4.93303 | failed:   60
batch:      32360 | loss: 4.85736 | failed:   60
batch:      32370 | loss: 4.70347 | failed:   60
batch:      32380 | loss: 4.70138 | failed:   60
batch:      32390 | loss: 4.40771 | failed:   60
batch:      32400 | loss: 4.03156 | failed:   60
batch:      32410 | loss: 4.17700 | failed:   60
batch:      32420 | loss: 4.41734 | failed:   60
batch:      32430 | loss: 5.06338 | failed:   60
batch:      32440 | loss: 5.09572 | failed:   60
batch:      32450 | loss: 5.15705 | failed:   60
batch:      32460 | loss: 5.30885 | failed:   60
batch:      32470 | loss: 5.21032 | failed:   60
batch:      32480 | loss: 5.31007 | failed:   60
batch:      32490 | loss: 5.35512 | failed:   60
batch:      32500 | loss: 5.23111 | failed:   60
batch:      32510 | loss: 5.05314 | failed:   60
batch:      32520 | loss: 5.23434 | failed:   60
batch:      32530 | loss: 5.29036 | failed:   60
batch:      32540 | loss: 5.21715 | failed:   60
batch:      32550 | loss: 5.20937 | failed:   60
batch:      32560 | loss: 5.13146 | failed:   60
batch:      32570 | loss: 5.00546 | failed:   60
batch:      32580 | loss: 5.10062 | failed:   60
batch:      32590 | loss: 5.00094 | failed:   60
batch:      32600 | loss: 5.10742 | failed:   60
batch:      32610 | loss: 5.12893 | failed:   60
batch:      32620 | loss: 5.12357 | failed:   60
batch:      32630 | loss: 5.11280 | failed:   60
batch:      32640 | loss: 5.16125 | failed:   60
batch:      32650 | loss: 5.04301 | failed:   60
batch:      32660 | loss: 5.11340 | failed:   60
batch:      32670 | loss: 5.03051 | failed:   60
batch:      32680 | loss: 4.93574 | failed:   60
batch:      32690 | loss: 4.88505 | failed:   60
batch:      32700 | loss: 5.19304 | failed:   60
batch:      32710 | loss: 5.16139 | failed:   60
batch:      32720 | loss: 5.12444 | failed:   60
batch:      32730 | loss: 5.22931 | failed:   60
batch:      32740 | loss: 5.22044 | failed:   60
batch:      32750 | loss: 5.31315 | failed:   60
batch:      32760 | loss: 5.27287 | failed:   60
batch:      32770 | loss: 5.29776 | failed:   60
batch:      32780 | loss: 5.24325 | failed:   60
batch:      32790 | loss: 5.24390 | failed:   60
batch:      32800 | loss: 5.27574 | failed:   60
batch:      32810 | loss: 5.20175 | failed:   60
batch:      32820 | loss: 5.07420 | failed:   60
batch:      32830 | loss: 5.12290 | failed:   60
batch:      32840 | loss: 5.16079 | failed:   60
batch:      32850 | loss: 5.06151 | failed:   60
batch:      32860 | loss: 5.28621 | failed:   60
batch:      32870 | loss: 5.23305 | failed:   60
batch:      32880 | loss: 5.21944 | failed:   60
batch:      32890 | loss: 5.32761 | failed:   60
batch:      32900 | loss: 5.18718 | failed:   60
batch:      32910 | loss: 5.18210 | failed:   60
batch:      32920 | loss: 5.28752 | failed:   60
batch:      32930 | loss: 5.14063 | failed:   60
batch:      32940 | loss: 5.15083 | failed:   60
batch:      32950 | loss: 5.07277 | failed:   60
batch:      32960 | loss: 5.09683 | failed:   60
batch:      32970 | loss: 5.17145 | failed:   60
batch:      32980 | loss: 5.11406 | failed:   60
batch:      32990 | loss: 5.22390 | failed:   60
batch:      33000 | loss: 5.10009 | failed:   60
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      33010 | loss: 4.92436 | failed:   60
batch:      33020 | loss: 5.22885 | failed:   60
batch:      33030 | loss: 5.24824 | failed:   60
batch:      33040 | loss: 5.18081 | failed:   60
batch:      33050 | loss: 5.17069 | failed:   60
batch:      33060 | loss: 5.10167 | failed:   60
batch:      33070 | loss: 5.16289 | failed:   60
batch:      33080 | loss: 4.87880 | failed:   60
batch:      33090 | loss: 4.78152 | failed:   60
batch:      33100 | loss: 5.04199 | failed:   60
batch:      33110 | loss: 5.27062 | failed:   60
batch:      33120 | loss: 5.30508 | failed:   60
batch:      33130 | loss: 5.04373 | failed:   60
batch:      33140 | loss: 5.09124 | failed:   60
batch:      33150 | loss: 5.22967 | failed:   60
batch:      33160 | loss: 5.19970 | failed:   60
batch:      33170 | loss: 5.20888 | failed:   60
batch:      33180 | loss: 5.26441 | failed:   60
batch:      33190 | loss: 5.19954 | failed:   60
batch:      33200 | loss: 5.24414 | failed:   60
batch:      33210 | loss: 5.15161 | failed:   60
batch:      33220 | loss: 5.08550 | failed:   60
batch:      33230 | loss: 5.20231 | failed:   60
batch:      33240 | loss: 5.23531 | failed:   60
batch:      33250 | loss: 5.19516 | failed:   60
batch:      33260 | loss: 5.19142 | failed:   60
batch:      33270 | loss: 5.19804 | failed:   60
batch:      33280 | loss: 5.23377 | failed:   60
batch:      33290 | loss: 5.21562 | failed:   60
batch:      33300 | loss: 4.80358 | failed:   60
batch:      33310 | loss: 5.20626 | failed:   60
batch:      33320 | loss: 5.20880 | failed:   60
batch:      33330 | loss: 5.18328 | failed:   60
batch:      33340 | loss: 5.11219 | failed:   60
batch:      33350 | loss: 5.13632 | failed:   60
batch:      33360 | loss: 5.11375 | failed:   60
batch:      33370 | loss: 5.06454 | failed:   60
batch:      33380 | loss: 5.21641 | failed:   60
batch:      33390 | loss: 5.14574 | failed:   60
batch:      33400 | loss: 5.19151 | failed:   60
batch:      33410 | loss: 5.16899 | failed:   60
batch:      33420 | loss: 5.14338 | failed:   60
batch:      33430 | loss: 5.09765 | failed:   60
batch:      33440 | loss: 5.25672 | failed:   60
batch:      33450 | loss: 5.21494 | failed:   60
batch:      33460 | loss: 5.20866 | failed:   60
batch:      33470 | loss: 5.19077 | failed:   60
batch:      33480 | loss: 5.18536 | failed:   60
batch:      33490 | loss: 5.15808 | failed:   60
batch:      33500 | loss: 5.24653 | failed:   60
batch:      33510 | loss: 5.21467 | failed:   60
batch:      33520 | loss: 5.19416 | failed:   60
batch:      33530 | loss: 5.03862 | failed:   60
batch:      33540 | loss: 5.19059 | failed:   60
batch:      33550 | loss: 5.23296 | failed:   60
batch:      33560 | loss: 5.18150 | failed:   60
batch:      33570 | loss: 5.27595 | failed:   60
batch:      33580 | loss: 5.17941 | failed:   60
batch:      33590 | loss: 5.15699 | failed:   60
batch:      33600 | loss: 5.07674 | failed:   60
batch:      33610 | loss: 5.16596 | failed:   60
batch:      33620 | loss: 5.15392 | failed:   60
batch:      33630 | loss: 5.21445 | failed:   60
batch:      33640 | loss: 4.85742 | failed:   60
batch:      33650 | loss: 5.09365 | failed:   60
batch:      33660 | loss: 5.26624 | failed:   60
batch:      33670 | loss: 5.10489 | failed:   60
batch:      33680 | loss: 5.29068 | failed:   60
batch:      33690 | loss: 5.21089 | failed:   60
batch:      33700 | loss: 5.14905 | failed:   60
batch:      33710 | loss: 5.07152 | failed:   60
batch:      33720 | loss: 5.10305 | failed:   60
batch:      33730 | loss: 4.96406 | failed:   60
batch:      33740 | loss: 5.04994 | failed:   60
batch:      33750 | loss: 4.92279 | failed:   60
batch:      33760 | loss: 5.09658 | failed:   60
batch:      33770 | loss: 5.03855 | failed:   60
batch:      33780 | loss: 4.96618 | failed:   60
batch:      33790 | loss: 5.19645 | failed:   60
batch:      33800 | loss: 5.18048 | failed:   60
batch:      33810 | loss: 5.14140 | failed:   60
batch:      33820 | loss: 5.15822 | failed:   60
batch:      33830 | loss: 5.35124 | failed:   60
batch:      33840 | loss: 5.30911 | failed:   60
batch:      33850 | loss: 5.15913 | failed:   60
batch:      33860 | loss: 5.13150 | failed:   60
batch:      33870 | loss: 5.16641 | failed:   60
batch:      33880 | loss: 4.89915 | failed:   60
batch:      33890 | loss: 5.02220 | failed:   60
batch:      33900 | loss: 5.09401 | failed:   60
batch:      33910 | loss: 5.04226 | failed:   60
batch:      33920 | loss: 5.03238 | failed:   60
batch:      33930 | loss: 5.07994 | failed:   60
batch:      33940 | loss: 5.24283 | failed:   60
batch:      33950 | loss: 5.21540 | failed:   60
batch:      33960 | loss: 5.27994 | failed:   60
batch:      33970 | loss: 5.25780 | failed:   60
batch:      33980 | loss: 5.19848 | failed:   60
batch:      33990 | loss: 5.00310 | failed:   60
batch:      34000 | loss: 5.18374 | failed:   60
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      34010 | loss: 5.28346 | failed:   60
batch:      34020 | loss: 5.26357 | failed:   60
batch:      34030 | loss: 5.19380 | failed:   60
batch:      34040 | loss: 5.20830 | failed:   60
batch:      34050 | loss: 5.20732 | failed:   60
batch:      34060 | loss: 5.20419 | failed:   60
batch:      34070 | loss: 5.07512 | failed:   60
batch:      34080 | loss: 5.13950 | failed:   60
batch:      34090 | loss: 4.93182 | failed:   60
batch:      34100 | loss: 5.19003 | failed:   60
batch:      34110 | loss: 5.19088 | failed:   60
batch:      34120 | loss: 5.07039 | failed:   60
batch:      34130 | loss: 4.63620 | failed:   60
batch:      34140 | loss: 4.03935 | failed:   60
batch:      34150 | loss: 5.39743 | failed:   60
batch:      34160 | loss: 5.16699 | failed:   60
batch:      34170 | loss: 5.12499 | failed:   60
batch:      34180 | loss: 5.23873 | failed:   60
batch:      34190 | loss: 5.16332 | failed:   60
batch:      34200 | loss: 5.20911 | failed:   60
batch:      34210 | loss: 5.11659 | failed:   60
batch:      34220 | loss: 5.27029 | failed:   60
batch:      34230 | loss: 5.15572 | failed:   60
batch:      34240 | loss: 5.25426 | failed:   60
batch:      34250 | loss: 5.12321 | failed:   60
batch:      34260 | loss: 5.20425 | failed:   60
batch:      34270 | loss: 5.09759 | failed:   60
batch:      34280 | loss: 5.26776 | failed:   60
batch:      34290 | loss: 5.03467 | failed:   60
batch:      34300 | loss: 5.09232 | failed:   60
batch:      34310 | loss: 5.18244 | failed:   60
batch:      34320 | loss: 5.21357 | failed:   60
batch:      34330 | loss: 5.21563 | failed:   60
batch:      34340 | loss: 5.20265 | failed:   60
batch:      34350 | loss: 5.31472 | failed:   60
batch:      34360 | loss: 5.26034 | failed:   60
batch:      34370 | loss: 5.25922 | failed:   60
batch:      34380 | loss: 5.10197 | failed:   60
batch:      34390 | loss: 5.17809 | failed:   60
batch:      34400 | loss: 5.17711 | failed:   60
batch:      34410 | loss: 5.17665 | failed:   60
batch:      34420 | loss: 5.07594 | failed:   60
batch:      34430 | loss: 5.09006 | failed:   60
batch:      34440 | loss: 5.18913 | failed:   60
batch:      34450 | loss: 5.17582 | failed:   60
batch:      34460 | loss: 5.23545 | failed:   60
batch:      34470 | loss: 5.17256 | failed:   60
batch:      34480 | loss: 5.27110 | failed:   60
batch:      34490 | loss: 5.19327 | failed:   60
batch:      34500 | loss: 5.12735 | failed:   60
batch:      34510 | loss: 5.18582 | failed:   60
batch:      34520 | loss: 4.82167 | failed:   60
batch:      34530 | loss: 5.19127 | failed:   60
batch:      34540 | loss: 5.14093 | failed:   60
batch:      34550 | loss: 5.22828 | failed:   60
batch:      34560 | loss: 5.10306 | failed:   60
batch:      34570 | loss: 5.17536 | failed:   60
batch:      34580 | loss: 5.24417 | failed:   60
batch:      34590 | loss: 5.28439 | failed:   60
batch:      34600 | loss: 5.05772 | failed:   60
batch:      34610 | loss: 5.13548 | failed:   60
batch:      34620 | loss: 5.21156 | failed:   60
batch:      34630 | loss: 5.15928 | failed:   60
batch:      34640 | loss: 5.14656 | failed:   60
batch:      34650 | loss: 5.10670 | failed:   60
batch:      34660 | loss: 5.18810 | failed:   60
batch:      34670 | loss: 5.26640 | failed:   60
batch:      34680 | loss: 5.14574 | failed:   60
batch:      34690 | loss: 5.20051 | failed:   60
batch:      34700 | loss: 5.01749 | failed:   60
batch:      34710 | loss: 5.13166 | failed:   60
batch:      34720 | loss: 5.06470 | failed:   60
batch:      34730 | loss: 5.27859 | failed:   60
batch:      34740 | loss: 5.00047 | failed:   60
batch:      34750 | loss: 5.03522 | failed:   60
batch:      34760 | loss: 5.17411 | failed:   60
batch:      34770 | loss: 5.15670 | failed:   60
batch:      34780 | loss: 5.26205 | failed:   60
batch:      34790 | loss: 4.97202 | failed:   60
batch:      34800 | loss: 5.20125 | failed:   60
batch:      34810 | loss: 5.04252 | failed:   60
batch:      34820 | loss: 5.19242 | failed:   60
batch:      34830 | loss: 5.01294 | failed:   60
batch:      34840 | loss: 5.22553 | failed:   60
batch:      34850 | loss: 5.24800 | failed:   60
batch:      34860 | loss: 5.22755 | failed:   60
batch:      34870 | loss: 5.10970 | failed:   60
batch:      34880 | loss: 5.20820 | failed:   60
batch:      34890 | loss: 5.14516 | failed:   60
batch:      34900 | loss: 5.27395 | failed:   60
batch:      34910 | loss: 4.98329 | failed:   60
batch:      34920 | loss: 5.14397 | failed:   60
batch:      34930 | loss: 5.12433 | failed:   60
batch:      34940 | loss: 5.13229 | failed:   60
batch:      34950 | loss: 5.03319 | failed:   60
batch:      34960 | loss: 5.08082 | failed:   60
batch:      34970 | loss: 5.11624 | failed:   60
batch:      34980 | loss: 5.16649 | failed:   60
batch:      34990 | loss: 5.25101 | failed:   60
batch:      35000 | loss: 5.26708 | failed:   60
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      35010 | loss: 5.21857 | failed:   60
batch:      35020 | loss: 5.18987 | failed:   60
batch:      35030 | loss: 5.06305 | failed:   60
batch:      35040 | loss: 5.17232 | failed:   60
batch:      35050 | loss: 5.22397 | failed:   60
batch:      35060 | loss: 5.04864 | failed:   60
batch:      35070 | loss: 5.24550 | failed:   60
batch:      35080 | loss: 5.19718 | failed:   60
batch:      35090 | loss: 5.17200 | failed:   60
batch:      35100 | loss: 5.12167 | failed:   60
batch:      35110 | loss: 5.12728 | failed:   60
batch:      35120 | loss: 5.17574 | failed:   60
batch:      35130 | loss: 5.17725 | failed:   60
batch:      35140 | loss: 5.22909 | failed:   60
batch:      35150 | loss: 5.01811 | failed:   60
batch:      35160 | loss: 5.16704 | failed:   60
batch:      35170 | loss: 5.20664 | failed:   60
batch:      35180 | loss: 5.06757 | failed:   60
batch:      35190 | loss: 5.14790 | failed:   60
batch:      35200 | loss: 5.13222 | failed:   60
batch:      35210 | loss: 5.10408 | failed:   60
batch:      35220 | loss: 5.15996 | failed:   60
batch:      35230 | loss: 5.19375 | failed:   60
batch:      35240 | loss: 5.11796 | failed:   60
batch:      35250 | loss: 5.11044 | failed:   60
batch:      35260 | loss: 5.18554 | failed:   60
batch:      35270 | loss: 5.23045 | failed:   60
batch:      35280 | loss: 5.17227 | failed:   60
batch:      35290 | loss: 5.13063 | failed:   60
batch:      35300 | loss: 5.11347 | failed:   60
batch:      35310 | loss: 5.00541 | failed:   60
batch:      35320 | loss: 5.26355 | failed:   60
batch:      35330 | loss: 5.14672 | failed:   60
batch:      35340 | loss: 5.08218 | failed:   60
batch:      35350 | loss: 5.13734 | failed:   60
batch:      35360 | loss: 5.18396 | failed:   60
batch:      35370 | loss: 5.17523 | failed:   60
batch:      35380 | loss: 5.20628 | failed:   60
batch:      35390 | loss: 5.13943 | failed:   60
batch:      35400 | loss: 5.21189 | failed:   60
batch:      35410 | loss: 5.17466 | failed:   60
batch:      35420 | loss: 5.25273 | failed:   60
batch:      35430 | loss: 5.22721 | failed:   60
batch:      35440 | loss: 5.22142 | failed:   60
batch:      35450 | loss: 5.14725 | failed:   60
batch:      35460 | loss: 5.04936 | failed:   60
batch:      35470 | loss: 5.25417 | failed:   60
batch:      35480 | loss: 5.15334 | failed:   60
batch:      35490 | loss: 5.14208 | failed:   60
batch:      35500 | loss: 4.87727 | failed:   60
batch:      35510 | loss: 5.20853 | failed:   60
batch:      35520 | loss: 5.25041 | failed:   60
batch:      35530 | loss: 5.04130 | failed:   60
batch:      35540 | loss: 5.12420 | failed:   60
batch:      35550 | loss: 5.14581 | failed:   60
batch:      35560 | loss: 4.88032 | failed:   60
batch:      35570 | loss: 5.00703 | failed:   60
batch:      35580 | loss: 5.19455 | failed:   60
batch:      35590 | loss: 5.14877 | failed:   60
batch:      35600 | loss: 5.19241 | failed:   60
batch:      35610 | loss: 5.23596 | failed:   60
batch:      35620 | loss: 5.21586 | failed:   60
batch:      35630 | loss: 5.08748 | failed:   60
batch:      35640 | loss: 5.18206 | failed:   60
batch:      35650 | loss: 5.08488 | failed:   60
batch:      35660 | loss: 5.15093 | failed:   60
batch:      35670 | loss: 5.19056 | failed:   60
batch:      35680 | loss: 5.27446 | failed:   60
batch:      35690 | loss: 5.20204 | failed:   60
batch:      35700 | loss: 5.15064 | failed:   60
batch:      35710 | loss: 5.08184 | failed:   60
batch:      35720 | loss: 5.19349 | failed:   60
batch:      35730 | loss: 5.21685 | failed:   60
batch:      35740 | loss: 5.23394 | failed:   60
batch:      35750 | loss: 4.99762 | failed:   60
batch:      35760 | loss: 5.16978 | failed:   60
batch:      35770 | loss: 5.16179 | failed:   60
batch:      35780 | loss: 5.10721 | failed:   60
batch:      35790 | loss: 5.16988 | failed:   60
batch:      35800 | loss: 5.04634 | failed:   60
batch:      35810 | loss: 5.14242 | failed:   60
batch:      35820 | loss: 5.09717 | failed:   60
batch:      35830 | loss: 5.17372 | failed:   60
batch:      35840 | loss: 5.24318 | failed:   60
batch:      35850 | loss: 5.21263 | failed:   60
batch:      35860 | loss: 5.21402 | failed:   60
batch:      35870 | loss: 5.22695 | failed:   60
batch:      35880 | loss: 5.18087 | failed:   60
batch:      35890 | loss: 5.22304 | failed:   60
batch:      35900 | loss: 5.19203 | failed:   60
batch:      35910 | loss: 5.13364 | failed:   60
batch:      35920 | loss: 5.18527 | failed:   60
batch:      35930 | loss: 5.20068 | failed:   60
batch:      35940 | loss: 5.16669 | failed:   60
batch:      35950 | loss: 5.19072 | failed:   60
batch:      35960 | loss: 5.19287 | failed:   60
batch:      35970 | loss: 5.15400 | failed:   60
batch:      35980 | loss: 5.20889 | failed:   60
batch:      35990 | loss: 5.18139 | failed:   60
batch:      36000 | loss: 5.13998 | failed:   60
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      36010 | loss: 5.18402 | failed:   60
batch:      36020 | loss: 5.17492 | failed:   60
batch:      36030 | loss: 5.15642 | failed:   60
batch:      36040 | loss: 5.25474 | failed:   60
batch:      36050 | loss: 5.13134 | failed:   60
batch:      36060 | loss: 5.07763 | failed:   60
batch:      36070 | loss: 5.06867 | failed:   60
batch:      36080 | loss: 5.13050 | failed:   60
batch:      36090 | loss: 5.10177 | failed:   60
batch:      36100 | loss: 5.17747 | failed:   60
batch:      36110 | loss: 5.11792 | failed:   60
batch:      36120 | loss: 5.28776 | failed:   60
batch:      36130 | loss: 5.12614 | failed:   60
batch:      36140 | loss: 5.19701 | failed:   60
batch:      36150 | loss: 4.93859 | failed:   60
batch:      36160 | loss: 5.25744 | failed:   60
batch:      36170 | loss: 5.23380 | failed:   60
batch:      36180 | loss: 5.23316 | failed:   60
batch:      36190 | loss: 5.29028 | failed:   60
batch:      36200 | loss: 5.20607 | failed:   60
batch:      36210 | loss: 5.21458 | failed:   60
batch:      36220 | loss: 5.24324 | failed:   60
batch:      36230 | loss: 5.08685 | failed:   60
batch:      36240 | loss: 5.26491 | failed:   60
batch:      36250 | loss: 5.26331 | failed:   60
batch:      36260 | loss: 5.22886 | failed:   60
batch:      36270 | loss: 5.18827 | failed:   60
batch:      36280 | loss: 5.21996 | failed:   60
batch:      36290 | loss: 5.24874 | failed:   60
batch:      36300 | loss: 5.07343 | failed:   60
batch:      36310 | loss: 5.08821 | failed:   60
batch:      36320 | loss: 5.09260 | failed:   60
batch:      36330 | loss: 5.09872 | failed:   60
batch:      36340 | loss: 5.25092 | failed:   60
batch:      36350 | loss: 5.15613 | failed:   60
batch:      36360 | loss: 5.08067 | failed:   60
batch:      36370 | loss: 5.13618 | failed:   60
batch:      36380 | loss: 5.08659 | failed:   60
batch:      36390 | loss: 5.11881 | failed:   60
batch:      36400 | loss: 5.15454 | failed:   60
batch:      36410 | loss: 5.19254 | failed:   60
batch:      36420 | loss: 5.02474 | failed:   60
batch:      36430 | loss: 5.02564 | failed:   60
batch:      36440 | loss: 5.34186 | failed:   60
batch:      36450 | loss: 5.25113 | failed:   60
batch:      36460 | loss: 5.19561 | failed:   60
batch:      36470 | loss: 5.23688 | failed:   60
batch:      36480 | loss: 5.11145 | failed:   60
batch:      36490 | loss: 5.19862 | failed:   60
batch:      36500 | loss: 5.11687 | failed:   60
batch:      36510 | loss: 5.10768 | failed:   60
batch:      36520 | loss: 5.28674 | failed:   60
batch:      36530 | loss: 5.11146 | failed:   60
batch:      36540 | loss: 5.02934 | failed:   60
batch:      36550 | loss: 5.23886 | failed:   60
batch:      36560 | loss: 5.04366 | failed:   60
batch:      36570 | loss: 5.17341 | failed:   60
batch:      36580 | loss: 5.06155 | failed:   60
batch:      36590 | loss: 5.24031 | failed:   60
batch:      36600 | loss: 5.25678 | failed:   60
batch:      36610 | loss: 5.27184 | failed:   60
batch:      36620 | loss: 5.18723 | failed:   60
batch:      36630 | loss: 5.17860 | failed:   60
batch:      36640 | loss: 5.29356 | failed:   60
batch:      36650 | loss: 5.16086 | failed:   60
batch:      36660 | loss: 5.14301 | failed:   60
batch:      36670 | loss: 5.14675 | failed:   60
batch:      36680 | loss: 5.17320 | failed:   60
batch:      36690 | loss: 5.14437 | failed:   60
batch:      36700 | loss: 5.23082 | failed:   60
batch:      36710 | loss: 5.15890 | failed:   60
batch:      36720 | loss: 5.20433 | failed:   60
batch:      36730 | loss: 5.15056 | failed:   60
batch:      36740 | loss: 5.16851 | failed:   60
batch:      36750 | loss: 5.21753 | failed:   60
batch:      36760 | loss: 5.11611 | failed:   60
batch:      36770 | loss: 5.15555 | failed:   60
batch:      36780 | loss: 5.13758 | failed:   60
batch:      36790 | loss: 5.18060 | failed:   60
batch:      36800 | loss: 5.18630 | failed:   60
batch:      36810 | loss: 5.27370 | failed:   60
batch:      36820 | loss: 5.18790 | failed:   60
batch:      36830 | loss: 5.13455 | failed:   60
batch:      36840 | loss: 5.09694 | failed:   60
batch:      36850 | loss: 5.15859 | failed:   60
batch:      36860 | loss: 5.15606 | failed:   60
batch:      36870 | loss: 5.08231 | failed:   60
batch:      36880 | loss: 5.08292 | failed:   60
batch:      36890 | loss: 5.14566 | failed:   60
batch:      36900 | loss: 4.87554 | failed:   60
batch:      36910 | loss: 5.13086 | failed:   60
batch:      36920 | loss: 4.99283 | failed:   60
batch:      36930 | loss: 5.00087 | failed:   60
batch:      36940 | loss: 5.28150 | failed:   60
batch:      36950 | loss: 5.05541 | failed:   60
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      36960 | loss: 5.19539 | failed:   60
batch:      36970 | loss: 4.90663 | failed:   60
batch:      36980 | loss: 5.23555 | failed:   63
batch:      36990 | loss: 5.21569 | failed:   63
batch:      37000 | loss: 5.15143 | failed:   63
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      37010 | loss: 5.15690 | failed:   63
batch:      37020 | loss: 5.06501 | failed:   63
batch:      37030 | loss: 5.08593 | failed:   63
batch:      37040 | loss: 5.08128 | failed:   63
batch:      37050 | loss: 5.05823 | failed:   63
batch:      37060 | loss: 5.27459 | failed:   63
batch:      37070 | loss: 5.24628 | failed:   63
batch:      37080 | loss: 5.15118 | failed:   63
batch:      37090 | loss: 5.06179 | failed:   63
batch:      37100 | loss: 4.98576 | failed:   63
batch:      37110 | loss: 5.07674 | failed:   63
batch:      37120 | loss: 5.15884 | failed:   63
batch:      37130 | loss: 5.15857 | failed:   63
batch:      37140 | loss: 5.20611 | failed:   63
batch:      37150 | loss: 5.18397 | failed:   63
batch:      37160 | loss: 5.31743 | failed:   63
batch:      37170 | loss: 5.15261 | failed:   63
batch:      37180 | loss: 5.26448 | failed:   63
batch:      37190 | loss: 5.24082 | failed:   63
batch:      37200 | loss: 5.21608 | failed:   63
batch:      37210 | loss: 5.20411 | failed:   63
batch:      37220 | loss: 5.17358 | failed:   63
batch:      37230 | loss: 5.15289 | failed:   63
batch:      37240 | loss: 5.16373 | failed:   63
batch:      37250 | loss: 5.25136 | failed:   63
batch:      37260 | loss: 5.24720 | failed:   63
batch:      37270 | loss: 5.10057 | failed:   63
batch:      37280 | loss: 5.08881 | failed:   63
batch:      37290 | loss: 5.10954 | failed:   63
batch:      37300 | loss: 5.20542 | failed:   63
batch:      37310 | loss: 5.21380 | failed:   63
batch:      37320 | loss: 5.16742 | failed:   63
batch:      37330 | loss: 5.19297 | failed:   63
batch:      37340 | loss: 5.21534 | failed:   63
batch:      37350 | loss: 5.18046 | failed:   63
batch:      37360 | loss: 5.09057 | failed:   63
batch:      37370 | loss: 5.07580 | failed:   63
batch:      37380 | loss: 5.18800 | failed:   63
batch:      37390 | loss: 4.97401 | failed:   63
batch:      37400 | loss: 5.16644 | failed:   63
batch:      37410 | loss: 5.13596 | failed:   63
batch:      37420 | loss: 5.14372 | failed:   63
batch:      37430 | loss: 5.02284 | failed:   63
batch:      37440 | loss: 5.22105 | failed:   63
batch:      37450 | loss: 5.14992 | failed:   63
batch:      37460 | loss: 5.28510 | failed:   63
batch:      37470 | loss: 5.18789 | failed:   63
batch:      37480 | loss: 5.03066 | failed:   63
batch:      37490 | loss: 5.15160 | failed:   63
batch:      37500 | loss: 5.17728 | failed:   63
batch:      37510 | loss: 5.17299 | failed:   63
batch:      37520 | loss: 5.31555 | failed:   63
batch:      37530 | loss: 5.15090 | failed:   63
batch:      37540 | loss: 5.23510 | failed:   63
batch:      37550 | loss: 5.14405 | failed:   63
batch:      37560 | loss: 5.14515 | failed:   63
batch:      37570 | loss: 5.13862 | failed:   63
batch:      37580 | loss: 5.24196 | failed:   63
batch:      37590 | loss: 5.15708 | failed:   63
batch:      37600 | loss: 5.18451 | failed:   63
batch:      37610 | loss: 5.27927 | failed:   63
batch:      37620 | loss: 5.19239 | failed:   63
batch:      37630 | loss: 5.10518 | failed:   63
batch:      37640 | loss: 5.19590 | failed:   63
batch:      37650 | loss: 5.01407 | failed:   63
batch:      37660 | loss: 5.23322 | failed:   63
batch:      37670 | loss: 5.09592 | failed:   63
batch:      37680 | loss: 5.21061 | failed:   63
batch:      37690 | loss: 5.10959 | failed:   63
batch:      37700 | loss: 5.16257 | failed:   63
batch:      37710 | loss: 5.19024 | failed:   63
batch:      37720 | loss: 5.13314 | failed:   63
batch:      37730 | loss: 5.17229 | failed:   63
batch:      37740 | loss: 5.20261 | failed:   63
batch:      37750 | loss: 5.19225 | failed:   63
batch:      37760 | loss: 5.12178 | failed:   63
batch:      37770 | loss: 5.14645 | failed:   63
batch:      37780 | loss: 5.21638 | failed:   63
batch:      37790 | loss: 5.21079 | failed:   63
batch:      37800 | loss: 4.96755 | failed:   63
batch:      37810 | loss: 5.03756 | failed:   63
batch:      37820 | loss: 5.12393 | failed:   63
batch:      37830 | loss: 5.09055 | failed:   63
batch:      37840 | loss: 5.01898 | failed:   63
batch:      37850 | loss: 5.29560 | failed:   63
batch:      37860 | loss: 4.80396 | failed:   63
batch:      37870 | loss: 5.00022 | failed:   63
batch:      37880 | loss: 5.21785 | failed:   63
batch:      37890 | loss: 5.17313 | failed:   63
batch:      37900 | loss: 5.21215 | failed:   63
batch:      37910 | loss: 5.15891 | failed:   63
batch:      37920 | loss: 5.03110 | failed:   63
batch:      37930 | loss: 5.26827 | failed:   63
batch:      37940 | loss: 5.22983 | failed:   63
batch:      37950 | loss: 5.26140 | failed:   63
batch:      37960 | loss: 5.23296 | failed:   63
batch:      37970 | loss: 5.19627 | failed:   63
batch:      37980 | loss: 5.19290 | failed:   63
batch:      37990 | loss: 5.17575 | failed:   63
batch:      38000 | loss: 5.20705 | failed:   63
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      38010 | loss: 5.08235 | failed:   63
batch:      38020 | loss: 5.10546 | failed:   63
batch:      38030 | loss: 5.09660 | failed:   63
batch:      38040 | loss: 5.13966 | failed:   63
batch:      38050 | loss: 5.11450 | failed:   63
batch:      38060 | loss: 5.06512 | failed:   63
batch:      38070 | loss: 5.21660 | failed:   63
batch:      38080 | loss: 5.11903 | failed:   63
batch:      38090 | loss: 5.22248 | failed:   63
batch:      38100 | loss: 5.19150 | failed:   63
batch:      38110 | loss: 5.42129 | failed:   63
batch:      38120 | loss: 5.16721 | failed:   63
batch:      38130 | loss: 5.24419 | failed:   63
batch:      38140 | loss: 5.16130 | failed:   63
batch:      38150 | loss: 5.14166 | failed:   63
batch:      38160 | loss: 5.16984 | failed:   63
batch:      38170 | loss: 5.21775 | failed:   63
batch:      38180 | loss: 5.29348 | failed:   63
batch:      38190 | loss: 5.20431 | failed:   63
batch:      38200 | loss: 5.16121 | failed:   63
batch:      38210 | loss: 5.20946 | failed:   63
batch:      38220 | loss: 5.16177 | failed:   63
batch:      38230 | loss: 5.13191 | failed:   63
batch:      38240 | loss: 4.90018 | failed:   63
batch:      38250 | loss: 5.13472 | failed:   63
batch:      38260 | loss: 5.05146 | failed:   63
batch:      38270 | loss: 5.03587 | failed:   63
batch:      38280 | loss: 5.20941 | failed:   63
batch:      38290 | loss: 5.21140 | failed:   63
batch:      38300 | loss: 5.23486 | failed:   63
batch:      38310 | loss: 5.12508 | failed:   63
batch:      38320 | loss: 5.33020 | failed:   63
batch:      38330 | loss: 5.30777 | failed:   63
batch:      38340 | loss: 5.25153 | failed:   63
batch:      38350 | loss: 5.22213 | failed:   63
batch:      38360 | loss: 5.20519 | failed:   63
batch:      38370 | loss: 5.18309 | failed:   63
batch:      38380 | loss: 4.94850 | failed:   63
batch:      38390 | loss: 5.11480 | failed:   63
batch:      38400 | loss: 5.21105 | failed:   63
batch:      38410 | loss: 5.20590 | failed:   63
batch:      38420 | loss: 5.18540 | failed:   63
batch:      38430 | loss: 5.20652 | failed:   63
batch:      38440 | loss: 5.24499 | failed:   63
batch:      38450 | loss: 5.09805 | failed:   63
batch:      38460 | loss: 5.22082 | failed:   63
batch:      38470 | loss: 5.23059 | failed:   63
batch:      38480 | loss: 5.25196 | failed:   63
batch:      38490 | loss: 5.17625 | failed:   63
batch:      38500 | loss: 5.12739 | failed:   63
batch:      38510 | loss: 5.08471 | failed:   63
batch:      38520 | loss: 5.25230 | failed:   63
batch:      38530 | loss: 5.21300 | failed:   63
batch:      38540 | loss: 5.16822 | failed:   63
batch:      38550 | loss: 5.01627 | failed:   63
batch:      38560 | loss: 5.13875 | failed:   63
batch:      38570 | loss: 5.16360 | failed:   63
batch:      38580 | loss: 5.14624 | failed:   63
batch:      38590 | loss: 5.11732 | failed:   63
batch:      38600 | loss: 5.14349 | failed:   63
batch:      38610 | loss: 5.08100 | failed:   63
batch:      38620 | loss: 5.25596 | failed:   63
batch:      38630 | loss: 5.18776 | failed:   63
batch:      38640 | loss: 5.21854 | failed:   63
batch:      38650 | loss: 5.12747 | failed:   63
batch:      38660 | loss: 5.23821 | failed:   63
batch:      38670 | loss: 5.12374 | failed:   63
batch:      38680 | loss: 4.79599 | failed:   63
batch:      38690 | loss: 5.23131 | failed:   63
batch:      38700 | loss: 5.03439 | failed:   63
batch:      38710 | loss: 5.04278 | failed:   63
batch:      38720 | loss: 5.15662 | failed:   63
batch:      38730 | loss: 5.30368 | failed:   63
batch:      38740 | loss: 5.08120 | failed:   63
batch:      38750 | loss: 5.17782 | failed:   63
batch:      38760 | loss: 5.19898 | failed:   63
batch:      38770 | loss: 5.02992 | failed:   63
batch:      38780 | loss: 5.06542 | failed:   63
batch:      38790 | loss: 5.10149 | failed:   63
batch:      38800 | loss: 5.21860 | failed:   63
batch:      38810 | loss: 5.21677 | failed:   63
batch:      38820 | loss: 5.07651 | failed:   63
batch:      38830 | loss: 5.14008 | failed:   63
batch:      38840 | loss: 5.10041 | failed:   63
batch:      38850 | loss: 5.11336 | failed:   63
batch:      38860 | loss: 5.18780 | failed:   63
batch:      38870 | loss: 4.89592 | failed:   63
batch:      38880 | loss: 5.17977 | failed:   63
batch:      38890 | loss: 4.96734 | failed:   63
batch:      38900 | loss: 5.03873 | failed:   63
batch:      38910 | loss: 5.16114 | failed:   63
batch:      38920 | loss: 5.18671 | failed:   63
batch:      38930 | loss: 5.17617 | failed:   63
batch:      38940 | loss: 5.23648 | failed:   63
batch:      38950 | loss: 5.22530 | failed:   63
batch:      38960 | loss: 5.14405 | failed:   63
batch:      38970 | loss: 5.18327 | failed:   63
batch:      38980 | loss: 5.22462 | failed:   63
batch:      38990 | loss: 5.10016 | failed:   63
batch:      39000 | loss: 5.08272 | failed:   63
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      39010 | loss: 5.17835 | failed:   63
batch:      39020 | loss: 5.15368 | failed:   63
batch:      39030 | loss: 5.14435 | failed:   63
batch:      39040 | loss: 5.22227 | failed:   63
batch:      39050 | loss: 5.26151 | failed:   63
batch:      39060 | loss: 5.14718 | failed:   63
batch:      39070 | loss: 5.20228 | failed:   63
batch:      39080 | loss: 5.11789 | failed:   63
batch:      39090 | loss: 5.22296 | failed:   63
batch:      39100 | loss: 5.21979 | failed:   63
batch:      39110 | loss: 5.05956 | failed:   63
batch:      39120 | loss: 5.17103 | failed:   63
batch:      39130 | loss: 5.09911 | failed:   63
batch:      39140 | loss: 5.12016 | failed:   63
batch:      39150 | loss: 5.17421 | failed:   63
batch:      39160 | loss: 5.07035 | failed:   63
batch:      39170 | loss: 5.16028 | failed:   63
batch:      39180 | loss: 5.15010 | failed:   63
batch:      39190 | loss: 5.29395 | failed:   63
batch:      39200 | loss: 5.09726 | failed:   63
batch:      39210 | loss: 5.15435 | failed:   63
batch:      39220 | loss: 5.07082 | failed:   63
batch:      39230 | loss: 5.08452 | failed:   63
batch:      39240 | loss: 5.11171 | failed:   63
batch:      39250 | loss: 5.20885 | failed:   63
batch:      39260 | loss: 5.16256 | failed:   63
batch:      39270 | loss: 5.16887 | failed:   63
batch:      39280 | loss: 5.18515 | failed:   63
batch:      39290 | loss: 5.21250 | failed:   63
batch:      39300 | loss: 5.24982 | failed:   63
batch:      39310 | loss: 5.19792 | failed:   63
batch:      39320 | loss: 5.16344 | failed:   63
batch:      39330 | loss: 5.18366 | failed:   63
batch:      39340 | loss: 5.11678 | failed:   63
batch:      39350 | loss: 5.12265 | failed:   63
batch:      39360 | loss: 5.09273 | failed:   63
batch:      39370 | loss: 5.23162 | failed:   63
batch:      39380 | loss: 5.17175 | failed:   63
batch:      39390 | loss: 5.18700 | failed:   63
batch:      39400 | loss: 4.81786 | failed:   63
batch:      39410 | loss: 5.20961 | failed:   63
batch:      39420 | loss: 5.19478 | failed:   63
batch:      39430 | loss: 5.23147 | failed:   63
batch:      39440 | loss: 5.26275 | failed:   63
batch:      39450 | loss: 5.01218 | failed:   63
batch:      39460 | loss: 5.03527 | failed:   63
batch:      39470 | loss: 4.91871 | failed:   63
batch:      39480 | loss: 4.80443 | failed:   63
batch:      39490 | loss: 4.50670 | failed:   63
batch:      39500 | loss: 4.57790 | failed:   63
batch:      39510 | loss: 4.49111 | failed:   63
batch:      39520 | loss: 5.23006 | failed:   63
batch:      39530 | loss: 5.19071 | failed:   63
batch:      39540 | loss: 5.27596 | failed:   63
batch:      39550 | loss: 5.25128 | failed:   63
batch:      39560 | loss: 5.15817 | failed:   63
batch:      39570 | loss: 5.11463 | failed:   63
batch:      39580 | loss: 5.07792 | failed:   63
batch:      39590 | loss: 5.14154 | failed:   63
batch:      39600 | loss: 5.26400 | failed:   63
batch:      39610 | loss: 5.22025 | failed:   63
batch:      39620 | loss: 5.25381 | failed:   63
batch:      39630 | loss: 5.20404 | failed:   63
batch:      39640 | loss: 5.13241 | failed:   63
batch:      39650 | loss: 5.21748 | failed:   63
batch:      39660 | loss: 5.05005 | failed:   63
batch:      39670 | loss: 5.18378 | failed:   63
batch:      39680 | loss: 4.94116 | failed:   63
batch:      39690 | loss: 5.18555 | failed:   63
batch:      39700 | loss: 5.14036 | failed:   63
batch:      39710 | loss: 5.15310 | failed:   63
batch:      39720 | loss: 5.19445 | failed:   63
batch:      39730 | loss: 5.24563 | failed:   63
batch:      39740 | loss: 5.22038 | failed:   63
batch:      39750 | loss: 4.92718 | failed:   63
batch:      39760 | loss: 5.20708 | failed:   63
batch:      39770 | loss: 4.92331 | failed:   63
batch:      39780 | loss: 4.83477 | failed:   63
batch:      39790 | loss: 5.19815 | failed:   63
batch:      39800 | loss: 5.24947 | failed:   63
batch:      39810 | loss: 5.13796 | failed:   63
batch:      39820 | loss: 5.14898 | failed:   63
batch:      39830 | loss: 5.24791 | failed:   63
batch:      39840 | loss: 5.20390 | failed:   63
batch:      39850 | loss: 5.08094 | failed:   63
batch:      39860 | loss: 5.23614 | failed:   63
batch:      39870 | loss: 5.18239 | failed:   63
batch:      39880 | loss: 5.19708 | failed:   63
batch:      39890 | loss: 5.13110 | failed:   63
batch:      39900 | loss: 5.17921 | failed:   63
batch:      39910 | loss: 5.20530 | failed:   63
batch:      39920 | loss: 5.20954 | failed:   63
batch:      39930 | loss: 4.82431 | failed:   63
batch:      39940 | loss: 5.18311 | failed:   63
batch:      39950 | loss: 5.07616 | failed:   63
batch:      39960 | loss: 5.08121 | failed:   63
batch:      39970 | loss: 5.13656 | failed:   63
batch:      39980 | loss: 5.15249 | failed:   63
batch:      39990 | loss: 5.23256 | failed:   63
batch:      40000 | loss: 5.20762 | failed:   63
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      40010 | loss: 5.07691 | failed:   63
batch:      40020 | loss: 5.13689 | failed:   63
batch:      40030 | loss: 5.11501 | failed:   63
batch:      40040 | loss: 5.17408 | failed:   63
batch:      40050 | loss: 5.22937 | failed:   63
batch:      40060 | loss: 5.04431 | failed:   63
batch:      40070 | loss: 5.19468 | failed:   63
batch:      40080 | loss: 4.99877 | failed:   63
batch:      40090 | loss: 4.53076 | failed:   63
batch:      40100 | loss: 4.36918 | failed:   63
batch:      40110 | loss: 5.30916 | failed:   63
batch:      40120 | loss: 5.20459 | failed:   63
batch:      40130 | loss: 5.19497 | failed:   63
batch:      40140 | loss: 5.22775 | failed:   63
batch:      40150 | loss: 5.22190 | failed:   63
batch:      40160 | loss: 5.23000 | failed:   63
batch:      40170 | loss: 5.22346 | failed:   63
batch:      40180 | loss: 5.20211 | failed:   63
batch:      40190 | loss: 5.22060 | failed:   63
batch:      40200 | loss: 5.19738 | failed:   63
batch:      40210 | loss: 5.20776 | failed:   63
batch:      40220 | loss: 5.16133 | failed:   63
batch:      40230 | loss: 5.23341 | failed:   63
batch:      40240 | loss: 5.06943 | failed:   63
batch:      40250 | loss: 5.17052 | failed:   63
batch:      40260 | loss: 5.17522 | failed:   63
batch:      40270 | loss: 5.27215 | failed:   63
batch:      40280 | loss: 5.22890 | failed:   63
batch:      40290 | loss: 5.24543 | failed:   63
batch:      40300 | loss: 5.02734 | failed:   63
batch:      40310 | loss: 5.03382 | failed:   63
batch:      40320 | loss: 5.17487 | failed:   63
batch:      40330 | loss: 5.18570 | failed:   63
batch:      40340 | loss: 5.26481 | failed:   63
batch:      40350 | loss: 5.17472 | failed:   63
batch:      40360 | loss: 5.20115 | failed:   63
batch:      40370 | loss: 5.20757 | failed:   63
batch:      40380 | loss: 5.19544 | failed:   63
batch:      40390 | loss: 5.19752 | failed:   63
batch:      40400 | loss: 5.23285 | failed:   63
batch:      40410 | loss: 5.20842 | failed:   63
batch:      40420 | loss: 5.09765 | failed:   63
batch:      40430 | loss: 5.12735 | failed:   63
batch:      40440 | loss: 5.13966 | failed:   63
batch:      40450 | loss: 5.17627 | failed:   63
batch:      40460 | loss: 5.26689 | failed:   63
batch:      40470 | loss: 5.24636 | failed:   63
batch:      40480 | loss: 5.20607 | failed:   63
batch:      40490 | loss: 5.24167 | failed:   63
batch:      40500 | loss: 5.27386 | failed:   63
batch:      40510 | loss: 5.12969 | failed:   63
batch:      40520 | loss: 5.08944 | failed:   63
batch:      40530 | loss: 5.14753 | failed:   63
batch:      40540 | loss: 5.05619 | failed:   63
batch:      40550 | loss: 5.07234 | failed:   63
batch:      40560 | loss: 5.17418 | failed:   63
batch:      40570 | loss: 5.07402 | failed:   63
batch:      40580 | loss: 5.17312 | failed:   63
batch:      40590 | loss: 5.06081 | failed:   63
batch:      40600 | loss: 5.07306 | failed:   63
batch:      40610 | loss: 5.16709 | failed:   63
batch:      40620 | loss: 5.21498 | failed:   63
batch:      40630 | loss: 5.33125 | failed:   63
batch:      40640 | loss: 5.23163 | failed:   63
batch:      40650 | loss: 5.21102 | failed:   63
batch:      40660 | loss: 5.16229 | failed:   63
batch:      40670 | loss: 5.18265 | failed:   63
batch:      40680 | loss: 5.12407 | failed:   63
batch:      40690 | loss: 5.26048 | failed:   63
batch:      40700 | loss: 5.30063 | failed:   63
batch:      40710 | loss: 5.24572 | failed:   63
batch:      40720 | loss: 4.98964 | failed:   63
batch:      40730 | loss: 5.30724 | failed:   63
batch:      40740 | loss: 5.16381 | failed:   63
batch:      40750 | loss: 5.00064 | failed:   63
batch:      40760 | loss: 5.07165 | failed:   63
batch:      40770 | loss: 5.19569 | failed:   63
batch:      40780 | loss: 5.21726 | failed:   63
batch:      40790 | loss: 5.24538 | failed:   63
batch:      40800 | loss: 5.07009 | failed:   63
batch:      40810 | loss: 5.11291 | failed:   63
batch:      40820 | loss: 5.27305 | failed:   63
batch:      40830 | loss: 5.15145 | failed:   63
batch:      40840 | loss: 5.13409 | failed:   63
batch:      40850 | loss: 5.27690 | failed:   63
batch:      40860 | loss: 5.23523 | failed:   63
batch:      40870 | loss: 5.09565 | failed:   63
batch:      40880 | loss: 5.20276 | failed:   63
batch:      40890 | loss: 5.16825 | failed:   63
batch:      40900 | loss: 5.17686 | failed:   63
batch:      40910 | loss: 5.11184 | failed:   63
batch:      40920 | loss: 5.20267 | failed:   63
batch:      40930 | loss: 5.20376 | failed:   63
batch:      40940 | loss: 5.12107 | failed:   63
batch:      40950 | loss: 5.06860 | failed:   63
batch:      40960 | loss: 5.13175 | failed:   63
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      40970 | loss: 5.22420 | failed:   63
batch:      40980 | loss: 5.28238 | failed:   63
batch:      41000 | loss: 5.21146 | failed:   64
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      41010 | loss: 5.08658 | failed:   64
batch:      41020 | loss: 5.21553 | failed:   64
batch:      41030 | loss: 5.25483 | failed:   64
batch:      41040 | loss: 5.21189 | failed:   64
batch:      41050 | loss: 5.50295 | failed:   64
batch:      41060 | loss: 5.21635 | failed:   64
batch:      41070 | loss: 5.17236 | failed:   64
batch:      41080 | loss: 5.12437 | failed:   64
batch:      41090 | loss: 4.99906 | failed:   64
batch:      41100 | loss: 5.20850 | failed:   64
batch:      41110 | loss: 5.23172 | failed:   64
batch:      41120 | loss: 5.21512 | failed:   64
batch:      41130 | loss: 5.26617 | failed:   64
batch:      41140 | loss: 5.25138 | failed:   64
batch:      41150 | loss: 5.19595 | failed:   64
batch:      41160 | loss: 5.14114 | failed:   64
batch:      41170 | loss: 5.25147 | failed:   64
batch:      41180 | loss: 5.11392 | failed:   64
batch:      41190 | loss: 5.27632 | failed:   64
batch:      41200 | loss: 5.23155 | failed:   64
batch:      41210 | loss: 5.11320 | failed:   64
batch:      41220 | loss: 5.14901 | failed:   64
batch:      41230 | loss: 5.06055 | failed:   64
batch:      41240 | loss: 5.18182 | failed:   64
batch:      41250 | loss: 5.21991 | failed:   64
batch:      41260 | loss: 5.25381 | failed:   64
batch:      41270 | loss: 5.18787 | failed:   64
batch:      41280 | loss: 5.22718 | failed:   64
batch:      41290 | loss: 5.16845 | failed:   64
batch:      41300 | loss: 5.18250 | failed:   64
batch:      41310 | loss: 5.11491 | failed:   64
batch:      41320 | loss: 5.18023 | failed:   64
batch:      41330 | loss: 5.15298 | failed:   64
batch:      41340 | loss: 5.07376 | failed:   64
batch:      41350 | loss: 5.14449 | failed:   64
batch:      41360 | loss: 5.12989 | failed:   64
batch:      41370 | loss: 5.11118 | failed:   64
batch:      41380 | loss: 5.10700 | failed:   64
batch:      41390 | loss: 5.07523 | failed:   64
batch:      41400 | loss: 5.19924 | failed:   64
batch:      41410 | loss: 5.00823 | failed:   64
batch:      41420 | loss: 5.23757 | failed:   64
batch:      41430 | loss: 4.42997 | failed:   64
batch:      41440 | loss: 5.13800 | failed:   64
batch:      41450 | loss: 5.01226 | failed:   64
batch:      41460 | loss: 5.28744 | failed:   64
batch:      41470 | loss: 5.24025 | failed:   64
batch:      41480 | loss: 5.25115 | failed:   64
batch:      41490 | loss: 5.25141 | failed:   64
batch:      41500 | loss: 5.24893 | failed:   64
batch:      41510 | loss: 5.24204 | failed:   64
batch:      41520 | loss: 5.04555 | failed:   64
batch:      41530 | loss: 5.09966 | failed:   64
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      41540 | loss: 5.21054 | failed:   64
batch:      41550 | loss: 5.17317 | failed:   64
batch:      41560 | loss: 5.18975 | failed:   65
batch:      41570 | loss: 5.13854 | failed:   65
batch:      41580 | loss: 5.05765 | failed:   65
batch:      41590 | loss: 5.16832 | failed:   65
batch:      41600 | loss: 5.17635 | failed:   65
batch:      41610 | loss: 5.20448 | failed:   65
batch:      41620 | loss: 5.19606 | failed:   65
batch:      41630 | loss: 5.18605 | failed:   65
batch:      41640 | loss: 5.19448 | failed:   65
batch:      41650 | loss: 5.26781 | failed:   65
batch:      41660 | loss: 5.21504 | failed:   65
batch:      41670 | loss: 5.17689 | failed:   65
batch:      41680 | loss: 5.18258 | failed:   65
batch:      41690 | loss: 5.07706 | failed:   65
batch:      41700 | loss: 5.21687 | failed:   65
batch:      41710 | loss: 5.13600 | failed:   65
batch:      41720 | loss: 5.23103 | failed:   65
batch:      41730 | loss: 5.12788 | failed:   65
batch:      41740 | loss: 5.20121 | failed:   65
batch:      41750 | loss: 5.17356 | failed:   65
batch:      41760 | loss: 5.21085 | failed:   65
batch:      41770 | loss: 4.99408 | failed:   65
batch:      41780 | loss: 5.22633 | failed:   65
batch:      41790 | loss: 5.09992 | failed:   65
batch:      41800 | loss: 4.95623 | failed:   65
batch:      41810 | loss: 5.26462 | failed:   65
batch:      41820 | loss: 5.27242 | failed:   65
batch:      41830 | loss: 5.10095 | failed:   65
batch:      41840 | loss: 5.16203 | failed:   65
batch:      41850 | loss: 4.94660 | failed:   65
batch:      41860 | loss: 5.12879 | failed:   65
batch:      41870 | loss: 4.98021 | failed:   65
batch:      41880 | loss: 5.16166 | failed:   65
batch:      41890 | loss: 5.11465 | failed:   65
batch:      41900 | loss: 5.20721 | failed:   65
batch:      41910 | loss: 5.31136 | failed:   65
batch:      41920 | loss: 5.18984 | failed:   65
batch:      41930 | loss: 5.01846 | failed:   65
batch:      41940 | loss: 5.12323 | failed:   65
batch:      41950 | loss: 5.18068 | failed:   65
batch:      41960 | loss: 5.17644 | failed:   65
batch:      41970 | loss: 5.22464 | failed:   65
batch:      41980 | loss: 5.25472 | failed:   65
batch:      41990 | loss: 5.22298 | failed:   65
batch:      42000 | loss: 5.18456 | failed:   65
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      42010 | loss: 5.14771 | failed:   65
batch:      42020 | loss: 5.23912 | failed:   65
batch:      42030 | loss: 5.12601 | failed:   65
batch:      42040 | loss: 5.16266 | failed:   65
batch:      42050 | loss: 5.18725 | failed:   65
batch:      42060 | loss: 5.19173 | failed:   65
batch:      42070 | loss: 5.09510 | failed:   65
batch:      42080 | loss: 5.17772 | failed:   65
batch:      42090 | loss: 5.15413 | failed:   65
batch:      42100 | loss: 5.21060 | failed:   65
batch:      42110 | loss: 5.05356 | failed:   65
batch:      42120 | loss: 5.13655 | failed:   65
batch:      42130 | loss: 4.86795 | failed:   65
batch:      42140 | loss: 4.99687 | failed:   65
batch:      42150 | loss: 5.06584 | failed:   65
batch:      42160 | loss: 5.27690 | failed:   65
batch:      42170 | loss: 5.11368 | failed:   65
batch:      42180 | loss: 5.09844 | failed:   65
batch:      42190 | loss: 5.15147 | failed:   65
batch:      42200 | loss: 5.03172 | failed:   65
batch:      42210 | loss: 4.99963 | failed:   65
batch:      42220 | loss: 4.98119 | failed:   65
batch:      42230 | loss: 5.01045 | failed:   65
batch:      42240 | loss: 5.28303 | failed:   65
batch:      42250 | loss: 5.24691 | failed:   65
batch:      42260 | loss: 5.05933 | failed:   65
batch:      42270 | loss: 5.16510 | failed:   65
batch:      42280 | loss: 5.25935 | failed:   65
batch:      42290 | loss: 5.16639 | failed:   65
batch:      42300 | loss: 5.13872 | failed:   65
batch:      42310 | loss: 5.18367 | failed:   65
batch:      42320 | loss: 5.09243 | failed:   65
batch:      42330 | loss: 5.22566 | failed:   65
batch:      42340 | loss: 5.07394 | failed:   65
batch:      42350 | loss: 5.14209 | failed:   65
batch:      42360 | loss: 5.33671 | failed:   65
batch:      42370 | loss: 5.17689 | failed:   65
batch:      42380 | loss: 5.15316 | failed:   65
batch:      42390 | loss: 5.08162 | failed:   65
batch:      42400 | loss: 5.03357 | failed:   65
batch:      42410 | loss: 5.17000 | failed:   65
batch:      42420 | loss: 5.09263 | failed:   65
batch:      42430 | loss: 5.26548 | failed:   65
batch:      42440 | loss: 5.04373 | failed:   65
batch:      42450 | loss: 5.16521 | failed:   65
batch:      42460 | loss: 5.24819 | failed:   65
batch:      42470 | loss: 5.11638 | failed:   65
batch:      42480 | loss: 5.12416 | failed:   65
batch:      42490 | loss: 5.19028 | failed:   65
batch:      42500 | loss: 5.12628 | failed:   65
batch:      42510 | loss: 5.29750 | failed:   65
batch:      42520 | loss: 5.18469 | failed:   65
batch:      42530 | loss: 5.21338 | failed:   65
batch:      42540 | loss: 5.08977 | failed:   65
batch:      42550 | loss: 5.11188 | failed:   65
batch:      42560 | loss: 5.10412 | failed:   65
batch:      42570 | loss: 5.08270 | failed:   65
batch:      42580 | loss: 5.22886 | failed:   65
batch:      42590 | loss: 5.09578 | failed:   65
batch:      42600 | loss: 5.15721 | failed:   65
batch:      42610 | loss: 5.22809 | failed:   65
batch:      42620 | loss: 5.15474 | failed:   65
batch:      42630 | loss: 5.29936 | failed:   65
batch:      42640 | loss: 5.17290 | failed:   65
batch:      42650 | loss: 5.15993 | failed:   65
batch:      42660 | loss: 5.17091 | failed:   65
batch:      42670 | loss: 5.16605 | failed:   65
batch:      42680 | loss: 5.17984 | failed:   65
batch:      42690 | loss: 5.21342 | failed:   65
batch:      42700 | loss: 5.17192 | failed:   65
batch:      42710 | loss: 5.24830 | failed:   65
batch:      42720 | loss: 5.22586 | failed:   65
batch:      42730 | loss: 5.10731 | failed:   65
batch:      42740 | loss: 5.05749 | failed:   65
batch:      42750 | loss: 4.95428 | failed:   65
batch:      42760 | loss: 5.18527 | failed:   65
batch:      42770 | loss: 5.16826 | failed:   65
batch:      42780 | loss: 5.18682 | failed:   65
batch:      42790 | loss: 5.10191 | failed:   65
batch:      42800 | loss: 5.13931 | failed:   65
batch:      42810 | loss: 5.14417 | failed:   65
batch:      42820 | loss: 5.14970 | failed:   65
batch:      42830 | loss: 5.16469 | failed:   65
batch:      42840 | loss: 5.22903 | failed:   65
batch:      42850 | loss: 5.09953 | failed:   65
batch:      42860 | loss: 5.15036 | failed:   65
batch:      42870 | loss: 5.11640 | failed:   65
batch:      42880 | loss: 5.14603 | failed:   65
batch:      42890 | loss: 5.05790 | failed:   65
batch:      42900 | loss: 5.23026 | failed:   65
batch:      42910 | loss: 5.23072 | failed:   65
batch:      42920 | loss: 5.28315 | failed:   65
batch:      42930 | loss: 5.23784 | failed:   65
batch:      42940 | loss: 5.11945 | failed:   65
batch:      42950 | loss: 5.06139 | failed:   65
batch:      42960 | loss: 5.12901 | failed:   65
batch:      42970 | loss: 5.25164 | failed:   65
batch:      42980 | loss: 5.03488 | failed:   65
batch:      42990 | loss: 5.09082 | failed:   65
batch:      43000 | loss: 5.20854 | failed:   65
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      43010 | loss: 5.22731 | failed:   65
batch:      43020 | loss: 5.24348 | failed:   65
batch:      43030 | loss: 5.28985 | failed:   65
batch:      43040 | loss: 5.10465 | failed:   65
batch:      43050 | loss: 5.15473 | failed:   65
batch:      43060 | loss: 4.50035 | failed:   65
batch:      43070 | loss: 4.66375 | failed:   65
batch:      43080 | loss: 4.47605 | failed:   65
batch:      43090 | loss: 4.34942 | failed:   65
batch:      43100 | loss: 4.49937 | failed:   65
batch:      43110 | loss: 4.34323 | failed:   65
batch:      43120 | loss: 4.32779 | failed:   65
batch:      43130 | loss: 4.56840 | failed:   65
batch:      43140 | loss: 4.78348 | failed:   65
batch:      43150 | loss: 5.20920 | failed:   65
batch:      43160 | loss: 5.25987 | failed:   65
batch:      43170 | loss: 5.21570 | failed:   65
batch:      43180 | loss: 5.15749 | failed:   65
batch:      43190 | loss: 5.02189 | failed:   65
batch:      43200 | loss: 5.12043 | failed:   65
batch:      43210 | loss: 5.13727 | failed:   65
batch:      43220 | loss: 5.21731 | failed:   65
batch:      43230 | loss: 5.14839 | failed:   65
batch:      43240 | loss: 5.11036 | failed:   65
batch:      43250 | loss: 5.25733 | failed:   65
batch:      43260 | loss: 5.08622 | failed:   65
batch:      43270 | loss: 5.17457 | failed:   65
batch:      43280 | loss: 5.15908 | failed:   65
batch:      43290 | loss: 5.11862 | failed:   65
batch:      43300 | loss: 5.16520 | failed:   65
batch:      43310 | loss: 5.10707 | failed:   65
batch:      43320 | loss: 5.20853 | failed:   65
batch:      43330 | loss: 5.35140 | failed:   65
batch:      43340 | loss: 5.16946 | failed:   65
batch:      43350 | loss: 5.22405 | failed:   65
batch:      43360 | loss: 5.25623 | failed:   65
batch:      43370 | loss: 5.12609 | failed:   65
batch:      43380 | loss: 5.16632 | failed:   65
batch:      43390 | loss: 5.14984 | failed:   65
batch:      43400 | loss: 5.07027 | failed:   65
batch:      43410 | loss: 5.24547 | failed:   65
batch:      43420 | loss: 5.20011 | failed:   65
batch:      43430 | loss: 5.22799 | failed:   65
batch:      43440 | loss: 4.89164 | failed:   65
batch:      43450 | loss: 5.24574 | failed:   65
batch:      43460 | loss: 5.12621 | failed:   65
batch:      43470 | loss: 5.14437 | failed:   65
batch:      43480 | loss: 5.30191 | failed:   65
batch:      43490 | loss: 5.22260 | failed:   65
batch:      43500 | loss: 5.15568 | failed:   65
batch:      43510 | loss: 5.11987 | failed:   65
batch:      43520 | loss: 5.15055 | failed:   65
batch:      43530 | loss: 5.21843 | failed:   65
batch:      43540 | loss: 5.19087 | failed:   65
batch:      43550 | loss: 4.98473 | failed:   65
batch:      43560 | loss: 5.23264 | failed:   65
batch:      43570 | loss: 5.17734 | failed:   65
batch:      43580 | loss: 5.11072 | failed:   65
batch:      43590 | loss: 5.26883 | failed:   65
batch:      43600 | loss: 5.02619 | failed:   65
batch:      43610 | loss: 5.25320 | failed:   65
batch:      43620 | loss: 5.06864 | failed:   65
batch:      43630 | loss: 5.13442 | failed:   65
batch:      43640 | loss: 5.26394 | failed:   65
batch:      43650 | loss: 5.15502 | failed:   65
batch:      43660 | loss: 5.19379 | failed:   65
batch:      43670 | loss: 5.22885 | failed:   65
batch:      43680 | loss: 5.09584 | failed:   65
batch:      43690 | loss: 5.17499 | failed:   65
batch:      43700 | loss: 5.14862 | failed:   65
batch:      43710 | loss: 5.13916 | failed:   65
batch:      43720 | loss: 5.15909 | failed:   65
batch:      43730 | loss: 5.23729 | failed:   65
batch:      43740 | loss: 5.19910 | failed:   65
batch:      43750 | loss: 5.16156 | failed:   65
batch:      43760 | loss: 5.14717 | failed:   65
batch:      43770 | loss: 5.11419 | failed:   65
batch:      43780 | loss: 5.06647 | failed:   65
batch:      43790 | loss: 5.15328 | failed:   65
batch:      43800 | loss: 5.16112 | failed:   65
batch:      43810 | loss: 5.08104 | failed:   65
batch:      43820 | loss: 5.27803 | failed:   65
batch:      43830 | loss: 5.15618 | failed:   65
batch:      43840 | loss: 5.28077 | failed:   65
batch:      43850 | loss: 5.22174 | failed:   65
batch:      43860 | loss: 5.01941 | failed:   65
batch:      43870 | loss: 5.11393 | failed:   65
batch:      43880 | loss: 5.15720 | failed:   65
batch:      43890 | loss: 5.23764 | failed:   65
batch:      43900 | loss: 5.22625 | failed:   65
batch:      43910 | loss: 5.03204 | failed:   65
batch:      43920 | loss: 5.22049 | failed:   65
batch:      43930 | loss: 5.19554 | failed:   65
batch:      43940 | loss: 5.13635 | failed:   65
batch:      43950 | loss: 5.21885 | failed:   65
batch:      43960 | loss: 5.18113 | failed:   65
batch:      43970 | loss: 5.10713 | failed:   65
batch:      43980 | loss: 5.30093 | failed:   65
batch:      43990 | loss: 5.11589 | failed:   65
batch:      44000 | loss: 5.14454 | failed:   65
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      44010 | loss: 5.13283 | failed:   65
batch:      44020 | loss: 5.08856 | failed:   65
batch:      44030 | loss: 5.22134 | failed:   65
batch:      44040 | loss: 5.19634 | failed:   65
batch:      44050 | loss: 5.13856 | failed:   65
batch:      44060 | loss: 5.20921 | failed:   65
batch:      44070 | loss: 5.20417 | failed:   65
batch:      44080 | loss: 5.17453 | failed:   65
batch:      44090 | loss: 5.17138 | failed:   65
batch:      44100 | loss: 5.21544 | failed:   65
batch:      44110 | loss: 5.19636 | failed:   65
batch:      44120 | loss: 5.23377 | failed:   65
batch:      44130 | loss: 5.16376 | failed:   65
batch:      44140 | loss: 5.04364 | failed:   65
batch:      44150 | loss: 5.16051 | failed:   65
batch:      44160 | loss: 5.24573 | failed:   65
batch:      44170 | loss: 5.22426 | failed:   65
batch:      44180 | loss: 5.18803 | failed:   65
batch:      44190 | loss: 5.18954 | failed:   65
batch:      44200 | loss: 5.19742 | failed:   65
batch:      44210 | loss: 5.15176 | failed:   65
batch:      44220 | loss: 5.17849 | failed:   65
batch:      44230 | loss: 5.21457 | failed:   65
batch:      44240 | loss: 5.17522 | failed:   65
batch:      44250 | loss: 5.27779 | failed:   65
batch:      44260 | loss: 5.15918 | failed:   65
batch:      44270 | loss: 5.18383 | failed:   65
batch:      44280 | loss: 5.10219 | failed:   65
batch:      44290 | loss: 5.06261 | failed:   65
batch:      44300 | loss: 5.08921 | failed:   65
batch:      44310 | loss: 5.03139 | failed:   65
batch:      44320 | loss: 4.97229 | failed:   65
batch:      44330 | loss: 5.22287 | failed:   65
batch:      44340 | loss: 5.24764 | failed:   65
batch:      44350 | loss: 5.21402 | failed:   65
batch:      44360 | loss: 5.24392 | failed:   65
batch:      44370 | loss: 5.16923 | failed:   65
batch:      44380 | loss: 5.09270 | failed:   65
batch:      44390 | loss: 5.22749 | failed:   65
batch:      44400 | loss: 5.07750 | failed:   65
batch:      44410 | loss: 5.16009 | failed:   65
batch:      44420 | loss: 5.17481 | failed:   65
batch:      44430 | loss: 5.06550 | failed:   65
batch:      44440 | loss: 5.15618 | failed:   65
batch:      44450 | loss: 5.02547 | failed:   65
batch:      44460 | loss: 4.99675 | failed:   65
batch:      44470 | loss: 4.97212 | failed:   65
batch:      44480 | loss: 4.87904 | failed:   65
batch:      44490 | loss: 5.12213 | failed:   65
batch:      44500 | loss: 5.18733 | failed:   65
batch:      44510 | loss: 4.90008 | failed:   65
batch:      44520 | loss: 5.17738 | failed:   65
batch:      44530 | loss: 5.22643 | failed:   65
batch:      44540 | loss: 5.19557 | failed:   65
batch:      44550 | loss: 5.20827 | failed:   65
batch:      44560 | loss: 5.10155 | failed:   65
batch:      44570 | loss: 5.06406 | failed:   65
batch:      44580 | loss: 5.05793 | failed:   65
batch:      44590 | loss: 5.23246 | failed:   65
batch:      44600 | loss: 5.15173 | failed:   65
batch:      44610 | loss: 5.03616 | failed:   65
batch:      44620 | loss: 5.11723 | failed:   65
batch:      44630 | loss: 5.07479 | failed:   65
batch:      44640 | loss: 5.12218 | failed:   65
batch:      44650 | loss: 5.18879 | failed:   65
batch:      44660 | loss: 5.19831 | failed:   65
batch:      44670 | loss: 5.05067 | failed:   65
batch:      44680 | loss: 5.20965 | failed:   65
batch:      44690 | loss: 5.20721 | failed:   65
batch:      44700 | loss: 5.12995 | failed:   65
batch:      44710 | loss: 5.06369 | failed:   65
batch:      44720 | loss: 5.11395 | failed:   65
batch:      44730 | loss: 5.11427 | failed:   65
batch:      44740 | loss: 5.22100 | failed:   65
batch:      44750 | loss: 5.18823 | failed:   65
batch:      44760 | loss: 5.13053 | failed:   65
batch:      44770 | loss: 5.08301 | failed:   65
batch:      44780 | loss: 5.16624 | failed:   65
batch:      44790 | loss: 5.14736 | failed:   65
batch:      44800 | loss: 5.13160 | failed:   65
batch:      44810 | loss: 4.99334 | failed:   65
batch:      44820 | loss: 5.17434 | failed:   65
batch:      44830 | loss: 5.24708 | failed:   65
batch:      44840 | loss: 5.15167 | failed:   65
batch:      44850 | loss: 5.07655 | failed:   65
batch:      44860 | loss: 4.98363 | failed:   65
batch:      44870 | loss: 5.20928 | failed:   65
batch:      44880 | loss: 5.06717 | failed:   65
batch:      44890 | loss: 5.10871 | failed:   65
batch:      44900 | loss: 5.31505 | failed:   65
batch:      44910 | loss: 5.22095 | failed:   65
batch:      44920 | loss: 5.13472 | failed:   65
batch:      44930 | loss: 4.98259 | failed:   65
batch:      44940 | loss: 5.22152 | failed:   65
batch:      44950 | loss: 5.22666 | failed:   65
batch:      44960 | loss: 5.10745 | failed:   65
batch:      44970 | loss: 5.15982 | failed:   65
batch:      44980 | loss: 4.99954 | failed:   65
batch:      44990 | loss: 5.18181 | failed:   65
batch:      45000 | loss: 5.21109 | failed:   65
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      45010 | loss: 5.19496 | failed:   65
batch:      45020 | loss: 5.25095 | failed:   65
batch:      45030 | loss: 5.02860 | failed:   65
batch:      45040 | loss: 5.24086 | failed:   65
batch:      45050 | loss: 5.23298 | failed:   65
batch:      45060 | loss: 5.15350 | failed:   65
batch:      45070 | loss: 5.10518 | failed:   65
batch:      45080 | loss: 5.21300 | failed:   65
batch:      45090 | loss: 5.15014 | failed:   65
batch:      45100 | loss: 5.07920 | failed:   65
batch:      45110 | loss: 5.11115 | failed:   65
batch:      45120 | loss: 5.09609 | failed:   65
batch:      45130 | loss: 5.01410 | failed:   65
batch:      45140 | loss: 5.19318 | failed:   65
batch:      45150 | loss: 5.06976 | failed:   65
batch:      45160 | loss: 5.13475 | failed:   65
batch:      45170 | loss: 5.06238 | failed:   65
batch:      45180 | loss: 4.89357 | failed:   65
batch:      45190 | loss: 5.14651 | failed:   65
batch:      45200 | loss: 5.02984 | failed:   65
batch:      45210 | loss: 5.11211 | failed:   65
batch:      45220 | loss: 5.12399 | failed:   65
batch:      45230 | loss: 5.23640 | failed:   65
batch:      45240 | loss: 4.98761 | failed:   65
batch:      45250 | loss: 5.14848 | failed:   65
batch:      45260 | loss: 5.17301 | failed:   65
batch:      45270 | loss: 5.14556 | failed:   65
batch:      45280 | loss: 5.21211 | failed:   65
batch:      45290 | loss: 5.10335 | failed:   65
batch:      45300 | loss: 5.07139 | failed:   65
batch:      45310 | loss: 5.17455 | failed:   65
batch:      45320 | loss: 5.22046 | failed:   65
batch:      45330 | loss: 5.16061 | failed:   65
batch:      45340 | loss: 5.22098 | failed:   65
batch:      45350 | loss: 5.20295 | failed:   65
batch:      45360 | loss: 5.30271 | failed:   65
batch:      45370 | loss: 5.19260 | failed:   65
batch:      45380 | loss: 5.01263 | failed:   65
batch:      45390 | loss: 5.10145 | failed:   65
batch:      45400 | loss: 5.28283 | failed:   65
batch:      45410 | loss: 4.97854 | failed:   65
batch:      45420 | loss: 5.23002 | failed:   65
batch:      45430 | loss: 5.14805 | failed:   65
batch:      45440 | loss: 5.13682 | failed:   65
batch:      45450 | loss: 5.09103 | failed:   65
batch:      45460 | loss: 5.17140 | failed:   65
batch:      45470 | loss: 5.17524 | failed:   65
batch:      45480 | loss: 5.21169 | failed:   65
batch:      45490 | loss: 5.20405 | failed:   65
batch:      45500 | loss: 5.16665 | failed:   65
batch:      45510 | loss: 5.25578 | failed:   65
batch:      45520 | loss: 5.21253 | failed:   65
batch:      45530 | loss: 5.13517 | failed:   65
batch:      45540 | loss: 5.13256 | failed:   65
batch:      45550 | loss: 4.95771 | failed:   65
batch:      45560 | loss: 5.05016 | failed:   65
batch:      45570 | loss: 5.19691 | failed:   65
batch:      45580 | loss: 5.16459 | failed:   65
batch:      45590 | loss: 5.19821 | failed:   65
batch:      45600 | loss: 4.97050 | failed:   65
batch:      45610 | loss: 5.17757 | failed:   65
batch:      45620 | loss: 5.13584 | failed:   65
batch:      45630 | loss: 5.07695 | failed:   65
batch:      45640 | loss: 5.10201 | failed:   65
batch:      45650 | loss: 5.02410 | failed:   65
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      45660 | loss: 5.23814 | failed:   65
batch:      45670 | loss: 5.17782 | failed:   65
batch:      45690 | loss: 5.17379 | failed:   67
batch:      45700 | loss: 5.25139 | failed:   67
batch:      45710 | loss: 5.13352 | failed:   67
batch:      45720 | loss: 5.09720 | failed:   67
batch:      45730 | loss: 5.15101 | failed:   67
batch:      45740 | loss: 5.29332 | failed:   67
batch:      45750 | loss: 5.11914 | failed:   67
batch:      45760 | loss: 5.10051 | failed:   67
batch:      45770 | loss: 4.98985 | failed:   67
batch:      45780 | loss: 5.09028 | failed:   67
batch:      45790 | loss: 5.01658 | failed:   67
batch:      45800 | loss: 5.28450 | failed:   67
batch:      45810 | loss: 5.14692 | failed:   67
batch:      45820 | loss: 5.18149 | failed:   67
batch:      45830 | loss: 5.10789 | failed:   67
batch:      45840 | loss: 5.06437 | failed:   67
batch:      45850 | loss: 5.02206 | failed:   67
batch:      45860 | loss: 5.29467 | failed:   67
batch:      45870 | loss: 5.29552 | failed:   67
batch:      45880 | loss: 5.21224 | failed:   67
batch:      45890 | loss: 5.19400 | failed:   67
batch:      45900 | loss: 5.13278 | failed:   67
batch:      45910 | loss: 5.08261 | failed:   67
batch:      45920 | loss: 5.13268 | failed:   67
batch:      45930 | loss: 5.08299 | failed:   67
batch:      45940 | loss: 5.01811 | failed:   67
batch:      45950 | loss: 5.06851 | failed:   67
batch:      45960 | loss: 5.18017 | failed:   67
batch:      45970 | loss: 5.13847 | failed:   67
batch:      45980 | loss: 5.07908 | failed:   67
batch:      45990 | loss: 5.09815 | failed:   67
batch:      46000 | loss: 5.23688 | failed:   67
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      46010 | loss: 5.15936 | failed:   67
batch:      46020 | loss: 5.02494 | failed:   67
batch:      46030 | loss: 4.85379 | failed:   67
batch:      46040 | loss: 4.76360 | failed:   67
batch:      46050 | loss: 4.78092 | failed:   67
batch:      46060 | loss: 5.23876 | failed:   67
batch:      46070 | loss: 5.19875 | failed:   67
batch:      46080 | loss: 5.24015 | failed:   67
batch:      46090 | loss: 5.10161 | failed:   67
batch:      46100 | loss: 5.30165 | failed:   67
batch:      46110 | loss: 5.19122 | failed:   67
batch:      46120 | loss: 5.13231 | failed:   67
batch:      46130 | loss: 5.10596 | failed:   67
batch:      46140 | loss: 5.13309 | failed:   67
batch:      46150 | loss: 4.90664 | failed:   67
batch:      46160 | loss: 5.01043 | failed:   67
batch:      46170 | loss: 5.21817 | failed:   67
batch:      46180 | loss: 5.28743 | failed:   67
batch:      46190 | loss: 5.18279 | failed:   67
batch:      46200 | loss: 5.02077 | failed:   67
batch:      46210 | loss: 5.11407 | failed:   67
batch:      46220 | loss: 5.17468 | failed:   67
batch:      46230 | loss: 5.09182 | failed:   67
batch:      46240 | loss: 5.15497 | failed:   67
batch:      46250 | loss: 5.15903 | failed:   67
batch:      46260 | loss: 5.14494 | failed:   67
batch:      46270 | loss: 5.12053 | failed:   67
batch:      46280 | loss: 5.31261 | failed:   67
batch:      46290 | loss: 5.20549 | failed:   67
batch:      46300 | loss: 4.90519 | failed:   67
batch:      46310 | loss: 5.19564 | failed:   67
batch:      46320 | loss: 5.13247 | failed:   67
batch:      46330 | loss: 5.23777 | failed:   67
batch:      46340 | loss: 5.09127 | failed:   67
batch:      46350 | loss: 5.16444 | failed:   67
batch:      46360 | loss: 5.16602 | failed:   67
batch:      46370 | loss: 5.07980 | failed:   67
batch:      46380 | loss: 5.18776 | failed:   67
batch:      46390 | loss: 5.20755 | failed:   67
batch:      46400 | loss: 5.14506 | failed:   67
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      46410 | loss: 5.06046 | failed:   67
batch:      46420 | loss: 5.07962 | failed:   67
batch:      46430 | loss: 4.84788 | failed:   68
batch:      46440 | loss: 5.07117 | failed:   68
batch:      46450 | loss: 5.24467 | failed:   68
batch:      46460 | loss: 5.14709 | failed:   68
batch:      46470 | loss: 5.06096 | failed:   68
batch:      46480 | loss: 5.18790 | failed:   68
batch:      46490 | loss: 5.02574 | failed:   68
batch:      46500 | loss: 5.13108 | failed:   68
batch:      46510 | loss: 5.16583 | failed:   68
batch:      46520 | loss: 5.14359 | failed:   68
batch:      46530 | loss: 5.24013 | failed:   68
batch:      46540 | loss: 5.11991 | failed:   68
batch:      46550 | loss: 5.16789 | failed:   68
batch:      46560 | loss: 5.12842 | failed:   68
batch:      46570 | loss: 5.20091 | failed:   68
batch:      46580 | loss: 5.30637 | failed:   68
batch:      46590 | loss: 5.15803 | failed:   68
batch:      46600 | loss: 5.11354 | failed:   68
batch:      46610 | loss: 4.95783 | failed:   68
batch:      46620 | loss: 5.36430 | failed:   68
batch:      46630 | loss: 5.21778 | failed:   68
batch:      46640 | loss: 5.12281 | failed:   68
batch:      46650 | loss: 5.15782 | failed:   68
batch:      46660 | loss: 4.95627 | failed:   68
batch:      46670 | loss: 5.19524 | failed:   68
batch:      46680 | loss: 5.17874 | failed:   68
batch:      46690 | loss: 5.24974 | failed:   68
batch:      46700 | loss: 5.08551 | failed:   68
batch:      46710 | loss: 5.13511 | failed:   68
batch:      46720 | loss: 5.17199 | failed:   68
batch:      46730 | loss: 5.16503 | failed:   68
batch:      46740 | loss: 4.93470 | failed:   68
batch:      46750 | loss: 5.13527 | failed:   68
batch:      46760 | loss: 5.18784 | failed:   68
batch:      46770 | loss: 5.25950 | failed:   68
batch:      46780 | loss: 5.18009 | failed:   68
batch:      46790 | loss: 5.20345 | failed:   68
batch:      46800 | loss: 5.19572 | failed:   68
batch:      46810 | loss: 5.17463 | failed:   68
batch:      46820 | loss: 5.09655 | failed:   68
batch:      46830 | loss: 5.07319 | failed:   68
batch:      46840 | loss: 5.16685 | failed:   68
batch:      46850 | loss: 5.36972 | failed:   68
batch:      46860 | loss: 5.10799 | failed:   68
batch:      46870 | loss: 5.11637 | failed:   68
batch:      46880 | loss: 5.14111 | failed:   68
batch:      46890 | loss: 5.19895 | failed:   68
batch:      46900 | loss: 5.14865 | failed:   68
batch:      46910 | loss: 5.14586 | failed:   68
batch:      46920 | loss: 5.12366 | failed:   68
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      46930 | loss: 5.13949 | failed:   68
batch:      46940 | loss: 5.11723 | failed:   68
batch:      46950 | loss: 5.30933 | failed:   69
batch:      46960 | loss: 5.24225 | failed:   69
batch:      46970 | loss: 5.28433 | failed:   69
batch:      46980 | loss: 5.24460 | failed:   69
batch:      46990 | loss: 5.27616 | failed:   69
batch:      47000 | loss: 5.24528 | failed:   69
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      47010 | loss: 5.25541 | failed:   69
batch:      47020 | loss: 5.21056 | failed:   69
batch:      47030 | loss: 5.23754 | failed:   69
batch:      47040 | loss: 5.10368 | failed:   69
batch:      47050 | loss: 5.24765 | failed:   69
batch:      47060 | loss: 5.13357 | failed:   69
batch:      47070 | loss: 4.88600 | failed:   69
batch:      47080 | loss: 5.08900 | failed:   69
batch:      47090 | loss: 5.07422 | failed:   69
batch:      47100 | loss: 5.23401 | failed:   69
batch:      47110 | loss: 5.10759 | failed:   69
batch:      47120 | loss: 5.07773 | failed:   69
batch:      47130 | loss: 5.16053 | failed:   69
batch:      47140 | loss: 5.15325 | failed:   69
batch:      47150 | loss: 5.20085 | failed:   69
batch:      47160 | loss: 5.06924 | failed:   69
batch:      47170 | loss: 5.07726 | failed:   69
batch:      47180 | loss: 5.05946 | failed:   69
batch:      47190 | loss: 4.99574 | failed:   69
batch:      47200 | loss: 5.09443 | failed:   69
batch:      47210 | loss: 5.22155 | failed:   69
batch:      47220 | loss: 5.09990 | failed:   69
batch:      47230 | loss: 5.14928 | failed:   69
batch:      47240 | loss: 5.17165 | failed:   69
batch:      47250 | loss: 5.14364 | failed:   69
batch:      47260 | loss: 5.14856 | failed:   69
batch:      47270 | loss: 5.12096 | failed:   69
batch:      47280 | loss: 5.07981 | failed:   69
batch:      47290 | loss: 5.14359 | failed:   69
batch:      47300 | loss: 5.16767 | failed:   69
batch:      47310 | loss: 5.30464 | failed:   69
batch:      47320 | loss: 5.23022 | failed:   69
batch:      47330 | loss: 5.15326 | failed:   69
batch:      47340 | loss: 5.10892 | failed:   69
batch:      47350 | loss: 5.22177 | failed:   69
batch:      47360 | loss: 5.20080 | failed:   69
batch:      47370 | loss: 5.01601 | failed:   69
batch:      47380 | loss: 5.11851 | failed:   69
batch:      47390 | loss: 4.92546 | failed:   69
batch:      47400 | loss: 4.99017 | failed:   69
batch:      47410 | loss: 5.21977 | failed:   69
batch:      47420 | loss: 5.17187 | failed:   69
batch:      47430 | loss: 5.10077 | failed:   69
batch:      47440 | loss: 5.11585 | failed:   69
batch:      47450 | loss: 5.05712 | failed:   69
batch:      47460 | loss: 5.06208 | failed:   69
batch:      47470 | loss: 5.10324 | failed:   69
batch:      47480 | loss: 5.17176 | failed:   69
batch:      47490 | loss: 5.03962 | failed:   69
batch:      47500 | loss: 5.25889 | failed:   69
batch:      47510 | loss: 5.19044 | failed:   69
batch:      47520 | loss: 5.01854 | failed:   69
batch:      47530 | loss: 4.89292 | failed:   69
batch:      47540 | loss: 5.08363 | failed:   69
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      47550 | loss: 5.26249 | failed:   69
batch:      47560 | loss: 5.20681 | failed:   69
batch:      47570 | loss: 5.20796 | failed:   73
batch:      47580 | loss: 5.16948 | failed:   73
batch:      47590 | loss: 5.24248 | failed:   73
batch:      47600 | loss: 5.26615 | failed:   73
batch:      47610 | loss: 5.18492 | failed:   73
batch:      47620 | loss: 5.16660 | failed:   73
batch:      47630 | loss: 5.01921 | failed:   73
batch:      47640 | loss: 5.22984 | failed:   73
batch:      47650 | loss: 5.14245 | failed:   73
batch:      47660 | loss: 5.11868 | failed:   73
batch:      47670 | loss: 5.13217 | failed:   73
batch:      47680 | loss: 5.18448 | failed:   73
batch:      47690 | loss: 5.05960 | failed:   73
batch:      47700 | loss: 5.17220 | failed:   73
batch:      47710 | loss: 5.17997 | failed:   73
batch:      47720 | loss: 5.23090 | failed:   73
batch:      47730 | loss: 5.17753 | failed:   73
batch:      47740 | loss: 5.24783 | failed:   73
batch:      47750 | loss: 5.15482 | failed:   73
batch:      47760 | loss: 5.11005 | failed:   73
batch:      47770 | loss: 5.02328 | failed:   73
batch:      47780 | loss: 5.22761 | failed:   73
batch:      47790 | loss: 5.26165 | failed:   73
batch:      47800 | loss: 5.23457 | failed:   73
batch:      47810 | loss: 5.22139 | failed:   73
batch:      47820 | loss: 5.22171 | failed:   73
batch:      47830 | loss: 5.02067 | failed:   73
batch:      47840 | loss: 5.06892 | failed:   73
batch:      47850 | loss: 5.21174 | failed:   73
batch:      47860 | loss: 5.19715 | failed:   73
batch:      47870 | loss: 5.12120 | failed:   73
batch:      47880 | loss: 4.73434 | failed:   73
batch:      47890 | loss: 4.45201 | failed:   73
batch:      47900 | loss: 4.40426 | failed:   73
batch:      47910 | loss: 4.06540 | failed:   73
batch:      47920 | loss: 4.15339 | failed:   73
batch:      47930 | loss: 5.06149 | failed:   73
batch:      47940 | loss: 5.31493 | failed:   73
batch:      47950 | loss: 5.24832 | failed:   73
batch:      47960 | loss: 5.14689 | failed:   73
batch:      47970 | loss: 5.18494 | failed:   73
batch:      47980 | loss: 5.18902 | failed:   73
batch:      47990 | loss: 5.13049 | failed:   73
batch:      48000 | loss: 5.23122 | failed:   73
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      48010 | loss: 5.12003 | failed:   73
batch:      48020 | loss: 5.22153 | failed:   73
batch:      48030 | loss: 5.19340 | failed:   73
batch:      48040 | loss: 4.44553 | failed:   73
batch:      48050 | loss: 5.11998 | failed:   73
batch:      48060 | loss: 5.16967 | failed:   73
batch:      48070 | loss: 5.23438 | failed:   73
batch:      48080 | loss: 5.09951 | failed:   73
batch:      48090 | loss: 5.22883 | failed:   73
batch:      48100 | loss: 5.15077 | failed:   73
batch:      48110 | loss: 5.14524 | failed:   73
batch:      48120 | loss: 5.11196 | failed:   73
batch:      48130 | loss: 5.09028 | failed:   73
batch:      48140 | loss: 5.12903 | failed:   73
batch:      48150 | loss: 5.23548 | failed:   73
batch:      48160 | loss: 5.19186 | failed:   73
batch:      48170 | loss: 5.10001 | failed:   73
batch:      48180 | loss: 5.18016 | failed:   73
batch:      48190 | loss: 5.22148 | failed:   73
batch:      48200 | loss: 5.09638 | failed:   73
batch:      48210 | loss: 5.09767 | failed:   73
batch:      48220 | loss: 5.01145 | failed:   73
batch:      48230 | loss: 5.27521 | failed:   73
batch:      48240 | loss: 5.27252 | failed:   73
batch:      48250 | loss: 5.20017 | failed:   73
batch:      48260 | loss: 5.24923 | failed:   73
batch:      48270 | loss: 5.09084 | failed:   73
batch:      48280 | loss: 5.23629 | failed:   73
batch:      48290 | loss: 5.15595 | failed:   73
batch:      48300 | loss: 5.17717 | failed:   73
batch:      48310 | loss: 5.16865 | failed:   73
batch:      48320 | loss: 5.13536 | failed:   73
batch:      48330 | loss: 5.14264 | failed:   73
batch:      48340 | loss: 5.06306 | failed:   73
batch:      48350 | loss: 5.21207 | failed:   73
batch:      48360 | loss: 5.10979 | failed:   73
batch:      48370 | loss: 5.25721 | failed:   73
batch:      48380 | loss: 5.16693 | failed:   73
batch:      48390 | loss: 4.87374 | failed:   73
batch:      48400 | loss: 5.11406 | failed:   73
batch:      48410 | loss: 5.05813 | failed:   73
batch:      48420 | loss: 5.08681 | failed:   73
batch:      48430 | loss: 5.23220 | failed:   73
batch:      48440 | loss: 5.25463 | failed:   73
batch:      48450 | loss: 5.17988 | failed:   73
batch:      48460 | loss: 5.09244 | failed:   73
batch:      48470 | loss: 5.15230 | failed:   73
batch:      48480 | loss: 5.18518 | failed:   73
batch:      48490 | loss: 5.20224 | failed:   73
batch:      48500 | loss: 5.09275 | failed:   73
batch:      48510 | loss: 4.98075 | failed:   73
batch:      48520 | loss: 5.25718 | failed:   73
batch:      48530 | loss: 5.06411 | failed:   73
batch:      48540 | loss: 5.16362 | failed:   73
batch:      48550 | loss: 5.13423 | failed:   73
batch:      48560 | loss: 5.20292 | failed:   73
batch:      48570 | loss: 4.92719 | failed:   73
batch:      48580 | loss: 4.99117 | failed:   73
batch:      48590 | loss: 5.03453 | failed:   73
batch:      48600 | loss: 5.11654 | failed:   73
batch:      48610 | loss: 5.04875 | failed:   73
batch:      48620 | loss: 5.22183 | failed:   73
batch:      48630 | loss: 5.21320 | failed:   73
batch:      48640 | loss: 5.25653 | failed:   73
batch:      48650 | loss: 5.24156 | failed:   73
batch:      48660 | loss: 5.14853 | failed:   73
batch:      48670 | loss: 5.20904 | failed:   73
batch:      48680 | loss: 4.96222 | failed:   73
batch:      48690 | loss: 5.27091 | failed:   73
batch:      48700 | loss: 5.22451 | failed:   73
batch:      48710 | loss: 5.23240 | failed:   73
batch:      48720 | loss: 4.81021 | failed:   73
batch:      48730 | loss: 5.17037 | failed:   73
batch:      48740 | loss: 5.12682 | failed:   73
batch:      48750 | loss: 5.04806 | failed:   73
batch:      48760 | loss: 5.09979 | failed:   73
batch:      48770 | loss: 5.21636 | failed:   73
batch:      48780 | loss: 5.03808 | failed:   73
batch:      48790 | loss: 5.21606 | failed:   73
batch:      48800 | loss: 5.06322 | failed:   73
batch:      48810 | loss: 5.11050 | failed:   73
batch:      48820 | loss: 5.14353 | failed:   73
batch:      48830 | loss: 5.19342 | failed:   73
batch:      48840 | loss: 5.19309 | failed:   73
batch:      48850 | loss: 5.07695 | failed:   73
batch:      48860 | loss: 5.03908 | failed:   73
batch:      48870 | loss: 5.24813 | failed:   73
batch:      48880 | loss: 5.00153 | failed:   73
batch:      48890 | loss: 5.20994 | failed:   73
batch:      48900 | loss: 5.16520 | failed:   73
batch:      48910 | loss: 5.09008 | failed:   73
batch:      48920 | loss: 5.16645 | failed:   73
batch:      48930 | loss: 5.17297 | failed:   73
batch:      48940 | loss: 5.08909 | failed:   73
batch:      48950 | loss: 5.20905 | failed:   73
batch:      48960 | loss: 5.12538 | failed:   73
batch:      48970 | loss: 5.17175 | failed:   73
batch:      48980 | loss: 5.07665 | failed:   73
batch:      48990 | loss: 5.25228 | failed:   73
batch:      49000 | loss: 5.03902 | failed:   73
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      49010 | loss: 5.14342 | failed:   73
batch:      49020 | loss: 5.18053 | failed:   73
batch:      49030 | loss: 5.12414 | failed:   73
batch:      49040 | loss: 5.13014 | failed:   73
batch:      49050 | loss: 5.15036 | failed:   73
batch:      49060 | loss: 5.09851 | failed:   73
batch:      49070 | loss: 5.15523 | failed:   73
batch:      49080 | loss: 5.28778 | failed:   73
batch:      49090 | loss: 5.23052 | failed:   73
batch:      49100 | loss: 5.20007 | failed:   73
batch:      49110 | loss: 5.05313 | failed:   73
batch:      49120 | loss: 5.07273 | failed:   73
batch:      49130 | loss: 5.13187 | failed:   73
batch:      49140 | loss: 5.18421 | failed:   73
batch:      49150 | loss: 5.02433 | failed:   73
batch:      49160 | loss: 5.02023 | failed:   73
batch:      49170 | loss: 5.06787 | failed:   73
batch:      49180 | loss: 5.07290 | failed:   73
batch:      49190 | loss: 5.00779 | failed:   73
batch:      49200 | loss: 5.08096 | failed:   73
batch:      49210 | loss: 5.14783 | failed:   73
batch:      49220 | loss: 4.91174 | failed:   73
batch:      49230 | loss: 5.09582 | failed:   73
batch:      49240 | loss: 5.16203 | failed:   73
batch:      49250 | loss: 5.10043 | failed:   73
batch:      49260 | loss: 4.97140 | failed:   73
batch:      49270 | loss: 5.07350 | failed:   73
batch:      49280 | loss: 5.12241 | failed:   73
batch:      49290 | loss: 5.10099 | failed:   73
batch:      49300 | loss: 5.14556 | failed:   73
batch:      49310 | loss: 5.21444 | failed:   73
batch:      49320 | loss: 5.19448 | failed:   73
batch:      49330 | loss: 5.16481 | failed:   73
batch:      49340 | loss: 5.21783 | failed:   73
batch:      49350 | loss: 5.07065 | failed:   73
batch:      49360 | loss: 5.06651 | failed:   73
batch:      49370 | loss: 5.08948 | failed:   73
batch:      49380 | loss: 5.04995 | failed:   73
batch:      49390 | loss: 5.05861 | failed:   73
batch:      49400 | loss: 4.93741 | failed:   73
batch:      49410 | loss: 5.13949 | failed:   73
batch:      49420 | loss: 5.11197 | failed:   73
batch:      49430 | loss: 5.04341 | failed:   73
batch:      49440 | loss: 5.12635 | failed:   73
batch:      49450 | loss: 5.19210 | failed:   73
batch:      49460 | loss: 5.04031 | failed:   73
batch:      49470 | loss: 5.02397 | failed:   73
batch:      49480 | loss: 5.13414 | failed:   73
batch:      49490 | loss: 5.16118 | failed:   73
batch:      49500 | loss: 5.11511 | failed:   73
batch:      49510 | loss: 5.13318 | failed:   73
batch:      49520 | loss: 5.04057 | failed:   73
batch:      49530 | loss: 5.12750 | failed:   73
batch:      49540 | loss: 5.11515 | failed:   73
batch:      49550 | loss: 5.17457 | failed:   73
batch:      49560 | loss: 5.22130 | failed:   73
batch:      49570 | loss: 5.08605 | failed:   73
batch:      49580 | loss: 5.27718 | failed:   73
batch:      49590 | loss: 5.24476 | failed:   73
batch:      49600 | loss: 5.19323 | failed:   73
batch:      49610 | loss: 5.16857 | failed:   73
batch:      49620 | loss: 5.00748 | failed:   73
batch:      49630 | loss: 5.09643 | failed:   73
batch:      49640 | loss: 5.12280 | failed:   73
batch:      49650 | loss: 5.00535 | failed:   73
batch:      49660 | loss: 5.15730 | failed:   73
batch:      49670 | loss: 5.14066 | failed:   73
batch:      49680 | loss: 5.12808 | failed:   73
batch:      49690 | loss: 5.04006 | failed:   73
batch:      49700 | loss: 5.22238 | failed:   73
batch:      49710 | loss: 5.24321 | failed:   73
batch:      49720 | loss: 5.21314 | failed:   73
batch:      49730 | loss: 5.16479 | failed:   73
batch:      49740 | loss: 4.89531 | failed:   73
batch:      49750 | loss: 5.13627 | failed:   73
batch:      49760 | loss: 5.08934 | failed:   73
batch:      49770 | loss: 5.16296 | failed:   73
batch:      49780 | loss: 5.22834 | failed:   73
batch:      49790 | loss: 5.18883 | failed:   73
batch:      49800 | loss: 5.07408 | failed:   73
batch:      49810 | loss: 5.10874 | failed:   73
batch:      49820 | loss: 5.25232 | failed:   73
batch:      49830 | loss: 5.15271 | failed:   73
batch:      49840 | loss: 5.10254 | failed:   73
batch:      49850 | loss: 5.22741 | failed:   73
batch:      49860 | loss: 5.21410 | failed:   73
batch:      49870 | loss: 5.18981 | failed:   73
batch:      49880 | loss: 5.24406 | failed:   73
batch:      49890 | loss: 5.17310 | failed:   73
batch:      49900 | loss: 4.97844 | failed:   73
batch:      49910 | loss: 5.15469 | failed:   73
batch:      49920 | loss: 5.13791 | failed:   73
batch:      49930 | loss: 5.18623 | failed:   73
batch:      49940 | loss: 5.23029 | failed:   73
batch:      49950 | loss: 5.26956 | failed:   73
batch:      49960 | loss: 5.23277 | failed:   73
batch:      49970 | loss: 5.16741 | failed:   73
batch:      49980 | loss: 5.09181 | failed:   73
batch:      49990 | loss: 5.11271 | failed:   73
batch:      50000 | loss: 4.56532 | failed:   73
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      50010 | loss: 5.22016 | failed:   73
batch:      50020 | loss: 5.01976 | failed:   73
batch:      50030 | loss: 5.22259 | failed:   73
batch:      50040 | loss: 5.11389 | failed:   73
batch:      50050 | loss: 5.11728 | failed:   73
batch:      50060 | loss: 5.17160 | failed:   73
batch:      50070 | loss: 5.13447 | failed:   73
batch:      50080 | loss: 3.95176 | failed:   73
batch:      50090 | loss: 5.25992 | failed:   73
batch:      50100 | loss: 5.04919 | failed:   73
batch:      50110 | loss: 5.18678 | failed:   73
batch:      50120 | loss: 5.19239 | failed:   73
batch:      50130 | loss: 5.19333 | failed:   73
batch:      50140 | loss: 5.22239 | failed:   73
batch:      50150 | loss: 5.16108 | failed:   73
batch:      50160 | loss: 5.17162 | failed:   73
batch:      50170 | loss: 5.17789 | failed:   73
batch:      50180 | loss: 5.00724 | failed:   73
batch:      50190 | loss: 5.12053 | failed:   73
batch:      50200 | loss: 5.12215 | failed:   73
batch:      50210 | loss: 5.06557 | failed:   73
batch:      50220 | loss: 5.10879 | failed:   73
batch:      50230 | loss: 5.15112 | failed:   73
batch:      50240 | loss: 5.23188 | failed:   73
batch:      50250 | loss: 4.96745 | failed:   73
batch:      50260 | loss: 5.12813 | failed:   73
batch:      50270 | loss: 5.24475 | failed:   73
batch:      50280 | loss: 5.21540 | failed:   73
batch:      50290 | loss: 5.13044 | failed:   73
batch:      50300 | loss: 5.17075 | failed:   73
batch:      50310 | loss: 5.14223 | failed:   73
batch:      50320 | loss: 5.19818 | failed:   73
batch:      50330 | loss: 5.11625 | failed:   73
batch:      50340 | loss: 5.19180 | failed:   73
batch:      50350 | loss: 5.13093 | failed:   73
batch:      50360 | loss: 5.05167 | failed:   73
batch:      50370 | loss: 5.10501 | failed:   73
batch:      50380 | loss: 5.19567 | failed:   73
batch:      50390 | loss: 5.00101 | failed:   73
batch:      50400 | loss: 5.02445 | failed:   73
batch:      50410 | loss: 5.13533 | failed:   73
batch:      50420 | loss: 5.17445 | failed:   73
batch:      50430 | loss: 5.05350 | failed:   73
batch:      50440 | loss: 5.26966 | failed:   73
batch:      50450 | loss: 5.29390 | failed:   73
batch:      50460 | loss: 5.22982 | failed:   73
batch:      50470 | loss: 5.21333 | failed:   73
batch:      50480 | loss: 5.02499 | failed:   73
batch:      50490 | loss: 4.91176 | failed:   73
batch:      50500 | loss: 5.13240 | failed:   73
batch:      50510 | loss: 4.96800 | failed:   73
batch:      50520 | loss: 5.07044 | failed:   73
batch:      50530 | loss: 5.21688 | failed:   73
batch:      50540 | loss: 5.17956 | failed:   73
batch:      50550 | loss: 5.19059 | failed:   73
batch:      50560 | loss: 5.21275 | failed:   73
batch:      50570 | loss: 5.10071 | failed:   73
batch:      50580 | loss: 5.11301 | failed:   73
batch:      50590 | loss: 4.90987 | failed:   73
batch:      50600 | loss: 5.15394 | failed:   73
batch:      50610 | loss: 5.22566 | failed:   73
batch:      50620 | loss: 5.16141 | failed:   73
batch:      50630 | loss: 4.87731 | failed:   73
batch:      50640 | loss: 5.19529 | failed:   73
batch:      50650 | loss: 5.24099 | failed:   73
batch:      50660 | loss: 5.20624 | failed:   73
batch:      50670 | loss: 5.01911 | failed:   73
batch:      50680 | loss: 5.08995 | failed:   73
batch:      50690 | loss: 5.11736 | failed:   73
batch:      50700 | loss: 4.96947 | failed:   73
batch:      50710 | loss: 5.17928 | failed:   73
batch:      50720 | loss: 5.12933 | failed:   73
batch:      50730 | loss: 5.05818 | failed:   73
batch:      50740 | loss: 5.20216 | failed:   73
batch:      50750 | loss: 5.14966 | failed:   73
batch:      50760 | loss: 5.08021 | failed:   73
batch:      50770 | loss: 4.94822 | failed:   73
batch:      50780 | loss: 5.08351 | failed:   73
batch:      50790 | loss: 5.17597 | failed:   73
batch:      50800 | loss: 5.00028 | failed:   73
batch:      50810 | loss: 5.20384 | failed:   73
batch:      50820 | loss: 5.12203 | failed:   73
batch:      50830 | loss: 5.19234 | failed:   73
batch:      50840 | loss: 5.06302 | failed:   73
batch:      50850 | loss: 5.06680 | failed:   73
batch:      50860 | loss: 5.17242 | failed:   73
batch:      50870 | loss: 5.20275 | failed:   73
batch:      50880 | loss: 5.21950 | failed:   73
batch:      50890 | loss: 5.20574 | failed:   73
batch:      50900 | loss: 5.15481 | failed:   73
batch:      50910 | loss: 5.08888 | failed:   73
batch:      50920 | loss: 5.29665 | failed:   73
batch:      50930 | loss: 5.19824 | failed:   73
batch:      50940 | loss: 5.15656 | failed:   73
batch:      50950 | loss: 5.19362 | failed:   73
batch:      50960 | loss: 5.18754 | failed:   73
batch:      50970 | loss: 5.24093 | failed:   73
batch:      50980 | loss: 4.91963 | failed:   73
batch:      50990 | loss: 5.15003 | failed:   73
batch:      51000 | loss: 5.10150 | failed:   73
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      51010 | loss: 5.01336 | failed:   73
batch:      51020 | loss: 5.15950 | failed:   73
batch:      51030 | loss: 5.24396 | failed:   73
batch:      51040 | loss: 5.22718 | failed:   73
batch:      51050 | loss: 5.14795 | failed:   73
batch:      51060 | loss: 5.14105 | failed:   73
batch:      51070 | loss: 5.16936 | failed:   73
batch:      51080 | loss: 5.09399 | failed:   73
batch:      51090 | loss: 5.18170 | failed:   73
batch:      51100 | loss: 5.27723 | failed:   73
batch:      51110 | loss: 5.18946 | failed:   73
batch:      51120 | loss: 5.00424 | failed:   73
batch:      51130 | loss: 5.09382 | failed:   73
batch:      51140 | loss: 5.02942 | failed:   73
batch:      51150 | loss: 5.10899 | failed:   73
batch:      51160 | loss: 5.19949 | failed:   73
batch:      51170 | loss: 5.19414 | failed:   73
batch:      51180 | loss: 5.14990 | failed:   73
batch:      51190 | loss: 5.18072 | failed:   73
batch:      51200 | loss: 5.09652 | failed:   73
batch:      51210 | loss: 4.86217 | failed:   73
batch:      51220 | loss: 5.08153 | failed:   73
batch:      51230 | loss: 5.13640 | failed:   73
batch:      51240 | loss: 5.20809 | failed:   73
batch:      51250 | loss: 5.12982 | failed:   73
batch:      51260 | loss: 5.24900 | failed:   73
batch:      51270 | loss: 5.16335 | failed:   73
batch:      51280 | loss: 5.20451 | failed:   73
batch:      51290 | loss: 5.13567 | failed:   73
batch:      51300 | loss: 5.16721 | failed:   73
batch:      51310 | loss: 5.24295 | failed:   73
batch:      51320 | loss: 5.25074 | failed:   73
batch:      51330 | loss: 5.14273 | failed:   73
batch:      51340 | loss: 5.09011 | failed:   73
batch:      51350 | loss: 4.95495 | failed:   73
batch:      51360 | loss: 5.14292 | failed:   73
batch:      51370 | loss: 5.13336 | failed:   73
batch:      51380 | loss: 5.21878 | failed:   73
batch:      51390 | loss: 5.14052 | failed:   73
batch:      51400 | loss: 5.15881 | failed:   73
batch:      51410 | loss: 5.15066 | failed:   73
batch:      51420 | loss: 5.19388 | failed:   73
batch:      51430 | loss: 5.14946 | failed:   73
batch:      51440 | loss: 5.22741 | failed:   73
batch:      51450 | loss: 5.04513 | failed:   73
batch:      51460 | loss: 4.90170 | failed:   73
batch:      51470 | loss: 4.90704 | failed:   73
batch:      51480 | loss: 5.18036 | failed:   73
batch:      51490 | loss: 5.21598 | failed:   73
batch:      51500 | loss: 5.17701 | failed:   73
batch:      51510 | loss: 5.09695 | failed:   73
batch:      51520 | loss: 5.13891 | failed:   73
batch:      51530 | loss: 5.04610 | failed:   73
batch:      51540 | loss: 5.16472 | failed:   73
batch:      51550 | loss: 5.18832 | failed:   73
batch:      51560 | loss: 5.09218 | failed:   73
batch:      51570 | loss: 5.22505 | failed:   73
batch:      51580 | loss: 5.11303 | failed:   73
batch:      51590 | loss: 5.17797 | failed:   73
batch:      51600 | loss: 5.06811 | failed:   73
batch:      51610 | loss: 5.20286 | failed:   73
batch:      51620 | loss: 5.04851 | failed:   73
batch:      51630 | loss: 5.27245 | failed:   73
batch:      51640 | loss: 5.19537 | failed:   73
batch:      51650 | loss: 5.22214 | failed:   73
batch:      51660 | loss: 5.21900 | failed:   73
batch:      51670 | loss: 5.24488 | failed:   73
batch:      51680 | loss: 5.03153 | failed:   73
batch:      51690 | loss: 5.19727 | failed:   73
batch:      51700 | loss: 5.16094 | failed:   73
batch:      51710 | loss: 5.10554 | failed:   73
batch:      51720 | loss: 5.12391 | failed:   73
batch:      51730 | loss: 5.14499 | failed:   73
batch:      51740 | loss: 5.12715 | failed:   73
batch:      51750 | loss: 5.19763 | failed:   73
batch:      51760 | loss: 4.99240 | failed:   73
batch:      51770 | loss: 5.23655 | failed:   73
batch:      51780 | loss: 5.21673 | failed:   73
batch:      51790 | loss: 5.03942 | failed:   73
batch:      51800 | loss: 5.18953 | failed:   73
batch:      51810 | loss: 5.13625 | failed:   73
batch:      51820 | loss: 5.25311 | failed:   73
batch:      51830 | loss: 5.23610 | failed:   73
batch:      51840 | loss: 5.06084 | failed:   73
batch:      51850 | loss: 5.24811 | failed:   73
batch:      51860 | loss: 5.24912 | failed:   73
batch:      51870 | loss: 5.03365 | failed:   73
batch:      51880 | loss: 5.12477 | failed:   73
batch:      51890 | loss: 5.19921 | failed:   73
batch:      51900 | loss: 5.24107 | failed:   73
batch:      51910 | loss: 5.22229 | failed:   73
batch:      51920 | loss: 5.23792 | failed:   73
batch:      51930 | loss: 5.19648 | failed:   73
batch:      51940 | loss: 5.20353 | failed:   73
batch:      51950 | loss: 5.14367 | failed:   73
batch:      51960 | loss: 4.93535 | failed:   73
batch:      51970 | loss: 5.26002 | failed:   73
batch:      51980 | loss: 5.13474 | failed:   73
batch:      51990 | loss: 5.11981 | failed:   73
batch:      52000 | loss: 5.15188 | failed:   73
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      52010 | loss: 5.17346 | failed:   73
batch:      52020 | loss: 5.12238 | failed:   73
batch:      52030 | loss: 5.09501 | failed:   73
batch:      52040 | loss: 5.11001 | failed:   73
batch:      52050 | loss: 5.22144 | failed:   73
batch:      52060 | loss: 5.19625 | failed:   73
batch:      52070 | loss: 4.89632 | failed:   73
batch:      52080 | loss: 5.16954 | failed:   73
batch:      52090 | loss: 5.27233 | failed:   73
batch:      52100 | loss: 5.27919 | failed:   73
batch:      52110 | loss: 5.21145 | failed:   73
batch:      52120 | loss: 5.12768 | failed:   73
batch:      52130 | loss: 5.18646 | failed:   73
batch:      52140 | loss: 5.05532 | failed:   73
batch:      52150 | loss: 5.12695 | failed:   73
batch:      52160 | loss: 5.15592 | failed:   73
batch:      52170 | loss: 5.07341 | failed:   73
batch:      52180 | loss: 5.19241 | failed:   73
batch:      52190 | loss: 5.11688 | failed:   73
batch:      52200 | loss: 5.19418 | failed:   73
batch:      52210 | loss: 5.16424 | failed:   73
batch:      52220 | loss: 5.14485 | failed:   73
batch:      52230 | loss: 5.08750 | failed:   73
batch:      52240 | loss: 5.17750 | failed:   73
batch:      52250 | loss: 5.22195 | failed:   73
batch:      52260 | loss: 4.74460 | failed:   73
batch:      52270 | loss: 5.10276 | failed:   73
batch:      52280 | loss: 5.20553 | failed:   73
batch:      52290 | loss: 5.06871 | failed:   73
batch:      52300 | loss: 5.05707 | failed:   73
batch:      52310 | loss: 5.16395 | failed:   73
batch:      52320 | loss: 5.17656 | failed:   73
batch:      52330 | loss: 5.17334 | failed:   73
batch:      52340 | loss: 5.07576 | failed:   73
batch:      52350 | loss: 5.16499 | failed:   73
batch:      52360 | loss: 5.03054 | failed:   73
batch:      52370 | loss: 4.93262 | failed:   73
batch:      52380 | loss: 5.07322 | failed:   73
batch:      52390 | loss: 5.19895 | failed:   73
batch:      52400 | loss: 5.02988 | failed:   73
batch:      52410 | loss: 5.12178 | failed:   73
batch:      52420 | loss: 5.18556 | failed:   73
batch:      52430 | loss: 5.15521 | failed:   73
batch:      52440 | loss: 5.02796 | failed:   73
batch:      52450 | loss: 5.00950 | failed:   73
batch:      52460 | loss: 5.26381 | failed:   73
batch:      52470 | loss: 5.21488 | failed:   73
batch:      52480 | loss: 5.22906 | failed:   73
batch:      52490 | loss: 5.00196 | failed:   73
batch:      52500 | loss: 4.80523 | failed:   73
batch:      52510 | loss: 4.74035 | failed:   73
batch:      52520 | loss: 4.60559 | failed:   73
batch:      52530 | loss: 4.45707 | failed:   73
batch:      52540 | loss: 4.38953 | failed:   73
batch:      52550 | loss: 4.38212 | failed:   73
batch:      52560 | loss: 4.34432 | failed:   73
batch:      52570 | loss: 4.18214 | failed:   73
batch:      52580 | loss: 4.15219 | failed:   73
batch:      52590 | loss: 4.27557 | failed:   73
batch:      52600 | loss: 4.61092 | failed:   73
batch:      52610 | loss: 4.18185 | failed:   73
batch:      52620 | loss: 4.19695 | failed:   73
batch:      52630 | loss: 4.08661 | failed:   73
batch:      52640 | loss: 4.12178 | failed:   73
batch:      52650 | loss: 5.33507 | failed:   73
batch:      52660 | loss: 5.15334 | failed:   73
batch:      52670 | loss: 5.09683 | failed:   73
batch:      52680 | loss: 5.09328 | failed:   73
batch:      52690 | loss: 5.05628 | failed:   73
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      52700 | loss: 5.01532 | failed:   73
batch:      52710 | loss: 4.92619 | failed:   73
batch:      52720 | loss: 4.98907 | failed:   75
batch:      52730 | loss: 5.08598 | failed:   75
batch:      52740 | loss: 5.19512 | failed:   75
batch:      52750 | loss: 5.07303 | failed:   75
batch:      52760 | loss: 5.09902 | failed:   75
batch:      52770 | loss: 5.09029 | failed:   75
batch:      52780 | loss: 5.28865 | failed:   75
batch:      52790 | loss: 5.21143 | failed:   75
batch:      52800 | loss: 4.97459 | failed:   75
batch:      52810 | loss: 5.20929 | failed:   75
batch:      52820 | loss: 5.09588 | failed:   75
batch:      52830 | loss: 5.24928 | failed:   75
batch:      52840 | loss: 5.18161 | failed:   75
batch:      52850 | loss: 5.18276 | failed:   75
batch:      52860 | loss: 5.12498 | failed:   75
batch:      52870 | loss: 5.29803 | failed:   75
batch:      52880 | loss: 5.15832 | failed:   75
batch:      52890 | loss: 5.01212 | failed:   75
batch:      52900 | loss: 5.10939 | failed:   75
batch:      52910 | loss: 5.06033 | failed:   75
batch:      52920 | loss: 5.27380 | failed:   75
batch:      52930 | loss: 5.28110 | failed:   75
batch:      52940 | loss: 5.05067 | failed:   75
batch:      52950 | loss: 5.25220 | failed:   75
batch:      52960 | loss: 5.19961 | failed:   75
batch:      52970 | loss: 5.19498 | failed:   75
batch:      52980 | loss: 5.07160 | failed:   75
batch:      52990 | loss: 5.14547 | failed:   75
batch:      53000 | loss: 5.10310 | failed:   75
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      53010 | loss: 5.09908 | failed:   75
batch:      53020 | loss: 5.06296 | failed:   75
batch:      53030 | loss: 5.08160 | failed:   75
batch:      53040 | loss: 5.15810 | failed:   75
batch:      53050 | loss: 5.18300 | failed:   75
batch:      53060 | loss: 5.24332 | failed:   75
batch:      53070 | loss: 5.20429 | failed:   75
batch:      53080 | loss: 5.07060 | failed:   75
batch:      53090 | loss: 5.11448 | failed:   75
batch:      53100 | loss: 5.23773 | failed:   75
batch:      53110 | loss: 5.17150 | failed:   75
batch:      53120 | loss: 5.14275 | failed:   75
batch:      53130 | loss: 5.27568 | failed:   75
batch:      53140 | loss: 5.18960 | failed:   75
batch:      53150 | loss: 5.08197 | failed:   75
batch:      53160 | loss: 4.92295 | failed:   75
batch:      53170 | loss: 5.20591 | failed:   75
batch:      53180 | loss: 5.18129 | failed:   75
batch:      53190 | loss: 5.17020 | failed:   75
batch:      53200 | loss: 5.07851 | failed:   75
batch:      53210 | loss: 5.13563 | failed:   75
batch:      53220 | loss: 5.26734 | failed:   75
batch:      53230 | loss: 5.26332 | failed:   75
batch:      53240 | loss: 5.22109 | failed:   75
batch:      53250 | loss: 5.16340 | failed:   75
batch:      53260 | loss: 5.19458 | failed:   75
batch:      53270 | loss: 5.24489 | failed:   75
batch:      53280 | loss: 5.19235 | failed:   75
batch:      53290 | loss: 5.19776 | failed:   75
batch:      53300 | loss: 5.23152 | failed:   75
batch:      53310 | loss: 5.17704 | failed:   75
batch:      53320 | loss: 5.06627 | failed:   75
batch:      53330 | loss: 5.20534 | failed:   75
batch:      53340 | loss: 5.08478 | failed:   75
batch:      53350 | loss: 5.21533 | failed:   75
batch:      53360 | loss: 5.15640 | failed:   75
batch:      53370 | loss: 5.14349 | failed:   75
batch:      53380 | loss: 5.06752 | failed:   75
batch:      53390 | loss: 5.16753 | failed:   75
batch:      53400 | loss: 5.18271 | failed:   75
batch:      53410 | loss: 5.06542 | failed:   75
batch:      53420 | loss: 5.18746 | failed:   75
batch:      53430 | loss: 5.14176 | failed:   75
batch:      53440 | loss: 5.22366 | failed:   75
batch:      53450 | loss: 5.14795 | failed:   75
batch:      53460 | loss: 5.12611 | failed:   75
batch:      53470 | loss: 5.13306 | failed:   75
batch:      53480 | loss: 5.28288 | failed:   75
batch:      53490 | loss: 5.28902 | failed:   75
batch:      53500 | loss: 5.27955 | failed:   75
batch:      53510 | loss: 5.21746 | failed:   75
batch:      53520 | loss: 5.16491 | failed:   75
batch:      53530 | loss: 5.16441 | failed:   75
batch:      53540 | loss: 5.17595 | failed:   75
batch:      53550 | loss: 5.16662 | failed:   75
batch:      53560 | loss: 5.15738 | failed:   75
batch:      53570 | loss: 5.22231 | failed:   75
batch:      53580 | loss: 4.84745 | failed:   75
batch:      53590 | loss: 5.13714 | failed:   75
batch:      53600 | loss: 5.08688 | failed:   75
batch:      53610 | loss: 5.12020 | failed:   75
batch:      53620 | loss: 5.31555 | failed:   75
batch:      53630 | loss: 5.10657 | failed:   75
batch:      53640 | loss: 5.18570 | failed:   75
batch:      53650 | loss: 5.03883 | failed:   75
batch:      53660 | loss: 5.14879 | failed:   75
batch:      53670 | loss: 5.17537 | failed:   75
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      53680 | loss: 5.16775 | failed:   75
batch:      53690 | loss: 5.14475 | failed:   75
batch:      53700 | loss: 5.22642 | failed:   76
batch:      53710 | loss: 5.17587 | failed:   76
batch:      53720 | loss: 5.20733 | failed:   76
batch:      53730 | loss: 5.18734 | failed:   76
batch:      53740 | loss: 5.19137 | failed:   76
batch:      53750 | loss: 5.21950 | failed:   76
batch:      53760 | loss: 5.23628 | failed:   76
batch:      53770 | loss: 5.15228 | failed:   76
batch:      53780 | loss: 5.26134 | failed:   76
batch:      53790 | loss: 5.03538 | failed:   76
batch:      53800 | loss: 5.24800 | failed:   76
batch:      53810 | loss: 5.19281 | failed:   76
batch:      53820 | loss: 5.24688 | failed:   76
batch:      53830 | loss: 5.16661 | failed:   76
batch:      53840 | loss: 5.20255 | failed:   76
batch:      53850 | loss: 5.13785 | failed:   76
batch:      53860 | loss: 5.05180 | failed:   76
batch:      53870 | loss: 5.17103 | failed:   76
batch:      53880 | loss: 5.23483 | failed:   76
batch:      53890 | loss: 5.11085 | failed:   76
batch:      53900 | loss: 5.25146 | failed:   76
batch:      53910 | loss: 5.15479 | failed:   76
batch:      53920 | loss: 5.18966 | failed:   76
batch:      53930 | loss: 5.19096 | failed:   76
batch:      53940 | loss: 5.21482 | failed:   76
batch:      53950 | loss: 5.10894 | failed:   76
batch:      53960 | loss: 5.12584 | failed:   76
batch:      53970 | loss: 5.18227 | failed:   76
batch:      53980 | loss: 5.26389 | failed:   76
batch:      53990 | loss: 5.02525 | failed:   76
batch:      54000 | loss: 5.13004 | failed:   76
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      54010 | loss: 5.25842 | failed:   76
batch:      54020 | loss: 5.24518 | failed:   76
batch:      54030 | loss: 5.20264 | failed:   76
batch:      54040 | loss: 5.21258 | failed:   76
batch:      54050 | loss: 5.16795 | failed:   76
batch:      54060 | loss: 5.14850 | failed:   76
batch:      54070 | loss: 5.11994 | failed:   76
batch:      54080 | loss: 5.14991 | failed:   76
batch:      54090 | loss: 5.28283 | failed:   76
batch:      54100 | loss: 5.13898 | failed:   76
batch:      54110 | loss: 5.14368 | failed:   76
batch:      54120 | loss: 5.13775 | failed:   76
batch:      54130 | loss: 5.07867 | failed:   76
batch:      54140 | loss: 5.10073 | failed:   76
batch:      54150 | loss: 5.05750 | failed:   76
batch:      54160 | loss: 4.93889 | failed:   76
batch:      54170 | loss: 5.18887 | failed:   76
batch:      54180 | loss: 5.18279 | failed:   76
batch:      54190 | loss: 5.09836 | failed:   76
batch:      54200 | loss: 5.21239 | failed:   76
batch:      54210 | loss: 5.14788 | failed:   76
batch:      54220 | loss: 5.16822 | failed:   76
batch:      54230 | loss: 5.21757 | failed:   76
batch:      54240 | loss: 5.08529 | failed:   76
batch:      54250 | loss: 5.15765 | failed:   76
batch:      54260 | loss: 5.11437 | failed:   76
batch:      54270 | loss: 5.13241 | failed:   76
batch:      54280 | loss: 5.24289 | failed:   76
batch:      54290 | loss: 5.07880 | failed:   76
batch:      54300 | loss: 5.10756 | failed:   76
batch:      54310 | loss: 5.14834 | failed:   76
batch:      54320 | loss: 5.11280 | failed:   76
batch:      54330 | loss: 5.03820 | failed:   76
batch:      54340 | loss: 5.20470 | failed:   76
batch:      54350 | loss: 5.25151 | failed:   76
batch:      54360 | loss: 5.19106 | failed:   76
batch:      54370 | loss: 5.14722 | failed:   76
batch:      54380 | loss: 5.03761 | failed:   76
batch:      54390 | loss: 5.14467 | failed:   76
batch:      54400 | loss: 5.21042 | failed:   76
batch:      54410 | loss: 5.27440 | failed:   76
batch:      54420 | loss: 5.02681 | failed:   76
batch:      54430 | loss: 5.05140 | failed:   76
batch:      54440 | loss: 5.22077 | failed:   76
batch:      54450 | loss: 5.12164 | failed:   76
batch:      54460 | loss: 5.05168 | failed:   76
batch:      54470 | loss: 5.24983 | failed:   76
batch:      54480 | loss: 5.19408 | failed:   76
batch:      54490 | loss: 5.12429 | failed:   76
batch:      54500 | loss: 5.14281 | failed:   76
batch:      54510 | loss: 5.21218 | failed:   76
batch:      54520 | loss: 5.06427 | failed:   76
batch:      54530 | loss: 5.11966 | failed:   76
batch:      54540 | loss: 5.09890 | failed:   76
batch:      54550 | loss: 5.17145 | failed:   76
batch:      54560 | loss: 5.15341 | failed:   76
batch:      54570 | loss: 5.24979 | failed:   76
batch:      54580 | loss: 5.22097 | failed:   76
batch:      54590 | loss: 5.08015 | failed:   76
batch:      54600 | loss: 5.11152 | failed:   76
batch:      54610 | loss: 5.27304 | failed:   76
batch:      54620 | loss: 4.97324 | failed:   76
batch:      54630 | loss: 5.12292 | failed:   76
batch:      54640 | loss: 5.14606 | failed:   76
batch:      54650 | loss: 5.10495 | failed:   76
batch:      54660 | loss: 5.15062 | failed:   76
batch:      54670 | loss: 5.03233 | failed:   76
batch:      54680 | loss: 4.93299 | failed:   76
batch:      54690 | loss: 5.20075 | failed:   76
batch:      54700 | loss: 5.05832 | failed:   76
batch:      54710 | loss: 5.21540 | failed:   76
batch:      54720 | loss: 5.13229 | failed:   76
batch:      54730 | loss: 5.17200 | failed:   76
batch:      54740 | loss: 5.22518 | failed:   76
batch:      54750 | loss: 5.08341 | failed:   76
batch:      54760 | loss: 5.17808 | failed:   76
batch:      54770 | loss: 5.18172 | failed:   76
batch:      54780 | loss: 5.21279 | failed:   76
batch:      54790 | loss: 4.96868 | failed:   76
batch:      54800 | loss: 5.12640 | failed:   76
batch:      54810 | loss: 5.11888 | failed:   76
batch:      54820 | loss: 5.25130 | failed:   76
batch:      54830 | loss: 5.11038 | failed:   76
batch:      54840 | loss: 5.19096 | failed:   76
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      54850 | loss: 5.07057 | failed:   76
batch:      54860 | loss: 5.17408 | failed:   76
batch:      54870 | loss: 5.14146 | failed:   77
batch:      54880 | loss: 5.21915 | failed:   77
batch:      54890 | loss: 5.23889 | failed:   77
batch:      54900 | loss: 5.15744 | failed:   77
batch:      54910 | loss: 5.23812 | failed:   77
batch:      54920 | loss: 5.11143 | failed:   77
batch:      54930 | loss: 5.20562 | failed:   77
batch:      54940 | loss: 5.11098 | failed:   77
batch:      54950 | loss: 5.12104 | failed:   77
batch:      54960 | loss: 5.12809 | failed:   77
batch:      54970 | loss: 5.21646 | failed:   77
batch:      54980 | loss: 5.08362 | failed:   77
batch:      54990 | loss: 5.00098 | failed:   77
batch:      55000 | loss: 5.12180 | failed:   77
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      55010 | loss: 5.10165 | failed:   77
batch:      55020 | loss: 5.13554 | failed:   77
batch:      55030 | loss: 5.15015 | failed:   77
batch:      55040 | loss: 5.15124 | failed:   77
batch:      55050 | loss: 5.15658 | failed:   77
batch:      55060 | loss: 5.10448 | failed:   77
batch:      55070 | loss: 5.07687 | failed:   77
batch:      55080 | loss: 4.99729 | failed:   77
batch:      55090 | loss: 5.18306 | failed:   77
batch:      55100 | loss: 5.19312 | failed:   77
batch:      55110 | loss: 5.20427 | failed:   77
batch:      55120 | loss: 5.14780 | failed:   77
batch:      55130 | loss: 5.19901 | failed:   77
batch:      55140 | loss: 5.09192 | failed:   77
batch:      55150 | loss: 5.20326 | failed:   77
batch:      55160 | loss: 5.16843 | failed:   77
batch:      55170 | loss: 5.22426 | failed:   77
batch:      55180 | loss: 5.15190 | failed:   77
batch:      55190 | loss: 4.96423 | failed:   77
batch:      55200 | loss: 5.25404 | failed:   77
batch:      55210 | loss: 5.14585 | failed:   77
batch:      55220 | loss: 5.09646 | failed:   77
batch:      55230 | loss: 5.12223 | failed:   77
batch:      55240 | loss: 5.23990 | failed:   77
batch:      55250 | loss: 5.16944 | failed:   77
batch:      55260 | loss: 5.13884 | failed:   77
batch:      55270 | loss: 5.08689 | failed:   77
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      55280 | loss: 5.22820 | failed:   77
batch:      55290 | loss: 5.10212 | failed:   77
batch:      55300 | loss: 5.22332 | failed:   78
batch:      55310 | loss: 5.13272 | failed:   78
batch:      55320 | loss: 5.14338 | failed:   78
batch:      55330 | loss: 5.05919 | failed:   78
batch:      55340 | loss: 5.16687 | failed:   78
batch:      55350 | loss: 5.13739 | failed:   78
batch:      55360 | loss: 5.09685 | failed:   78
batch:      55370 | loss: 5.20088 | failed:   78
batch:      55380 | loss: 5.20825 | failed:   78
batch:      55390 | loss: 5.09809 | failed:   78
batch:      55400 | loss: 5.17879 | failed:   78
batch:      55410 | loss: 5.24470 | failed:   78
batch:      55420 | loss: 5.15494 | failed:   78
batch:      55430 | loss: 5.11071 | failed:   78
batch:      55440 | loss: 5.25272 | failed:   78
batch:      55450 | loss: 5.13936 | failed:   78
batch:      55460 | loss: 5.10383 | failed:   78
batch:      55470 | loss: 5.16520 | failed:   78
batch:      55480 | loss: 5.12236 | failed:   78
batch:      55490 | loss: 5.20290 | failed:   78
batch:      55500 | loss: 5.08942 | failed:   78
batch:      55510 | loss: 5.19423 | failed:   78
batch:      55520 | loss: 5.23841 | failed:   78
batch:      55530 | loss: 5.25763 | failed:   78
batch:      55540 | loss: 5.10612 | failed:   78
batch:      55550 | loss: 5.17575 | failed:   78
batch:      55560 | loss: 5.19344 | failed:   78
batch:      55570 | loss: 5.13576 | failed:   78
batch:      55580 | loss: 5.13746 | failed:   78
batch:      55590 | loss: 5.18993 | failed:   78
batch:      55600 | loss: 5.13978 | failed:   78
batch:      55610 | loss: 4.87965 | failed:   78
batch:      55620 | loss: 5.22662 | failed:   78
batch:      55630 | loss: 4.89902 | failed:   78
batch:      55640 | loss: 5.12575 | failed:   78
batch:      55650 | loss: 5.15793 | failed:   78
batch:      55660 | loss: 5.24224 | failed:   78
batch:      55670 | loss: 4.89455 | failed:   78
batch:      55680 | loss: 5.26940 | failed:   78
batch:      55690 | loss: 5.26908 | failed:   78
batch:      55700 | loss: 5.18486 | failed:   78
batch:      55710 | loss: 4.92908 | failed:   78
batch:      55720 | loss: 5.15081 | failed:   78
batch:      55730 | loss: 5.15366 | failed:   78
batch:      55740 | loss: 5.22732 | failed:   78
batch:      55750 | loss: 5.29146 | failed:   78
batch:      55760 | loss: 5.22296 | failed:   78
batch:      55770 | loss: 5.05034 | failed:   78
batch:      55780 | loss: 5.07800 | failed:   78
batch:      55790 | loss: 4.56675 | failed:   78
batch:      55800 | loss: 5.16272 | failed:   78
batch:      55810 | loss: 5.20218 | failed:   78
batch:      55820 | loss: 5.22520 | failed:   78
batch:      55830 | loss: 5.19053 | failed:   78
batch:      55840 | loss: 5.09936 | failed:   78
batch:      55850 | loss: 4.95733 | failed:   78
batch:      55860 | loss: 4.96122 | failed:   78
batch:      55870 | loss: 5.16587 | failed:   78
batch:      55880 | loss: 5.28884 | failed:   78
batch:      55890 | loss: 5.15369 | failed:   78
batch:      55900 | loss: 5.12408 | failed:   78
batch:      55910 | loss: 5.11748 | failed:   78
batch:      55920 | loss: 5.22102 | failed:   78
batch:      55930 | loss: 5.29589 | failed:   78
batch:      55940 | loss: 5.28454 | failed:   78
batch:      55950 | loss: 5.22990 | failed:   78
batch:      55960 | loss: 5.18975 | failed:   78
batch:      55970 | loss: 5.04958 | failed:   78
batch:      55980 | loss: 5.19973 | failed:   78
batch:      55990 | loss: 5.13292 | failed:   78
batch:      56000 | loss: 5.23174 | failed:   78
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      56010 | loss: 5.21584 | failed:   78
batch:      56020 | loss: 5.22641 | failed:   78
batch:      56030 | loss: 5.18108 | failed:   78
batch:      56040 | loss: 5.21074 | failed:   78
batch:      56050 | loss: 5.21488 | failed:   78
batch:      56060 | loss: 5.17890 | failed:   78
batch:      56070 | loss: 5.21250 | failed:   78
batch:      56080 | loss: 5.23516 | failed:   78
batch:      56090 | loss: 5.30108 | failed:   78
batch:      56100 | loss: 5.16037 | failed:   78
batch:      56110 | loss: 5.08360 | failed:   78
batch:      56120 | loss: 5.14619 | failed:   78
batch:      56130 | loss: 5.18730 | failed:   78
batch:      56140 | loss: 5.12958 | failed:   78
batch:      56150 | loss: 5.07961 | failed:   78
batch:      56160 | loss: 4.12612 | failed:   78
batch:      56170 | loss: 5.22625 | failed:   78
batch:      56180 | loss: 5.09030 | failed:   78
batch:      56190 | loss: 5.11178 | failed:   78
batch:      56200 | loss: 5.21571 | failed:   78
batch:      56210 | loss: 5.17790 | failed:   78
batch:      56220 | loss: 5.19931 | failed:   78
batch:      56230 | loss: 5.21045 | failed:   78
batch:      56240 | loss: 5.20263 | failed:   78
batch:      56250 | loss: 5.23836 | failed:   78
batch:      56260 | loss: 5.16339 | failed:   78
batch:      56270 | loss: 5.16239 | failed:   78
batch:      56280 | loss: 5.14604 | failed:   78
batch:      56290 | loss: 5.21161 | failed:   78
batch:      56300 | loss: 5.21312 | failed:   78
batch:      56310 | loss: 5.15981 | failed:   78
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56320 | loss: 5.16520 | failed:   78
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56330 | loss: 5.21571 | failed:   78
batch:      56350 | loss: 5.14626 | failed:   85
batch:      56360 | loss: 5.14101 | failed:   85
batch:      56370 | loss: 5.09585 | failed:   85
batch:      56380 | loss: 5.06528 | failed:   85
batch:      56390 | loss: 5.07990 | failed:   85
batch:      56400 | loss: 5.13346 | failed:   85
batch:      56410 | loss: 5.06889 | failed:   85
batch:      56420 | loss: 4.33136 | failed:   85
batch:      56430 | loss: 5.19446 | failed:   85
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56440 | loss: 5.14257 | failed:   85
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56450 | loss: 5.02603 | failed:   85
batch:      56470 | loss: 5.13244 | failed:   93
batch:      56480 | loss: 5.23094 | failed:   93
batch:      56490 | loss: 5.16385 | failed:   93
batch:      56500 | loss: 5.15891 | failed:   93
batch:      56510 | loss: 5.15648 | failed:   93
batch:      56520 | loss: 5.14792 | failed:   93
batch:      56530 | loss: 5.17967 | failed:   93
batch:      56540 | loss: 5.14051 | failed:   93
batch:      56550 | loss: 5.19062 | failed:   93
batch:      56560 | loss: 5.06701 | failed:   93
batch:      56570 | loss: 5.06408 | failed:   93
batch:      56580 | loss: 5.14939 | failed:   93
batch:      56590 | loss: 5.15753 | failed:   93
batch:      56600 | loss: 5.04883 | failed:   93
batch:      56610 | loss: 5.25262 | failed:   93
batch:      56620 | loss: 5.25448 | failed:   93
batch:      56630 | loss: 5.16706 | failed:   93
batch:      56640 | loss: 5.28507 | failed:   93
batch:      56650 | loss: 5.20232 | failed:   93
batch:      56660 | loss: 4.97632 | failed:   93
batch:      56670 | loss: 5.12163 | failed:   93
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56680 | loss: 5.19159 | failed:   93
batch:      56690 | loss: 5.16899 | failed:   93
batch:      56700 | loss: 5.16215 | failed:   94
batch:      56710 | loss: 5.14994 | failed:   94
batch:      56720 | loss: 5.18759 | failed:   94
batch:      56730 | loss: 5.11487 | failed:   94
batch:      56740 | loss: 5.01780 | failed:   94
batch:      56750 | loss: 5.15186 | failed:   94
batch:      56760 | loss: 4.97008 | failed:   94
batch:      56770 | loss: 5.10424 | failed:   94
batch:      56780 | loss: 5.17350 | failed:   94
batch:      56790 | loss: 5.11985 | failed:   94
batch:      56800 | loss: 5.25964 | failed:   94
batch:      56810 | loss: 5.07170 | failed:   94
batch:      56820 | loss: 5.04382 | failed:   94
batch:      56830 | loss: 5.14839 | failed:   94
batch:      56840 | loss: 5.28021 | failed:   94
batch:      56850 | loss: 5.03841 | failed:   94
batch:      56860 | loss: 5.20004 | failed:   94
batch:      56870 | loss: 5.14080 | failed:   94
batch:      56880 | loss: 5.02298 | failed:   94
batch:      56890 | loss: 5.22050 | failed:   94
batch:      56900 | loss: 5.19879 | failed:   94
batch:      56910 | loss: 5.12255 | failed:   94
batch:      56920 | loss: 5.19442 | failed:   94
batch:      56930 | loss: 5.20647 | failed:   94
batch:      56940 | loss: 5.03303 | failed:   94
batch:      56950 | loss: 5.11551 | failed:   94
batch:      56960 | loss: 5.14968 | failed:   94
batch:      56970 | loss: 5.11651 | failed:   94
batch:      56980 | loss: 5.02707 | failed:   94
batch:      56990 | loss: 5.26640 | failed:   94
batch:      57000 | loss: 5.14381 | failed:   94
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      57010 | loss: 5.12238 | failed:   94
batch:      57020 | loss: 5.30652 | failed:   94
batch:      57030 | loss: 5.25686 | failed:   94
batch:      57040 | loss: 5.10954 | failed:   94
batch:      57050 | loss: 5.19062 | failed:   94
batch:      57060 | loss: 5.17361 | failed:   94
batch:      57070 | loss: 5.12626 | failed:   94
batch:      57080 | loss: 5.13048 | failed:   94
batch:      57090 | loss: 5.13544 | failed:   94
batch:      57100 | loss: 5.08496 | failed:   94
batch:      57110 | loss: 5.17087 | failed:   94
batch:      57120 | loss: 5.06467 | failed:   94
batch:      57130 | loss: 5.06229 | failed:   94
batch:      57140 | loss: 5.03315 | failed:   94
batch:      57150 | loss: 5.12218 | failed:   94
batch:      57160 | loss: 5.26638 | failed:   94
batch:      57170 | loss: 5.20069 | failed:   94
batch:      57180 | loss: 5.12710 | failed:   94
batch:      57190 | loss: 5.14173 | failed:   94
batch:      57200 | loss: 4.93797 | failed:   94
batch:      57210 | loss: 5.22834 | failed:   94
batch:      57220 | loss: 5.06829 | failed:   94
batch:      57230 | loss: 5.23132 | failed:   94
batch:      57240 | loss: 5.14565 | failed:   94
batch:      57250 | loss: 5.16923 | failed:   94
batch:      57260 | loss: 5.15421 | failed:   94
batch:      57270 | loss: 4.94311 | failed:   94
batch:      57280 | loss: 4.67795 | failed:   94
batch:      57290 | loss: 4.58009 | failed:   94
batch:      57300 | loss: 4.63899 | failed:   94
batch:      57310 | loss: 5.22815 | failed:   94
batch:      57320 | loss: 5.00027 | failed:   94
batch:      57330 | loss: 5.11541 | failed:   94
batch:      57340 | loss: 5.01084 | failed:   94
batch:      57350 | loss: 5.16346 | failed:   94
batch:      57360 | loss: 5.11752 | failed:   94
batch:      57370 | loss: 4.88984 | failed:   94
batch:      57380 | loss: 5.13803 | failed:   94
batch:      57390 | loss: 5.19635 | failed:   94
batch:      57400 | loss: 5.15941 | failed:   94
batch:      57410 | loss: 4.95859 | failed:   94
batch:      57420 | loss: 5.15921 | failed:   94
batch:      57430 | loss: 4.89875 | failed:   94
batch:      57440 | loss: 5.23969 | failed:   94
batch:      57450 | loss: 5.29236 | failed:   94
batch:      57460 | loss: 5.16864 | failed:   94
batch:      57470 | loss: 5.10875 | failed:   94
batch:      57480 | loss: 4.87939 | failed:   94
batch:      57490 | loss: 5.27402 | failed:   94
batch:      57500 | loss: 5.13551 | failed:   94
batch:      57510 | loss: 5.12306 | failed:   94
batch:      57520 | loss: 5.12319 | failed:   94
batch:      57530 | loss: 5.24442 | failed:   94
batch:      57540 | loss: 5.11545 | failed:   94
batch:      57550 | loss: 5.19903 | failed:   94
batch:      57560 | loss: 5.16918 | failed:   94
batch:      57570 | loss: 5.22751 | failed:   94
batch:      57580 | loss: 5.18487 | failed:   94
batch:      57590 | loss: 5.13815 | failed:   94
batch:      57600 | loss: 5.12383 | failed:   94
batch:      57610 | loss: 5.12955 | failed:   94
batch:      57620 | loss: 5.10973 | failed:   94
batch:      57630 | loss: 5.15151 | failed:   94
batch:      57640 | loss: 5.16451 | failed:   94
batch:      57650 | loss: 5.19750 | failed:   94
batch:      57660 | loss: 5.04630 | failed:   94
batch:      57670 | loss: 5.07102 | failed:   94
batch:      57680 | loss: 5.14422 | failed:   94
batch:      57690 | loss: 5.20133 | failed:   94
batch:      57700 | loss: 5.12471 | failed:   94
batch:      57710 | loss: 5.16391 | failed:   94
batch:      57720 | loss: 5.10906 | failed:   94
batch:      57730 | loss: 5.12187 | failed:   94
batch:      57740 | loss: 5.23943 | failed:   94
batch:      57750 | loss: 5.23231 | failed:   94
batch:      57760 | loss: 5.25798 | failed:   94
batch:      57770 | loss: 5.22718 | failed:   94
batch:      57780 | loss: 5.20737 | failed:   94
batch:      57790 | loss: 5.11852 | failed:   94
batch:      57800 | loss: 5.17661 | failed:   94
batch:      57810 | loss: 5.15524 | failed:   94
batch:      57820 | loss: 5.06648 | failed:   94
batch:      57830 | loss: 4.63443 | failed:   94
batch:      57840 | loss: 4.33776 | failed:   94
batch:      57850 | loss: 4.24089 | failed:   94
batch:      57860 | loss: 4.17410 | failed:   94
batch:      57870 | loss: 3.94912 | failed:   94
batch:      57880 | loss: 4.05278 | failed:   94
batch:      57890 | loss: 4.02477 | failed:   94
batch:      57900 | loss: 5.20419 | failed:   94
batch:      57910 | loss: 5.05043 | failed:   94
batch:      57920 | loss: 5.27968 | failed:   94
batch:      57930 | loss: 5.26413 | failed:   94
batch:      57940 | loss: 5.26791 | failed:   94
batch:      57950 | loss: 5.12425 | failed:   94
batch:      57960 | loss: 5.01786 | failed:   94
batch:      57970 | loss: 5.21471 | failed:   94
batch:      57980 | loss: 5.12552 | failed:   94
batch:      57990 | loss: 5.12875 | failed:   94
batch:      58000 | loss: 5.03308 | failed:   94
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      58010 | loss: 4.95806 | failed:   94
batch:      58020 | loss: 4.94354 | failed:   94
batch:      58030 | loss: 5.05216 | failed:   94
batch:      58040 | loss: 5.16517 | failed:   94
batch:      58050 | loss: 5.08952 | failed:   94
batch:      58060 | loss: 5.01876 | failed:   94
batch:      58070 | loss: 5.07768 | failed:   94
batch:      58080 | loss: 5.22064 | failed:   94
batch:      58090 | loss: 4.91970 | failed:   94
batch:      58100 | loss: 5.22731 | failed:   94
batch:      58110 | loss: 5.24211 | failed:   94
batch:      58120 | loss: 5.08212 | failed:   94
batch:      58130 | loss: 5.15642 | failed:   94
batch:      58140 | loss: 5.01387 | failed:   94
batch:      58150 | loss: 5.07208 | failed:   94
batch:      58160 | loss: 5.05394 | failed:   94
batch:      58170 | loss: 5.06012 | failed:   94
batch:      58180 | loss: 5.11173 | failed:   94
batch:      58190 | loss: 5.12825 | failed:   94
batch:      58200 | loss: 5.20569 | failed:   94
batch:      58210 | loss: 5.13305 | failed:   94
batch:      58220 | loss: 5.17920 | failed:   94
batch:      58230 | loss: 5.03394 | failed:   94
batch:      58240 | loss: 5.18265 | failed:   94
batch:      58250 | loss: 5.19009 | failed:   94
batch:      58260 | loss: 5.02887 | failed:   94
batch:      58270 | loss: 5.20963 | failed:   94
batch:      58280 | loss: 5.27617 | failed:   94
batch:      58290 | loss: 5.08205 | failed:   94
batch:      58300 | loss: 5.23793 | failed:   94
batch:      58310 | loss: 5.19052 | failed:   94
batch:      58320 | loss: 5.19168 | failed:   94
batch:      58330 | loss: 5.10221 | failed:   94
batch:      58340 | loss: 5.13652 | failed:   94
batch:      58350 | loss: 5.06781 | failed:   94
batch:      58360 | loss: 5.23803 | failed:   94
batch:      58370 | loss: 5.26813 | failed:   94
batch:      58380 | loss: 5.25998 | failed:   94
batch:      58390 | loss: 5.25752 | failed:   94
batch:      58400 | loss: 5.08953 | failed:   94
batch:      58410 | loss: 5.00605 | failed:   94
batch:      58420 | loss: 5.05028 | failed:   94
batch:      58430 | loss: 5.20965 | failed:   94
batch:      58440 | loss: 5.13969 | failed:   94
batch:      58450 | loss: 5.13244 | failed:   94
batch:      58460 | loss: 5.13861 | failed:   94
batch:      58470 | loss: 5.08834 | failed:   94
batch:      58480 | loss: 5.13627 | failed:   94
batch:      58490 | loss: 4.84055 | failed:   94
batch:      58500 | loss: 5.10862 | failed:   94
batch:      58510 | loss: 5.01103 | failed:   94
batch:      58520 | loss: 5.31188 | failed:   94
batch:      58530 | loss: 5.26964 | failed:   94
batch:      58540 | loss: 5.15121 | failed:   94
batch:      58550 | loss: 4.88768 | failed:   94
batch:      58560 | loss: 5.20670 | failed:   94
batch:      58570 | loss: 5.24192 | failed:   94
batch:      58580 | loss: 5.26351 | failed:   94
batch:      58590 | loss: 5.15471 | failed:   94
batch:      58600 | loss: 5.05711 | failed:   94
batch:      58610 | loss: 4.98337 | failed:   94
batch:      58620 | loss: 4.73199 | failed:   94
batch:      58630 | loss: 5.14515 | failed:   94
batch:      58640 | loss: 5.16789 | failed:   94
batch:      58650 | loss: 4.93862 | failed:   94
batch:      58660 | loss: 5.10429 | failed:   94
batch:      58670 | loss: 5.08406 | failed:   94
batch:      58680 | loss: 5.33310 | failed:   94
batch:      58690 | loss: 5.21753 | failed:   94
batch:      58700 | loss: 5.22578 | failed:   94
batch:      58710 | loss: 5.28578 | failed:   94
batch:      58720 | loss: 5.19394 | failed:   94
batch:      58730 | loss: 5.19062 | failed:   94
batch:      58740 | loss: 5.18789 | failed:   94
batch:      58750 | loss: 5.15134 | failed:   94
batch:      58760 | loss: 5.09253 | failed:   94
batch:      58770 | loss: 5.16552 | failed:   94
batch:      58780 | loss: 5.19584 | failed:   94
batch:      58790 | loss: 5.12764 | failed:   94
batch:      58800 | loss: 4.85537 | failed:   94
batch:      58810 | loss: 5.13788 | failed:   94
batch:      58820 | loss: 4.97248 | failed:   94
batch:      58830 | loss: 5.05633 | failed:   94
batch:      58840 | loss: 5.10584 | failed:   94
batch:      58850 | loss: 5.24330 | failed:   94
batch:      58860 | loss: 5.27742 | failed:   94
batch:      58870 | loss: 5.24859 | failed:   94
batch:      58880 | loss: 5.04838 | failed:   94
batch:      58890 | loss: 5.23490 | failed:   94
batch:      58900 | loss: 5.10563 | failed:   94
batch:      58910 | loss: 4.97385 | failed:   94
batch:      58920 | loss: 5.09904 | failed:   94
batch:      58930 | loss: 5.10294 | failed:   94
batch:      58940 | loss: 5.17774 | failed:   94
batch:      58950 | loss: 5.17384 | failed:   94
batch:      58960 | loss: 5.25183 | failed:   94
batch:      58970 | loss: 5.20201 | failed:   94
batch:      58980 | loss: 5.12574 | failed:   94
batch:      58990 | loss: 5.02334 | failed:   94
batch:      59000 | loss: 5.20794 | failed:   94
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      59010 | loss: 5.12614 | failed:   94
batch:      59020 | loss: 5.10411 | failed:   94
batch:      59030 | loss: 5.22795 | failed:   94
batch:      59040 | loss: 5.18394 | failed:   94
batch:      59050 | loss: 5.18067 | failed:   94
batch:      59060 | loss: 5.19368 | failed:   94
batch:      59070 | loss: 5.21271 | failed:   94
batch:      59080 | loss: 5.04032 | failed:   94
batch:      59090 | loss: 5.28768 | failed:   94
batch:      59100 | loss: 5.10563 | failed:   94
batch:      59110 | loss: 5.10814 | failed:   94
batch:      59120 | loss: 5.13905 | failed:   94
batch:      59130 | loss: 5.15947 | failed:   94
batch:      59140 | loss: 5.05706 | failed:   94
batch:      59150 | loss: 5.03567 | failed:   94
batch:      59160 | loss: 5.09203 | failed:   94
batch:      59170 | loss: 5.11016 | failed:   94
batch:      59180 | loss: 5.17834 | failed:   94
batch:      59190 | loss: 5.29103 | failed:   94
batch:      59200 | loss: 5.20931 | failed:   94
batch:      59210 | loss: 5.06097 | failed:   94
batch:      59220 | loss: 5.08134 | failed:   94
batch:      59230 | loss: 5.16539 | failed:   94
batch:      59240 | loss: 5.19561 | failed:   94
batch:      59250 | loss: 5.22083 | failed:   94
batch:      59260 | loss: 5.18063 | failed:   94
batch:      59270 | loss: 5.24617 | failed:   94
batch:      59280 | loss: 5.05708 | failed:   94
batch:      59290 | loss: 5.15519 | failed:   94
batch:      59300 | loss: 5.21163 | failed:   94
batch:      59310 | loss: 5.36735 | failed:   94
batch:      59320 | loss: 5.27762 | failed:   94
batch:      59330 | loss: 5.23288 | failed:   94
batch:      59340 | loss: 5.27411 | failed:   94
batch:      59350 | loss: 5.28900 | failed:   94
batch:      59360 | loss: 5.19615 | failed:   94
batch:      59370 | loss: 5.07722 | failed:   94
batch:      59380 | loss: 5.19199 | failed:   94
batch:      59390 | loss: 5.23117 | failed:   94
batch:      59400 | loss: 4.98629 | failed:   94
batch:      59410 | loss: 5.19606 | failed:   94
batch:      59420 | loss: 4.98442 | failed:   94
batch:      59430 | loss: 5.10712 | failed:   94
batch:      59440 | loss: 5.11683 | failed:   94
batch:      59450 | loss: 5.27862 | failed:   94
batch:      59460 | loss: 5.23426 | failed:   94
batch:      59470 | loss: 5.09822 | failed:   94
batch:      59480 | loss: 5.17359 | failed:   94
batch:      59490 | loss: 5.13323 | failed:   94
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      59500 | loss: 5.11674 | failed:   94
batch:      59510 | loss: 5.22088 | failed:   94
batch:      59520 | loss: 5.16270 | failed:   95
batch:      59530 | loss: 5.19185 | failed:   95
batch:      59540 | loss: 5.18504 | failed:   95
batch:      59550 | loss: 5.22964 | failed:   95
batch:      59560 | loss: 5.16477 | failed:   95
batch:      59570 | loss: 5.18776 | failed:   95
batch:      59580 | loss: 5.15370 | failed:   95
batch:      59590 | loss: 5.15861 | failed:   95
batch:      59600 | loss: 5.19295 | failed:   95
batch:      59610 | loss: 5.19558 | failed:   95
batch:      59620 | loss: 4.97275 | failed:   95
batch:      59630 | loss: 5.18328 | failed:   95
batch:      59640 | loss: 5.22391 | failed:   95
batch:      59650 | loss: 5.13811 | failed:   95
batch:      59660 | loss: 5.18997 | failed:   95
batch:      59670 | loss: 5.11123 | failed:   95
batch:      59680 | loss: 5.28019 | failed:   95
batch:      59690 | loss: 5.24643 | failed:   95
batch:      59700 | loss: 5.07464 | failed:   95
batch:      59710 | loss: 5.17444 | failed:   95
batch:      59720 | loss: 5.17420 | failed:   95
batch:      59730 | loss: 5.15805 | failed:   95
batch:      59740 | loss: 5.31888 | failed:   95
batch:      59750 | loss: 5.26516 | failed:   95
batch:      59760 | loss: 5.00424 | failed:   95
batch:      59770 | loss: 5.14448 | failed:   95
batch:      59780 | loss: 5.12260 | failed:   95
batch:      59790 | loss: 5.20626 | failed:   95
batch:      59800 | loss: 5.04683 | failed:   95
batch:      59810 | loss: 5.01807 | failed:   95
batch:      59820 | loss: 5.33776 | failed:   95
batch:      59830 | loss: 5.09951 | failed:   95
batch:      59840 | loss: 5.11840 | failed:   95
batch:      59850 | loss: 4.97899 | failed:   95
batch:      59860 | loss: 5.10341 | failed:   95
batch:      59870 | loss: 5.07609 | failed:   95
batch:      59880 | loss: 5.13530 | failed:   95
batch:      59890 | loss: 5.08261 | failed:   95
batch:      59900 | loss: 5.24801 | failed:   95
batch:      59910 | loss: 5.19668 | failed:   95
batch:      59920 | loss: 5.27216 | failed:   95
batch:      59930 | loss: 5.14887 | failed:   95
batch:      59940 | loss: 5.10079 | failed:   95
batch:      59950 | loss: 5.12387 | failed:   95
batch:      59960 | loss: 5.17500 | failed:   95
batch:      59970 | loss: 5.25960 | failed:   95
batch:      59980 | loss: 5.15167 | failed:   95
batch:      59990 | loss: 5.13665 | failed:   95
batch:      60000 | loss: 5.07142 | failed:   95
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      60010 | loss: 5.06483 | failed:   95
batch:      60020 | loss: 5.08186 | failed:   95
batch:      60030 | loss: 4.84447 | failed:   95
batch:      60040 | loss: 5.16169 | failed:   95
batch:      60050 | loss: 5.20854 | failed:   95
batch:      60060 | loss: 5.15935 | failed:   95
batch:      60070 | loss: 5.05883 | failed:   95
batch:      60080 | loss: 5.15551 | failed:   95
batch:      60090 | loss: 5.17312 | failed:   95
batch:      60100 | loss: 5.21105 | failed:   95
batch:      60110 | loss: 5.16543 | failed:   95
batch:      60120 | loss: 5.16000 | failed:   95
batch:      60130 | loss: 5.22267 | failed:   95
batch:      60140 | loss: 5.17764 | failed:   95
batch:      60150 | loss: 5.09915 | failed:   95
batch:      60160 | loss: 5.20527 | failed:   95
batch:      60170 | loss: 5.11179 | failed:   95
batch:      60180 | loss: 5.09788 | failed:   95
batch:      60190 | loss: 5.19991 | failed:   95
batch:      60200 | loss: 5.06334 | failed:   95
batch:      60210 | loss: 5.07920 | failed:   95
batch:      60220 | loss: 5.31121 | failed:   95
batch:      60230 | loss: 5.12707 | failed:   95
batch:      60240 | loss: 5.41019 | failed:   95
batch:      60250 | loss: 5.02731 | failed:   95
batch:      60260 | loss: 5.09257 | failed:   95
batch:      60270 | loss: 5.22155 | failed:   95
batch:      60280 | loss: 5.16288 | failed:   95
batch:      60290 | loss: 5.11922 | failed:   95
batch:      60300 | loss: 5.21798 | failed:   95
batch:      60310 | loss: 4.90221 | failed:   95
batch:      60320 | loss: 5.19891 | failed:   95
batch:      60330 | loss: 5.10428 | failed:   95
batch:      60340 | loss: 5.15345 | failed:   95
batch:      60350 | loss: 5.14883 | failed:   95
batch:      60360 | loss: 5.23170 | failed:   95
batch:      60370 | loss: 5.14473 | failed:   95
batch:      60380 | loss: 5.14856 | failed:   95
batch:      60390 | loss: 5.24754 | failed:   95
batch:      60400 | loss: 5.22769 | failed:   95
batch:      60410 | loss: 5.09802 | failed:   95
batch:      60420 | loss: 5.24182 | failed:   95
batch:      60430 | loss: 5.26119 | failed:   95
batch:      60440 | loss: 5.14786 | failed:   95
batch:      60450 | loss: 4.97751 | failed:   95
batch:      60460 | loss: 5.18344 | failed:   95
batch:      60470 | loss: 5.22700 | failed:   95
batch:      60480 | loss: 5.19909 | failed:   95
batch:      60490 | loss: 5.20593 | failed:   95
batch:      60500 | loss: 5.21383 | failed:   95
batch:      60510 | loss: 5.18135 | failed:   95
batch:      60520 | loss: 5.19183 | failed:   95
batch:      60530 | loss: 5.17338 | failed:   95
batch:      60540 | loss: 5.15255 | failed:   95
batch:      60550 | loss: 5.19161 | failed:   95
batch:      60560 | loss: 5.28922 | failed:   95
batch:      60570 | loss: 5.12067 | failed:   95
batch:      60580 | loss: 4.96722 | failed:   95
batch:      60590 | loss: 5.22586 | failed:   95
batch:      60600 | loss: 5.24994 | failed:   95
batch:      60610 | loss: 5.16667 | failed:   95
batch:      60620 | loss: 5.12309 | failed:   95
batch:      60630 | loss: 5.19063 | failed:   95
batch:      60640 | loss: 5.16097 | failed:   95
batch:      60650 | loss: 5.18724 | failed:   95
batch:      60660 | loss: 5.15768 | failed:   95
batch:      60670 | loss: 5.12275 | failed:   95
batch:      60680 | loss: 5.15616 | failed:   95
batch:      60690 | loss: 5.20022 | failed:   95
batch:      60700 | loss: 5.09803 | failed:   95
batch:      60710 | loss: 5.20121 | failed:   95
batch:      60720 | loss: 5.11514 | failed:   95
batch:      60730 | loss: 5.13297 | failed:   95
batch:      60740 | loss: 5.09455 | failed:   95
batch:      60750 | loss: 5.18694 | failed:   95
batch:      60760 | loss: 5.14968 | failed:   95
batch:      60770 | loss: 5.30214 | failed:   95
batch:      60780 | loss: 5.12556 | failed:   95
batch:      60790 | loss: 5.18559 | failed:   95
batch:      60800 | loss: 5.20233 | failed:   95
batch:      60810 | loss: 5.18587 | failed:   95
batch:      60820 | loss: 5.23143 | failed:   95
batch:      60830 | loss: 5.21212 | failed:   95
batch:      60840 | loss: 5.10138 | failed:   95
batch:      60850 | loss: 5.18337 | failed:   95
batch:      60860 | loss: 5.14316 | failed:   95
batch:      60870 | loss: 5.21706 | failed:   95
batch:      60880 | loss: 5.25904 | failed:   95
batch:      60890 | loss: 5.10192 | failed:   95
batch:      60900 | loss: 5.12736 | failed:   95
batch:      60910 | loss: 5.15912 | failed:   95
batch:      60920 | loss: 5.24331 | failed:   95
batch:      60930 | loss: 5.03206 | failed:   95
batch:      60940 | loss: 5.14999 | failed:   95
batch:      60950 | loss: 5.15375 | failed:   95
batch:      60960 | loss: 5.20318 | failed:   95
batch:      60970 | loss: 5.19937 | failed:   95
batch:      60980 | loss: 5.15032 | failed:   95
batch:      60990 | loss: 5.18112 | failed:   95
batch:      61000 | loss: 5.11960 | failed:   95
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      61010 | loss: 4.98928 | failed:   95
batch:      61020 | loss: 5.14965 | failed:   95
batch:      61030 | loss: 5.05815 | failed:   95
batch:      61040 | loss: 5.03920 | failed:   95
batch:      61050 | loss: 5.14226 | failed:   95
batch:      61060 | loss: 5.01904 | failed:   95
batch:      61070 | loss: 5.17377 | failed:   95
batch:      61080 | loss: 5.22417 | failed:   95
batch:      61090 | loss: 5.15758 | failed:   95
batch:      61100 | loss: 5.18668 | failed:   95
batch:      61110 | loss: 5.04635 | failed:   95
batch:      61120 | loss: 5.05220 | failed:   95
batch:      61130 | loss: 5.01220 | failed:   95
batch:      61140 | loss: 5.26804 | failed:   95
batch:      61150 | loss: 5.12846 | failed:   95
batch:      61160 | loss: 5.23735 | failed:   95
batch:      61170 | loss: 5.07004 | failed:   95
batch:      61180 | loss: 5.27795 | failed:   95
batch:      61190 | loss: 5.11617 | failed:   95
batch:      61200 | loss: 5.23728 | failed:   95
batch:      61210 | loss: 4.98353 | failed:   95
batch:      61220 | loss: 5.18770 | failed:   95
batch:      61230 | loss: 5.02478 | failed:   95
batch:      61240 | loss: 5.10503 | failed:   95
batch:      61250 | loss: 5.14982 | failed:   95
batch:      61260 | loss: 5.18895 | failed:   95
batch:      61270 | loss: 5.04884 | failed:   95
batch:      61280 | loss: 5.26823 | failed:   95
batch:      61290 | loss: 5.22389 | failed:   95
batch:      61300 | loss: 5.15877 | failed:   95
batch:      61310 | loss: 5.10331 | failed:   95
batch:      61320 | loss: 5.14250 | failed:   95
batch:      61330 | loss: 5.14489 | failed:   95
batch:      61340 | loss: 5.17888 | failed:   95
batch:      61350 | loss: 5.22768 | failed:   95
batch:      61360 | loss: 5.22223 | failed:   95
batch:      61370 | loss: 5.15566 | failed:   95
batch:      61380 | loss: 5.13180 | failed:   95
batch:      61390 | loss: 5.11676 | failed:   95
batch:      61400 | loss: 5.17405 | failed:   95
batch:      61410 | loss: 5.14314 | failed:   95
batch:      61420 | loss: 5.16568 | failed:   95
batch:      61430 | loss: 5.08819 | failed:   95
batch:      61440 | loss: 5.24716 | failed:   95
batch:      61450 | loss: 5.17376 | failed:   95
batch:      61460 | loss: 5.16749 | failed:   95
batch:      61470 | loss: 5.11021 | failed:   95
batch:      61480 | loss: 5.04709 | failed:   95
batch:      61490 | loss: 5.17475 | failed:   95
batch:      61500 | loss: 5.21854 | failed:   95
batch:      61510 | loss: 5.19422 | failed:   95
batch:      61520 | loss: 5.13020 | failed:   95
batch:      61530 | loss: 5.09225 | failed:   95
batch:      61540 | loss: 5.12784 | failed:   95
batch:      61550 | loss: 5.19860 | failed:   95
batch:      61560 | loss: 5.20503 | failed:   95
batch:      61570 | loss: 5.19945 | failed:   95
batch:      61580 | loss: 5.17887 | failed:   95
batch:      61590 | loss: 5.17980 | failed:   95
batch:      61600 | loss: 5.17572 | failed:   95
batch:      61610 | loss: 5.11403 | failed:   95
batch:      61620 | loss: 4.92452 | failed:   95
batch:      61630 | loss: 4.97089 | failed:   95
batch:      61640 | loss: 5.24716 | failed:   95
batch:      61650 | loss: 5.03997 | failed:   95
batch:      61660 | loss: 5.07982 | failed:   95
batch:      61670 | loss: 5.08139 | failed:   95
batch:      61680 | loss: 5.11229 | failed:   95
batch:      61690 | loss: 5.15681 | failed:   95
batch:      61700 | loss: 5.31370 | failed:   95
batch:      61710 | loss: 5.14521 | failed:   95
batch:      61720 | loss: 5.21559 | failed:   95
batch:      61730 | loss: 5.16806 | failed:   95
batch:      61740 | loss: 5.18362 | failed:   95
batch:      61750 | loss: 5.11293 | failed:   95
batch:      61760 | loss: 5.22643 | failed:   95
batch:      61770 | loss: 5.08095 | failed:   95
batch:      61780 | loss: 5.02403 | failed:   95
batch:      61790 | loss: 5.18215 | failed:   95
batch:      61800 | loss: 5.10613 | failed:   95
batch:      61810 | loss: 5.10366 | failed:   95
batch:      61820 | loss: 5.01361 | failed:   95
batch:      61830 | loss: 5.11633 | failed:   95
batch:      61840 | loss: 5.22109 | failed:   95
batch:      61850 | loss: 5.16106 | failed:   95
batch:      61860 | loss: 5.11008 | failed:   95
batch:      61870 | loss: 5.21114 | failed:   95
batch:      61880 | loss: 5.18969 | failed:   95
batch:      61890 | loss: 5.05179 | failed:   95
batch:      61900 | loss: 5.02868 | failed:   95
batch:      61910 | loss: 5.17600 | failed:   95
batch:      61920 | loss: 5.00369 | failed:   95
batch:      61930 | loss: 5.08982 | failed:   95
batch:      61940 | loss: 5.11074 | failed:   95
batch:      61950 | loss: 5.08804 | failed:   95
batch:      61960 | loss: 5.22086 | failed:   95
batch:      61970 | loss: 5.16805 | failed:   95
batch:      61980 | loss: 4.97035 | failed:   95
batch:      61990 | loss: 5.22523 | failed:   95
batch:      62000 | loss: 5.07738 | failed:   95
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      62010 | loss: 5.17630 | failed:   95
batch:      62020 | loss: 5.10382 | failed:   95
batch:      62030 | loss: 5.07122 | failed:   95
batch:      62040 | loss: 5.14413 | failed:   95
batch:      62050 | loss: 5.15263 | failed:   95
batch:      62060 | loss: 5.15828 | failed:   95
batch:      62070 | loss: 5.18614 | failed:   95
batch:      62080 | loss: 5.06923 | failed:   95
batch:      62090 | loss: 5.20404 | failed:   95
batch:      62100 | loss: 5.23908 | failed:   95
batch:      62110 | loss: 5.23022 | failed:   95
batch:      62120 | loss: 5.26695 | failed:   95
batch:      62130 | loss: 5.01060 | failed:   95
batch:      62140 | loss: 5.24801 | failed:   95
batch:      62150 | loss: 5.15617 | failed:   95
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      62160 | loss: 5.10047 | failed:   95
batch:      62170 | loss: 5.13036 | failed:   95
batch:      62180 | loss: 5.24526 | failed:   96
batch:      62190 | loss: 5.14470 | failed:   96
batch:      62200 | loss: 5.19207 | failed:   96
batch:      62210 | loss: 5.19611 | failed:   96
batch:      62220 | loss: 5.17138 | failed:   96
batch:      62230 | loss: 5.20598 | failed:   96
batch:      62240 | loss: 5.12351 | failed:   96
batch:      62250 | loss: 5.16553 | failed:   96
batch:      62260 | loss: 5.10941 | failed:   96
batch:      62270 | loss: 5.10905 | failed:   96
batch:      62280 | loss: 5.24046 | failed:   96
batch:      62290 | loss: 5.23439 | failed:   96
batch:      62300 | loss: 5.01412 | failed:   96
batch:      62310 | loss: 4.98660 | failed:   96
batch:      62320 | loss: 4.95067 | failed:   96
batch:      62330 | loss: 5.21626 | failed:   96
batch:      62340 | loss: 5.18829 | failed:   96
batch:      62350 | loss: 5.20662 | failed:   96
batch:      62360 | loss: 5.09364 | failed:   96
batch:      62370 | loss: 5.06133 | failed:   96
batch:      62380 | loss: 5.15220 | failed:   96
batch:      62390 | loss: 5.13777 | failed:   96
batch:      62400 | loss: 5.24025 | failed:   96
batch:      62410 | loss: 5.22158 | failed:   96
batch:      62420 | loss: 5.20423 | failed:   96
batch:      62430 | loss: 5.22960 | failed:   96
batch:      62440 | loss: 5.19684 | failed:   96
batch:      62450 | loss: 5.12446 | failed:   96
batch:      62460 | loss: 5.14442 | failed:   96
batch:      62470 | loss: 5.14564 | failed:   96
batch:      62480 | loss: 5.04217 | failed:   96
batch:      62490 | loss: 5.13272 | failed:   96
batch:      62500 | loss: 5.12712 | failed:   96
batch:      62510 | loss: 5.09552 | failed:   96
batch:      62520 | loss: 4.97182 | failed:   96
batch:      62530 | loss: 5.18115 | failed:   96
batch:      62540 | loss: 4.75697 | failed:   96
batch:      62550 | loss: 5.21433 | failed:   96
batch:      62560 | loss: 5.15722 | failed:   96
batch:      62570 | loss: 5.06879 | failed:   96
batch:      62580 | loss: 5.21856 | failed:   96
batch:      62590 | loss: 5.15742 | failed:   96
batch:      62600 | loss: 5.23064 | failed:   96
batch:      62610 | loss: 5.21942 | failed:   96
batch:      62620 | loss: 5.12417 | failed:   96
batch:      62630 | loss: 5.10354 | failed:   96
batch:      62640 | loss: 5.18305 | failed:   96
batch:      62650 | loss: 5.11581 | failed:   96
batch:      62660 | loss: 5.22159 | failed:   96
batch:      62670 | loss: 5.16954 | failed:   96
batch:      62680 | loss: 4.99912 | failed:   96
batch:      62690 | loss: 5.11768 | failed:   96
batch:      62700 | loss: 5.09335 | failed:   96
batch:      62710 | loss: 5.09030 | failed:   96
batch:      62720 | loss: 5.06820 | failed:   96
batch:      62730 | loss: 5.11585 | failed:   96
batch:      62740 | loss: 5.03200 | failed:   96
batch:      62750 | loss: 4.97633 | failed:   96
batch:      62760 | loss: 5.14232 | failed:   96
batch:      62770 | loss: 5.14214 | failed:   96
batch:      62780 | loss: 5.25475 | failed:   96
batch:      62790 | loss: 5.22067 | failed:   96
batch:      62800 | loss: 5.21829 | failed:   96
batch:      62810 | loss: 5.25818 | failed:   96
batch:      62820 | loss: 5.24437 | failed:   96
batch:      62830 | loss: 5.13862 | failed:   96
batch:      62840 | loss: 5.09680 | failed:   96
batch:      62850 | loss: 5.14924 | failed:   96
batch:      62860 | loss: 5.18572 | failed:   96
batch:      62870 | loss: 5.17658 | failed:   96
batch:      62880 | loss: 5.11976 | failed:   96
batch:      62890 | loss: 5.16724 | failed:   96
batch:      62900 | loss: 5.22263 | failed:   96
batch:      62910 | loss: 5.16390 | failed:   96
batch:      62920 | loss: 5.17040 | failed:   96
batch:      62930 | loss: 5.07752 | failed:   96
batch:      62940 | loss: 5.19541 | failed:   96
batch:      62950 | loss: 5.16734 | failed:   96
batch:      62960 | loss: 4.99542 | failed:   96
batch:      62970 | loss: 5.28878 | failed:   96
batch:      62980 | loss: 5.27176 | failed:   96
batch:      62990 | loss: 5.21893 | failed:   96
batch:      63000 | loss: 5.12173 | failed:   96
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      63010 | loss: 5.22165 | failed:   96
batch:      63020 | loss: 5.16161 | failed:   96
batch:      63030 | loss: 4.99904 | failed:   96
batch:      63040 | loss: 5.20202 | failed:   96
batch:      63050 | loss: 5.19871 | failed:   96
batch:      63060 | loss: 5.13222 | failed:   96
batch:      63070 | loss: 5.11029 | failed:   96
batch:      63080 | loss: 5.00091 | failed:   96
batch:      63090 | loss: 5.14456 | failed:   96
batch:      63100 | loss: 5.10904 | failed:   96
batch:      63110 | loss: 5.23821 | failed:   96
batch:      63120 | loss: 5.17926 | failed:   96
batch:      63130 | loss: 5.20728 | failed:   96
batch:      63140 | loss: 5.16522 | failed:   96
batch:      63150 | loss: 5.12923 | failed:   96
batch:      63160 | loss: 5.10360 | failed:   96
batch:      63170 | loss: 5.00912 | failed:   96
batch:      63180 | loss: 5.14305 | failed:   96
batch:      63190 | loss: 5.19963 | failed:   96
batch:      63200 | loss: 5.19784 | failed:   96
batch:      63210 | loss: 5.16785 | failed:   96
batch:      63220 | loss: 5.19961 | failed:   96
batch:      63230 | loss: 5.10411 | failed:   96
batch:      63240 | loss: 5.06298 | failed:   96
batch:      63250 | loss: 5.12933 | failed:   96
batch:      63260 | loss: 5.11073 | failed:   96
batch:      63270 | loss: 5.19272 | failed:   96
batch:      63280 | loss: 5.23051 | failed:   96
batch:      63290 | loss: 5.11540 | failed:   96
batch:      63300 | loss: 5.13378 | failed:   96
batch:      63310 | loss: 5.16752 | failed:   96
batch:      63320 | loss: 5.20668 | failed:   96
batch:      63330 | loss: 5.25422 | failed:   96
batch:      63340 | loss: 5.11988 | failed:   96
batch:      63350 | loss: 5.11725 | failed:   96
batch:      63360 | loss: 4.92921 | failed:   96
batch:      63370 | loss: 5.19932 | failed:   96
batch:      63380 | loss: 5.30395 | failed:   96
batch:      63390 | loss: 5.19002 | failed:   96
batch:      63400 | loss: 5.10762 | failed:   96
batch:      63410 | loss: 5.29545 | failed:   96
batch:      63420 | loss: 5.27309 | failed:   96
batch:      63430 | loss: 5.24700 | failed:   96
batch:      63440 | loss: 5.22027 | failed:   96
batch:      63450 | loss: 5.19546 | failed:   96
batch:      63460 | loss: 5.30604 | failed:   96
batch:      63470 | loss: 5.16567 | failed:   96
batch:      63480 | loss: 5.14060 | failed:   96
batch:      63490 | loss: 5.12654 | failed:   96
batch:      63500 | loss: 5.24476 | failed:   96
batch:      63510 | loss: 5.10559 | failed:   96
batch:      63520 | loss: 5.10531 | failed:   96
batch:      63530 | loss: 4.98834 | failed:   96
batch:      63540 | loss: 5.23963 | failed:   96
batch:      63550 | loss: 5.16252 | failed:   96
batch:      63560 | loss: 5.17630 | failed:   96
batch:      63570 | loss: 5.26084 | failed:   96
batch:      63580 | loss: 5.19590 | failed:   96
batch:      63590 | loss: 5.13299 | failed:   96
batch:      63600 | loss: 5.23438 | failed:   96
batch:      63610 | loss: 4.97493 | failed:   96
batch:      63620 | loss: 5.16548 | failed:   96
batch:      63630 | loss: 4.91338 | failed:   96
batch:      63640 | loss: 5.07023 | failed:   96
batch:      63650 | loss: 5.14697 | failed:   96
batch:      63660 | loss: 5.21526 | failed:   96
batch:      63670 | loss: 5.17307 | failed:   96
batch:      63680 | loss: 5.07832 | failed:   96
batch:      63690 | loss: 5.04718 | failed:   96
batch:      63700 | loss: 5.02350 | failed:   96
batch:      63710 | loss: 5.12502 | failed:   96
batch:      63720 | loss: 4.99780 | failed:   96
batch:      63730 | loss: 4.95126 | failed:   96
batch:      63740 | loss: 4.95491 | failed:   96
batch:      63750 | loss: 5.23069 | failed:   96
batch:      63760 | loss: 5.21431 | failed:   96
batch:      63770 | loss: 5.15340 | failed:   96
batch:      63780 | loss: 5.13736 | failed:   96
batch:      63790 | loss: 5.10939 | failed:   96
batch:      63800 | loss: 5.02719 | failed:   96
batch:      63810 | loss: 5.08599 | failed:   96
batch:      63820 | loss: 5.05975 | failed:   96
batch:      63830 | loss: 5.10264 | failed:   96
batch:      63840 | loss: 4.99554 | failed:   96
batch:      63850 | loss: 5.18116 | failed:   96
batch:      63860 | loss: 5.15514 | failed:   96
batch:      63870 | loss: 5.19365 | failed:   96
batch:      63880 | loss: 5.04860 | failed:   96
batch:      63890 | loss: 4.98175 | failed:   96
batch:      63900 | loss: 5.07377 | failed:   96
batch:      63910 | loss: 5.11397 | failed:   96
batch:      63920 | loss: 5.10394 | failed:   96
batch:      63930 | loss: 4.86943 | failed:   96
batch:      63940 | loss: 5.17924 | failed:   96
batch:      63950 | loss: 5.03918 | failed:   96
batch:      63960 | loss: 5.04965 | failed:   96
batch:      63970 | loss: 5.26177 | failed:   96
batch:      63980 | loss: 5.16036 | failed:   96
batch:      63990 | loss: 5.17756 | failed:   96
batch:      64000 | loss: 5.16799 | failed:   96
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      64010 | loss: 5.05622 | failed:   96
batch:      64020 | loss: 5.19495 | failed:   96
batch:      64030 | loss: 5.17827 | failed:   96
batch:      64040 | loss: 5.34366 | failed:   96
batch:      64050 | loss: 5.20955 | failed:   96
batch:      64060 | loss: 5.14342 | failed:   96
batch:      64070 | loss: 5.06256 | failed:   96
batch:      64080 | loss: 5.16525 | failed:   96
batch:      64090 | loss: 5.15090 | failed:   96
batch:      64100 | loss: 5.13180 | failed:   96
batch:      64110 | loss: 5.21366 | failed:   96
batch:      64120 | loss: 5.20027 | failed:   96
batch:      64130 | loss: 5.02571 | failed:   96
batch:      64140 | loss: 4.98093 | failed:   96
batch:      64150 | loss: 5.12328 | failed:   96
batch:      64160 | loss: 5.14696 | failed:   96
batch:      64170 | loss: 5.08642 | failed:   96
batch:      64180 | loss: 5.09869 | failed:   96
batch:      64190 | loss: 5.17141 | failed:   96
batch:      64200 | loss: 5.11463 | failed:   96
batch:      64210 | loss: 5.15846 | failed:   96
batch:      64220 | loss: 5.15662 | failed:   96
batch:      64230 | loss: 5.19331 | failed:   96
batch:      64240 | loss: 5.25243 | failed:   96
batch:      64250 | loss: 5.11756 | failed:   96
batch:      64260 | loss: 5.22639 | failed:   96
batch:      64270 | loss: 5.11394 | failed:   96
batch:      64280 | loss: 5.24413 | failed:   96
batch:      64290 | loss: 5.20618 | failed:   96
batch:      64300 | loss: 5.16808 | failed:   96
batch:      64310 | loss: 5.15745 | failed:   96
batch:      64320 | loss: 5.17653 | failed:   96
batch:      64330 | loss: 5.19205 | failed:   96
batch:      64340 | loss: 4.93484 | failed:   96
batch:      64350 | loss: 5.19481 | failed:   96
batch:      64360 | loss: 5.09629 | failed:   96
batch:      64370 | loss: 5.27674 | failed:   96
batch:      64380 | loss: 5.19733 | failed:   96
batch:      64390 | loss: 5.13776 | failed:   96
batch:      64400 | loss: 5.16842 | failed:   96
batch:      64410 | loss: 5.22980 | failed:   96
batch:      64420 | loss: 5.17854 | failed:   96
batch:      64430 | loss: 5.16237 | failed:   96
batch:      64440 | loss: 5.18592 | failed:   96
batch:      64450 | loss: 5.15843 | failed:   96
batch:      64460 | loss: 5.24013 | failed:   96
batch:      64470 | loss: 5.00611 | failed:   96
batch:      64480 | loss: 4.93857 | failed:   96
batch:      64490 | loss: 4.91127 | failed:   96
batch:      64500 | loss: 4.88917 | failed:   96
batch:      64510 | loss: 5.13588 | failed:   96
batch:      64520 | loss: 5.18666 | failed:   96
batch:      64530 | loss: 5.22757 | failed:   96
batch:      64540 | loss: 5.18936 | failed:   96
batch:      64550 | loss: 5.17542 | failed:   96
batch:      64560 | loss: 5.20812 | failed:   96
batch:      64570 | loss: 5.17777 | failed:   96
batch:      64580 | loss: 5.18766 | failed:   96
batch:      64590 | loss: 5.06529 | failed:   96
batch:      64600 | loss: 5.29785 | failed:   96
batch:      64610 | loss: 5.20484 | failed:   96
batch:      64620 | loss: 5.14463 | failed:   96
batch:      64630 | loss: 5.04578 | failed:   96
batch:      64640 | loss: 5.12973 | failed:   96
batch:      64650 | loss: 5.19040 | failed:   96
batch:      64660 | loss: 5.21975 | failed:   96
batch:      64670 | loss: 5.10917 | failed:   96
batch:      64680 | loss: 5.17586 | failed:   96
batch:      64690 | loss: 5.24793 | failed:   96
batch:      64700 | loss: 4.98190 | failed:   96
batch:      64710 | loss: 5.26085 | failed:   96
batch:      64720 | loss: 5.20579 | failed:   96
batch:      64730 | loss: 5.12781 | failed:   96
batch:      64740 | loss: 5.08438 | failed:   96
batch:      64750 | loss: 5.06016 | failed:   96
batch:      64760 | loss: 5.19704 | failed:   96
batch:      64770 | loss: 5.23216 | failed:   96
batch:      64780 | loss: 5.15513 | failed:   96
batch:      64790 | loss: 5.20462 | failed:   96
batch:      64800 | loss: 5.21275 | failed:   96
batch:      64810 | loss: 5.26889 | failed:   96
batch:      64820 | loss: 5.21588 | failed:   96
batch:      64830 | loss: 5.24017 | failed:   96
batch:      64840 | loss: 5.10705 | failed:   96
batch:      64850 | loss: 4.97016 | failed:   96
batch:      64860 | loss: 5.14661 | failed:   96
batch:      64870 | loss: 5.18453 | failed:   96
batch:      64880 | loss: 5.10213 | failed:   96
batch:      64890 | loss: 5.11418 | failed:   96
batch:      64900 | loss: 5.13334 | failed:   96
batch:      64910 | loss: 5.04564 | failed:   96
batch:      64920 | loss: 5.04396 | failed:   96
batch:      64930 | loss: 4.68744 | failed:   96
batch:      64940 | loss: 5.19332 | failed:   96
batch:      64950 | loss: 5.02826 | failed:   96
batch:      64960 | loss: 5.20001 | failed:   96
batch:      64970 | loss: 5.18800 | failed:   96
batch:      64980 | loss: 5.12187 | failed:   96
batch:      64990 | loss: 4.97624 | failed:   96
batch:      65000 | loss: 5.13357 | failed:   96
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      65010 | loss: 5.23607 | failed:   96
batch:      65020 | loss: 5.16134 | failed:   96
batch:      65030 | loss: 5.16000 | failed:   96
batch:      65040 | loss: 5.12242 | failed:   96
batch:      65050 | loss: 5.12937 | failed:   96
batch:      65060 | loss: 5.21321 | failed:   96
batch:      65070 | loss: 5.15145 | failed:   96
batch:      65080 | loss: 5.16228 | failed:   96
batch:      65090 | loss: 5.19700 | failed:   96
batch:      65100 | loss: 5.10785 | failed:   96
batch:      65110 | loss: 5.12283 | failed:   96
batch:      65120 | loss: 5.00743 | failed:   96
batch:      65130 | loss: 5.18222 | failed:   96
batch:      65140 | loss: 5.10905 | failed:   96
batch:      65150 | loss: 5.15166 | failed:   96
batch:      65160 | loss: 5.12508 | failed:   96
batch:      65170 | loss: 5.07069 | failed:   96
batch:      65180 | loss: 5.10148 | failed:   96
batch:      65190 | loss: 5.16518 | failed:   96
batch:      65200 | loss: 5.07626 | failed:   96
batch:      65210 | loss: 5.14306 | failed:   96
batch:      65220 | loss: 5.11644 | failed:   96
batch:      65230 | loss: 5.08816 | failed:   96
batch:      65240 | loss: 5.09856 | failed:   96
batch:      65250 | loss: 5.15332 | failed:   96
batch:      65260 | loss: 5.29235 | failed:   96
batch:      65270 | loss: 4.99440 | failed:   96
batch:      65280 | loss: 5.02885 | failed:   96
batch:      65290 | loss: 5.20100 | failed:   96
batch:      65300 | loss: 5.08300 | failed:   96
batch:      65310 | loss: 5.09842 | failed:   96
batch:      65320 | loss: 4.94591 | failed:   96
batch:      65330 | loss: 5.08656 | failed:   96
batch:      65340 | loss: 5.12382 | failed:   96
batch:      65350 | loss: 5.18488 | failed:   96
batch:      65360 | loss: 5.16297 | failed:   96
batch:      65370 | loss: 5.25808 | failed:   96
batch:      65380 | loss: 5.21525 | failed:   96
batch:      65390 | loss: 5.01822 | failed:   96
batch:      65400 | loss: 5.14809 | failed:   96
batch:      65410 | loss: 5.09443 | failed:   96
batch:      65420 | loss: 5.05191 | failed:   96
batch:      65430 | loss: 5.10466 | failed:   96
batch:      65440 | loss: 5.23620 | failed:   96
batch:      65450 | loss: 5.12277 | failed:   96
batch:      65460 | loss: 5.02524 | failed:   96
batch:      65470 | loss: 5.17699 | failed:   96
batch:      65480 | loss: 5.16907 | failed:   96
batch:      65490 | loss: 5.03212 | failed:   96
batch:      65500 | loss: 4.92880 | failed:   96
batch:      65510 | loss: 5.36641 | failed:   96
batch:      65520 | loss: 5.19369 | failed:   96
batch:      65530 | loss: 5.17810 | failed:   96
batch:      65540 | loss: 5.19659 | failed:   96
batch:      65550 | loss: 5.21872 | failed:   96
batch:      65560 | loss: 5.16283 | failed:   96
batch:      65570 | loss: 5.17822 | failed:   96
batch:      65580 | loss: 5.05169 | failed:   96
batch:      65590 | loss: 5.16932 | failed:   96
batch:      65600 | loss: 5.15234 | failed:   96
batch:      65610 | loss: 5.18625 | failed:   96
batch:      65620 | loss: 5.28181 | failed:   96
batch:      65630 | loss: 5.20159 | failed:   96
batch:      65640 | loss: 5.24048 | failed:   96
batch:      65650 | loss: 5.11711 | failed:   96
batch:      65660 | loss: 5.23911 | failed:   96
batch:      65670 | loss: 5.22271 | failed:   96
batch:      65680 | loss: 5.13351 | failed:   96
batch:      65690 | loss: 4.98061 | failed:   96
batch:      65700 | loss: 5.23131 | failed:   96
batch:      65710 | loss: 5.14905 | failed:   96
batch:      65720 | loss: 5.27851 | failed:   96
batch:      65730 | loss: 5.21162 | failed:   96
batch:      65740 | loss: 5.17007 | failed:   96
batch:      65750 | loss: 5.23887 | failed:   96
batch:      65760 | loss: 5.18964 | failed:   96
batch:      65770 | loss: 5.10849 | failed:   96
batch:      65780 | loss: 5.22088 | failed:   96
batch:      65790 | loss: 5.11603 | failed:   96
batch:      65800 | loss: 4.96378 | failed:   96
batch:      65810 | loss: 5.18109 | failed:   96
batch:      65820 | loss: 5.14274 | failed:   96
batch:      65830 | loss: 5.15413 | failed:   96
batch:      65840 | loss: 5.16721 | failed:   96
batch:      65850 | loss: 5.19187 | failed:   96
batch:      65860 | loss: 5.20601 | failed:   96
batch:      65870 | loss: 5.14450 | failed:   96
batch:      65880 | loss: 5.15674 | failed:   96
batch:      65890 | loss: 5.15698 | failed:   96
batch:      65900 | loss: 5.13612 | failed:   96
batch:      65910 | loss: 4.99010 | failed:   96
batch:      65920 | loss: 5.02625 | failed:   96
batch:      65930 | loss: 5.02851 | failed:   96
batch:      65940 | loss: 5.10892 | failed:   96
batch:      65950 | loss: 5.18110 | failed:   96
batch:      65960 | loss: 5.18677 | failed:   96
batch:      65970 | loss: 5.24788 | failed:   96
batch:      65980 | loss: 5.18760 | failed:   96
batch:      65990 | loss: 5.36557 | failed:   96
batch:      66000 | loss: 5.24059 | failed:   96
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      66010 | loss: 5.23249 | failed:   96
batch:      66020 | loss: 5.11122 | failed:   96
batch:      66030 | loss: 5.16977 | failed:   96
batch:      66040 | loss: 5.19314 | failed:   96
batch:      66050 | loss: 5.12129 | failed:   96
batch:      66060 | loss: 5.17268 | failed:   96
batch:      66070 | loss: 5.22205 | failed:   96
batch:      66080 | loss: 5.04798 | failed:   96
batch:      66090 | loss: 5.10449 | failed:   96
batch:      66100 | loss: 5.20085 | failed:   96
batch:      66110 | loss: 5.15420 | failed:   96
batch:      66120 | loss: 5.25328 | failed:   96
batch:      66130 | loss: 5.20513 | failed:   96
batch:      66140 | loss: 5.19393 | failed:   96
batch:      66150 | loss: 5.19231 | failed:   96
batch:      66160 | loss: 5.12332 | failed:   96
batch:      66170 | loss: 5.09920 | failed:   96
batch:      66180 | loss: 5.21647 | failed:   96
batch:      66190 | loss: 5.16003 | failed:   96
batch:      66200 | loss: 5.12563 | failed:   96
batch:      66210 | loss: 5.02508 | failed:   96
batch:      66220 | loss: 5.15194 | failed:   96
batch:      66230 | loss: 5.03776 | failed:   96
batch:      66240 | loss: 5.13696 | failed:   96
batch:      66250 | loss: 5.15220 | failed:   96
batch:      66260 | loss: 5.20756 | failed:   96
batch:      66270 | loss: 5.16558 | failed:   96
batch:      66280 | loss: 5.15089 | failed:   96
batch:      66290 | loss: 5.09169 | failed:   96
batch:      66300 | loss: 5.14746 | failed:   96
batch:      66310 | loss: 4.83126 | failed:   96
batch:      66320 | loss: 5.22368 | failed:   96
batch:      66330 | loss: 5.04361 | failed:   96
batch:      66340 | loss: 5.18797 | failed:   96
batch:      66350 | loss: 5.11635 | failed:   96
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      66360 | loss: 5.20028 | failed:   96
batch:      66370 | loss: 5.20865 | failed:   96
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      66380 | loss: 5.21695 | failed:   97
batch:      66390 | loss: 5.22860 | failed:   97
batch:      66400 | loss: 5.12149 | failed:   98
batch:      66410 | loss: 5.20476 | failed:   98
batch:      66420 | loss: 5.15424 | failed:   98
batch:      66430 | loss: 5.09592 | failed:   98
batch:      66440 | loss: 5.09419 | failed:   98
batch:      66450 | loss: 5.09523 | failed:   98
batch:      66460 | loss: 5.20298 | failed:   98
batch:      66470 | loss: 5.11861 | failed:   98
batch:      66480 | loss: 5.13509 | failed:   98
batch:      66490 | loss: 4.84993 | failed:   98
batch:      66500 | loss: 5.08400 | failed:   98
batch:      66510 | loss: 4.99393 | failed:   98
batch:      66520 | loss: 5.16559 | failed:   98
batch:      66530 | loss: 5.14227 | failed:   98
batch:      66540 | loss: 5.30601 | failed:   98
batch:      66550 | loss: 5.08686 | failed:   98
batch:      66560 | loss: 4.94589 | failed:   98
batch:      66570 | loss: 5.07397 | failed:   98
batch:      66580 | loss: 5.15104 | failed:   98
batch:      66590 | loss: 5.04554 | failed:   98
batch:      66600 | loss: 5.06658 | failed:   98
batch:      66610 | loss: 5.01138 | failed:   98
batch:      66620 | loss: 4.99328 | failed:   98
batch:      66630 | loss: 4.78134 | failed:   98
batch:      66640 | loss: 5.15381 | failed:   98
batch:      66650 | loss: 5.06123 | failed:   98
batch:      66660 | loss: 5.25750 | failed:   98
batch:      66670 | loss: 5.08430 | failed:   98
batch:      66680 | loss: 5.06510 | failed:   98
batch:      66690 | loss: 5.17862 | failed:   98
batch:      66700 | loss: 5.18157 | failed:   98
batch:      66710 | loss: 5.18042 | failed:   98
batch:      66720 | loss: 5.04650 | failed:   98
batch:      66730 | loss: 5.05235 | failed:   98
batch:      66740 | loss: 5.05429 | failed:   98
batch:      66750 | loss: 5.22886 | failed:   98
batch:      66760 | loss: 5.23536 | failed:   98
batch:      66770 | loss: 5.25358 | failed:   98
batch:      66780 | loss: 5.23981 | failed:   98
batch:      66790 | loss: 5.18090 | failed:   98
batch:      66800 | loss: 5.18237 | failed:   98
batch:      66810 | loss: 5.26042 | failed:   98
batch:      66820 | loss: 5.10130 | failed:   98
batch:      66830 | loss: 5.05804 | failed:   98
batch:      66840 | loss: 5.11706 | failed:   98
batch:      66850 | loss: 4.86456 | failed:   98
batch:      66860 | loss: 5.10052 | failed:   98
batch:      66870 | loss: 5.21930 | failed:   98
batch:      66880 | loss: 5.19012 | failed:   98
batch:      66890 | loss: 5.06855 | failed:   98
batch:      66900 | loss: 5.17244 | failed:   98
batch:      66910 | loss: 5.10398 | failed:   98
batch:      66920 | loss: 5.15604 | failed:   98
batch:      66930 | loss: 5.21538 | failed:   98
batch:      66940 | loss: 5.17227 | failed:   98
batch:      66950 | loss: 5.18260 | failed:   98
batch:      66960 | loss: 5.13830 | failed:   98
batch:      66970 | loss: 5.25981 | failed:   98
batch:      66980 | loss: 5.24664 | failed:   98
batch:      66990 | loss: 5.16509 | failed:   98
batch:      67000 | loss: 5.05052 | failed:   98
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      67010 | loss: 5.10451 | failed:   98
batch:      67020 | loss: 5.05157 | failed:   98
batch:      67030 | loss: 5.19952 | failed:   98
batch:      67040 | loss: 5.09677 | failed:   98
batch:      67050 | loss: 5.10635 | failed:   98
batch:      67060 | loss: 5.16406 | failed:   98
batch:      67070 | loss: 5.14445 | failed:   98
batch:      67080 | loss: 5.19780 | failed:   98
batch:      67090 | loss: 5.23518 | failed:   98
batch:      67100 | loss: 5.26762 | failed:   98
batch:      67110 | loss: 5.06397 | failed:   98
batch:      67120 | loss: 5.15724 | failed:   98
batch:      67130 | loss: 5.11757 | failed:   98
batch:      67140 | loss: 5.10095 | failed:   98
batch:      67150 | loss: 5.21328 | failed:   98
batch:      67160 | loss: 5.05137 | failed:   98
batch:      67170 | loss: 5.22352 | failed:   98
batch:      67180 | loss: 5.25353 | failed:   98
batch:      67190 | loss: 5.14963 | failed:   98
batch:      67200 | loss: 5.14865 | failed:   98
batch:      67210 | loss: 5.20210 | failed:   98
batch:      67220 | loss: 5.07171 | failed:   98
batch:      67230 | loss: 5.20708 | failed:   98
batch:      67240 | loss: 5.13468 | failed:   98
batch:      67250 | loss: 4.86254 | failed:   98
batch:      67260 | loss: 5.05944 | failed:   98
batch:      67270 | loss: 5.07634 | failed:   98
batch:      67280 | loss: 5.12411 | failed:   98
batch:      67290 | loss: 5.25874 | failed:   98
batch:      67300 | loss: 5.10985 | failed:   98
batch:      67310 | loss: 5.00840 | failed:   98
batch:      67320 | loss: 5.12260 | failed:   98
batch:      67330 | loss: 5.06389 | failed:   98
batch:      67340 | loss: 5.12472 | failed:   98
batch:      67350 | loss: 5.19518 | failed:   98
batch:      67360 | loss: 5.18125 | failed:   98
batch:      67370 | loss: 4.90170 | failed:   98
batch:      67380 | loss: 5.15658 | failed:   98
batch:      67390 | loss: 5.24552 | failed:   98
batch:      67400 | loss: 5.16823 | failed:   98
batch:      67410 | loss: 5.11662 | failed:   98
batch:      67420 | loss: 4.89466 | failed:   98
batch:      67430 | loss: 5.10204 | failed:   98
batch:      67440 | loss: 5.13106 | failed:   98
batch:      67450 | loss: 5.21243 | failed:   98
batch:      67460 | loss: 5.18181 | failed:   98
batch:      67470 | loss: 5.29907 | failed:   98
batch:      67480 | loss: 5.13697 | failed:   98
batch:      67490 | loss: 5.03782 | failed:   98
batch:      67500 | loss: 5.08823 | failed:   98
batch:      67510 | loss: 5.15702 | failed:   98
batch:      67520 | loss: 5.10324 | failed:   98
batch:      67530 | loss: 5.20116 | failed:   98
batch:      67540 | loss: 5.10526 | failed:   98
batch:      67550 | loss: 5.13952 | failed:   98
batch:      67560 | loss: 5.19036 | failed:   98
batch:      67570 | loss: 5.18700 | failed:   98
batch:      67580 | loss: 5.16291 | failed:   98
batch:      67590 | loss: 5.23285 | failed:   98
batch:      67600 | loss: 5.18575 | failed:   98
batch:      67610 | loss: 5.12313 | failed:   98
batch:      67620 | loss: 5.30716 | failed:   98
batch:      67630 | loss: 5.14533 | failed:   98
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      67640 | loss: 5.22391 | failed:   98
batch:      67650 | loss: 5.15955 | failed:   98
batch:      67670 | loss: 5.17278 | failed:  100
batch:      67680 | loss: 5.19571 | failed:  100
batch:      67690 | loss: 5.13600 | failed:  100
batch:      67700 | loss: 5.04977 | failed:  100
batch:      67710 | loss: 5.17651 | failed:  100
batch:      67720 | loss: 4.84605 | failed:  100
batch:      67730 | loss: 5.11259 | failed:  100
batch:      67740 | loss: 5.13781 | failed:  100
batch:      67750 | loss: 5.12449 | failed:  100
batch:      67760 | loss: 5.25689 | failed:  100
batch:      67770 | loss: 5.26048 | failed:  100
batch:      67780 | loss: 5.16533 | failed:  100
batch:      67790 | loss: 5.19111 | failed:  100
batch:      67800 | loss: 5.16753 | failed:  100
batch:      67810 | loss: 4.98558 | failed:  100
batch:      67820 | loss: 5.15565 | failed:  100
batch:      67830 | loss: 5.25414 | failed:  100
batch:      67840 | loss: 5.21317 | failed:  100
batch:      67850 | loss: 5.07135 | failed:  100
batch:      67860 | loss: 5.06919 | failed:  100
batch:      67870 | loss: 5.25773 | failed:  100
batch:      67880 | loss: 5.13017 | failed:  100
batch:      67890 | loss: 5.21494 | failed:  100
batch:      67900 | loss: 5.37841 | failed:  100
batch:      67910 | loss: 5.32393 | failed:  100
batch:      67920 | loss: 5.15614 | failed:  100
batch:      67930 | loss: 5.21629 | failed:  100
batch:      67940 | loss: 5.13042 | failed:  100
batch:      67950 | loss: 5.14340 | failed:  100
batch:      67960 | loss: 5.09158 | failed:  100
batch:      67970 | loss: 5.10153 | failed:  100
batch:      67980 | loss: 5.04358 | failed:  100
batch:      67990 | loss: 4.93153 | failed:  100
batch:      68000 | loss: 5.05157 | failed:  100
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      68010 | loss: 5.12401 | failed:  100
batch:      68020 | loss: 5.17621 | failed:  100
batch:      68030 | loss: 5.19179 | failed:  100
batch:      68040 | loss: 5.28235 | failed:  100
batch:      68050 | loss: 5.13576 | failed:  100
batch:      68060 | loss: 5.14216 | failed:  100
batch:      68070 | loss: 5.19329 | failed:  100
batch:      68080 | loss: 5.13284 | failed:  100
batch:      68090 | loss: 5.16749 | failed:  100
batch:      68100 | loss: 5.16201 | failed:  100
batch:      68110 | loss: 5.08391 | failed:  100
batch:      68120 | loss: 4.96410 | failed:  100
batch:      68130 | loss: 5.21136 | failed:  100
batch:      68140 | loss: 5.06203 | failed:  100
batch:      68150 | loss: 5.03053 | failed:  100
batch:      68160 | loss: 5.13529 | failed:  100
batch:      68170 | loss: 5.08919 | failed:  100
batch:      68180 | loss: 5.12625 | failed:  100
batch:      68190 | loss: 5.05718 | failed:  100
batch:      68200 | loss: 5.07134 | failed:  100
batch:      68210 | loss: 5.13969 | failed:  100
batch:      68220 | loss: 5.06534 | failed:  100
batch:      68230 | loss: 5.06531 | failed:  100
batch:      68240 | loss: 5.30049 | failed:  100
batch:      68250 | loss: 4.95838 | failed:  100
batch:      68260 | loss: 5.13189 | failed:  100
batch:      68270 | loss: 5.14473 | failed:  100
batch:      68280 | loss: 5.17035 | failed:  100
batch:      68290 | loss: 5.23740 | failed:  100
batch:      68300 | loss: 5.17773 | failed:  100
batch:      68310 | loss: 5.20815 | failed:  100
batch:      68320 | loss: 5.28396 | failed:  100
batch:      68330 | loss: 5.20644 | failed:  100
batch:      68340 | loss: 5.00294 | failed:  100
batch:      68350 | loss: 5.13308 | failed:  100
batch:      68360 | loss: 4.92062 | failed:  100
batch:      68370 | loss: 5.12558 | failed:  100
batch:      68380 | loss: 5.15210 | failed:  100
batch:      68390 | loss: 5.20778 | failed:  100
batch:      68400 | loss: 5.11549 | failed:  100
batch:      68410 | loss: 5.12605 | failed:  100
batch:      68420 | loss: 5.13441 | failed:  100
batch:      68430 | loss: 5.03605 | failed:  100
batch:      68440 | loss: 5.22626 | failed:  100
batch:      68450 | loss: 5.10279 | failed:  100
batch:      68460 | loss: 5.05567 | failed:  100
batch:      68470 | loss: 5.02151 | failed:  100
batch:      68480 | loss: 5.07967 | failed:  100
batch:      68490 | loss: 5.09639 | failed:  100
batch:      68500 | loss: 4.97121 | failed:  100
batch:      68510 | loss: 5.10442 | failed:  100
batch:      68520 | loss: 5.11333 | failed:  100
batch:      68530 | loss: 5.20431 | failed:  100
batch:      68540 | loss: 5.14917 | failed:  100
batch:      68550 | loss: 5.21244 | failed:  100
batch:      68560 | loss: 5.06523 | failed:  100
batch:      68570 | loss: 5.20522 | failed:  100
batch:      68580 | loss: 5.21358 | failed:  100
batch:      68590 | loss: 5.14867 | failed:  100
batch:      68600 | loss: 5.16153 | failed:  100
batch:      68610 | loss: 5.18697 | failed:  100
batch:      68620 | loss: 5.12594 | failed:  100
batch:      68630 | loss: 5.17770 | failed:  100
batch:      68640 | loss: 5.09088 | failed:  100
batch:      68650 | loss: 5.11784 | failed:  100
batch:      68660 | loss: 5.18323 | failed:  100
batch:      68670 | loss: 5.12716 | failed:  100
batch:      68680 | loss: 5.11965 | failed:  100
batch:      68690 | loss: 5.14938 | failed:  100
batch:      68700 | loss: 5.09758 | failed:  100
batch:      68710 | loss: 5.01003 | failed:  100
batch:      68720 | loss: 5.13134 | failed:  100
batch:      68730 | loss: 5.19377 | failed:  100
batch:      68740 | loss: 4.89316 | failed:  100
batch:      68750 | loss: 5.09450 | failed:  100
batch:      68760 | loss: 5.30665 | failed:  100
batch:      68770 | loss: 5.23960 | failed:  100
batch:      68780 | loss: 5.02592 | failed:  100
batch:      68790 | loss: 4.82252 | failed:  100
batch:      68800 | loss: 5.08468 | failed:  100
batch:      68810 | loss: 5.14334 | failed:  100
batch:      68820 | loss: 5.08223 | failed:  100
batch:      68830 | loss: 5.38481 | failed:  100
batch:      68840 | loss: 5.28714 | failed:  100
batch:      68850 | loss: 5.13836 | failed:  100
batch:      68860 | loss: 5.21045 | failed:  100
batch:      68870 | loss: 5.09614 | failed:  100
batch:      68880 | loss: 5.09602 | failed:  100
batch:      68890 | loss: 5.11333 | failed:  100
batch:      68900 | loss: 5.12904 | failed:  100
batch:      68910 | loss: 4.91472 | failed:  100
batch:      68920 | loss: 5.16112 | failed:  100
batch:      68930 | loss: 5.10887 | failed:  100
batch:      68940 | loss: 5.16956 | failed:  100
batch:      68950 | loss: 5.14006 | failed:  100
batch:      68960 | loss: 5.08285 | failed:  100
batch:      68970 | loss: 4.98644 | failed:  100
batch:      68980 | loss: 5.11969 | failed:  100
batch:      68990 | loss: 4.95829 | failed:  100
batch:      69000 | loss: 5.25713 | failed:  100
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      69010 | loss: 5.02578 | failed:  100
batch:      69020 | loss: 5.19771 | failed:  100
batch:      69030 | loss: 5.10523 | failed:  100
batch:      69040 | loss: 5.21881 | failed:  100
batch:      69050 | loss: 5.25217 | failed:  100
batch:      69060 | loss: 5.17149 | failed:  100
batch:      69070 | loss: 5.17436 | failed:  100
batch:      69080 | loss: 5.16348 | failed:  100
batch:      69090 | loss: 5.24585 | failed:  100
batch:      69100 | loss: 5.14931 | failed:  100
batch:      69110 | loss: 5.27963 | failed:  100
batch:      69120 | loss: 5.26945 | failed:  100
batch:      69130 | loss: 5.21130 | failed:  100
batch:      69140 | loss: 5.14714 | failed:  100
batch:      69150 | loss: 5.19154 | failed:  100
batch:      69160 | loss: 5.16982 | failed:  100
batch:      69170 | loss: 5.26802 | failed:  100
batch:      69180 | loss: 5.25152 | failed:  100
batch:      69190 | loss: 5.31235 | failed:  100
batch:      69200 | loss: 5.14331 | failed:  100
batch:      69210 | loss: 5.16876 | failed:  100
batch:      69220 | loss: 5.10464 | failed:  100
batch:      69230 | loss: 4.87233 | failed:  100
batch:      69240 | loss: 5.26018 | failed:  100
batch:      69250 | loss: 5.17804 | failed:  100
batch:      69260 | loss: 5.15314 | failed:  100
batch:      69270 | loss: 5.20297 | failed:  100
batch:      69280 | loss: 5.13624 | failed:  100
batch:      69290 | loss: 5.11814 | failed:  100
batch:      69300 | loss: 5.27441 | failed:  100
batch:      69310 | loss: 5.24122 | failed:  100
batch:      69320 | loss: 5.29886 | failed:  100
batch:      69330 | loss: 5.18735 | failed:  100
batch:      69340 | loss: 5.12560 | failed:  100
batch:      69350 | loss: 5.09335 | failed:  100
batch:      69360 | loss: 5.14022 | failed:  100
batch:      69370 | loss: 5.15796 | failed:  100
batch:      69380 | loss: 5.18953 | failed:  100
batch:      69390 | loss: 5.14021 | failed:  100
batch:      69400 | loss: 5.27647 | failed:  100
batch:      69410 | loss: 5.18041 | failed:  100
batch:      69420 | loss: 5.15770 | failed:  100
batch:      69430 | loss: 5.03662 | failed:  100
batch:      69440 | loss: 5.11208 | failed:  100
batch:      69450 | loss: 4.98481 | failed:  100
batch:      69460 | loss: 5.23429 | failed:  100
batch:      69470 | loss: 5.16959 | failed:  100
batch:      69480 | loss: 5.07477 | failed:  100
batch:      69490 | loss: 5.04026 | failed:  100
batch:      69500 | loss: 5.17950 | failed:  100
batch:      69510 | loss: 5.14207 | failed:  100
batch:      69520 | loss: 5.26774 | failed:  100
batch:      69530 | loss: 5.19978 | failed:  100
batch:      69540 | loss: 5.21541 | failed:  100
batch:      69550 | loss: 5.00301 | failed:  100
batch:      69560 | loss: 5.01513 | failed:  100
batch:      69570 | loss: 4.90652 | failed:  100
batch:      69580 | loss: 4.18321 | failed:  100
batch:      69590 | loss: 5.19920 | failed:  100
batch:      69600 | loss: 5.22316 | failed:  100
batch:      69610 | loss: 5.20179 | failed:  100
batch:      69620 | loss: 5.15014 | failed:  100
batch:      69630 | loss: 5.17928 | failed:  100
batch:      69640 | loss: 5.25520 | failed:  100
batch:      69650 | loss: 5.25017 | failed:  100
batch:      69660 | loss: 5.12304 | failed:  100
batch:      69670 | loss: 5.06486 | failed:  100
batch:      69680 | loss: 5.14700 | failed:  100
batch:      69690 | loss: 4.88868 | failed:  100
batch:      69700 | loss: 5.06031 | failed:  100
batch:      69710 | loss: 5.17273 | failed:  100
batch:      69720 | loss: 5.16087 | failed:  100
batch:      69730 | loss: 5.09718 | failed:  100
batch:      69740 | loss: 5.10435 | failed:  100
batch:      69750 | loss: 5.20683 | failed:  100
batch:      69760 | loss: 4.91798 | failed:  100
batch:      69770 | loss: 4.98129 | failed:  100
batch:      69780 | loss: 5.01928 | failed:  100
batch:      69790 | loss: 5.13960 | failed:  100
batch:      69800 | loss: 5.03496 | failed:  100
batch:      69810 | loss: 4.95270 | failed:  100
batch:      69820 | loss: 4.93629 | failed:  100
batch:      69830 | loss: 5.13983 | failed:  100
batch:      69840 | loss: 5.14161 | failed:  100
batch:      69850 | loss: 5.15327 | failed:  100
batch:      69860 | loss: 5.04922 | failed:  100
batch:      69870 | loss: 4.97563 | failed:  100
batch:      69880 | loss: 5.18759 | failed:  100
batch:      69890 | loss: 4.95406 | failed:  100
batch:      69900 | loss: 5.22414 | failed:  100
batch:      69910 | loss: 5.08340 | failed:  100
batch:      69920 | loss: 5.22281 | failed:  100
batch:      69930 | loss: 5.14543 | failed:  100
batch:      69940 | loss: 5.07603 | failed:  100
batch:      69950 | loss: 5.25032 | failed:  100
batch:      69960 | loss: 5.19569 | failed:  100
batch:      69970 | loss: 5.21471 | failed:  100
batch:      69980 | loss: 4.99435 | failed:  100
batch:      69990 | loss: 5.23303 | failed:  100
batch:      70000 | loss: 4.86514 | failed:  100
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      70010 | loss: 5.29265 | failed:  100
batch:      70020 | loss: 5.12154 | failed:  100
batch:      70030 | loss: 5.19226 | failed:  100
batch:      70040 | loss: 5.20726 | failed:  100
batch:      70050 | loss: 5.06086 | failed:  100
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      70060 | loss: 5.13241 | failed:  100
batch:      70070 | loss: 4.94882 | failed:  100
batch:      70080 | loss: 5.22252 | failed:  101
batch:      70090 | loss: 5.24515 | failed:  101
batch:      70100 | loss: 5.07551 | failed:  101
batch:      70110 | loss: 5.13447 | failed:  101
batch:      70120 | loss: 5.06282 | failed:  101
batch:      70130 | loss: 5.16144 | failed:  101
batch:      70140 | loss: 5.15769 | failed:  101
batch:      70150 | loss: 5.20527 | failed:  101
batch:      70160 | loss: 5.14325 | failed:  101
batch:      70170 | loss: 5.09875 | failed:  101
batch:      70180 | loss: 5.08576 | failed:  101
batch:      70190 | loss: 5.12314 | failed:  101
batch:      70200 | loss: 5.05105 | failed:  101
batch:      70210 | loss: 5.20987 | failed:  101
batch:      70220 | loss: 5.23512 | failed:  101
batch:      70230 | loss: 5.10677 | failed:  101
batch:      70240 | loss: 4.96633 | failed:  101
batch:      70250 | loss: 5.23438 | failed:  101
batch:      70260 | loss: 5.22682 | failed:  101
batch:      70270 | loss: 5.14568 | failed:  101
batch:      70280 | loss: 5.05852 | failed:  101
batch:      70290 | loss: 5.17750 | failed:  101
batch:      70300 | loss: 5.16898 | failed:  101
batch:      70310 | loss: 5.02984 | failed:  101
batch:      70320 | loss: 5.13561 | failed:  101
batch:      70330 | loss: 5.18568 | failed:  101
batch:      70340 | loss: 5.03937 | failed:  101
batch:      70350 | loss: 5.12190 | failed:  101
batch:      70360 | loss: 5.07888 | failed:  101
batch:      70370 | loss: 5.12003 | failed:  101
batch:      70380 | loss: 5.16411 | failed:  101
batch:      70390 | loss: 5.13413 | failed:  101
batch:      70400 | loss: 5.13655 | failed:  101
batch:      70410 | loss: 5.28015 | failed:  101
batch:      70420 | loss: 4.98199 | failed:  101
batch:      70430 | loss: 5.01526 | failed:  101
batch:      70440 | loss: 5.13877 | failed:  101
batch:      70450 | loss: 5.06971 | failed:  101
batch:      70460 | loss: 5.04101 | failed:  101
batch:      70470 | loss: 4.83997 | failed:  101
batch:      70480 | loss: 5.13094 | failed:  101
batch:      70490 | loss: 5.18270 | failed:  101
batch:      70500 | loss: 5.11219 | failed:  101
batch:      70510 | loss: 4.98961 | failed:  101
batch:      70520 | loss: 5.20723 | failed:  101
batch:      70530 | loss: 5.18837 | failed:  101
batch:      70540 | loss: 5.21505 | failed:  101
batch:      70550 | loss: 5.11586 | failed:  101
batch:      70560 | loss: 5.10236 | failed:  101
batch:      70570 | loss: 5.09651 | failed:  101
batch:      70580 | loss: 5.05993 | failed:  101
batch:      70590 | loss: 5.11318 | failed:  101
batch:      70600 | loss: 5.03624 | failed:  101
batch:      70610 | loss: 5.19573 | failed:  101
batch:      70620 | loss: 5.06486 | failed:  101
batch:      70630 | loss: 5.12599 | failed:  101
batch:      70640 | loss: 5.12298 | failed:  101
batch:      70650 | loss: 5.13268 | failed:  101
batch:      70660 | loss: 4.92710 | failed:  101
batch:      70670 | loss: 5.32507 | failed:  101
batch:      70680 | loss: 5.28526 | failed:  101
batch:      70690 | loss: 5.28929 | failed:  101
batch:      70700 | loss: 5.14051 | failed:  101
batch:      70710 | loss: 5.16527 | failed:  101
batch:      70720 | loss: 5.15993 | failed:  101
batch:      70730 | loss: 5.15290 | failed:  101
batch:      70740 | loss: 5.13773 | failed:  101
batch:      70750 | loss: 5.18613 | failed:  101
batch:      70760 | loss: 5.06222 | failed:  101
batch:      70770 | loss: 5.14180 | failed:  101
batch:      70780 | loss: 5.01852 | failed:  101
batch:      70790 | loss: 4.86210 | failed:  101
batch:      70800 | loss: 5.32931 | failed:  101
batch:      70810 | loss: 5.00873 | failed:  101
batch:      70820 | loss: 5.19580 | failed:  101
batch:      70830 | loss: 5.21532 | failed:  101
batch:      70840 | loss: 5.18308 | failed:  101
batch:      70850 | loss: 5.17437 | failed:  101
batch:      70860 | loss: 5.17923 | failed:  101
batch:      70870 | loss: 5.12023 | failed:  101
batch:      70880 | loss: 5.17177 | failed:  101
batch:      70890 | loss: 5.10479 | failed:  101
batch:      70900 | loss: 5.07311 | failed:  101
batch:      70910 | loss: 5.13254 | failed:  101
batch:      70920 | loss: 5.17676 | failed:  101
batch:      70930 | loss: 5.14980 | failed:  101
batch:      70940 | loss: 5.02124 | failed:  101
batch:      70950 | loss: 5.07417 | failed:  101
batch:      70960 | loss: 5.11039 | failed:  101
batch:      70970 | loss: 5.26091 | failed:  101
batch:      70980 | loss: 5.25904 | failed:  101
batch:      70990 | loss: 5.18823 | failed:  101
batch:      71000 | loss: 5.05097 | failed:  101
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      71010 | loss: 5.21048 | failed:  101
batch:      71020 | loss: 5.19549 | failed:  101
batch:      71030 | loss: 5.12872 | failed:  101
batch:      71040 | loss: 5.20334 | failed:  101
batch:      71050 | loss: 5.25493 | failed:  101
batch:      71060 | loss: 5.12456 | failed:  101
batch:      71070 | loss: 5.02720 | failed:  101
batch:      71080 | loss: 5.02013 | failed:  101
batch:      71090 | loss: 5.23155 | failed:  101
batch:      71100 | loss: 5.21235 | failed:  101
batch:      71110 | loss: 5.24558 | failed:  101
batch:      71120 | loss: 5.14071 | failed:  101
batch:      71130 | loss: 5.22045 | failed:  101
batch:      71140 | loss: 5.06389 | failed:  101
batch:      71150 | loss: 5.12328 | failed:  101
batch:      71160 | loss: 5.14913 | failed:  101
batch:      71170 | loss: 5.07044 | failed:  101
batch:      71180 | loss: 5.14218 | failed:  101
batch:      71190 | loss: 5.17262 | failed:  101
batch:      71200 | loss: 5.16524 | failed:  101
batch:      71210 | loss: 5.00686 | failed:  101
batch:      71220 | loss: 5.10339 | failed:  101
batch:      71230 | loss: 5.04281 | failed:  101
batch:      71240 | loss: 5.21558 | failed:  101
batch:      71250 | loss: 5.06950 | failed:  101
batch:      71260 | loss: 5.02902 | failed:  101
batch:      71270 | loss: 4.99895 | failed:  101
batch:      71280 | loss: 5.07621 | failed:  101
batch:      71290 | loss: 5.10526 | failed:  101
batch:      71300 | loss: 5.24881 | failed:  101
batch:      71310 | loss: 5.26638 | failed:  101
batch:      71320 | loss: 5.14285 | failed:  101
batch:      71330 | loss: 5.11404 | failed:  101
batch:      71340 | loss: 5.24347 | failed:  101
batch:      71350 | loss: 5.18629 | failed:  101
batch:      71360 | loss: 5.18065 | failed:  101
batch:      71370 | loss: 5.19050 | failed:  101
batch:      71380 | loss: 5.23020 | failed:  101
batch:      71390 | loss: 5.20862 | failed:  101
batch:      71400 | loss: 5.13281 | failed:  101
batch:      71410 | loss: 5.15199 | failed:  101
batch:      71420 | loss: 5.15058 | failed:  101
batch:      71430 | loss: 5.12273 | failed:  101
batch:      71440 | loss: 5.15047 | failed:  101
batch:      71450 | loss: 4.97540 | failed:  101
batch:      71460 | loss: 4.92377 | failed:  101
batch:      71470 | loss: 5.14375 | failed:  101
batch:      71480 | loss: 5.20587 | failed:  101
batch:      71490 | loss: 5.12206 | failed:  101
batch:      71500 | loss: 5.07876 | failed:  101
batch:      71510 | loss: 5.15015 | failed:  101
batch:      71520 | loss: 5.12776 | failed:  101
batch:      71530 | loss: 4.98947 | failed:  101
batch:      71540 | loss: 5.11161 | failed:  101
batch:      71550 | loss: 5.24953 | failed:  101
batch:      71560 | loss: 5.16180 | failed:  101
batch:      71570 | loss: 5.26695 | failed:  101
batch:      71580 | loss: 5.17555 | failed:  101
batch:      71590 | loss: 5.12539 | failed:  101
batch:      71600 | loss: 5.08904 | failed:  101
batch:      71610 | loss: 5.16647 | failed:  101
batch:      71620 | loss: 5.14952 | failed:  101
batch:      71630 | loss: 5.23034 | failed:  101
batch:      71640 | loss: 5.17202 | failed:  101
batch:      71650 | loss: 5.11956 | failed:  101
batch:      71660 | loss: 5.12138 | failed:  101
batch:      71670 | loss: 5.04374 | failed:  101
batch:      71680 | loss: 5.11804 | failed:  101
batch:      71690 | loss: 5.10463 | failed:  101
batch:      71700 | loss: 5.19245 | failed:  101
batch:      71710 | loss: 5.11282 | failed:  101
batch:      71720 | loss: 5.04831 | failed:  101
batch:      71730 | loss: 5.16196 | failed:  101
batch:      71740 | loss: 5.10588 | failed:  101
batch:      71750 | loss: 5.07885 | failed:  101
batch:      71760 | loss: 5.20553 | failed:  101
batch:      71770 | loss: 5.03034 | failed:  101
batch:      71780 | loss: 5.15886 | failed:  101
batch:      71790 | loss: 5.06787 | failed:  101
batch:      71800 | loss: 5.12560 | failed:  101
batch:      71810 | loss: 5.18872 | failed:  101
batch:      71820 | loss: 5.12506 | failed:  101
batch:      71830 | loss: 5.13303 | failed:  101
batch:      71840 | loss: 5.22122 | failed:  101
batch:      71850 | loss: 5.22380 | failed:  101
batch:      71860 | loss: 5.14317 | failed:  101
batch:      71870 | loss: 5.14468 | failed:  101
batch:      71880 | loss: 5.23712 | failed:  101
batch:      71890 | loss: 5.16845 | failed:  101
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      71900 | loss: 5.11640 | failed:  101
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      71910 | loss: 5.20867 | failed:  101
batch:      71930 | loss: 5.20597 | failed:  114
batch:      71940 | loss: 5.00419 | failed:  114
batch:      71950 | loss: 5.07790 | failed:  114
batch:      71960 | loss: 5.13326 | failed:  114
batch:      71970 | loss: 5.21572 | failed:  114
batch:      71980 | loss: 5.23938 | failed:  114
batch:      71990 | loss: 5.18303 | failed:  114
batch:      72000 | loss: 5.21589 | failed:  114
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      72010 | loss: 5.15205 | failed:  114
batch:      72020 | loss: 5.34615 | failed:  114
batch:      72030 | loss: 5.35247 | failed:  114
batch:      72040 | loss: 5.30750 | failed:  114
batch:      72050 | loss: 5.31335 | failed:  114
batch:      72060 | loss: 5.19546 | failed:  114
batch:      72070 | loss: 5.19780 | failed:  114
batch:      72080 | loss: 5.30917 | failed:  114
batch:      72090 | loss: 5.17165 | failed:  114
batch:      72100 | loss: 5.13819 | failed:  114
batch:      72110 | loss: 5.24063 | failed:  114
batch:      72120 | loss: 5.18537 | failed:  114
batch:      72130 | loss: 5.10213 | failed:  114
batch:      72140 | loss: 5.09052 | failed:  114
batch:      72150 | loss: 5.05147 | failed:  114
batch:      72160 | loss: 5.24459 | failed:  114
batch:      72170 | loss: 5.13751 | failed:  114
batch:      72180 | loss: 5.25584 | failed:  114
batch:      72190 | loss: 5.15650 | failed:  114
batch:      72200 | loss: 5.19324 | failed:  114
batch:      72210 | loss: 5.11136 | failed:  114
batch:      72220 | loss: 5.22719 | failed:  114
batch:      72230 | loss: 4.88455 | failed:  114
batch:      72240 | loss: 5.19932 | failed:  114
batch:      72250 | loss: 5.09591 | failed:  114
batch:      72260 | loss: 5.11617 | failed:  114
batch:      72270 | loss: 5.14695 | failed:  114
batch:      72280 | loss: 5.04588 | failed:  114
batch:      72290 | loss: 5.12532 | failed:  114
batch:      72300 | loss: 5.17846 | failed:  114
batch:      72310 | loss: 5.11575 | failed:  114
batch:      72320 | loss: 5.17451 | failed:  114
batch:      72330 | loss: 5.07636 | failed:  114
batch:      72340 | loss: 5.01116 | failed:  114
batch:      72350 | loss: 5.10638 | failed:  114
batch:      72360 | loss: 5.13118 | failed:  114
batch:      72370 | loss: 5.10068 | failed:  114
batch:      72380 | loss: 5.13653 | failed:  114
batch:      72390 | loss: 4.97035 | failed:  114
batch:      72400 | loss: 5.06444 | failed:  114
batch:      72410 | loss: 5.10364 | failed:  114
batch:      72420 | loss: 5.18279 | failed:  114
batch:      72430 | loss: 5.09354 | failed:  114
batch:      72440 | loss: 5.11473 | failed:  114
batch:      72450 | loss: 5.06520 | failed:  114
batch:      72460 | loss: 5.13981 | failed:  114
batch:      72470 | loss: 5.17477 | failed:  114
batch:      72480 | loss: 5.13798 | failed:  114
batch:      72490 | loss: 5.07078 | failed:  114
batch:      72500 | loss: 4.94905 | failed:  114
batch:      72510 | loss: 5.07262 | failed:  114
batch:      72520 | loss: 5.25639 | failed:  114
batch:      72530 | loss: 5.04138 | failed:  114
batch:      72540 | loss: 5.20999 | failed:  114
batch:      72550 | loss: 5.03495 | failed:  114
batch:      72560 | loss: 5.16251 | failed:  114
batch:      72570 | loss: 5.23092 | failed:  114
batch:      72580 | loss: 5.12354 | failed:  114
batch:      72590 | loss: 5.16860 | failed:  114
batch:      72600 | loss: 5.08410 | failed:  114
batch:      72610 | loss: 5.10148 | failed:  114
batch:      72620 | loss: 5.26709 | failed:  114
batch:      72630 | loss: 5.06849 | failed:  114
batch:      72640 | loss: 5.03996 | failed:  114
batch:      72650 | loss: 5.18965 | failed:  114
batch:      72660 | loss: 5.07966 | failed:  114
batch:      72670 | loss: 5.23015 | failed:  114
batch:      72680 | loss: 4.91140 | failed:  114
batch:      72690 | loss: 5.09692 | failed:  114
batch:      72700 | loss: 5.11344 | failed:  114
batch:      72710 | loss: 4.92307 | failed:  114
batch:      72720 | loss: 5.17665 | failed:  114
batch:      72730 | loss: 5.11255 | failed:  114
batch:      72740 | loss: 5.13118 | failed:  114
batch:      72750 | loss: 5.12783 | failed:  114
batch:      72760 | loss: 5.21211 | failed:  114
batch:      72770 | loss: 5.16764 | failed:  114
batch:      72780 | loss: 5.27496 | failed:  114
batch:      72790 | loss: 5.11905 | failed:  114
batch:      72800 | loss: 5.14982 | failed:  114
batch:      72810 | loss: 5.25005 | failed:  114
batch:      72820 | loss: 5.14541 | failed:  114
batch:      72830 | loss: 5.13025 | failed:  114
batch:      72840 | loss: 5.17993 | failed:  114
batch:      72850 | loss: 5.16757 | failed:  114
batch:      72860 | loss: 5.24786 | failed:  114
batch:      72870 | loss: 5.26627 | failed:  114
batch:      72880 | loss: 5.15213 | failed:  114
batch:      72890 | loss: 5.11026 | failed:  114
batch:      72900 | loss: 5.09067 | failed:  114
batch:      72910 | loss: 5.16848 | failed:  114
batch:      72920 | loss: 5.25280 | failed:  114
batch:      72930 | loss: 5.03888 | failed:  114
batch:      72940 | loss: 5.23264 | failed:  114
batch:      72950 | loss: 4.98554 | failed:  114
batch:      72960 | loss: 5.24987 | failed:  114
batch:      72970 | loss: 5.08748 | failed:  114
batch:      72980 | loss: 5.27641 | failed:  114
batch:      72990 | loss: 4.84160 | failed:  114
batch:      73000 | loss: 4.95724 | failed:  114
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      73010 | loss: 5.05927 | failed:  114
batch:      73020 | loss: 5.18574 | failed:  114
batch:      73030 | loss: 5.09888 | failed:  114
batch:      73040 | loss: 5.12578 | failed:  114
batch:      73050 | loss: 5.14364 | failed:  114
batch:      73060 | loss: 5.19913 | failed:  114
batch:      73070 | loss: 5.18559 | failed:  114
batch:      73080 | loss: 5.19227 | failed:  114
batch:      73090 | loss: 5.07632 | failed:  114
batch:      73100 | loss: 5.21149 | failed:  114
batch:      73110 | loss: 5.19366 | failed:  114
batch:      73120 | loss: 5.27115 | failed:  114
batch:      73130 | loss: 5.21280 | failed:  114
batch:      73140 | loss: 5.15553 | failed:  114
batch:      73150 | loss: 5.23489 | failed:  114
batch:      73160 | loss: 5.04041 | failed:  114
batch:      73170 | loss: 5.21441 | failed:  114
batch:      73180 | loss: 5.15059 | failed:  114
batch:      73190 | loss: 5.15327 | failed:  114
batch:      73200 | loss: 5.17046 | failed:  114
batch:      73210 | loss: 5.07968 | failed:  114
batch:      73220 | loss: 5.16625 | failed:  114
batch:      73230 | loss: 4.98374 | failed:  114
batch:      73240 | loss: 5.25435 | failed:  114
batch:      73250 | loss: 5.23191 | failed:  114
batch:      73260 | loss: 5.23674 | failed:  114
batch:      73270 | loss: 5.16879 | failed:  114
batch:      73280 | loss: 5.18840 | failed:  114
batch:      73290 | loss: 5.20116 | failed:  114
batch:      73300 | loss: 5.19485 | failed:  114
batch:      73310 | loss: 5.18149 | failed:  114
batch:      73320 | loss: 5.18318 | failed:  114
batch:      73330 | loss: 5.19687 | failed:  114
batch:      73340 | loss: 5.04624 | failed:  114
batch:      73350 | loss: 5.06245 | failed:  114
batch:      73360 | loss: 4.96344 | failed:  114
batch:      73370 | loss: 5.17771 | failed:  114
batch:      73380 | loss: 5.25753 | failed:  114
batch:      73390 | loss: 5.18760 | failed:  114
batch:      73400 | loss: 5.05410 | failed:  114
batch:      73410 | loss: 5.11218 | failed:  114
batch:      73420 | loss: 5.16752 | failed:  114
batch:      73430 | loss: 5.00669 | failed:  114
batch:      73440 | loss: 5.12228 | failed:  114
batch:      73450 | loss: 5.07844 | failed:  114
batch:      73460 | loss: 5.29863 | failed:  114
batch:      73470 | loss: 5.12839 | failed:  114
batch:      73480 | loss: 4.99232 | failed:  114
batch:      73490 | loss: 5.20373 | failed:  114
batch:      73500 | loss: 5.02711 | failed:  114
batch:      73510 | loss: 5.16634 | failed:  114
batch:      73520 | loss: 5.05464 | failed:  114
batch:      73530 | loss: 5.10427 | failed:  114
batch:      73540 | loss: 5.24021 | failed:  114
batch:      73550 | loss: 5.23865 | failed:  114
batch:      73560 | loss: 5.15061 | failed:  114
batch:      73570 | loss: 5.18129 | failed:  114
batch:      73580 | loss: 5.13216 | failed:  114
batch:      73590 | loss: 5.00319 | failed:  114
batch:      73600 | loss: 5.10390 | failed:  114
batch:      73610 | loss: 5.17982 | failed:  114
batch:      73620 | loss: 5.02634 | failed:  114
batch:      73630 | loss: 4.80735 | failed:  114
batch:      73640 | loss: 5.14486 | failed:  114
batch:      73650 | loss: 5.09767 | failed:  114
batch:      73660 | loss: 5.26220 | failed:  114
batch:      73670 | loss: 5.03474 | failed:  114
batch:      73680 | loss: 5.16826 | failed:  114
batch:      73690 | loss: 5.20305 | failed:  114
batch:      73700 | loss: 5.24940 | failed:  114
batch:      73710 | loss: 5.04994 | failed:  114
batch:      73720 | loss: 5.10918 | failed:  114
batch:      73730 | loss: 5.17343 | failed:  114
batch:      73740 | loss: 5.13270 | failed:  114
batch:      73750 | loss: 5.16911 | failed:  114
batch:      73760 | loss: 5.23812 | failed:  114
batch:      73770 | loss: 5.19079 | failed:  114
batch:      73780 | loss: 4.98912 | failed:  114
batch:      73790 | loss: 5.03587 | failed:  114
batch:      73800 | loss: 5.17065 | failed:  114
