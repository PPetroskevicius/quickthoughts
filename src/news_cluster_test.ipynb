{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import *\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "torch.cuda.set_device(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset 20 NEWS with total lines: 11314\n",
      "[0 4 4 4 6 5 6 4 4 4 0 5 4 6 3]\n",
      "[ 7  4  4  1 14 16 13  3  2  4  8 19  4 14  6]\n"
     ]
    }
   ],
   "source": [
    "# get data \n",
    "newsgroups_train = fetch_20newsgroups(data_home=\"~/workspace/scikit_learn_data\", subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(data_home=\"~/workspace/scikit_learn_data\", subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "text, labels = newsgroups_train.data, newsgroups_train.target\n",
    "test_batch_size=1000\n",
    "size = len(labels)\n",
    "print(\"Loaded dataset {} with total lines: {}\".format(\"20 NEWS\", size))\n",
    "\n",
    "top_level_labels = np.copy(labels)\n",
    "top_categories = dict((name, i) for (i, name) in enumerate(set(map(lambda x: x.split('.')[0], newsgroups_test.target_names))))\n",
    "#print(top_categories)\n",
    "for i, name in enumerate(newsgroups_test.target_names):\n",
    "    #print(i, name)\n",
    "    top = name.split('.')[0]\n",
    "    top_level_labels[labels == i ] = top_categories[top]\n",
    "\n",
    "print(top_level_labels[:15])\n",
    "print(labels[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-19 14:59:11 INFO     loading Doc2Vec object from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model\n",
      "2019-07-19 14:59:12 INFO     loading vocabulary recursively from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.vocabulary.* with mmap=None\n",
      "2019-07-19 14:59:12 INFO     loading trainables recursively from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.trainables.* with mmap=None\n",
      "2019-07-19 14:59:12 INFO     loading syn1neg from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-07-19 14:59:12 INFO     loading wv recursively from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.wv.* with mmap=None\n",
      "2019-07-19 14:59:12 INFO     loading vectors from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.wv.vectors.npy with mmap=None\n",
      "2019-07-19 14:59:13 INFO     loading docvecs recursively from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.docvecs.* with mmap=None\n",
      "2019-07-19 14:59:13 INFO     loading vectors_docs from /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model.docvecs.vectors_docs.npy with mmap=None\n",
      "2019-07-19 14:59:17 INFO     loaded /home/diq/all/bert_expe/tabooola_gensim_model/gensim_doc2vec.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 600)\n"
     ]
    }
   ],
   "source": [
    "#load taboola model and encode \n",
    "taboola_model_dir = \"/home/diq/all/bert_expe/tabooola_gensim_model\"\n",
    "d2v = Doc2Vec.load(\"{}/{}\".format(taboola_model_dir, \"gensim_doc2vec.model\"))\n",
    "\n",
    "d2v_features = np.vstack([d2v.infer_vector(simple_preprocess(doc)) for doc in text])\n",
    "print(d2v_features.shape)\n",
    "\n",
    "#assert np.array_equal(d2v_features[0, :],  d2v.infer_vector(simple_preprocess(text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-19 14:59:37 INFO     loading projection weights from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2019-07-19 15:01:38 INFO     loaded (400000, 300) matrix from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored successfully from /home/jcjessecai/workspace/taboola/quickthoughts/checkpoints\n"
     ]
    }
   ],
   "source": [
    "#load qt model\n",
    "checkpoint_dir = '/home/jcjessecai/workspace/taboola/quickthoughts/checkpoints'\n",
    "with open(\"{}/config.json\".format(checkpoint_dir)) as fp:\n",
    "    CONFIG = json.load(fp)\n",
    "\n",
    "WV_MODEL = api.load(CONFIG['embedding'])\n",
    "qt = QuickThoughts(WV_MODEL, hidden_size=CONFIG['hidden_size'])\n",
    "trained_params = torch.load(\"{}/checkpoint_latest.pth\".format(checkpoint_dir))\n",
    "qt.load_state_dict(trained_params['state_dict'])\n",
    "qt = qt.cuda()\n",
    "qt.eval()\n",
    "print(\"Restored successfully from {}\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing    12 batches of size  1000\n",
      "Test feature matrix of shape: (11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "#encode data\n",
    "def make_batch(j):\n",
    "    \"\"\"Processes one test batch of the test datset\"\"\"\n",
    "    stop_idx = min(size, j+test_batch_size)\n",
    "    batch_text, batch_labels  = text[j:stop_idx], labels[j:stop_idx]\n",
    "    data = list(map(lambda x: torch.LongTensor(prepare_sequence(x, WV_MODEL.vocab, no_zeros=True)), batch_text))\n",
    "    for i in data:\n",
    "        if len(i) == 0:\n",
    "            print(i)\n",
    "            input()\n",
    "    packed = safe_pack_sequence(data).cuda()\n",
    "    return qt(packed).cpu().detach().numpy()\n",
    "\n",
    "feature_list = [make_batch(i) for i in range(0, size, test_batch_size)]\n",
    "print(\"Processing {:5d} batches of size {:5d}\".format(len(feature_list), test_batch_size))\n",
    "qt_features = np.concatenate(feature_list)\n",
    "print(\"Test feature matrix of shape: {}\".format(qt_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit logistic model on d2v with s:   1 and train acc: 79.84% test acc: 53.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#first we compare embedding performance by fitting binary classifier on top\n",
    "s=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(d2v_features, labels)\n",
    "clf = LogisticRegression(solver='sag', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Fit logistic model on d2v with s: {:3d} and train acc: {:.2%} test acc: {:.2%}\".format(s, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit logistic model on qt with s:   1 and train acc: 93.68% test acc: 47.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(qt_features, labels)\n",
    "clf = LogisticRegression(solver='sag', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Fit logistic model on qt with s: {:3d} and train acc: {:.2%} test acc: {:.2%}\".format(s, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_predicted  = KMeans(n_clusters=7, n_jobs=20).fit_predict(qt_features)\n",
    "d2v_predicted  = KMeans(n_clusters=7, n_jobs=20).fit_predict(d2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12372624528161412\n",
      "0.10892941037885245\n",
      "0.16328080524560495\n",
      "0.1614446379480039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(adjusted_rand_score(top_level_labels, qt_predicted))\n",
    "print(adjusted_rand_score(top_level_labels, d2v_predicted))\n",
    "\n",
    "print(adjusted_mutual_info_score(top_level_labels, qt_predicted))\n",
    "print(adjusted_mutual_info_score(top_level_labels, d2v_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_embedded = TSNE(n_components=2, verbose=1).fit_transform(features[:5000])\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[labels[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[predicted_spectral[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[predicted_kmeans[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
