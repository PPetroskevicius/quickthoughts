{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import *\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import orth\n",
    "import csv\n",
    "torch.cuda.set_device(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taboola_data_dir = \"/local/diq/all/StepContent__nocontent_title_desc\"\n",
    "step_lines = \"/local/diq/all/StepLines__nocontent_title_desc\"\n",
    "\n",
    "import os\n",
    "#print(os.listdir(step_lines))\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "for file in os.listdir(step_lines):\n",
    "    sc_len = file_len(\"{}/{}\".format(taboola_data_dir, file))\n",
    "    sl_len = file_len(\"{}/{}\".format(step_lines, file))\n",
    "    #print(sc_len, sl_len)\n",
    "    assert sc_len == sl_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:11<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'travel', 'finance', 'home and garden', 'art and entertainment', 'society', 'business and industrial', 'automotive and vehicles', 'news', 'law, govt and politics', 'food and drink', 'shopping', 'real estate', 'science', 'technology and computing', 'pets', 'sports', 'health and fitness', 'hobbies and interests', 'religion and spirituality', 'family and parenting', 'style and fashion', 'careers', 'education'}\n",
      "{'travel': 0, 'finance': 1, 'home and garden': 2, 'art and entertainment': 3, 'society': 4, 'business and industrial': 5, 'automotive and vehicles': 6, 'news': 7, 'law, govt and politics': 8, 'food and drink': 9, 'shopping': 10, 'real estate': 11, 'science': 12, 'technology and computing': 13, 'pets': 14, 'sports': 15, 'health and fitness': 16, 'hobbies and interests': 17, 'religion and spirituality': 18, 'family and parenting': 19, 'style and fashion': 20, 'careers': 21, 'education': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "taxonomies = set()\n",
    "#hack for the moment (jupuyter nb in folder)\n",
    "for file in tqdm(os.listdir(step_lines)):\n",
    "    with open(\"{}/{}\".format(taboola_data_dir, file), \"r\") as f:\n",
    "        data = set(map(lambda json_obj: json_obj.get('first_level_taxonomy'), map(lambda x: json.loads(x), f.readlines())))\n",
    "        taxonomies= taxonomies.union(data)\n",
    "\n",
    "print(taxonomies)\n",
    "mapping = dict([(name, i) for i, name in enumerate(taxonomies)])\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "map() must have at least two arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-66d7d8c1fe33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"alllines.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: map() must have at least two arguments."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:18<00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569893\n",
      "569893\n",
      "Loaded dataset TABOOLA with total lines: 569893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "labels = []\n",
    "\n",
    "for file in tqdm(sorted(os.listdir(step_lines))):\n",
    "    data = pd.read_csv(\"{}/{}\".format(step_lines, file), header=None, sep='\\t', quoting=csv.QUOTE_NONE)[2].tolist()\n",
    "    text+=data\n",
    "    \n",
    "    with open(\"{}/{}\".format(taboola_data_dir, file), \"r\") as f:\n",
    "        data = map(lambda json_obj: mapping.get(json_obj.get('first_level_taxonomy'), -1), map(lambda x: json.loads(x), f.readlines()))\n",
    "        labels += list(data)\n",
    "        \n",
    "    #print(len(text), len(labels))\n",
    "    assert(len(text) == len(labels))\n",
    "\n",
    "print(len(labels))\n",
    "print(len(text))\n",
    "\n",
    "test_batch_size=5000\n",
    "size = len(labels)\n",
    "print(\"Loaded dataset {} with total lines: {}\".format(\"TABOOLA\", size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-19 15:07:06 INFO     loading projection weights from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-19 15:09:09 INFO     loaded (400000, 300) matrix from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored successfully from /home/jcjessecai/workspace/taboola/quickthoughts/checkpoints\n"
     ]
    }
   ],
   "source": [
    "#load qt model\n",
    "checkpoint_dir = '/home/jcjessecai/workspace/taboola/quickthoughts/checkpoints'\n",
    "with open(\"{}/config.json\".format(checkpoint_dir)) as fp:\n",
    "    CONFIG = json.load(fp)\n",
    "\n",
    "WV_MODEL = api.load(CONFIG['embedding'])\n",
    "qt = QuickThoughts(WV_MODEL, hidden_size=CONFIG['hidden_size'])\n",
    "trained_params = torch.load(\"{}/checkpoint_latest.pth\".format(checkpoint_dir))\n",
    "qt.load_state_dict(trained_params['state_dict'])\n",
    "qt = qt.cuda()\n",
    "qt.eval()\n",
    "print(\"Restored successfully from {}\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07658999 -0.09340584  0.01470476 ...  0.01451465  0.03383676\n",
      "   0.03353591]\n",
      " [-0.15760107 -0.01548272 -0.07186207 ... -0.04399835  0.00394282\n",
      "   0.06472579]\n",
      " [ 0.22808631  0.5511722  -0.11740495 ... -0.01527937 -0.00725939\n",
      "   0.0724398 ]\n",
      " ...\n",
      " [-0.0219017  -0.15206882 -0.09195305 ... -0.05870578 -0.05670162\n",
      "   0.05115658]\n",
      " [-0.05923571 -0.14442426 -0.23165044 ... -0.0316557   0.02349955\n",
      "   0.12594706]\n",
      " [-0.1402694   0.05879774 -0.08240668 ... -0.04093776 -0.02952008\n",
      "   0.16207318]]\n"
     ]
    }
   ],
   "source": [
    "data = list(map(lambda x: torch.LongTensor(prepare_sequence(x, WV_MODEL.vocab, no_zeros=True)), mapping.keys()))\n",
    "packed = safe_pack_sequence(data).cuda()\n",
    "            \n",
    "vec_categories = qt(packed).cpu().detach().numpy()\n",
    "print(vec_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed   114 batches of size  5000\n",
      "Test feature matrix of shape: (569893, 1000)\n"
     ]
    }
   ],
   "source": [
    "#encode data\n",
    "def make_batch(j):\n",
    "    \"\"\"Processes one test batch of the test datset\"\"\"\n",
    "    stop_idx = min(size, j+test_batch_size)\n",
    "    batch_text, batch_labels  = text[j:stop_idx], labels[j:stop_idx]\n",
    "    data = list(map(lambda x: torch.LongTensor(prepare_sequence(x, WV_MODEL.vocab, no_zeros=True)), batch_text))\n",
    "    for i in data:\n",
    "        if len(i) == 0:\n",
    "            print(i)\n",
    "            input()\n",
    "    packed = safe_pack_sequence(data).cuda()\n",
    "    return qt(packed).cpu().detach().numpy()\n",
    "\n",
    "feature_list = [make_batch(i) for i in range(0, size, test_batch_size)]\n",
    "print(\"Processed {:5d} batches of size {:5d}\".format(len(feature_list), test_batch_size))\n",
    "qt_features = np.concatenate(feature_list)\n",
    "print(\"Test feature matrix of shape: {}\".format(qt_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-19 15:11:23 INFO     loading Doc2Vec object from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model\n",
      "2019-07-19 15:11:24 INFO     loading vocabulary recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.vocabulary.* with mmap=None\n",
      "2019-07-19 15:11:24 INFO     loading trainables recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.trainables.* with mmap=None\n",
      "2019-07-19 15:11:24 INFO     loading syn1neg from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-07-19 15:11:25 INFO     loading wv recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.wv.* with mmap=None\n",
      "2019-07-19 15:11:25 INFO     loading vectors from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.wv.vectors.npy with mmap=None\n",
      "2019-07-19 15:11:25 INFO     loading docvecs recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.docvecs.* with mmap=None\n",
      "2019-07-19 15:11:25 INFO     loading vectors_docs from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.docvecs.vectors_docs.npy with mmap=None\n",
      "2019-07-19 15:11:29 INFO     loaded /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model\n"
     ]
    }
   ],
   "source": [
    "taboola_model_dir = \"/home/jcjessecai/workspace/taboola_model\"\n",
    "d2v = Doc2Vec.load(\"{}/{}\".format(taboola_model_dir, \"gensim_doc2vec.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569893, 600)\n"
     ]
    }
   ],
   "source": [
    "d2v_features = np.vstack([d2v.infer_vector(simple_preprocess(doc)) for doc in text])\n",
    "print(d2v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit logistic model on d2v with train acc: 49.60% test acc: 49.21%\n",
      "Fit logistic model on qt with train acc: 55.59% test acc: 55.06%\n"
     ]
    }
   ],
   "source": [
    "#first we compare embedding performance by fitting binary classifier on top\n",
    "from sklearn import linear_model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(d2v_features, labels)\n",
    "clf = linear_model.SGDClassifier(loss='log', max_iter=1000, n_jobs=20)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Fit logistic model on d2v with train acc: {:.2%} test acc: {:.2%}\".format(train_acc, test_acc))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(qt_features, labels)\n",
    "clf = linear_model.SGDClassifier(loss='log', max_iter=1000, n_jobs=20)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Fit logistic model on qt with train acc: {:.2%} test acc: {:.2%}\".format(train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
