{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import *\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "torch.cuda.set_device(7)\n",
    "#pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part000004', 'part000073', 'part000109', 'part000079', 'part000103', 'part000097', 'part000041', 'part000036', 'part000110', 'part000084', 'part000017', 'part000060', 'part000058', 'part000052', 'part000025', 'part000028', 'part000022', 'part000055', 'part000083', 'part000067', 'part000010', 'part000089', 'part000031', 'part000046', 'part000074', 'part000003', 'part000090', 'part000104', 'part000009', 'part000100', 'part000094', 'part000070', 'part000007', 'part000048', 'part000035', 'part000042', 'part000063', 'part000014', 'part000113', 'part000087', 'part000069', 'part000026', 'part000051', 'part000056', 'part000021', 'part000013', 'part000064', 'part000019', 'part000080', 'part000114', 'part000038', 'part000045', 'part000032', 'part000093', 'part000107', 'part000099', 'part000077', 'part000047', 'part000030', 'part000091', 'part000105', 'part000008', 'part000002', 'part000075', 'part000054', 'part000023', 'part000029', 'part000011', 'part000066', 'part000088', 'part000082', 'part000061', 'part000016', 'part000111', 'part000085', 'part000024', 'part000053', 'part000059', 'part000078', 'part000102', 'part000096', 'part000072', 'part000005', 'part000108', 'part000037', 'part000040', 'part000033', 'part000044', 'part000039', 'part000098', 'part000076', 'part000001', 'part000092', 'part000106', 'part000020', 'part000057', 'part000018', 'part000081', 'part000065', 'part000012', 'part000112', 'part000086', 'part000068', 'part000015', 'part000062', 'part000050', 'part000027', 'part000006', 'part000071', 'part000101', 'part000095', 'part000043', 'part000034', 'part000049']\n"
     ]
    }
   ],
   "source": [
    "taboola_data_dir = \"/local/diq/all/StepContent__nocontent_title_desc\"\n",
    "step_lines = \"/local/diq/all/StepLines__nocontent_title_desc\"\n",
    "\n",
    "import os\n",
    "print(os.listdir(step_lines))\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "4893 4893\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n",
      "5000 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-17c08bb27d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaboola_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msl_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msl_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a1f617091509>\u001b[0m in \u001b[0;36mfile_len\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfile_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in os.listdir(step_lines):\n",
    "    sc_len = file_len(\"{}/{}\".format(taboola_data_dir, file))\n",
    "    sl_len = file_len(\"{}/{}\".format(step_lines, file))\n",
    "    print(sc_len, sl_len)\n",
    "    assert sc_len == sl_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:09<00:00, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sports', 'style and fashion', 'science', 'shopping', 'automotive and vehicles', 'business and industrial', 'food and drink', 'hobbies and interests', 'travel', 'education', 'society', 'family and parenting', 'law, govt and politics', 'real estate', 'pets', 'news', 'finance', 'health and fitness', 'home and garden', 'technology and computing', 'careers', 'art and entertainment', 'religion and spirituality'}\n",
      "{'sports': 0, 'style and fashion': 1, 'science': 2, 'shopping': 3, 'automotive and vehicles': 4, 'business and industrial': 5, 'food and drink': 6, 'hobbies and interests': 7, 'travel': 8, 'education': 9, 'society': 10, 'family and parenting': 11, 'law, govt and politics': 12, 'real estate': 13, 'pets': 14, 'news': 15, 'finance': 16, 'health and fitness': 17, 'home and garden': 18, 'technology and computing': 19, 'careers': 20, 'art and entertainment': 21, 'religion and spirituality': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "taxonomies = set()\n",
    "\n",
    "#hack for the moment (jupuyter nb in folder)\n",
    "for file in tqdm(os.listdir(step_lines)):\n",
    "    with open(\"{}/{}\".format(taboola_data_dir, file), \"r\") as f:\n",
    "        data = set(map(lambda json_obj: json_obj.get('first_level_taxonomy'), map(lambda x: json.loads(x), f.readlines())))\n",
    "        taxonomies= taxonomies.union(data)\n",
    "\n",
    "print(taxonomies)\n",
    "mapping = dict([(name, i) for i, name in enumerate(taxonomies)])\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:16<00:00,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569893\n",
      "569893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "labels = []\n",
    "\n",
    "for file in tqdm(sorted(os.listdir(step_lines))):\n",
    "    data = pd.read_csv(\"{}/{}\".format(step_lines, file), header=None, sep='\\t', quoting=csv.QUOTE_NONE)[2].tolist()\n",
    "    text+=data\n",
    "    \n",
    "    with open(\"{}/{}\".format(taboola_data_dir, file), \"r\") as f:\n",
    "        data = map(lambda json_obj: mapping.get(json_obj.get('first_level_taxonomy'), -1), map(lambda x: json.loads(x), f.readlines()))\n",
    "        labels += list(data)\n",
    "        \n",
    "    #print(len(text), len(labels))\n",
    "    assert(len(text) == len(labels))\n",
    "\n",
    "print(len(labels))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset TABOOLA with total lines: 569893\n"
     ]
    }
   ],
   "source": [
    "test_batch_size=5000\n",
    "size = len(labels)\n",
    "print(\"Loaded dataset {} with total lines: {}\".format(\"TABOOLA\", size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-18 15:00:40 INFO     loading projection weights from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-18 15:02:46 INFO     loaded (400000, 300) matrix from /home/jcjessecai/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored successfully from /home/jcjessecai/workspace/taboola/quickthoughts/checkpoints\n"
     ]
    }
   ],
   "source": [
    "#load qt model\n",
    "checkpoint_dir = '/home/jcjessecai/workspace/taboola/quickthoughts/checkpoints'\n",
    "with open(\"{}/config.json\".format(checkpoint_dir)) as fp:\n",
    "    CONFIG = json.load(fp)\n",
    "\n",
    "WV_MODEL = api.load(CONFIG['embedding'])\n",
    "qt = QuickThoughts(WV_MODEL, hidden_size=CONFIG['hidden_size'])\n",
    "trained_params = torch.load(\"{}/checkpoint_latest.pth\".format(checkpoint_dir))\n",
    "qt.load_state_dict(trained_params['state_dict'])\n",
    "qt = qt.cuda()\n",
    "qt.eval()\n",
    "print(\"Restored successfully from {}\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed   114 batches of size  5000\n",
      "Test feature matrix of shape: (569893, 1000)\n"
     ]
    }
   ],
   "source": [
    "#encode data\n",
    "def make_batch(j):\n",
    "    \"\"\"Processes one test batch of the test datset\"\"\"\n",
    "    stop_idx = min(size, j+test_batch_size)\n",
    "    batch_text, batch_labels  = text[j:stop_idx], labels[j:stop_idx]\n",
    "    data = list(map(lambda x: torch.LongTensor(prepare_sequence(x, WV_MODEL.vocab, no_zeros=True)), batch_text))\n",
    "    for i in data:\n",
    "        if len(i) == 0:\n",
    "            print(i)\n",
    "            input()\n",
    "    packed = safe_pack_sequence(data).cuda()\n",
    "    return qt(packed).cpu().detach().numpy()\n",
    "\n",
    "feature_list = [make_batch(i) for i in range(0, size, test_batch_size)]\n",
    "print(\"Processed {:5d} batches of size {:5d}\".format(len(feature_list), test_batch_size))\n",
    "qt_features = np.concatenate(feature_list)\n",
    "print(\"Test feature matrix of shape: {}\".format(qt_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-18 16:06:20 INFO     loading Doc2Vec object from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model\n",
      "/home/jcjessecai/workspace/miniconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-07-18 16:06:22 INFO     loading vocabulary recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.vocabulary.* with mmap=None\n",
      "2019-07-18 16:06:22 INFO     loading trainables recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.trainables.* with mmap=None\n",
      "2019-07-18 16:06:22 INFO     loading syn1neg from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-07-18 16:06:25 INFO     loading wv recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.wv.* with mmap=None\n",
      "2019-07-18 16:06:25 INFO     loading vectors from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.wv.vectors.npy with mmap=None\n",
      "2019-07-18 16:06:30 INFO     loading docvecs recursively from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.docvecs.* with mmap=None\n",
      "2019-07-18 16:06:30 INFO     loading vectors_docs from /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model.docvecs.vectors_docs.npy with mmap=None\n",
      "2019-07-18 16:06:59 INFO     loaded /home/jcjessecai/workspace/taboola_model/gensim_doc2vec.model\n"
     ]
    }
   ],
   "source": [
    "taboola_model_dir = \"/home/jcjessecai/workspace/taboola_model\"\n",
    "d2v = Doc2Vec.load(\"{}/{}\".format(taboola_model_dir, \"gensim_doc2vec.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569893, 600)\n"
     ]
    }
   ],
   "source": [
    "d2v_features = np.vstack([d2v.infer_vector(doc) for doc in text])\n",
    "print(d2v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first we compare embedding performance by fitting binary classifier on top\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(d2v_features, labels)\n",
    "clf = linear_model.SGDClassifier(loss='log', max_iter=1000, tol=1e-3, n_jobs=20)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Fit logistic model on d2v with s: {:3d} and train acc: {:.2%} test acc: {:.2%}\".format(s, train_acc, test_acc))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(qt_features, labels)\n",
    "clf = linear_model.SGDClassifier(loss='log', max_iter=1000, tol=1e-3, n_jobs=20)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Fit logistic model on qt with s: {:3d} and train acc: {:.2%} test acc: {:.2%}\".format(s, train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_predicted  = KMeans(n_clusters=7, n_jobs=20).fit_predict(qt_features)\n",
    "d2v_predicted  = KMeans(n_clusters=7, n_jobs=20).fit_predict(d2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjusted_rand_score(top_level_labels, qt_predicted))\n",
    "print(adjusted_rand_score(top_level_labels, d2v_predicted))\n",
    "\n",
    "print(adjusted_mutual_info_score(top_level_labels, qt_predicted))\n",
    "print(adjusted_mutual_info_score(top_level_labels, d2v_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_embedded = TSNE(n_components=2, verbose=1).fit_transform(features[:5000])\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[labels[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[predicted_spectral[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "\n",
    "fixed_xlim, fixed_ylim = (-85.6129455010451, 110.84618372125996), (-90.00594782812799, 80.691349744632)\n",
    "\n",
    "for i in range(20):\n",
    "    selected = feature_embedded[predicted_kmeans[:5000] == i]\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.scatter(selected[:, 0], selected[:, 1])\n",
    "    ax.set_ylim(fixed_ylim)\n",
    "    ax.set_xlim(fixed_xlim)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
