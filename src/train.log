Starting training
batch:          0 | loss: 5.30328 | failed:    0
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:         10 | loss: 5.30889 | failed:    0
batch:         20 | loss: 5.29623 | failed:    0
batch:         30 | loss: 5.29813 | failed:    0
batch:         40 | loss: 5.29636 | failed:    0
batch:         50 | loss: 5.29655 | failed:    0
batch:         60 | loss: 5.30172 | failed:    0
batch:         70 | loss: 5.30087 | failed:    0
batch:         80 | loss: 5.29342 | failed:    0
batch:         90 | loss: 5.29578 | failed:    0
batch:        100 | loss: 5.29751 | failed:    0
batch:        110 | loss: 5.29847 | failed:    0
batch:        120 | loss: 5.29963 | failed:    0
batch:        130 | loss: 5.29791 | failed:    0
batch:        140 | loss: 5.29203 | failed:    0
batch:        150 | loss: 5.29739 | failed:    0
batch:        160 | loss: 5.29966 | failed:    0
batch:        170 | loss: 5.30120 | failed:    0
batch:        180 | loss: 5.27735 | failed:    0
batch:        190 | loss: 5.29919 | failed:    0
batch:        200 | loss: 5.29477 | failed:    0
batch:        210 | loss: 5.30519 | failed:    0
batch:        220 | loss: 5.29645 | failed:    0
batch:        230 | loss: 5.25388 | failed:    0
batch:        240 | loss: 5.29512 | failed:    0
batch:        250 | loss: 5.29649 | failed:    0
batch:        260 | loss: 5.26965 | failed:    0
batch:        270 | loss: 5.29403 | failed:    0
batch:        280 | loss: 5.24643 | failed:    0
batch:        290 | loss: 5.24688 | failed:    0
batch:        300 | loss: 5.30258 | failed:    0
batch:        310 | loss: 5.31342 | failed:    0
batch:        320 | loss: 5.29203 | failed:    0
batch:        330 | loss: 5.29299 | failed:    0
batch:        340 | loss: 5.29336 | failed:    0
batch:        350 | loss: 5.29676 | failed:    0
batch:        360 | loss: 5.25263 | failed:    0
batch:        370 | loss: 5.27959 | failed:    0
batch:        380 | loss: 5.26601 | failed:    0
batch:        390 | loss: 5.29796 | failed:    0
batch:        400 | loss: 5.29458 | failed:    0
batch:        410 | loss: 5.30118 | failed:    0
batch:        420 | loss: 5.28701 | failed:    0
batch:        430 | loss: 5.28770 | failed:    0
batch:        440 | loss: 5.26119 | failed:    0
batch:        450 | loss: 5.27811 | failed:    0
batch:        460 | loss: 5.23902 | failed:    0
batch:        470 | loss: 5.26592 | failed:    0
batch:        480 | loss: 5.28335 | failed:    0
batch:        490 | loss: 5.29325 | failed:    0
batch:        500 | loss: 5.25889 | failed:    0
batch:        510 | loss: 5.29230 | failed:    0
batch:        520 | loss: 5.24574 | failed:    0
batch:        530 | loss: 5.22576 | failed:    0
batch:        540 | loss: 5.29271 | failed:    0
batch:        550 | loss: 5.27728 | failed:    0
batch:        560 | loss: 5.05636 | failed:    0
batch:        570 | loss: 5.34276 | failed:    0
batch:        580 | loss: 5.26428 | failed:    0
batch:        590 | loss: 5.26704 | failed:    0
batch:        600 | loss: 5.29428 | failed:    0
batch:        610 | loss: 5.26921 | failed:    0
batch:        620 | loss: 5.28971 | failed:    0
batch:        630 | loss: 5.30122 | failed:    0
batch:        640 | loss: 5.25429 | failed:    0
batch:        650 | loss: 5.27583 | failed:    0
batch:        660 | loss: 5.27486 | failed:    0
batch:        670 | loss: 5.21319 | failed:    0
batch:        680 | loss: 5.26021 | failed:    0
batch:        690 | loss: 5.26631 | failed:    0
batch:        700 | loss: 5.27930 | failed:    0
batch:        710 | loss: 5.27902 | failed:    0
batch:        720 | loss: 5.28683 | failed:    0
batch:        730 | loss: 5.30945 | failed:    0
batch:        740 | loss: 5.29373 | failed:    0
batch:        750 | loss: 5.28508 | failed:    0
batch:        760 | loss: 5.25470 | failed:    0
batch:        770 | loss: 5.28802 | failed:    0
batch:        780 | loss: 5.26828 | failed:    0
batch:        790 | loss: 5.26762 | failed:    0
batch:        800 | loss: 5.24395 | failed:    0
batch:        810 | loss: 5.30732 | failed:    0
batch:        820 | loss: 5.27184 | failed:    0
batch:        830 | loss: 5.22297 | failed:    0
batch:        840 | loss: 5.28932 | failed:    0
batch:        850 | loss: 5.27244 | failed:    0
batch:        860 | loss: 5.29105 | failed:    0
batch:        870 | loss: 5.27379 | failed:    0
batch:        880 | loss: 5.25510 | failed:    0
batch:        890 | loss: 5.25981 | failed:    0
batch:        900 | loss: 5.28021 | failed:    0
batch:        910 | loss: 5.26845 | failed:    0
batch:        920 | loss: 5.23606 | failed:    0
batch:        930 | loss: 5.23167 | failed:    0
batch:        940 | loss: 5.29966 | failed:    0
batch:        950 | loss: 5.23473 | failed:    0
batch:        960 | loss: 5.22775 | failed:    0
batch:        970 | loss: 5.25147 | failed:    0
batch:        980 | loss: 5.21472 | failed:    0
batch:        990 | loss: 5.02743 | failed:    0
batch:       1000 | loss: 5.30905 | failed:    0
batch:       1010 | loss: 5.26500 | failed:    0
batch:       1020 | loss: 5.27550 | failed:    0
batch:       1030 | loss: 5.27731 | failed:    0
batch:       1040 | loss: 5.30808 | failed:    0
batch:       1050 | loss: 5.28935 | failed:    0
batch:       1060 | loss: 5.26901 | failed:    0
batch:       1070 | loss: 5.27695 | failed:    0
batch:       1080 | loss: 5.25743 | failed:    0
batch:       1090 | loss: 5.27177 | failed:    0
batch:       1100 | loss: 5.28182 | failed:    0
batch:       1110 | loss: 5.27379 | failed:    0
batch:       1120 | loss: 5.24618 | failed:    0
batch:       1130 | loss: 5.24271 | failed:    0
batch:       1140 | loss: 5.17134 | failed:    0
batch:       1150 | loss: 5.25629 | failed:    0
batch:       1160 | loss: 5.27534 | failed:    0
batch:       1170 | loss: 5.22769 | failed:    0
batch:       1180 | loss: 5.29661 | failed:    0
batch:       1190 | loss: 5.27810 | failed:    0
batch:       1200 | loss: 5.24141 | failed:    0
batch:       1210 | loss: 5.26229 | failed:    0
batch:       1220 | loss: 5.10784 | failed:    0
batch:       1230 | loss: 5.25106 | failed:    0
batch:       1240 | loss: 5.27607 | failed:    0
batch:       1250 | loss: 5.26098 | failed:    0
batch:       1260 | loss: 5.26477 | failed:    0
batch:       1270 | loss: 5.21274 | failed:    0
batch:       1280 | loss: 5.26042 | failed:    0
batch:       1290 | loss: 5.25147 | failed:    0
batch:       1300 | loss: 5.25676 | failed:    0
batch:       1310 | loss: 5.28892 | failed:    0
batch:       1320 | loss: 5.25997 | failed:    0
batch:       1330 | loss: 5.30074 | failed:    0
batch:       1340 | loss: 5.25839 | failed:    0
batch:       1350 | loss: 5.28244 | failed:    0
batch:       1360 | loss: 5.26116 | failed:    0
batch:       1370 | loss: 5.23104 | failed:    0
batch:       1380 | loss: 5.25950 | failed:    0
batch:       1390 | loss: 5.23280 | failed:    0
batch:       1400 | loss: 5.25312 | failed:    0
batch:       1410 | loss: 5.25247 | failed:    0
batch:       1420 | loss: 5.28309 | failed:    0
batch:       1430 | loss: 5.22389 | failed:    0
batch:       1440 | loss: 5.24672 | failed:    0
batch:       1450 | loss: 5.30226 | failed:    0
batch:       1460 | loss: 5.27086 | failed:    0
batch:       1470 | loss: 5.25853 | failed:    0
batch:       1480 | loss: 5.24470 | failed:    0
batch:       1490 | loss: 5.30564 | failed:    0
batch:       1500 | loss: 5.30421 | failed:    0
batch:       1510 | loss: 5.28777 | failed:    0
batch:       1520 | loss: 5.28367 | failed:    0
batch:       1530 | loss: 5.27848 | failed:    0
batch:       1540 | loss: 5.25094 | failed:    0
batch:       1550 | loss: 5.27041 | failed:    0
batch:       1560 | loss: 5.29851 | failed:    0
batch:       1570 | loss: 5.24466 | failed:    0
batch:       1580 | loss: 5.23326 | failed:    0
batch:       1590 | loss: 5.29356 | failed:    0
batch:       1600 | loss: 5.23132 | failed:    0
batch:       1610 | loss: 5.15862 | failed:    0
batch:       1620 | loss: 5.22406 | failed:    0
batch:       1630 | loss: 5.26934 | failed:    0
batch:       1640 | loss: 5.27107 | failed:    0
batch:       1650 | loss: 5.23798 | failed:    0
batch:       1660 | loss: 5.23396 | failed:    0
batch:       1670 | loss: 5.28093 | failed:    0
batch:       1680 | loss: 5.21892 | failed:    0
batch:       1690 | loss: 5.24491 | failed:    0
batch:       1700 | loss: 5.23129 | failed:    0
batch:       1710 | loss: 5.20958 | failed:    0
batch:       1720 | loss: 5.35149 | failed:    0
batch:       1730 | loss: 5.30765 | failed:    0
batch:       1740 | loss: 5.29455 | failed:    0
batch:       1750 | loss: 5.29002 | failed:    0
batch:       1760 | loss: 5.27096 | failed:    0
batch:       1770 | loss: 5.27467 | failed:    0
batch:       1780 | loss: 5.13454 | failed:    0
batch:       1790 | loss: 5.25914 | failed:    0
batch:       1800 | loss: 5.22528 | failed:    0
batch:       1810 | loss: 5.23809 | failed:    0
batch:       1820 | loss: 5.23784 | failed:    0
batch:       1830 | loss: 5.24810 | failed:    0
batch:       1840 | loss: 5.26704 | failed:    0
batch:       1850 | loss: 5.23768 | failed:    0
batch:       1860 | loss: 5.26641 | failed:    0
batch:       1870 | loss: 5.28982 | failed:    0
batch:       1880 | loss: 5.26739 | failed:    0
batch:       1890 | loss: 5.23708 | failed:    0
batch:       1900 | loss: 5.32569 | failed:    0
batch:       1910 | loss: 5.26237 | failed:    0
batch:       1920 | loss: 5.26973 | failed:    0
batch:       1930 | loss: 5.24794 | failed:    0
batch:       1940 | loss: 5.22669 | failed:    0
batch:       1950 | loss: 5.28309 | failed:    0
batch:       1960 | loss: 5.24245 | failed:    0
batch:       1970 | loss: 5.18689 | failed:    0
batch:       1980 | loss: 5.26862 | failed:    0
batch:       1990 | loss: 5.25425 | failed:    0
batch:       2000 | loss: 5.26788 | failed:    0
batch:       2010 | loss: 5.23480 | failed:    0
batch:       2020 | loss: 5.14341 | failed:    0
batch:       2030 | loss: 5.24808 | failed:    0
batch:       2040 | loss: 5.22826 | failed:    0
batch:       2050 | loss: 5.22179 | failed:    0
batch:       2060 | loss: 5.27876 | failed:    0
batch:       2070 | loss: 5.21316 | failed:    0
batch:       2080 | loss: 5.34032 | failed:    0
batch:       2090 | loss: 5.26516 | failed:    0
batch:       2100 | loss: 5.19843 | failed:    0
batch:       2110 | loss: 5.29148 | failed:    0
batch:       2120 | loss: 5.27814 | failed:    0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       2130 | loss: 5.25380 | failed:    1
batch:       2140 | loss: 5.27157 | failed:    1
batch:       2150 | loss: 5.25770 | failed:    1
batch:       2160 | loss: 5.23676 | failed:    1
batch:       2170 | loss: 5.27219 | failed:    1
batch:       2180 | loss: 5.20231 | failed:    1
batch:       2190 | loss: 5.08215 | failed:    1
batch:       2200 | loss: 4.88050 | failed:    1
batch:       2210 | loss: 5.28791 | failed:    1
batch:       2220 | loss: 5.16967 | failed:    1
batch:       2230 | loss: 5.20934 | failed:    1
batch:       2240 | loss: 5.28141 | failed:    1
batch:       2250 | loss: 5.26256 | failed:    1
batch:       2260 | loss: 5.20869 | failed:    1
batch:       2270 | loss: 5.23390 | failed:    1
batch:       2280 | loss: 5.22514 | failed:    1
batch:       2290 | loss: 5.27781 | failed:    1
batch:       2300 | loss: 5.27719 | failed:    1
batch:       2310 | loss: 5.25549 | failed:    1
batch:       2320 | loss: 5.25259 | failed:    1
batch:       2330 | loss: 5.26287 | failed:    1
batch:       2340 | loss: 5.24863 | failed:    1
batch:       2350 | loss: 5.30978 | failed:    1
batch:       2360 | loss: 5.28333 | failed:    1
batch:       2370 | loss: 5.28372 | failed:    1
batch:       2380 | loss: 5.26646 | failed:    1
batch:       2390 | loss: 5.25711 | failed:    1
batch:       2400 | loss: 5.26840 | failed:    1
batch:       2410 | loss: 5.26611 | failed:    1
batch:       2420 | loss: 5.24554 | failed:    1
batch:       2430 | loss: 5.25514 | failed:    1
batch:       2440 | loss: 5.23946 | failed:    1
batch:       2450 | loss: 5.19891 | failed:    1
batch:       2460 | loss: 5.24057 | failed:    1
batch:       2470 | loss: 5.30913 | failed:    1
batch:       2480 | loss: 5.20641 | failed:    1
batch:       2490 | loss: 5.24515 | failed:    1
batch:       2500 | loss: 5.22722 | failed:    1
batch:       2510 | loss: 5.23026 | failed:    1
batch:       2520 | loss: 5.17015 | failed:    1
batch:       2530 | loss: 5.25894 | failed:    1
batch:       2540 | loss: 5.20881 | failed:    1
batch:       2550 | loss: 5.24255 | failed:    1
batch:       2560 | loss: 5.12950 | failed:    1
batch:       2570 | loss: 5.28938 | failed:    1
batch:       2580 | loss: 5.24516 | failed:    1
batch:       2590 | loss: 5.23879 | failed:    1
batch:       2600 | loss: 5.20401 | failed:    1
batch:       2610 | loss: 5.20788 | failed:    1
batch:       2620 | loss: 5.25797 | failed:    1
batch:       2630 | loss: 5.25681 | failed:    1
batch:       2640 | loss: 5.21025 | failed:    1
batch:       2650 | loss: 5.22613 | failed:    1
batch:       2660 | loss: 5.19301 | failed:    1
batch:       2670 | loss: 5.16446 | failed:    1
batch:       2680 | loss: 5.26632 | failed:    1
batch:       2690 | loss: 5.23721 | failed:    1
batch:       2700 | loss: 5.28855 | failed:    1
batch:       2710 | loss: 5.28717 | failed:    1
batch:       2720 | loss: 5.22665 | failed:    1
batch:       2730 | loss: 5.24063 | failed:    1
batch:       2740 | loss: 5.29465 | failed:    1
batch:       2750 | loss: 5.28693 | failed:    1
batch:       2760 | loss: 5.20975 | failed:    1
batch:       2770 | loss: 5.19332 | failed:    1
batch:       2780 | loss: 5.24238 | failed:    1
batch:       2790 | loss: 5.21141 | failed:    1
batch:       2800 | loss: 5.27691 | failed:    1
batch:       2810 | loss: 5.25645 | failed:    1
batch:       2820 | loss: 5.29782 | failed:    1
batch:       2830 | loss: 5.26031 | failed:    1
batch:       2840 | loss: 5.23217 | failed:    1
batch:       2850 | loss: 5.22320 | failed:    1
batch:       2860 | loss: 5.26302 | failed:    1
batch:       2870 | loss: 5.26477 | failed:    1
batch:       2880 | loss: 5.24132 | failed:    1
batch:       2890 | loss: 5.25052 | failed:    1
batch:       2900 | loss: 5.26249 | failed:    1
batch:       2910 | loss: 5.27224 | failed:    1
batch:       2920 | loss: 5.18422 | failed:    1
batch:       2930 | loss: 5.28150 | failed:    1
batch:       2940 | loss: 5.27899 | failed:    1
batch:       2950 | loss: 5.26125 | failed:    1
batch:       2960 | loss: 5.22583 | failed:    1
batch:       2970 | loss: 5.21564 | failed:    1
batch:       2980 | loss: 5.21617 | failed:    1
batch:       2990 | loss: 5.07416 | failed:    1
batch:       3000 | loss: 5.23705 | failed:    1
batch:       3010 | loss: 5.22136 | failed:    1
batch:       3020 | loss: 5.72307 | failed:    1
batch:       3030 | loss: 5.19530 | failed:    1
batch:       3040 | loss: 5.28120 | failed:    1
batch:       3050 | loss: 5.27397 | failed:    1
batch:       3060 | loss: 5.22229 | failed:    1
batch:       3070 | loss: 5.19569 | failed:    1
batch:       3080 | loss: 5.30191 | failed:    1
batch:       3090 | loss: 5.26786 | failed:    1
batch:       3100 | loss: 5.26961 | failed:    1
batch:       3110 | loss: 5.09148 | failed:    1
batch:       3120 | loss: 5.26292 | failed:    1
batch:       3130 | loss: 5.14927 | failed:    1
batch:       3140 | loss: 5.21823 | failed:    1
batch:       3150 | loss: 5.20224 | failed:    1
batch:       3160 | loss: 5.22990 | failed:    1
batch:       3170 | loss: 5.26977 | failed:    1
batch:       3180 | loss: 5.24902 | failed:    1
batch:       3190 | loss: 5.23907 | failed:    1
batch:       3200 | loss: 5.26057 | failed:    1
batch:       3210 | loss: 5.26812 | failed:    1
batch:       3220 | loss: 5.25350 | failed:    1
batch:       3230 | loss: 5.22170 | failed:    1
batch:       3240 | loss: 5.24348 | failed:    1
batch:       3250 | loss: 5.23517 | failed:    1
batch:       3260 | loss: 5.16825 | failed:    1
batch:       3270 | loss: 5.21116 | failed:    1
batch:       3280 | loss: 5.24496 | failed:    1
batch:       3290 | loss: 5.20728 | failed:    1
batch:       3300 | loss: 5.23982 | failed:    1
batch:       3310 | loss: 5.24180 | failed:    1
batch:       3320 | loss: 5.19222 | failed:    1
batch:       3330 | loss: 5.11985 | failed:    1
batch:       3340 | loss: 5.12440 | failed:    1
batch:       3350 | loss: 5.16865 | failed:    1
batch:       3360 | loss: 5.18298 | failed:    1
batch:       3370 | loss: 5.03472 | failed:    1
batch:       3380 | loss: 5.17102 | failed:    1
batch:       3390 | loss: 5.29348 | failed:    1
batch:       3400 | loss: 5.22949 | failed:    1
batch:       3410 | loss: 5.15502 | failed:    1
batch:       3420 | loss: 5.23254 | failed:    1
batch:       3430 | loss: 5.22576 | failed:    1
batch:       3440 | loss: 5.14403 | failed:    1
batch:       3450 | loss: 5.13545 | failed:    1
batch:       3460 | loss: 5.27387 | failed:    1
batch:       3470 | loss: 5.29572 | failed:    1
batch:       3480 | loss: 5.23684 | failed:    1
batch:       3490 | loss: 5.25296 | failed:    1
batch:       3500 | loss: 5.19469 | failed:    1
batch:       3510 | loss: 5.22563 | failed:    1
batch:       3520 | loss: 5.12092 | failed:    1
batch:       3530 | loss: 5.20551 | failed:    1
batch:       3540 | loss: 5.23601 | failed:    1
batch:       3550 | loss: 5.32467 | failed:    1
batch:       3560 | loss: 5.18369 | failed:    1
batch:       3570 | loss: 5.20330 | failed:    1
batch:       3580 | loss: 5.17362 | failed:    1
batch:       3590 | loss: 5.20899 | failed:    1
batch:       3600 | loss: 5.26791 | failed:    1
batch:       3610 | loss: 5.21288 | failed:    1
batch:       3620 | loss: 5.24763 | failed:    1
batch:       3630 | loss: 5.41735 | failed:    1
batch:       3640 | loss: 5.28234 | failed:    1
batch:       3650 | loss: 5.23892 | failed:    1
batch:       3660 | loss: 5.31918 | failed:    1
batch:       3670 | loss: 5.27841 | failed:    1
batch:       3680 | loss: 5.20423 | failed:    1
batch:       3690 | loss: 5.27778 | failed:    1
batch:       3700 | loss: 5.21630 | failed:    1
batch:       3710 | loss: 5.19106 | failed:    1
batch:       3720 | loss: 5.23846 | failed:    1
batch:       3730 | loss: 5.15512 | failed:    1
batch:       3740 | loss: 5.16038 | failed:    1
batch:       3750 | loss: 5.23786 | failed:    1
batch:       3760 | loss: 5.27030 | failed:    1
batch:       3770 | loss: 5.23232 | failed:    1
batch:       3780 | loss: 5.30764 | failed:    1
batch:       3790 | loss: 5.25291 | failed:    1
batch:       3800 | loss: 5.20264 | failed:    1
batch:       3810 | loss: 5.27001 | failed:    1
batch:       3820 | loss: 5.23993 | failed:    1
batch:       3830 | loss: 5.19300 | failed:    1
batch:       3840 | loss: 5.17399 | failed:    1
batch:       3850 | loss: 5.16446 | failed:    1
batch:       3860 | loss: 5.22410 | failed:    1
batch:       3870 | loss: 5.30795 | failed:    1
batch:       3880 | loss: 5.27218 | failed:    1
batch:       3890 | loss: 5.23307 | failed:    1
batch:       3900 | loss: 5.21099 | failed:    1
batch:       3910 | loss: 5.26490 | failed:    1
batch:       3920 | loss: 5.15833 | failed:    1
batch:       3930 | loss: 5.19734 | failed:    1
batch:       3940 | loss: 5.20738 | failed:    1
batch:       3950 | loss: 5.21372 | failed:    1
batch:       3960 | loss: 5.23924 | failed:    1
batch:       3970 | loss: 5.25216 | failed:    1
batch:       3980 | loss: 5.22324 | failed:    1
batch:       3990 | loss: 5.24187 | failed:    1
batch:       4000 | loss: 5.23861 | failed:    1
batch:       4010 | loss: 5.21885 | failed:    1
batch:       4020 | loss: 5.29017 | failed:    1
batch:       4030 | loss: 5.25548 | failed:    1
batch:       4040 | loss: 5.18965 | failed:    1
batch:       4050 | loss: 5.22789 | failed:    1
batch:       4060 | loss: 5.17370 | failed:    1
batch:       4070 | loss: 5.32262 | failed:    1
batch:       4080 | loss: 5.13702 | failed:    1
batch:       4090 | loss: 5.29170 | failed:    1
batch:       4100 | loss: 5.26291 | failed:    1
batch:       4110 | loss: 5.25607 | failed:    1
batch:       4120 | loss: 5.17755 | failed:    1
batch:       4130 | loss: 5.24196 | failed:    1
batch:       4140 | loss: 5.21315 | failed:    1
batch:       4150 | loss: 5.21065 | failed:    1
batch:       4160 | loss: 5.27471 | failed:    1
batch:       4170 | loss: 5.18100 | failed:    1
batch:       4180 | loss: 5.28390 | failed:    1
batch:       4190 | loss: 5.20350 | failed:    1
batch:       4200 | loss: 5.16502 | failed:    1
batch:       4210 | loss: 5.21635 | failed:    1
batch:       4220 | loss: 5.23072 | failed:    1
batch:       4230 | loss: 5.29600 | failed:    1
batch:       4240 | loss: 5.23799 | failed:    1
batch:       4250 | loss: 5.17650 | failed:    1
batch:       4260 | loss: 5.25079 | failed:    1
batch:       4270 | loss: 5.23213 | failed:    1
batch:       4280 | loss: 5.24100 | failed:    1
batch:       4290 | loss: 5.20468 | failed:    1
batch:       4300 | loss: 5.21148 | failed:    1
batch:       4310 | loss: 5.24334 | failed:    1
batch:       4320 | loss: 5.24036 | failed:    1
batch:       4330 | loss: 5.27358 | failed:    1
batch:       4340 | loss: 5.20460 | failed:    1
batch:       4350 | loss: 5.27731 | failed:    1
batch:       4360 | loss: 5.26135 | failed:    1
batch:       4370 | loss: 5.28164 | failed:    1
batch:       4380 | loss: 5.26189 | failed:    1
batch:       4390 | loss: 5.23705 | failed:    1
batch:       4400 | loss: 5.19942 | failed:    1
batch:       4410 | loss: 5.19512 | failed:    1
batch:       4420 | loss: 5.31258 | failed:    1
batch:       4430 | loss: 5.22547 | failed:    1
batch:       4440 | loss: 5.25858 | failed:    1
batch:       4450 | loss: 5.24231 | failed:    1
batch:       4460 | loss: 5.25457 | failed:    1
batch:       4470 | loss: 5.19179 | failed:    1
batch:       4480 | loss: 5.23553 | failed:    1
batch:       4490 | loss: 5.25378 | failed:    1
batch:       4500 | loss: 5.24707 | failed:    1
batch:       4510 | loss: 5.21744 | failed:    1
batch:       4520 | loss: 5.25367 | failed:    1
batch:       4530 | loss: 5.20827 | failed:    1
batch:       4540 | loss: 5.18932 | failed:    1
batch:       4550 | loss: 5.07979 | failed:    1
batch:       4560 | loss: 5.16146 | failed:    1
batch:       4570 | loss: 5.21175 | failed:    1
batch:       4580 | loss: 5.17701 | failed:    1
batch:       4590 | loss: 5.05189 | failed:    1
batch:       4600 | loss: 5.18676 | failed:    1
batch:       4610 | loss: 5.35626 | failed:    1
batch:       4620 | loss: 5.27740 | failed:    1
batch:       4630 | loss: 5.29616 | failed:    1
batch:       4640 | loss: 5.18579 | failed:    1
batch:       4650 | loss: 5.21429 | failed:    1
batch:       4660 | loss: 5.28454 | failed:    1
batch:       4670 | loss: 5.26043 | failed:    1
batch:       4680 | loss: 5.27262 | failed:    1
batch:       4690 | loss: 5.24704 | failed:    1
batch:       4700 | loss: 5.26510 | failed:    1
batch:       4710 | loss: 5.28004 | failed:    1
batch:       4720 | loss: 5.26552 | failed:    1
batch:       4730 | loss: 5.21318 | failed:    1
batch:       4740 | loss: 5.19998 | failed:    1
batch:       4750 | loss: 5.29214 | failed:    1
batch:       4760 | loss: 5.22807 | failed:    1
batch:       4770 | loss: 5.26174 | failed:    1
batch:       4780 | loss: 5.22732 | failed:    1
batch:       4790 | loss: 5.23553 | failed:    1
batch:       4800 | loss: 5.12497 | failed:    1
batch:       4810 | loss: 5.09752 | failed:    1
batch:       4820 | loss: 5.04439 | failed:    1
batch:       4830 | loss: 5.32149 | failed:    1
batch:       4840 | loss: 5.26095 | failed:    1
batch:       4850 | loss: 5.25405 | failed:    1
batch:       4860 | loss: 5.28302 | failed:    1
batch:       4870 | loss: 5.28133 | failed:    1
batch:       4880 | loss: 5.20661 | failed:    1
batch:       4890 | loss: 5.23836 | failed:    1
batch:       4900 | loss: 5.24244 | failed:    1
batch:       4910 | loss: 5.22818 | failed:    1
batch:       4920 | loss: 5.25646 | failed:    1
batch:       4930 | loss: 5.28220 | failed:    1
batch:       4940 | loss: 5.20526 | failed:    1
batch:       4950 | loss: 5.27993 | failed:    1
batch:       4960 | loss: 5.26581 | failed:    1
batch:       4970 | loss: 5.13971 | failed:    1
batch:       4980 | loss: 5.36486 | failed:    1
batch:       4990 | loss: 5.23939 | failed:    1
batch:       5000 | loss: 5.24714 | failed:    1
batch:       5010 | loss: 5.21293 | failed:    1
batch:       5020 | loss: 5.26165 | failed:    1
batch:       5030 | loss: 5.19325 | failed:    1
batch:       5040 | loss: 5.19978 | failed:    1
batch:       5050 | loss: 5.29050 | failed:    1
batch:       5060 | loss: 5.25739 | failed:    1
batch:       5070 | loss: 5.27577 | failed:    1
batch:       5080 | loss: 5.27532 | failed:    1
batch:       5090 | loss: 5.24417 | failed:    1
batch:       5100 | loss: 5.30629 | failed:    1
batch:       5110 | loss: 5.24981 | failed:    1
batch:       5120 | loss: 5.24886 | failed:    1
batch:       5130 | loss: 5.17835 | failed:    1
batch:       5140 | loss: 5.23638 | failed:    1
batch:       5150 | loss: 5.19796 | failed:    1
batch:       5160 | loss: 5.23817 | failed:    1
batch:       5170 | loss: 5.23611 | failed:    1
batch:       5180 | loss: 5.27701 | failed:    1
batch:       5190 | loss: 5.28278 | failed:    1
batch:       5200 | loss: 5.17396 | failed:    1
batch:       5210 | loss: 5.19612 | failed:    1
batch:       5220 | loss: 5.13997 | failed:    1
batch:       5230 | loss: 5.20318 | failed:    1
batch:       5240 | loss: 5.17283 | failed:    1
batch:       5250 | loss: 5.25247 | failed:    1
batch:       5260 | loss: 5.24799 | failed:    1
batch:       5270 | loss: 5.16300 | failed:    1
batch:       5280 | loss: 5.19657 | failed:    1
batch:       5290 | loss: 5.27157 | failed:    1
batch:       5300 | loss: 5.18315 | failed:    1
batch:       5310 | loss: 5.15334 | failed:    1
batch:       5320 | loss: 5.19108 | failed:    1
batch:       5330 | loss: 5.22645 | failed:    1
batch:       5340 | loss: 5.29199 | failed:    1
batch:       5350 | loss: 5.23956 | failed:    1
batch:       5360 | loss: 5.22632 | failed:    1
batch:       5370 | loss: 5.27714 | failed:    1
batch:       5380 | loss: 5.30716 | failed:    1
batch:       5390 | loss: 5.27746 | failed:    1
batch:       5400 | loss: 5.20102 | failed:    1
batch:       5410 | loss: 5.15101 | failed:    1
batch:       5420 | loss: 5.04211 | failed:    1
batch:       5430 | loss: 5.12190 | failed:    1
batch:       5440 | loss: 5.24162 | failed:    1
batch:       5450 | loss: 5.29959 | failed:    1
batch:       5460 | loss: 5.22040 | failed:    1
batch:       5470 | loss: 5.24151 | failed:    1
batch:       5480 | loss: 5.19979 | failed:    1
batch:       5490 | loss: 5.24474 | failed:    1
batch:       5500 | loss: 5.21110 | failed:    1
batch:       5510 | loss: 5.20969 | failed:    1
batch:       5520 | loss: 5.22394 | failed:    1
batch:       5530 | loss: 5.28456 | failed:    1
batch:       5540 | loss: 5.20552 | failed:    1
batch:       5550 | loss: 5.23140 | failed:    1
batch:       5560 | loss: 5.30240 | failed:    1
batch:       5570 | loss: 5.25115 | failed:    1
batch:       5580 | loss: 5.23762 | failed:    1
batch:       5590 | loss: 5.22873 | failed:    1
batch:       5600 | loss: 5.25824 | failed:    1
batch:       5610 | loss: 5.25157 | failed:    1
batch:       5620 | loss: 5.26769 | failed:    1
batch:       5630 | loss: 5.22023 | failed:    1
batch:       5640 | loss: 5.12806 | failed:    1
batch:       5650 | loss: 5.20780 | failed:    1
batch:       5660 | loss: 5.20801 | failed:    1
batch:       5670 | loss: 5.16537 | failed:    1
batch:       5680 | loss: 5.27136 | failed:    1
batch:       5690 | loss: 5.23236 | failed:    1
batch:       5700 | loss: 5.21675 | failed:    1
batch:       5710 | loss: 5.19121 | failed:    1
batch:       5720 | loss: 5.30309 | failed:    1
batch:       5730 | loss: 5.25000 | failed:    1
batch:       5740 | loss: 5.20092 | failed:    1
batch:       5750 | loss: 5.16176 | failed:    1
batch:       5760 | loss: 5.24124 | failed:    1
batch:       5770 | loss: 5.24066 | failed:    1
batch:       5780 | loss: 5.25523 | failed:    1
batch:       5790 | loss: 5.19129 | failed:    1
batch:       5800 | loss: 5.24600 | failed:    1
batch:       5810 | loss: 5.19559 | failed:    1
batch:       5820 | loss: 5.23231 | failed:    1
batch:       5830 | loss: 5.26540 | failed:    1
batch:       5840 | loss: 5.24095 | failed:    1
batch:       5850 | loss: 5.21336 | failed:    1
batch:       5860 | loss: 5.14131 | failed:    1
batch:       5870 | loss: 5.24907 | failed:    1
batch:       5880 | loss: 5.17290 | failed:    1
batch:       5890 | loss: 5.28987 | failed:    1
batch:       5900 | loss: 5.27090 | failed:    1
batch:       5910 | loss: 5.27124 | failed:    1
batch:       5920 | loss: 5.25769 | failed:    1
batch:       5930 | loss: 5.23797 | failed:    1
batch:       5940 | loss: 5.16314 | failed:    1
batch:       5950 | loss: 5.23650 | failed:    1
batch:       5960 | loss: 5.20501 | failed:    1
batch:       5970 | loss: 5.25007 | failed:    1
batch:       5980 | loss: 5.13174 | failed:    1
batch:       5990 | loss: 5.25813 | failed:    1
batch:       6000 | loss: 5.20560 | failed:    1
batch:       6010 | loss: 5.14313 | failed:    1
batch:       6020 | loss: 5.22443 | failed:    1
batch:       6030 | loss: 5.21616 | failed:    1
batch:       6040 | loss: 5.16615 | failed:    1
batch:       6050 | loss: 5.16631 | failed:    1
batch:       6060 | loss: 5.25364 | failed:    1
batch:       6070 | loss: 5.27157 | failed:    1
batch:       6080 | loss: 5.21819 | failed:    1
batch:       6090 | loss: 5.23098 | failed:    1
batch:       6100 | loss: 5.17965 | failed:    1
batch:       6110 | loss: 5.22087 | failed:    1
batch:       6120 | loss: 5.26227 | failed:    1
batch:       6130 | loss: 5.11665 | failed:    1
batch:       6140 | loss: 5.13916 | failed:    1
batch:       6150 | loss: 5.23008 | failed:    1
batch:       6160 | loss: 5.21614 | failed:    1
batch:       6170 | loss: 5.20004 | failed:    1
batch:       6180 | loss: 4.92112 | failed:    1
batch:       6190 | loss: 5.25360 | failed:    1
batch:       6200 | loss: 5.19691 | failed:    1
batch:       6210 | loss: 5.24935 | failed:    1
batch:       6220 | loss: 5.20389 | failed:    1
batch:       6230 | loss: 5.24079 | failed:    1
batch:       6240 | loss: 5.23366 | failed:    1
batch:       6250 | loss: 5.25061 | failed:    1
batch:       6260 | loss: 5.14214 | failed:    1
batch:       6270 | loss: 5.25868 | failed:    1
batch:       6280 | loss: 5.20381 | failed:    1
batch:       6290 | loss: 5.23946 | failed:    1
batch:       6300 | loss: 5.25150 | failed:    1
batch:       6310 | loss: 5.33334 | failed:    1
batch:       6320 | loss: 5.28833 | failed:    1
batch:       6330 | loss: 5.25571 | failed:    1
batch:       6340 | loss: 5.27622 | failed:    1
batch:       6350 | loss: 5.25135 | failed:    1
batch:       6360 | loss: 5.26060 | failed:    1
batch:       6370 | loss: 5.21993 | failed:    1
batch:       6380 | loss: 5.08593 | failed:    1
batch:       6390 | loss: 5.22142 | failed:    1
batch:       6400 | loss: 5.14146 | failed:    1
batch:       6410 | loss: 5.22722 | failed:    1
batch:       6420 | loss: 5.23070 | failed:    1
batch:       6430 | loss: 5.16903 | failed:    1
batch:       6440 | loss: 5.14675 | failed:    1
batch:       6450 | loss: 5.19758 | failed:    1
batch:       6460 | loss: 5.21112 | failed:    1
batch:       6470 | loss: 5.27438 | failed:    1
batch:       6480 | loss: 5.29071 | failed:    1
batch:       6490 | loss: 5.24664 | failed:    1
batch:       6500 | loss: 5.26859 | failed:    1
batch:       6510 | loss: 5.24818 | failed:    1
batch:       6520 | loss: 5.23990 | failed:    1
batch:       6530 | loss: 5.00467 | failed:    1
batch:       6540 | loss: 4.86263 | failed:    1
batch:       6550 | loss: 4.70050 | failed:    1
batch:       6560 | loss: 5.23673 | failed:    1
batch:       6570 | loss: 5.19811 | failed:    1
batch:       6580 | loss: 5.22485 | failed:    1
batch:       6590 | loss: 5.23550 | failed:    1
batch:       6600 | loss: 5.24157 | failed:    1
batch:       6610 | loss: 5.26788 | failed:    1
batch:       6620 | loss: 5.20110 | failed:    1
batch:       6630 | loss: 5.20253 | failed:    1
batch:       6640 | loss: 5.25782 | failed:    1
batch:       6650 | loss: 5.24062 | failed:    1
batch:       6660 | loss: 5.21939 | failed:    1
batch:       6670 | loss: 5.24371 | failed:    1
batch:       6680 | loss: 5.26488 | failed:    1
batch:       6690 | loss: 5.24077 | failed:    1
batch:       6700 | loss: 5.23132 | failed:    1
batch:       6710 | loss: 5.11203 | failed:    1
batch:       6720 | loss: 5.18160 | failed:    1
batch:       6730 | loss: 5.15137 | failed:    1
batch:       6740 | loss: 5.31392 | failed:    1
batch:       6750 | loss: 5.27232 | failed:    1
batch:       6760 | loss: 5.19064 | failed:    1
batch:       6770 | loss: 5.27472 | failed:    1
batch:       6780 | loss: 5.27423 | failed:    1
batch:       6790 | loss: 5.25052 | failed:    1
batch:       6800 | loss: 5.20142 | failed:    1
batch:       6810 | loss: 5.20419 | failed:    1
batch:       6820 | loss: 5.22042 | failed:    1
batch:       6830 | loss: 5.20813 | failed:    1
batch:       6840 | loss: 5.15742 | failed:    1
batch:       6850 | loss: 5.19935 | failed:    1
batch:       6860 | loss: 5.23355 | failed:    1
batch:       6870 | loss: 5.21299 | failed:    1
batch:       6880 | loss: 5.14177 | failed:    1
batch:       6890 | loss: 5.15376 | failed:    1
batch:       6900 | loss: 5.32124 | failed:    1
batch:       6910 | loss: 5.28090 | failed:    1
batch:       6920 | loss: 5.26907 | failed:    1
batch:       6930 | loss: 5.27207 | failed:    1
batch:       6940 | loss: 5.24486 | failed:    1
batch:       6950 | loss: 5.23830 | failed:    1
batch:       6960 | loss: 5.18307 | failed:    1
batch:       6970 | loss: 5.31293 | failed:    1
batch:       6980 | loss: 5.29236 | failed:    1
batch:       6990 | loss: 5.20181 | failed:    1
batch:       7000 | loss: 5.25874 | failed:    1
batch:       7010 | loss: 5.29617 | failed:    1
batch:       7020 | loss: 5.24844 | failed:    1
batch:       7030 | loss: 5.24350 | failed:    1
batch:       7040 | loss: 5.17761 | failed:    1
batch:       7050 | loss: 5.27210 | failed:    1
batch:       7060 | loss: 5.28528 | failed:    1
batch:       7070 | loss: 5.16212 | failed:    1
batch:       7080 | loss: 5.17618 | failed:    1
batch:       7090 | loss: 5.18923 | failed:    1
batch:       7100 | loss: 5.21449 | failed:    1
batch:       7110 | loss: 5.23998 | failed:    1
batch:       7120 | loss: 5.29715 | failed:    1
batch:       7130 | loss: 5.23844 | failed:    1
batch:       7140 | loss: 5.29481 | failed:    1
batch:       7150 | loss: 5.25687 | failed:    1
batch:       7160 | loss: 5.16777 | failed:    1
batch:       7170 | loss: 5.24831 | failed:    1
batch:       7180 | loss: 5.12547 | failed:    1
batch:       7190 | loss: 5.19367 | failed:    1
batch:       7200 | loss: 5.22871 | failed:    1
batch:       7210 | loss: 5.17304 | failed:    1
batch:       7220 | loss: 5.25761 | failed:    1
batch:       7230 | loss: 5.17385 | failed:    1
batch:       7240 | loss: 5.29192 | failed:    1
batch:       7250 | loss: 5.27959 | failed:    1
batch:       7260 | loss: 5.16517 | failed:    1
batch:       7270 | loss: 5.31264 | failed:    1
batch:       7280 | loss: 5.25940 | failed:    1
batch:       7290 | loss: 5.16803 | failed:    1
batch:       7300 | loss: 5.21222 | failed:    1
batch:       7310 | loss: 5.21202 | failed:    1
batch:       7320 | loss: 5.25879 | failed:    1
batch:       7330 | loss: 5.21726 | failed:    1
batch:       7340 | loss: 5.28022 | failed:    1
batch:       7350 | loss: 5.23883 | failed:    1
batch:       7360 | loss: 5.25459 | failed:    1
batch:       7370 | loss: 5.26437 | failed:    1
batch:       7380 | loss: 5.25961 | failed:    1
batch:       7390 | loss: 5.28110 | failed:    1
batch:       7400 | loss: 5.23520 | failed:    1
batch:       7410 | loss: 5.23592 | failed:    1
batch:       7420 | loss: 5.18045 | failed:    1
batch:       7430 | loss: 5.15744 | failed:    1
batch:       7440 | loss: 5.21097 | failed:    1
batch:       7450 | loss: 5.04226 | failed:    1
batch:       7460 | loss: 4.83957 | failed:    1
batch:       7470 | loss: 5.04567 | failed:    1
batch:       7480 | loss: 5.37265 | failed:    1
batch:       7490 | loss: 5.20120 | failed:    1
batch:       7500 | loss: 5.26691 | failed:    1
batch:       7510 | loss: 5.23105 | failed:    1
batch:       7520 | loss: 5.21703 | failed:    1
batch:       7530 | loss: 5.23813 | failed:    1
batch:       7540 | loss: 5.21484 | failed:    1
batch:       7550 | loss: 5.17341 | failed:    1
batch:       7560 | loss: 5.15464 | failed:    1
batch:       7570 | loss: 5.12893 | failed:    1
batch:       7580 | loss: 5.16991 | failed:    1
batch:       7590 | loss: 5.24391 | failed:    1
batch:       7600 | loss: 5.19855 | failed:    1
batch:       7610 | loss: 5.16414 | failed:    1
batch:       7620 | loss: 5.10279 | failed:    1
batch:       7630 | loss: 5.17402 | failed:    1
batch:       7640 | loss: 5.26728 | failed:    1
batch:       7650 | loss: 5.22162 | failed:    1
batch:       7660 | loss: 5.11301 | failed:    1
batch:       7670 | loss: 5.16180 | failed:    1
batch:       7680 | loss: 5.14131 | failed:    1
batch:       7690 | loss: 5.23947 | failed:    1
batch:       7700 | loss: 5.24840 | failed:    1
batch:       7710 | loss: 5.26573 | failed:    1
batch:       7720 | loss: 5.26334 | failed:    1
batch:       7730 | loss: 5.23333 | failed:    1
batch:       7740 | loss: 5.22601 | failed:    1
batch:       7750 | loss: 5.23797 | failed:    1
batch:       7760 | loss: 5.26809 | failed:    1
batch:       7770 | loss: 5.23392 | failed:    1
batch:       7780 | loss: 5.22552 | failed:    1
batch:       7790 | loss: 5.15436 | failed:    1
batch:       7800 | loss: 5.27599 | failed:    1
batch:       7810 | loss: 5.17295 | failed:    1
batch:       7820 | loss: 5.22015 | failed:    1
batch:       7830 | loss: 5.20764 | failed:    1
batch:       7840 | loss: 5.21382 | failed:    1
batch:       7850 | loss: 5.19536 | failed:    1
batch:       7860 | loss: 5.16456 | failed:    1
batch:       7870 | loss: 5.17903 | failed:    1
batch:       7880 | loss: 5.20072 | failed:    1
batch:       7890 | loss: 5.25298 | failed:    1
batch:       7900 | loss: 5.24283 | failed:    1
batch:       7910 | loss: 5.18728 | failed:    1
batch:       7920 | loss: 5.18625 | failed:    1
batch:       7930 | loss: 5.22932 | failed:    1
batch:       7940 | loss: 5.09659 | failed:    1
batch:       7950 | loss: 5.28026 | failed:    1
batch:       7960 | loss: 5.09550 | failed:    1
batch:       7970 | loss: 5.22590 | failed:    1
batch:       7980 | loss: 5.21879 | failed:    1
batch:       7990 | loss: 5.29480 | failed:    1
batch:       8000 | loss: 5.27388 | failed:    1
batch:       8010 | loss: 5.16797 | failed:    1
batch:       8020 | loss: 5.21998 | failed:    1
batch:       8030 | loss: 5.17248 | failed:    1
batch:       8040 | loss: 5.18875 | failed:    1
batch:       8050 | loss: 5.23510 | failed:    1
batch:       8060 | loss: 5.26171 | failed:    1
batch:       8070 | loss: 5.26754 | failed:    1
batch:       8080 | loss: 5.19755 | failed:    1
batch:       8090 | loss: 5.22537 | failed:    1
batch:       8100 | loss: 5.20173 | failed:    1
batch:       8110 | loss: 5.25238 | failed:    1
batch:       8120 | loss: 5.25018 | failed:    1
batch:       8130 | loss: 5.16904 | failed:    1
batch:       8140 | loss: 5.22101 | failed:    1
batch:       8150 | loss: 5.28031 | failed:    1
batch:       8160 | loss: 5.12474 | failed:    1
batch:       8170 | loss: 5.21771 | failed:    1
batch:       8180 | loss: 5.10114 | failed:    1
batch:       8190 | loss: 5.28245 | failed:    1
batch:       8200 | loss: 5.18555 | failed:    1
batch:       8210 | loss: 5.24194 | failed:    1
batch:       8220 | loss: 5.23547 | failed:    1
batch:       8230 | loss: 5.24589 | failed:    1
batch:       8240 | loss: 5.27370 | failed:    1
batch:       8250 | loss: 5.22443 | failed:    1
batch:       8260 | loss: 5.18710 | failed:    1
batch:       8270 | loss: 5.21196 | failed:    1
batch:       8280 | loss: 5.07380 | failed:    1
batch:       8290 | loss: 5.23248 | failed:    1
batch:       8300 | loss: 5.24911 | failed:    1
batch:       8310 | loss: 5.21391 | failed:    1
batch:       8320 | loss: 5.18570 | failed:    1
batch:       8330 | loss: 5.25534 | failed:    1
batch:       8340 | loss: 5.24489 | failed:    1
batch:       8350 | loss: 5.20402 | failed:    1
batch:       8360 | loss: 5.13566 | failed:    1
batch:       8370 | loss: 5.16254 | failed:    1
batch:       8380 | loss: 5.21601 | failed:    1
batch:       8390 | loss: 5.17696 | failed:    1
batch:       8400 | loss: 5.24965 | failed:    1
batch:       8410 | loss: 5.12109 | failed:    1
batch:       8420 | loss: 5.31126 | failed:    1
batch:       8430 | loss: 5.10056 | failed:    1
batch:       8440 | loss: 5.22266 | failed:    1
batch:       8450 | loss: 5.23781 | failed:    1
batch:       8460 | loss: 5.16579 | failed:    1
batch:       8470 | loss: 5.15797 | failed:    1
batch:       8480 | loss: 5.20586 | failed:    1
batch:       8490 | loss: 5.12750 | failed:    1
batch:       8500 | loss: 5.25376 | failed:    1
batch:       8510 | loss: 5.23855 | failed:    1
batch:       8520 | loss: 5.21480 | failed:    1
batch:       8530 | loss: 5.18631 | failed:    1
batch:       8540 | loss: 5.22912 | failed:    1
batch:       8550 | loss: 5.26170 | failed:    1
batch:       8560 | loss: 5.14276 | failed:    1
batch:       8570 | loss: 5.24867 | failed:    1
batch:       8580 | loss: 5.19639 | failed:    1
batch:       8590 | loss: 5.22068 | failed:    1
batch:       8600 | loss: 5.23103 | failed:    1
batch:       8610 | loss: 5.22553 | failed:    1
batch:       8620 | loss: 5.19953 | failed:    1
batch:       8630 | loss: 5.06805 | failed:    1
batch:       8640 | loss: 5.26468 | failed:    1
batch:       8650 | loss: 5.19967 | failed:    1
batch:       8660 | loss: 5.27259 | failed:    1
batch:       8670 | loss: 5.24177 | failed:    1
batch:       8680 | loss: 5.27394 | failed:    1
batch:       8690 | loss: 5.32292 | failed:    1
batch:       8700 | loss: 5.28137 | failed:    1
batch:       8710 | loss: 5.23763 | failed:    1
batch:       8720 | loss: 5.11868 | failed:    1
batch:       8730 | loss: 5.24770 | failed:    1
batch:       8740 | loss: 5.27631 | failed:    1
batch:       8750 | loss: 5.20740 | failed:    1
batch:       8760 | loss: 5.26324 | failed:    1
batch:       8770 | loss: 5.20353 | failed:    1
batch:       8780 | loss: 5.25312 | failed:    1
batch:       8790 | loss: 5.20143 | failed:    1
batch:       8800 | loss: 5.27182 | failed:    1
batch:       8810 | loss: 5.23500 | failed:    1
batch:       8820 | loss: 5.15438 | failed:    1
batch:       8830 | loss: 5.18406 | failed:    1
batch:       8840 | loss: 5.20568 | failed:    1
batch:       8850 | loss: 5.16716 | failed:    1
batch:       8860 | loss: 5.26771 | failed:    1
batch:       8870 | loss: 5.24826 | failed:    1
batch:       8880 | loss: 5.12818 | failed:    1
batch:       8890 | loss: 5.16299 | failed:    1
batch:       8900 | loss: 5.18920 | failed:    1
batch:       8910 | loss: 5.23879 | failed:    1
batch:       8920 | loss: 5.07683 | failed:    1
batch:       8930 | loss: 5.17609 | failed:    1
batch:       8940 | loss: 5.16516 | failed:    1
batch:       8950 | loss: 5.22090 | failed:    1
batch:       8960 | loss: 5.22721 | failed:    1
batch:       8970 | loss: 5.27111 | failed:    1
batch:       8980 | loss: 5.24256 | failed:    1
batch:       8990 | loss: 5.24412 | failed:    1
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9000 | loss: 5.16215 | failed:    2
batch:       9010 | loss: 5.25507 | failed:    2
batch:       9020 | loss: 5.20779 | failed:    2
batch:       9030 | loss: 5.27162 | failed:    2
batch:       9040 | loss: 5.16663 | failed:    2
batch:       9050 | loss: 5.22848 | failed:    2
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9060 | loss: 5.21803 | failed:    3
batch:       9070 | loss: 5.13573 | failed:    3
batch:       9080 | loss: 5.24466 | failed:    3
batch:       9090 | loss: 5.22339 | failed:    3
batch:       9100 | loss: 5.18858 | failed:    3
batch:       9110 | loss: 5.19973 | failed:    3
batch:       9120 | loss: 5.12995 | failed:    3
batch:       9130 | loss: 5.27025 | failed:    3
batch:       9140 | loss: 5.25927 | failed:    3
batch:       9150 | loss: 5.19864 | failed:    3
batch:       9160 | loss: 5.18221 | failed:    3
batch:       9170 | loss: 5.15646 | failed:    3
batch:       9180 | loss: 5.14404 | failed:    3
batch:       9190 | loss: 5.17663 | failed:    3
batch:       9200 | loss: 5.18954 | failed:    3
batch:       9210 | loss: 5.23149 | failed:    3
batch:       9220 | loss: 5.13609 | failed:    3
batch:       9230 | loss: 5.11612 | failed:    3
batch:       9240 | loss: 5.12736 | failed:    3
batch:       9250 | loss: 5.31133 | failed:    3
batch:       9260 | loss: 5.21460 | failed:    3
batch:       9270 | loss: 5.13267 | failed:    3
batch:       9280 | loss: 5.21410 | failed:    3
batch:       9290 | loss: 5.23250 | failed:    3
batch:       9300 | loss: 5.25132 | failed:    3
batch:       9310 | loss: 5.19918 | failed:    3
batch:       9320 | loss: 5.12877 | failed:    3
batch:       9330 | loss: 5.28324 | failed:    3
batch:       9340 | loss: 5.19966 | failed:    3
batch:       9350 | loss: 5.14755 | failed:    3
batch:       9360 | loss: 5.20994 | failed:    3
batch:       9370 | loss: 5.07556 | failed:    3
batch:       9380 | loss: 5.14689 | failed:    3
batch:       9390 | loss: 5.22061 | failed:    3
batch:       9400 | loss: 5.18740 | failed:    3
batch:       9410 | loss: 5.14793 | failed:    3
batch:       9420 | loss: 5.15857 | failed:    3
batch:       9430 | loss: 5.19095 | failed:    3
batch:       9440 | loss: 5.08668 | failed:    3
batch:       9450 | loss: 5.24486 | failed:    3
batch:       9460 | loss: 5.24133 | failed:    3
batch:       9470 | loss: 5.14379 | failed:    3
batch:       9480 | loss: 5.25157 | failed:    3
batch:       9490 | loss: 5.18566 | failed:    3
batch:       9500 | loss: 5.24368 | failed:    3
batch:       9510 | loss: 5.31644 | failed:    3
batch:       9520 | loss: 5.21538 | failed:    3
batch:       9530 | loss: 5.23142 | failed:    3
batch:       9540 | loss: 5.14835 | failed:    3
batch:       9550 | loss: 5.22211 | failed:    3
batch:       9560 | loss: 5.25958 | failed:    3
batch:       9570 | loss: 5.13875 | failed:    3
batch:       9580 | loss: 5.25522 | failed:    3
batch:       9590 | loss: 5.20997 | failed:    3
batch:       9600 | loss: 5.30239 | failed:    3
batch:       9610 | loss: 5.18599 | failed:    3
batch:       9620 | loss: 5.24886 | failed:    3
batch:       9630 | loss: 5.18689 | failed:    3
batch:       9640 | loss: 5.20261 | failed:    3
batch:       9650 | loss: 5.21973 | failed:    3
batch:       9660 | loss: 5.15077 | failed:    3
batch:       9670 | loss: 5.26879 | failed:    3
batch:       9680 | loss: 5.21843 | failed:    3
batch:       9690 | loss: 5.21065 | failed:    3
batch:       9700 | loss: 5.26313 | failed:    3
batch:       9710 | loss: 5.24802 | failed:    3
batch:       9720 | loss: 5.16563 | failed:    3
batch:       9730 | loss: 5.27363 | failed:    3
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9750 | loss: 5.24378 | failed:   16
batch:       9760 | loss: 5.17788 | failed:   16
batch:       9770 | loss: 5.27492 | failed:   16
batch:       9780 | loss: 5.25469 | failed:   16
batch:       9790 | loss: 5.17354 | failed:   16
batch:       9800 | loss: 5.22050 | failed:   16
batch:       9810 | loss: 5.19869 | failed:   16
batch:       9820 | loss: 5.18964 | failed:   16
batch:       9830 | loss: 5.20988 | failed:   16
batch:       9840 | loss: 5.24826 | failed:   16
batch:       9850 | loss: 5.27616 | failed:   16
batch:       9860 | loss: 5.23926 | failed:   16
batch:       9870 | loss: 5.24165 | failed:   16
batch:       9880 | loss: 5.16438 | failed:   16
batch:       9890 | loss: 5.47965 | failed:   16
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:       9900 | loss: 5.19418 | failed:   20
batch:       9910 | loss: 5.20755 | failed:   20
batch:       9920 | loss: 5.20029 | failed:   20
batch:       9930 | loss: 5.16765 | failed:   20
batch:       9940 | loss: 5.22637 | failed:   20
batch:       9950 | loss: 5.21872 | failed:   20
batch:       9960 | loss: 5.06569 | failed:   20
batch:       9970 | loss: 5.18194 | failed:   20
batch:       9980 | loss: 5.00296 | failed:   20
batch:       9990 | loss: 5.28350 | failed:   20
batch:      10000 | loss: 5.18489 | failed:   20
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      10010 | loss: 5.04640 | failed:   20
batch:      10020 | loss: 5.25815 | failed:   20
batch:      10030 | loss: 5.25626 | failed:   20
batch:      10040 | loss: 5.25254 | failed:   20
batch:      10050 | loss: 5.18158 | failed:   20
batch:      10060 | loss: 5.16702 | failed:   20
batch:      10070 | loss: 5.09786 | failed:   20
batch:      10080 | loss: 5.21595 | failed:   20
batch:      10090 | loss: 5.25231 | failed:   20
batch:      10100 | loss: 5.20768 | failed:   20
batch:      10110 | loss: 5.30294 | failed:   20
batch:      10120 | loss: 5.20649 | failed:   20
batch:      10130 | loss: 5.04642 | failed:   20
batch:      10140 | loss: 5.27403 | failed:   20
batch:      10150 | loss: 5.27252 | failed:   20
batch:      10160 | loss: 5.19265 | failed:   20
batch:      10170 | loss: 5.21730 | failed:   20
batch:      10180 | loss: 5.16888 | failed:   20
batch:      10190 | loss: 5.20476 | failed:   20
batch:      10200 | loss: 5.16596 | failed:   20
batch:      10210 | loss: 5.33742 | failed:   20
batch:      10220 | loss: 5.26733 | failed:   20
batch:      10230 | loss: 5.22024 | failed:   20
batch:      10240 | loss: 5.23585 | failed:   20
batch:      10250 | loss: 5.12279 | failed:   20
batch:      10260 | loss: 5.17712 | failed:   20
batch:      10270 | loss: 5.22278 | failed:   20
batch:      10280 | loss: 5.18792 | failed:   20
batch:      10290 | loss: 5.24440 | failed:   20
batch:      10300 | loss: 5.24982 | failed:   20
batch:      10310 | loss: 5.18671 | failed:   20
batch:      10320 | loss: 5.22886 | failed:   20
batch:      10330 | loss: 5.25548 | failed:   20
batch:      10340 | loss: 5.26980 | failed:   20
batch:      10350 | loss: 5.18789 | failed:   20
batch:      10360 | loss: 5.27985 | failed:   20
batch:      10370 | loss: 5.25966 | failed:   20
batch:      10380 | loss: 5.25685 | failed:   20
batch:      10390 | loss: 5.23877 | failed:   20
batch:      10400 | loss: 5.14836 | failed:   20
batch:      10410 | loss: 5.24492 | failed:   20
batch:      10420 | loss: 5.27365 | failed:   20
batch:      10430 | loss: 5.25588 | failed:   20
batch:      10440 | loss: 5.15281 | failed:   20
batch:      10450 | loss: 5.20619 | failed:   20
batch:      10460 | loss: 5.18974 | failed:   20
batch:      10470 | loss: 5.18790 | failed:   20
batch:      10480 | loss: 5.24461 | failed:   20
batch:      10490 | loss: 5.23713 | failed:   20
batch:      10500 | loss: 5.14378 | failed:   20
batch:      10510 | loss: 5.23660 | failed:   20
batch:      10520 | loss: 5.21151 | failed:   20
batch:      10530 | loss: 5.16708 | failed:   20
batch:      10540 | loss: 5.23873 | failed:   20
batch:      10550 | loss: 5.26658 | failed:   20
batch:      10560 | loss: 5.24371 | failed:   20
batch:      10570 | loss: 5.22774 | failed:   20
batch:      10580 | loss: 5.17569 | failed:   20
batch:      10590 | loss: 5.17555 | failed:   20
batch:      10600 | loss: 5.22660 | failed:   20
batch:      10610 | loss: 5.28268 | failed:   20
batch:      10620 | loss: 5.23639 | failed:   20
batch:      10630 | loss: 5.16397 | failed:   20
batch:      10640 | loss: 5.22461 | failed:   20
batch:      10650 | loss: 5.17582 | failed:   20
batch:      10660 | loss: 5.18272 | failed:   20
batch:      10670 | loss: 5.25665 | failed:   20
batch:      10680 | loss: 5.26320 | failed:   20
batch:      10690 | loss: 5.27232 | failed:   20
batch:      10700 | loss: 5.20704 | failed:   20
batch:      10710 | loss: 5.22057 | failed:   20
batch:      10720 | loss: 5.30226 | failed:   20
batch:      10730 | loss: 5.21230 | failed:   20
batch:      10740 | loss: 5.18878 | failed:   20
batch:      10750 | loss: 5.23722 | failed:   20
batch:      10760 | loss: 5.21026 | failed:   20
batch:      10770 | loss: 5.24476 | failed:   20
batch:      10780 | loss: 5.20625 | failed:   20
batch:      10790 | loss: 5.05412 | failed:   20
batch:      10800 | loss: 5.13314 | failed:   20
batch:      10810 | loss: 5.17311 | failed:   20
batch:      10820 | loss: 5.08954 | failed:   20
batch:      10830 | loss: 5.15700 | failed:   20
batch:      10840 | loss: 5.21350 | failed:   20
batch:      10850 | loss: 5.14722 | failed:   20
batch:      10860 | loss: 5.11392 | failed:   20
batch:      10870 | loss: 5.24091 | failed:   20
batch:      10880 | loss: 5.14513 | failed:   20
batch:      10890 | loss: 5.16661 | failed:   20
batch:      10900 | loss: 5.18517 | failed:   20
batch:      10910 | loss: 5.19916 | failed:   20
batch:      10920 | loss: 5.17777 | failed:   20
batch:      10930 | loss: 5.18385 | failed:   20
batch:      10940 | loss: 5.20837 | failed:   20
batch:      10950 | loss: 5.26117 | failed:   20
batch:      10960 | loss: 5.21949 | failed:   20
batch:      10970 | loss: 5.15803 | failed:   20
batch:      10980 | loss: 5.21252 | failed:   20
batch:      10990 | loss: 5.25267 | failed:   20
batch:      11000 | loss: 5.29836 | failed:   20
batch:      11010 | loss: 5.17136 | failed:   20
batch:      11020 | loss: 5.20092 | failed:   20
batch:      11030 | loss: 5.16063 | failed:   20
batch:      11040 | loss: 5.28711 | failed:   20
batch:      11050 | loss: 5.25283 | failed:   20
batch:      11060 | loss: 5.14590 | failed:   20
batch:      11070 | loss: 5.25115 | failed:   20
batch:      11080 | loss: 5.25403 | failed:   20
batch:      11090 | loss: 5.24516 | failed:   20
batch:      11100 | loss: 5.23946 | failed:   20
batch:      11110 | loss: 5.27221 | failed:   20
batch:      11120 | loss: 5.23762 | failed:   20
batch:      11130 | loss: 5.20174 | failed:   20
batch:      11140 | loss: 5.16406 | failed:   20
batch:      11150 | loss: 5.05484 | failed:   20
batch:      11160 | loss: 5.21700 | failed:   20
batch:      11170 | loss: 5.15985 | failed:   20
batch:      11180 | loss: 5.15348 | failed:   20
batch:      11190 | loss: 5.12546 | failed:   20
batch:      11200 | loss: 5.20635 | failed:   20
batch:      11210 | loss: 5.22890 | failed:   20
batch:      11220 | loss: 5.14225 | failed:   20
batch:      11230 | loss: 5.21450 | failed:   20
batch:      11240 | loss: 5.22127 | failed:   20
batch:      11250 | loss: 5.14809 | failed:   20
batch:      11260 | loss: 5.13552 | failed:   20
batch:      11270 | loss: 5.26194 | failed:   20
batch:      11280 | loss: 5.14835 | failed:   20
batch:      11290 | loss: 5.17993 | failed:   20
batch:      11300 | loss: 5.24150 | failed:   20
batch:      11310 | loss: 5.16577 | failed:   20
batch:      11320 | loss: 5.15019 | failed:   20
batch:      11330 | loss: 5.15098 | failed:   20
batch:      11340 | loss: 5.21397 | failed:   20
batch:      11350 | loss: 5.08393 | failed:   20
batch:      11360 | loss: 5.10252 | failed:   20
batch:      11370 | loss: 5.13729 | failed:   20
batch:      11380 | loss: 5.16116 | failed:   20
batch:      11390 | loss: 5.21592 | failed:   20
batch:      11400 | loss: 5.24545 | failed:   20
batch:      11410 | loss: 5.19533 | failed:   20
batch:      11420 | loss: 5.10410 | failed:   20
batch:      11430 | loss: 5.21633 | failed:   20
batch:      11440 | loss: 5.18444 | failed:   20
batch:      11450 | loss: 5.17327 | failed:   20
batch:      11460 | loss: 5.10377 | failed:   20
batch:      11470 | loss: 5.11299 | failed:   20
batch:      11480 | loss: 5.15826 | failed:   20
batch:      11490 | loss: 5.11296 | failed:   20
batch:      11500 | loss: 5.26385 | failed:   20
batch:      11510 | loss: 5.15397 | failed:   20
batch:      11520 | loss: 5.31343 | failed:   20
batch:      11530 | loss: 5.22511 | failed:   20
batch:      11540 | loss: 5.21623 | failed:   20
batch:      11550 | loss: 5.23100 | failed:   20
batch:      11560 | loss: 5.17452 | failed:   20
batch:      11570 | loss: 5.16352 | failed:   20
batch:      11580 | loss: 5.22649 | failed:   20
batch:      11590 | loss: 5.18906 | failed:   20
batch:      11600 | loss: 5.21786 | failed:   20
batch:      11610 | loss: 5.16744 | failed:   20
batch:      11620 | loss: 5.17130 | failed:   20
batch:      11630 | loss: 5.21546 | failed:   20
batch:      11640 | loss: 5.28248 | failed:   20
batch:      11650 | loss: 5.27906 | failed:   20
batch:      11660 | loss: 5.04896 | failed:   20
batch:      11670 | loss: 5.13216 | failed:   20
batch:      11680 | loss: 5.21690 | failed:   20
batch:      11690 | loss: 5.11643 | failed:   20
batch:      11700 | loss: 5.21507 | failed:   20
batch:      11710 | loss: 5.13311 | failed:   20
batch:      11720 | loss: 5.22546 | failed:   20
batch:      11730 | loss: 5.07013 | failed:   20
batch:      11740 | loss: 5.15046 | failed:   20
batch:      11750 | loss: 5.16887 | failed:   20
batch:      11760 | loss: 5.11428 | failed:   20
batch:      11770 | loss: 5.26536 | failed:   20
batch:      11780 | loss: 5.16752 | failed:   20
batch:      11790 | loss: 5.17491 | failed:   20
batch:      11800 | loss: 5.09142 | failed:   20
batch:      11810 | loss: 5.02025 | failed:   20
batch:      11820 | loss: 5.25938 | failed:   20
batch:      11830 | loss: 5.20526 | failed:   20
batch:      11840 | loss: 5.15807 | failed:   20
batch:      11850 | loss: 5.20019 | failed:   20
batch:      11860 | loss: 5.21971 | failed:   20
batch:      11870 | loss: 5.15522 | failed:   20
batch:      11880 | loss: 5.16240 | failed:   20
batch:      11890 | loss: 5.19517 | failed:   20
batch:      11900 | loss: 5.24933 | failed:   20
batch:      11910 | loss: 5.24755 | failed:   20
batch:      11920 | loss: 5.19617 | failed:   20
batch:      11930 | loss: 5.14700 | failed:   20
batch:      11940 | loss: 5.18288 | failed:   20
batch:      11950 | loss: 5.25723 | failed:   20
batch:      11960 | loss: 5.26734 | failed:   20
batch:      11970 | loss: 5.24769 | failed:   20
batch:      11980 | loss: 5.21163 | failed:   20
batch:      11990 | loss: 5.21369 | failed:   20
batch:      12000 | loss: 5.24565 | failed:   20
batch:      12010 | loss: 5.27083 | failed:   20
batch:      12020 | loss: 5.24763 | failed:   20
batch:      12030 | loss: 5.26285 | failed:   20
batch:      12040 | loss: 5.19728 | failed:   20
batch:      12050 | loss: 5.25895 | failed:   20
batch:      12060 | loss: 5.25129 | failed:   20
batch:      12070 | loss: 5.14685 | failed:   20
batch:      12080 | loss: 5.23394 | failed:   20
batch:      12090 | loss: 5.09485 | failed:   20
batch:      12100 | loss: 5.24166 | failed:   20
batch:      12110 | loss: 5.10681 | failed:   20
batch:      12120 | loss: 5.24520 | failed:   20
batch:      12130 | loss: 5.17439 | failed:   20
batch:      12140 | loss: 5.16401 | failed:   20
batch:      12150 | loss: 5.15978 | failed:   20
batch:      12160 | loss: 5.20083 | failed:   20
batch:      12170 | loss: 5.21505 | failed:   20
batch:      12180 | loss: 5.26744 | failed:   20
batch:      12190 | loss: 5.05733 | failed:   20
batch:      12200 | loss: 5.04002 | failed:   20
batch:      12210 | loss: 5.25349 | failed:   20
batch:      12220 | loss: 5.21661 | failed:   20
batch:      12230 | loss: 5.20166 | failed:   20
batch:      12240 | loss: 5.18039 | failed:   20
batch:      12250 | loss: 5.16747 | failed:   20
batch:      12260 | loss: 5.24408 | failed:   20
batch:      12270 | loss: 5.15168 | failed:   20
batch:      12280 | loss: 5.21058 | failed:   20
batch:      12290 | loss: 5.34503 | failed:   20
batch:      12300 | loss: 5.22697 | failed:   20
batch:      12310 | loss: 5.21444 | failed:   20
batch:      12320 | loss: 5.22357 | failed:   20
batch:      12330 | loss: 5.25861 | failed:   20
batch:      12340 | loss: 5.18086 | failed:   20
batch:      12350 | loss: 5.23379 | failed:   20
batch:      12360 | loss: 5.15326 | failed:   20
batch:      12370 | loss: 5.16787 | failed:   20
batch:      12380 | loss: 5.31161 | failed:   20
batch:      12390 | loss: 5.16042 | failed:   20
batch:      12400 | loss: 5.30810 | failed:   20
batch:      12410 | loss: 5.24075 | failed:   20
batch:      12420 | loss: 5.17720 | failed:   20
batch:      12430 | loss: 5.20551 | failed:   20
batch:      12440 | loss: 5.26612 | failed:   20
batch:      12450 | loss: 5.14989 | failed:   20
batch:      12460 | loss: 5.14195 | failed:   20
batch:      12470 | loss: 5.19831 | failed:   20
batch:      12480 | loss: 5.12619 | failed:   20
batch:      12490 | loss: 5.17256 | failed:   20
batch:      12500 | loss: 5.16654 | failed:   20
batch:      12510 | loss: 5.07273 | failed:   20
batch:      12520 | loss: 5.20152 | failed:   20
batch:      12530 | loss: 5.27493 | failed:   20
batch:      12540 | loss: 5.22002 | failed:   20
batch:      12550 | loss: 5.29823 | failed:   20
batch:      12560 | loss: 5.25435 | failed:   20
batch:      12570 | loss: 5.19902 | failed:   20
batch:      12580 | loss: 5.20553 | failed:   20
batch:      12590 | loss: 5.19381 | failed:   20
batch:      12600 | loss: 5.17005 | failed:   20
batch:      12610 | loss: 5.20879 | failed:   20
batch:      12620 | loss: 5.19265 | failed:   20
batch:      12630 | loss: 5.12805 | failed:   20
batch:      12640 | loss: 5.10612 | failed:   20
batch:      12650 | loss: 5.21936 | failed:   20
batch:      12660 | loss: 5.20564 | failed:   20
batch:      12670 | loss: 5.19181 | failed:   20
batch:      12680 | loss: 5.09704 | failed:   20
batch:      12690 | loss: 5.15176 | failed:   20
batch:      12700 | loss: 5.35977 | failed:   20
batch:      12710 | loss: 5.30280 | failed:   20
batch:      12720 | loss: 5.17767 | failed:   20
batch:      12730 | loss: 5.22125 | failed:   20
batch:      12740 | loss: 5.24884 | failed:   20
batch:      12750 | loss: 5.18183 | failed:   20
batch:      12760 | loss: 5.11875 | failed:   20
batch:      12770 | loss: 5.06491 | failed:   20
batch:      12780 | loss: 4.93719 | failed:   20
batch:      12790 | loss: 5.09180 | failed:   20
batch:      12800 | loss: 5.13651 | failed:   20
batch:      12810 | loss: 5.13747 | failed:   20
batch:      12820 | loss: 5.28669 | failed:   20
batch:      12830 | loss: 5.17828 | failed:   20
batch:      12840 | loss: 5.20007 | failed:   20
batch:      12850 | loss: 5.24160 | failed:   20
batch:      12860 | loss: 5.09981 | failed:   20
batch:      12870 | loss: 5.14496 | failed:   20
batch:      12880 | loss: 5.20928 | failed:   20
batch:      12890 | loss: 5.15827 | failed:   20
batch:      12900 | loss: 5.04073 | failed:   20
batch:      12910 | loss: 5.01063 | failed:   20
batch:      12920 | loss: 5.25511 | failed:   20
batch:      12930 | loss: 5.19425 | failed:   20
batch:      12940 | loss: 5.29059 | failed:   20
batch:      12950 | loss: 5.16060 | failed:   20
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      12960 | loss: 5.24816 | failed:   21
batch:      12970 | loss: 5.13794 | failed:   21
batch:      12980 | loss: 5.17622 | failed:   21
batch:      12990 | loss: 5.20916 | failed:   21
batch:      13000 | loss: 5.19967 | failed:   21
batch:      13010 | loss: 5.11767 | failed:   21
batch:      13020 | loss: 5.18366 | failed:   21
batch:      13030 | loss: 5.35628 | failed:   21
batch:      13040 | loss: 5.32168 | failed:   21
batch:      13050 | loss: 5.22238 | failed:   21
batch:      13060 | loss: 5.24977 | failed:   21
batch:      13070 | loss: 5.26611 | failed:   21
batch:      13080 | loss: 5.23294 | failed:   21
batch:      13090 | loss: 5.21537 | failed:   21
batch:      13100 | loss: 5.28955 | failed:   21
batch:      13110 | loss: 5.21168 | failed:   21
batch:      13120 | loss: 5.17425 | failed:   21
batch:      13130 | loss: 5.17112 | failed:   21
batch:      13140 | loss: 5.18068 | failed:   21
batch:      13150 | loss: 5.15040 | failed:   21
batch:      13160 | loss: 5.16630 | failed:   21
batch:      13170 | loss: 5.19964 | failed:   21
batch:      13180 | loss: 5.20377 | failed:   21
batch:      13190 | loss: 5.16988 | failed:   21
batch:      13200 | loss: 5.14035 | failed:   21
batch:      13210 | loss: 5.17264 | failed:   21
batch:      13220 | loss: 5.21662 | failed:   21
batch:      13230 | loss: 5.18556 | failed:   21
batch:      13240 | loss: 5.06139 | failed:   21
batch:      13250 | loss: 5.23193 | failed:   21
batch:      13260 | loss: 5.24071 | failed:   21
batch:      13270 | loss: 5.17878 | failed:   21
batch:      13280 | loss: 5.23463 | failed:   21
batch:      13290 | loss: 5.24365 | failed:   21
batch:      13300 | loss: 5.20910 | failed:   21
batch:      13310 | loss: 5.15917 | failed:   21
batch:      13320 | loss: 5.23859 | failed:   21
batch:      13330 | loss: 5.19062 | failed:   21
batch:      13340 | loss: 5.12153 | failed:   21
batch:      13350 | loss: 5.13609 | failed:   21
batch:      13360 | loss: 5.21929 | failed:   21
batch:      13370 | loss: 5.16939 | failed:   21
batch:      13380 | loss: 5.21965 | failed:   21
batch:      13390 | loss: 5.25336 | failed:   21
batch:      13400 | loss: 5.16623 | failed:   21
batch:      13410 | loss: 5.18672 | failed:   21
batch:      13420 | loss: 5.24806 | failed:   21
batch:      13430 | loss: 5.08802 | failed:   21
batch:      13440 | loss: 5.06733 | failed:   21
batch:      13450 | loss: 4.93350 | failed:   21
batch:      13460 | loss: 5.04701 | failed:   21
batch:      13470 | loss: 5.04420 | failed:   21
batch:      13480 | loss: 5.30725 | failed:   21
batch:      13490 | loss: 4.95909 | failed:   21
batch:      13500 | loss: 5.27917 | failed:   21
batch:      13510 | loss: 5.20158 | failed:   21
batch:      13520 | loss: 5.21811 | failed:   21
batch:      13530 | loss: 5.21034 | failed:   21
batch:      13540 | loss: 5.04936 | failed:   21
batch:      13550 | loss: 5.12391 | failed:   21
batch:      13560 | loss: 5.24747 | failed:   21
batch:      13570 | loss: 5.14169 | failed:   21
batch:      13580 | loss: 5.23274 | failed:   21
batch:      13590 | loss: 5.22998 | failed:   21
batch:      13600 | loss: 5.16927 | failed:   21
batch:      13610 | loss: 5.18870 | failed:   21
batch:      13620 | loss: 5.15157 | failed:   21
batch:      13630 | loss: 5.06472 | failed:   21
batch:      13640 | loss: 5.15519 | failed:   21
batch:      13650 | loss: 5.21143 | failed:   21
batch:      13660 | loss: 5.18916 | failed:   21
batch:      13670 | loss: 5.10467 | failed:   21
batch:      13680 | loss: 5.10837 | failed:   21
batch:      13690 | loss: 5.16387 | failed:   21
batch:      13700 | loss: 5.21012 | failed:   21
batch:      13710 | loss: 5.05524 | failed:   21
batch:      13720 | loss: 5.25586 | failed:   21
batch:      13730 | loss: 5.21127 | failed:   21
batch:      13740 | loss: 5.22731 | failed:   21
batch:      13750 | loss: 5.21200 | failed:   21
batch:      13760 | loss: 5.23210 | failed:   21
batch:      13770 | loss: 5.19520 | failed:   21
batch:      13780 | loss: 5.23382 | failed:   21
batch:      13790 | loss: 5.02528 | failed:   21
batch:      13800 | loss: 5.15833 | failed:   21
batch:      13810 | loss: 5.24381 | failed:   21
batch:      13820 | loss: 5.20207 | failed:   21
batch:      13830 | loss: 5.12601 | failed:   21
batch:      13840 | loss: 5.19659 | failed:   21
batch:      13850 | loss: 5.30554 | failed:   21
batch:      13860 | loss: 5.22936 | failed:   21
batch:      13870 | loss: 5.10043 | failed:   21
batch:      13880 | loss: 5.30402 | failed:   21
batch:      13890 | loss: 5.12959 | failed:   21
batch:      13900 | loss: 5.14163 | failed:   21
batch:      13910 | loss: 4.93476 | failed:   21
batch:      13920 | loss: 5.27266 | failed:   21
batch:      13930 | loss: 5.25203 | failed:   21
batch:      13940 | loss: 5.22756 | failed:   21
batch:      13950 | loss: 5.19187 | failed:   21
batch:      13960 | loss: 5.12813 | failed:   21
batch:      13970 | loss: 5.20021 | failed:   21
batch:      13980 | loss: 5.11427 | failed:   21
batch:      13990 | loss: 5.16228 | failed:   21
batch:      14000 | loss: 5.22054 | failed:   21
batch:      14010 | loss: 5.13599 | failed:   21
batch:      14020 | loss: 5.21562 | failed:   21
batch:      14030 | loss: 5.19246 | failed:   21
batch:      14040 | loss: 5.10301 | failed:   21
batch:      14050 | loss: 5.15688 | failed:   21
batch:      14060 | loss: 5.16901 | failed:   21
batch:      14070 | loss: 5.15361 | failed:   21
batch:      14080 | loss: 5.13258 | failed:   21
batch:      14090 | loss: 5.16350 | failed:   21
batch:      14100 | loss: 5.28762 | failed:   21
batch:      14110 | loss: 5.17094 | failed:   21
batch:      14120 | loss: 5.22204 | failed:   21
batch:      14130 | loss: 5.19222 | failed:   21
batch:      14140 | loss: 5.17055 | failed:   21
batch:      14150 | loss: 5.27494 | failed:   21
batch:      14160 | loss: 5.19874 | failed:   21
batch:      14170 | loss: 5.12467 | failed:   21
batch:      14180 | loss: 5.25571 | failed:   21
batch:      14190 | loss: 5.13266 | failed:   21
batch:      14200 | loss: 5.10846 | failed:   21
batch:      14210 | loss: 5.19956 | failed:   21
batch:      14220 | loss: 5.24484 | failed:   21
batch:      14230 | loss: 5.25169 | failed:   21
batch:      14240 | loss: 5.21170 | failed:   21
batch:      14250 | loss: 5.12701 | failed:   21
batch:      14260 | loss: 5.14739 | failed:   21
batch:      14270 | loss: 5.28554 | failed:   21
batch:      14280 | loss: 5.28285 | failed:   21
batch:      14290 | loss: 5.20278 | failed:   21
batch:      14300 | loss: 5.16093 | failed:   21
batch:      14310 | loss: 5.24230 | failed:   21
batch:      14320 | loss: 5.22049 | failed:   21
batch:      14330 | loss: 5.13004 | failed:   21
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      14350 | loss: 5.27860 | failed:   25
batch:      14360 | loss: 5.32656 | failed:   25
batch:      14370 | loss: 5.21522 | failed:   25
batch:      14380 | loss: 5.23320 | failed:   25
batch:      14390 | loss: 5.20829 | failed:   25
batch:      14400 | loss: 5.16488 | failed:   25
batch:      14410 | loss: 4.98351 | failed:   25
batch:      14420 | loss: 5.26258 | failed:   25
batch:      14430 | loss: 5.23927 | failed:   25
batch:      14440 | loss: 5.27070 | failed:   25
batch:      14450 | loss: 5.22294 | failed:   25
batch:      14460 | loss: 5.21831 | failed:   25
batch:      14470 | loss: 5.14752 | failed:   25
batch:      14480 | loss: 5.19709 | failed:   25
batch:      14490 | loss: 5.21270 | failed:   25
batch:      14500 | loss: 5.21592 | failed:   25
batch:      14510 | loss: 5.18781 | failed:   25
batch:      14520 | loss: 5.28276 | failed:   25
batch:      14530 | loss: 5.20184 | failed:   25
batch:      14540 | loss: 5.14128 | failed:   25
batch:      14550 | loss: 5.21304 | failed:   25
batch:      14560 | loss: 5.20940 | failed:   25
batch:      14570 | loss: 5.18463 | failed:   25
batch:      14580 | loss: 5.25787 | failed:   25
batch:      14590 | loss: 5.22723 | failed:   25
batch:      14600 | loss: 5.11514 | failed:   25
batch:      14610 | loss: 5.25852 | failed:   25
batch:      14620 | loss: 5.20931 | failed:   25
batch:      14630 | loss: 5.24444 | failed:   25
batch:      14640 | loss: 5.22819 | failed:   25
batch:      14650 | loss: 5.11925 | failed:   25
batch:      14660 | loss: 5.09291 | failed:   25
batch:      14670 | loss: 5.13250 | failed:   25
batch:      14680 | loss: 5.30896 | failed:   25
batch:      14690 | loss: 5.09510 | failed:   25
batch:      14700 | loss: 5.17456 | failed:   25
batch:      14710 | loss: 5.21991 | failed:   25
batch:      14720 | loss: 5.14777 | failed:   25
batch:      14730 | loss: 5.25595 | failed:   25
batch:      14740 | loss: 5.25800 | failed:   25
batch:      14750 | loss: 5.26220 | failed:   25
batch:      14760 | loss: 5.22642 | failed:   25
batch:      14770 | loss: 5.27211 | failed:   25
batch:      14780 | loss: 5.21038 | failed:   25
batch:      14790 | loss: 5.19913 | failed:   25
batch:      14800 | loss: 5.09268 | failed:   25
batch:      14810 | loss: 5.18378 | failed:   25
batch:      14820 | loss: 5.14141 | failed:   25
batch:      14830 | loss: 5.23403 | failed:   25
batch:      14840 | loss: 5.11881 | failed:   25
batch:      14850 | loss: 5.27811 | failed:   25
batch:      14860 | loss: 5.19485 | failed:   25
batch:      14870 | loss: 5.22427 | failed:   25
batch:      14880 | loss: 5.19441 | failed:   25
batch:      14890 | loss: 5.21025 | failed:   25
batch:      14900 | loss: 5.21930 | failed:   25
batch:      14910 | loss: 5.23025 | failed:   25
batch:      14920 | loss: 5.27744 | failed:   25
batch:      14930 | loss: 5.26776 | failed:   25
batch:      14940 | loss: 5.15046 | failed:   25
batch:      14950 | loss: 5.30057 | failed:   25
batch:      14960 | loss: 5.30911 | failed:   25
batch:      14970 | loss: 5.24365 | failed:   25
batch:      14980 | loss: 5.19391 | failed:   25
batch:      14990 | loss: 5.21326 | failed:   25
batch:      15000 | loss: 5.22026 | failed:   25
batch:      15010 | loss: 5.20548 | failed:   25
batch:      15020 | loss: 5.24113 | failed:   25
batch:      15030 | loss: 5.14809 | failed:   25
batch:      15040 | loss: 5.18192 | failed:   25
batch:      15050 | loss: 5.24011 | failed:   25
batch:      15060 | loss: 5.30408 | failed:   25
batch:      15070 | loss: 5.22994 | failed:   25
batch:      15080 | loss: 5.09327 | failed:   25
batch:      15090 | loss: 5.27024 | failed:   25
batch:      15100 | loss: 5.13381 | failed:   25
batch:      15110 | loss: 5.16932 | failed:   25
batch:      15120 | loss: 5.14190 | failed:   25
batch:      15130 | loss: 5.33455 | failed:   25
batch:      15140 | loss: 5.06733 | failed:   25
batch:      15150 | loss: 5.05777 | failed:   25
batch:      15160 | loss: 5.21789 | failed:   25
batch:      15170 | loss: 5.17371 | failed:   25
batch:      15180 | loss: 5.08397 | failed:   25
batch:      15190 | loss: 5.17655 | failed:   25
batch:      15200 | loss: 5.08337 | failed:   25
batch:      15210 | loss: 5.19868 | failed:   25
batch:      15220 | loss: 5.22525 | failed:   25
batch:      15230 | loss: 5.14694 | failed:   25
batch:      15240 | loss: 5.25724 | failed:   25
batch:      15250 | loss: 5.26689 | failed:   25
batch:      15260 | loss: 5.19370 | failed:   25
batch:      15270 | loss: 5.21505 | failed:   25
batch:      15280 | loss: 5.24609 | failed:   25
batch:      15290 | loss: 5.18471 | failed:   25
batch:      15300 | loss: 5.19720 | failed:   25
batch:      15310 | loss: 5.30131 | failed:   25
batch:      15320 | loss: 5.24455 | failed:   25
batch:      15330 | loss: 5.16887 | failed:   25
batch:      15340 | loss: 5.23081 | failed:   25
batch:      15350 | loss: 5.26340 | failed:   25
batch:      15360 | loss: 5.23025 | failed:   25
batch:      15370 | loss: 5.22670 | failed:   25
batch:      15380 | loss: 5.23511 | failed:   25
batch:      15390 | loss: 5.20046 | failed:   25
batch:      15400 | loss: 5.27307 | failed:   25
batch:      15410 | loss: 5.20984 | failed:   25
batch:      15420 | loss: 5.18430 | failed:   25
batch:      15430 | loss: 5.20459 | failed:   25
batch:      15440 | loss: 5.14504 | failed:   25
batch:      15450 | loss: 4.99680 | failed:   25
batch:      15460 | loss: 5.25595 | failed:   25
batch:      15470 | loss: 5.16579 | failed:   25
batch:      15480 | loss: 5.14076 | failed:   25
batch:      15490 | loss: 5.17365 | failed:   25
batch:      15500 | loss: 5.19282 | failed:   25
batch:      15510 | loss: 5.11455 | failed:   25
batch:      15520 | loss: 5.11913 | failed:   25
batch:      15530 | loss: 5.18038 | failed:   25
batch:      15540 | loss: 5.26158 | failed:   25
batch:      15550 | loss: 5.21562 | failed:   25
batch:      15560 | loss: 5.13933 | failed:   25
batch:      15570 | loss: 5.25578 | failed:   25
batch:      15580 | loss: 5.21598 | failed:   25
batch:      15590 | loss: 5.30480 | failed:   25
batch:      15600 | loss: 5.24770 | failed:   25
batch:      15610 | loss: 5.17098 | failed:   25
batch:      15620 | loss: 5.24171 | failed:   25
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      15650 | loss: 5.20772 | failed:   41
batch:      15660 | loss: 5.25640 | failed:   41
batch:      15670 | loss: 5.23281 | failed:   41
batch:      15680 | loss: 5.24510 | failed:   41
batch:      15690 | loss: 5.18146 | failed:   41
batch:      15700 | loss: 5.09496 | failed:   41
batch:      15710 | loss: 5.17878 | failed:   41
batch:      15720 | loss: 5.18628 | failed:   41
batch:      15730 | loss: 5.19975 | failed:   41
batch:      15740 | loss: 5.27792 | failed:   41
batch:      15750 | loss: 5.26581 | failed:   41
batch:      15760 | loss: 5.28975 | failed:   41
batch:      15770 | loss: 5.19615 | failed:   41
batch:      15780 | loss: 5.25792 | failed:   41
batch:      15790 | loss: 5.20576 | failed:   41
batch:      15800 | loss: 5.18653 | failed:   41
batch:      15810 | loss: 5.25351 | failed:   41
batch:      15820 | loss: 5.20416 | failed:   41
batch:      15830 | loss: 5.22958 | failed:   41
batch:      15840 | loss: 5.24227 | failed:   41
batch:      15850 | loss: 5.24053 | failed:   41
batch:      15860 | loss: 5.23131 | failed:   41
batch:      15870 | loss: 5.20719 | failed:   41
batch:      15880 | loss: 5.17179 | failed:   41
batch:      15890 | loss: 5.08046 | failed:   41
batch:      15900 | loss: 5.24510 | failed:   41
batch:      15910 | loss: 5.21655 | failed:   41
batch:      15920 | loss: 5.16647 | failed:   41
batch:      15930 | loss: 5.15792 | failed:   41
batch:      15940 | loss: 5.19535 | failed:   41
batch:      15950 | loss: 5.22848 | failed:   41
batch:      15960 | loss: 5.32139 | failed:   41
batch:      15970 | loss: 5.22660 | failed:   41
batch:      15980 | loss: 5.25804 | failed:   41
batch:      15990 | loss: 5.26924 | failed:   41
batch:      16000 | loss: 5.16525 | failed:   41
batch:      16010 | loss: 5.14934 | failed:   41
batch:      16020 | loss: 5.17827 | failed:   41
batch:      16030 | loss: 5.24550 | failed:   41
batch:      16040 | loss: 5.20470 | failed:   41
batch:      16050 | loss: 5.22335 | failed:   41
batch:      16060 | loss: 5.14518 | failed:   41
batch:      16070 | loss: 5.19794 | failed:   41
batch:      16080 | loss: 5.17774 | failed:   41
batch:      16090 | loss: 5.23207 | failed:   41
batch:      16100 | loss: 5.18328 | failed:   41
batch:      16110 | loss: 5.21048 | failed:   41
batch:      16120 | loss: 5.25325 | failed:   41
batch:      16130 | loss: 5.24170 | failed:   41
batch:      16140 | loss: 5.29183 | failed:   41
batch:      16150 | loss: 5.22813 | failed:   41
batch:      16160 | loss: 5.17655 | failed:   41
batch:      16170 | loss: 5.23430 | failed:   41
batch:      16180 | loss: 5.15491 | failed:   41
batch:      16190 | loss: 5.22547 | failed:   41
batch:      16200 | loss: 5.07448 | failed:   41
batch:      16210 | loss: 5.19985 | failed:   41
batch:      16220 | loss: 5.05570 | failed:   41
batch:      16230 | loss: 5.21719 | failed:   41
batch:      16240 | loss: 5.19843 | failed:   41
batch:      16250 | loss: 5.20317 | failed:   41
batch:      16260 | loss: 5.24291 | failed:   41
batch:      16270 | loss: 5.20198 | failed:   41
batch:      16280 | loss: 5.20859 | failed:   41
batch:      16290 | loss: 5.18849 | failed:   41
batch:      16300 | loss: 5.13028 | failed:   41
batch:      16310 | loss: 5.12697 | failed:   41
batch:      16320 | loss: 5.15881 | failed:   41
batch:      16330 | loss: 5.29671 | failed:   41
batch:      16340 | loss: 5.20630 | failed:   41
batch:      16350 | loss: 5.24171 | failed:   41
batch:      16360 | loss: 5.28458 | failed:   41
batch:      16370 | loss: 5.24638 | failed:   41
batch:      16380 | loss: 5.25943 | failed:   41
batch:      16390 | loss: 5.19714 | failed:   41
batch:      16400 | loss: 5.15929 | failed:   41
batch:      16410 | loss: 5.17392 | failed:   41
batch:      16420 | loss: 5.20232 | failed:   41
batch:      16430 | loss: 5.25075 | failed:   41
batch:      16440 | loss: 5.11342 | failed:   41
batch:      16450 | loss: 5.22173 | failed:   41
batch:      16460 | loss: 5.15934 | failed:   41
batch:      16470 | loss: 5.22574 | failed:   41
batch:      16480 | loss: 5.21778 | failed:   41
batch:      16490 | loss: 5.18242 | failed:   41
batch:      16500 | loss: 5.12389 | failed:   41
batch:      16510 | loss: 5.26607 | failed:   41
batch:      16520 | loss: 5.16605 | failed:   41
batch:      16530 | loss: 5.20608 | failed:   41
batch:      16540 | loss: 5.27673 | failed:   41
batch:      16550 | loss: 5.29190 | failed:   41
batch:      16560 | loss: 5.25544 | failed:   41
batch:      16570 | loss: 5.14749 | failed:   41
batch:      16580 | loss: 5.10180 | failed:   41
batch:      16590 | loss: 5.20439 | failed:   41
batch:      16600 | loss: 5.22570 | failed:   41
batch:      16610 | loss: 5.05915 | failed:   41
batch:      16620 | loss: 5.18668 | failed:   41
batch:      16630 | loss: 5.19925 | failed:   41
batch:      16640 | loss: 5.15216 | failed:   41
batch:      16650 | loss: 5.15670 | failed:   41
batch:      16660 | loss: 5.20205 | failed:   41
batch:      16670 | loss: 5.18061 | failed:   41
batch:      16680 | loss: 5.23867 | failed:   41
batch:      16690 | loss: 5.22696 | failed:   41
batch:      16700 | loss: 5.12312 | failed:   41
batch:      16710 | loss: 5.19605 | failed:   41
batch:      16720 | loss: 5.18567 | failed:   41
batch:      16730 | loss: 5.20300 | failed:   41
batch:      16740 | loss: 5.10817 | failed:   41
batch:      16750 | loss: 5.16921 | failed:   41
batch:      16760 | loss: 5.20357 | failed:   41
batch:      16770 | loss: 5.09943 | failed:   41
batch:      16780 | loss: 5.10172 | failed:   41
batch:      16790 | loss: 5.20464 | failed:   41
batch:      16800 | loss: 5.16471 | failed:   41
batch:      16810 | loss: 5.24086 | failed:   41
batch:      16820 | loss: 5.13946 | failed:   41
batch:      16830 | loss: 5.25724 | failed:   41
batch:      16840 | loss: 5.17358 | failed:   41
batch:      16850 | loss: 5.12206 | failed:   41
batch:      16860 | loss: 5.15608 | failed:   41
batch:      16870 | loss: 5.17728 | failed:   41
batch:      16880 | loss: 5.17971 | failed:   41
batch:      16890 | loss: 5.35620 | failed:   41
batch:      16900 | loss: 5.12182 | failed:   41
batch:      16910 | loss: 5.27213 | failed:   41
batch:      16920 | loss: 5.23452 | failed:   41
batch:      16930 | loss: 5.21592 | failed:   41
batch:      16940 | loss: 5.15971 | failed:   41
batch:      16950 | loss: 5.28792 | failed:   41
batch:      16960 | loss: 5.23818 | failed:   41
batch:      16970 | loss: 5.24132 | failed:   41
batch:      16980 | loss: 5.24731 | failed:   41
batch:      16990 | loss: 5.19908 | failed:   41
batch:      17000 | loss: 5.21274 | failed:   41
batch:      17010 | loss: 5.23730 | failed:   41
batch:      17020 | loss: 5.23537 | failed:   41
batch:      17030 | loss: 5.15775 | failed:   41
batch:      17040 | loss: 5.03838 | failed:   41
batch:      17050 | loss: 5.10447 | failed:   41
batch:      17060 | loss: 5.14773 | failed:   41
batch:      17070 | loss: 5.25984 | failed:   41
batch:      17080 | loss: 5.27672 | failed:   41
batch:      17090 | loss: 5.22946 | failed:   41
batch:      17100 | loss: 5.25612 | failed:   41
batch:      17110 | loss: 5.30138 | failed:   41
batch:      17120 | loss: 5.20231 | failed:   41
batch:      17130 | loss: 5.12026 | failed:   41
batch:      17140 | loss: 5.17524 | failed:   41
batch:      17150 | loss: 5.11866 | failed:   41
batch:      17160 | loss: 5.02677 | failed:   41
batch:      17170 | loss: 5.21838 | failed:   41
batch:      17180 | loss: 5.20501 | failed:   41
batch:      17190 | loss: 5.23049 | failed:   41
batch:      17200 | loss: 5.16298 | failed:   41
batch:      17210 | loss: 5.23730 | failed:   41
batch:      17220 | loss: 5.09549 | failed:   41
batch:      17230 | loss: 5.21142 | failed:   41
batch:      17240 | loss: 5.08377 | failed:   41
batch:      17250 | loss: 5.25989 | failed:   41
batch:      17260 | loss: 5.11616 | failed:   41
batch:      17270 | loss: 5.14772 | failed:   41
batch:      17280 | loss: 5.13335 | failed:   41
batch:      17290 | loss: 5.12527 | failed:   41
batch:      17300 | loss: 5.15348 | failed:   41
batch:      17310 | loss: 5.15913 | failed:   41
batch:      17320 | loss: 5.11628 | failed:   41
batch:      17330 | loss: 5.01384 | failed:   41
batch:      17340 | loss: 5.22155 | failed:   41
batch:      17350 | loss: 5.29800 | failed:   41
batch:      17360 | loss: 5.12418 | failed:   41
batch:      17370 | loss: 5.22020 | failed:   41
batch:      17380 | loss: 5.22678 | failed:   41
batch:      17390 | loss: 5.17667 | failed:   41
batch:      17400 | loss: 5.23854 | failed:   41
batch:      17410 | loss: 5.22330 | failed:   41
batch:      17420 | loss: 5.18808 | failed:   41
batch:      17430 | loss: 5.20677 | failed:   41
batch:      17440 | loss: 5.22376 | failed:   41
batch:      17450 | loss: 5.26446 | failed:   41
batch:      17460 | loss: 5.21794 | failed:   41
batch:      17470 | loss: 5.22309 | failed:   41
batch:      17480 | loss: 5.15886 | failed:   41
batch:      17490 | loss: 5.13867 | failed:   41
batch:      17500 | loss: 5.03997 | failed:   41
batch:      17510 | loss: 5.17649 | failed:   41
batch:      17520 | loss: 5.16891 | failed:   41
batch:      17530 | loss: 5.18940 | failed:   41
batch:      17540 | loss: 5.25319 | failed:   41
batch:      17550 | loss: 5.28491 | failed:   41
batch:      17560 | loss: 5.25896 | failed:   41
batch:      17570 | loss: 5.20893 | failed:   41
batch:      17580 | loss: 5.21363 | failed:   41
batch:      17590 | loss: 5.10314 | failed:   41
batch:      17600 | loss: 5.20351 | failed:   41
batch:      17610 | loss: 5.19410 | failed:   41
batch:      17620 | loss: 5.11356 | failed:   41
batch:      17630 | loss: 5.02813 | failed:   41
batch:      17640 | loss: 5.25968 | failed:   41
batch:      17650 | loss: 5.24452 | failed:   41
batch:      17660 | loss: 5.24117 | failed:   41
batch:      17670 | loss: 5.22949 | failed:   41
batch:      17680 | loss: 5.13037 | failed:   41
batch:      17690 | loss: 5.24461 | failed:   41
batch:      17700 | loss: 5.21763 | failed:   41
batch:      17710 | loss: 5.24213 | failed:   41
batch:      17720 | loss: 5.24113 | failed:   41
batch:      17730 | loss: 5.18192 | failed:   41
batch:      17740 | loss: 5.17599 | failed:   41
batch:      17750 | loss: 5.15590 | failed:   41
batch:      17760 | loss: 5.23873 | failed:   41
batch:      17770 | loss: 5.23335 | failed:   41
batch:      17780 | loss: 5.16513 | failed:   41
batch:      17790 | loss: 5.22280 | failed:   41
batch:      17800 | loss: 5.17905 | failed:   41
batch:      17810 | loss: 5.24225 | failed:   41
batch:      17820 | loss: 5.15312 | failed:   41
batch:      17830 | loss: 5.06846 | failed:   41
batch:      17840 | loss: 5.11181 | failed:   41
batch:      17850 | loss: 5.18596 | failed:   41
batch:      17860 | loss: 5.15731 | failed:   41
batch:      17870 | loss: 5.20557 | failed:   41
batch:      17880 | loss: 4.96497 | failed:   41
batch:      17890 | loss: 5.07152 | failed:   41
batch:      17900 | loss: 5.15729 | failed:   41
batch:      17910 | loss: 5.19968 | failed:   41
batch:      17920 | loss: 5.19710 | failed:   41
batch:      17930 | loss: 5.11544 | failed:   41
batch:      17940 | loss: 5.14941 | failed:   41
batch:      17950 | loss: 5.23730 | failed:   41
batch:      17960 | loss: 5.19445 | failed:   41
batch:      17970 | loss: 5.13021 | failed:   41
batch:      17980 | loss: 5.25377 | failed:   41
batch:      17990 | loss: 5.26660 | failed:   41
batch:      18000 | loss: 5.17937 | failed:   41
batch:      18010 | loss: 5.18876 | failed:   41
batch:      18020 | loss: 5.19714 | failed:   41
batch:      18030 | loss: 5.12051 | failed:   41
batch:      18040 | loss: 5.23690 | failed:   41
batch:      18050 | loss: 5.21626 | failed:   41
batch:      18060 | loss: 5.24579 | failed:   41
batch:      18070 | loss: 5.19678 | failed:   41
batch:      18080 | loss: 5.07140 | failed:   41
batch:      18090 | loss: 5.30167 | failed:   41
batch:      18100 | loss: 5.28555 | failed:   41
batch:      18110 | loss: 5.17980 | failed:   41
batch:      18120 | loss: 5.24344 | failed:   41
batch:      18130 | loss: 5.17008 | failed:   41
batch:      18140 | loss: 5.12238 | failed:   41
batch:      18150 | loss: 5.23813 | failed:   41
batch:      18160 | loss: 5.17819 | failed:   41
batch:      18170 | loss: 5.26516 | failed:   41
batch:      18180 | loss: 5.24666 | failed:   41
batch:      18190 | loss: 5.13420 | failed:   41
batch:      18200 | loss: 5.27380 | failed:   41
batch:      18210 | loss: 5.26566 | failed:   41
batch:      18220 | loss: 5.10035 | failed:   41
batch:      18230 | loss: 5.14317 | failed:   41
batch:      18240 | loss: 5.11102 | failed:   41
batch:      18250 | loss: 5.16688 | failed:   41
batch:      18260 | loss: 5.11461 | failed:   41
batch:      18270 | loss: 5.17135 | failed:   41
batch:      18280 | loss: 5.21866 | failed:   41
batch:      18290 | loss: 5.27925 | failed:   41
batch:      18300 | loss: 4.98812 | failed:   41
batch:      18310 | loss: 5.29760 | failed:   41
batch:      18320 | loss: 5.12270 | failed:   41
batch:      18330 | loss: 5.19797 | failed:   41
batch:      18340 | loss: 5.21073 | failed:   41
batch:      18350 | loss: 5.15832 | failed:   41
batch:      18360 | loss: 5.07005 | failed:   41
batch:      18370 | loss: 5.12633 | failed:   41
batch:      18380 | loss: 5.28575 | failed:   41
batch:      18390 | loss: 5.18411 | failed:   41
batch:      18400 | loss: 5.21404 | failed:   41
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      18410 | loss: 5.20240 | failed:   43
batch:      18420 | loss: 5.26136 | failed:   43
batch:      18430 | loss: 5.29198 | failed:   43
batch:      18440 | loss: 5.22617 | failed:   43
batch:      18450 | loss: 5.20698 | failed:   43
batch:      18460 | loss: 5.14006 | failed:   43
batch:      18470 | loss: 5.19477 | failed:   43
batch:      18480 | loss: 5.16764 | failed:   43
batch:      18490 | loss: 5.20496 | failed:   43
batch:      18500 | loss: 5.27075 | failed:   43
batch:      18510 | loss: 5.21804 | failed:   43
batch:      18520 | loss: 5.20287 | failed:   43
batch:      18530 | loss: 5.14258 | failed:   43
batch:      18540 | loss: 5.16695 | failed:   43
batch:      18550 | loss: 5.16939 | failed:   43
batch:      18560 | loss: 5.14696 | failed:   43
batch:      18570 | loss: 5.19063 | failed:   43
batch:      18580 | loss: 5.20034 | failed:   43
batch:      18590 | loss: 5.19661 | failed:   43
batch:      18600 | loss: 5.23455 | failed:   43
batch:      18610 | loss: 5.19912 | failed:   43
batch:      18620 | loss: 5.23769 | failed:   43
batch:      18630 | loss: 5.21124 | failed:   43
batch:      18640 | loss: 5.18404 | failed:   43
batch:      18650 | loss: 5.18329 | failed:   43
batch:      18660 | loss: 5.14712 | failed:   43
batch:      18670 | loss: 5.12210 | failed:   43
batch:      18680 | loss: 5.29288 | failed:   43
batch:      18690 | loss: 5.21692 | failed:   43
batch:      18700 | loss: 5.15974 | failed:   43
batch:      18710 | loss: 5.26420 | failed:   43
batch:      18720 | loss: 5.26460 | failed:   43
batch:      18730 | loss: 5.17495 | failed:   43
batch:      18740 | loss: 5.20999 | failed:   43
batch:      18750 | loss: 5.24962 | failed:   43
batch:      18760 | loss: 5.28005 | failed:   43
batch:      18770 | loss: 5.26757 | failed:   43
batch:      18780 | loss: 5.20366 | failed:   43
batch:      18790 | loss: 5.19918 | failed:   43
batch:      18800 | loss: 5.08591 | failed:   43
batch:      18810 | loss: 5.13615 | failed:   43
batch:      18820 | loss: 5.25559 | failed:   43
batch:      18830 | loss: 5.13016 | failed:   43
batch:      18840 | loss: 5.13476 | failed:   43
batch:      18850 | loss: 5.33224 | failed:   43
batch:      18860 | loss: 5.24305 | failed:   43
batch:      18870 | loss: 5.12462 | failed:   43
batch:      18880 | loss: 5.20125 | failed:   43
batch:      18890 | loss: 5.18520 | failed:   43
batch:      18900 | loss: 5.15075 | failed:   43
batch:      18910 | loss: 5.16371 | failed:   43
batch:      18920 | loss: 5.14278 | failed:   43
batch:      18930 | loss: 5.18645 | failed:   43
batch:      18940 | loss: 5.21313 | failed:   43
batch:      18950 | loss: 5.04642 | failed:   43
batch:      18960 | loss: 5.17763 | failed:   43
batch:      18970 | loss: 5.03634 | failed:   43
batch:      18980 | loss: 5.27236 | failed:   43
batch:      18990 | loss: 5.20855 | failed:   43
batch:      19000 | loss: 5.24519 | failed:   43
batch:      19010 | loss: 5.21509 | failed:   43
batch:      19020 | loss: 5.19213 | failed:   43
batch:      19030 | loss: 5.21939 | failed:   43
batch:      19040 | loss: 5.21937 | failed:   43
batch:      19050 | loss: 5.17763 | failed:   43
batch:      19060 | loss: 5.14279 | failed:   43
batch:      19070 | loss: 5.25223 | failed:   43
batch:      19080 | loss: 5.15474 | failed:   43
batch:      19090 | loss: 5.20507 | failed:   43
batch:      19100 | loss: 5.22391 | failed:   43
batch:      19110 | loss: 5.18697 | failed:   43
batch:      19120 | loss: 5.21150 | failed:   43
batch:      19130 | loss: 5.14587 | failed:   43
batch:      19140 | loss: 5.17721 | failed:   43
batch:      19150 | loss: 5.09677 | failed:   43
batch:      19160 | loss: 5.18632 | failed:   43
batch:      19170 | loss: 5.07039 | failed:   43
batch:      19180 | loss: 5.21932 | failed:   43
batch:      19190 | loss: 5.15873 | failed:   43
batch:      19200 | loss: 5.21916 | failed:   43
batch:      19210 | loss: 5.01473 | failed:   43
batch:      19220 | loss: 5.13286 | failed:   43
batch:      19230 | loss: 5.27905 | failed:   43
batch:      19240 | loss: 5.22774 | failed:   43
batch:      19250 | loss: 4.84995 | failed:   43
batch:      19260 | loss: 5.21101 | failed:   43
batch:      19270 | loss: 5.18799 | failed:   43
batch:      19280 | loss: 5.08986 | failed:   43
batch:      19290 | loss: 5.17952 | failed:   43
batch:      19300 | loss: 5.04964 | failed:   43
batch:      19310 | loss: 5.13400 | failed:   43
batch:      19320 | loss: 5.18614 | failed:   43
batch:      19330 | loss: 5.15448 | failed:   43
batch:      19340 | loss: 5.08726 | failed:   43
batch:      19350 | loss: 5.13898 | failed:   43
batch:      19360 | loss: 5.23550 | failed:   43
batch:      19370 | loss: 5.24239 | failed:   43
batch:      19380 | loss: 5.19220 | failed:   43
batch:      19390 | loss: 5.10793 | failed:   43
batch:      19400 | loss: 5.24269 | failed:   43
batch:      19410 | loss: 5.19184 | failed:   43
batch:      19420 | loss: 5.23788 | failed:   43
batch:      19430 | loss: 5.17480 | failed:   43
batch:      19440 | loss: 5.21325 | failed:   43
batch:      19450 | loss: 5.17567 | failed:   43
batch:      19460 | loss: 5.07435 | failed:   43
batch:      19470 | loss: 5.17874 | failed:   43
batch:      19480 | loss: 5.28381 | failed:   43
batch:      19490 | loss: 5.28226 | failed:   43
batch:      19500 | loss: 5.26675 | failed:   43
batch:      19510 | loss: 5.22587 | failed:   43
batch:      19520 | loss: 5.26533 | failed:   43
batch:      19530 | loss: 5.25656 | failed:   43
batch:      19540 | loss: 5.25104 | failed:   43
batch:      19550 | loss: 5.25836 | failed:   43
batch:      19560 | loss: 5.20926 | failed:   43
batch:      19570 | loss: 5.26907 | failed:   43
batch:      19580 | loss: 5.22773 | failed:   43
batch:      19590 | loss: 5.25140 | failed:   43
batch:      19600 | loss: 5.22777 | failed:   43
batch:      19610 | loss: 5.14368 | failed:   43
batch:      19620 | loss: 5.08664 | failed:   43
batch:      19630 | loss: 5.21048 | failed:   43
batch:      19640 | loss: 5.09248 | failed:   43
batch:      19650 | loss: 5.25130 | failed:   43
batch:      19660 | loss: 5.20801 | failed:   43
batch:      19670 | loss: 5.05792 | failed:   43
batch:      19680 | loss: 5.18737 | failed:   43
batch:      19690 | loss: 5.24360 | failed:   43
batch:      19700 | loss: 5.12596 | failed:   43
batch:      19710 | loss: 5.09883 | failed:   43
batch:      19720 | loss: 5.11496 | failed:   43
batch:      19730 | loss: 5.20577 | failed:   43
batch:      19740 | loss: 5.19826 | failed:   43
batch:      19750 | loss: 5.18961 | failed:   43
batch:      19760 | loss: 5.28578 | failed:   43
batch:      19770 | loss: 5.13064 | failed:   43
batch:      19780 | loss: 5.16890 | failed:   43
batch:      19790 | loss: 5.28766 | failed:   43
batch:      19800 | loss: 5.27171 | failed:   43
batch:      19810 | loss: 5.25245 | failed:   43
batch:      19820 | loss: 5.17637 | failed:   43
batch:      19830 | loss: 5.16446 | failed:   43
batch:      19840 | loss: 5.21088 | failed:   43
batch:      19850 | loss: 5.19690 | failed:   43
batch:      19860 | loss: 5.18961 | failed:   43
batch:      19870 | loss: 5.00220 | failed:   43
batch:      19880 | loss: 5.19809 | failed:   43
batch:      19890 | loss: 5.18673 | failed:   43
batch:      19900 | loss: 5.09620 | failed:   43
batch:      19910 | loss: 5.16315 | failed:   43
batch:      19920 | loss: 5.21329 | failed:   43
batch:      19930 | loss: 5.17768 | failed:   43
batch:      19940 | loss: 5.19283 | failed:   43
batch:      19950 | loss: 5.10426 | failed:   43
batch:      19960 | loss: 5.18623 | failed:   43
batch:      19970 | loss: 5.19682 | failed:   43
batch:      19980 | loss: 5.03064 | failed:   43
batch:      19990 | loss: 5.12436 | failed:   43
batch:      20000 | loss: 5.22741 | failed:   43
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      20010 | loss: 5.19538 | failed:   43
batch:      20020 | loss: 5.19521 | failed:   43
batch:      20030 | loss: 5.24703 | failed:   43
batch:      20040 | loss: 5.28801 | failed:   43
batch:      20050 | loss: 5.18957 | failed:   43
batch:      20060 | loss: 5.27155 | failed:   43
batch:      20070 | loss: 5.23324 | failed:   43
batch:      20080 | loss: 5.17576 | failed:   43
batch:      20090 | loss: 5.16139 | failed:   43
batch:      20100 | loss: 5.14956 | failed:   43
batch:      20110 | loss: 5.15045 | failed:   43
batch:      20120 | loss: 5.19627 | failed:   43
batch:      20130 | loss: 5.10564 | failed:   43
batch:      20140 | loss: 5.24037 | failed:   43
batch:      20150 | loss: 5.22048 | failed:   43
batch:      20160 | loss: 5.24123 | failed:   43
batch:      20170 | loss: 5.21089 | failed:   43
batch:      20180 | loss: 5.20509 | failed:   43
batch:      20190 | loss: 5.21030 | failed:   43
batch:      20200 | loss: 5.24648 | failed:   43
batch:      20210 | loss: 5.14338 | failed:   43
batch:      20220 | loss: 5.04132 | failed:   43
batch:      20230 | loss: 5.21110 | failed:   43
batch:      20240 | loss: 5.08406 | failed:   43
batch:      20250 | loss: 5.23088 | failed:   43
batch:      20260 | loss: 5.26993 | failed:   43
batch:      20270 | loss: 5.20500 | failed:   43
batch:      20280 | loss: 5.20421 | failed:   43
batch:      20290 | loss: 5.27796 | failed:   43
batch:      20300 | loss: 5.18724 | failed:   43
batch:      20310 | loss: 5.31049 | failed:   43
batch:      20320 | loss: 5.08053 | failed:   43
batch:      20330 | loss: 5.22560 | failed:   43
batch:      20340 | loss: 5.23564 | failed:   43
batch:      20350 | loss: 5.12927 | failed:   43
batch:      20360 | loss: 5.09734 | failed:   43
batch:      20370 | loss: 5.22193 | failed:   43
batch:      20380 | loss: 5.19800 | failed:   43
batch:      20390 | loss: 5.19125 | failed:   43
batch:      20400 | loss: 5.19009 | failed:   43
batch:      20410 | loss: 5.20507 | failed:   43
batch:      20420 | loss: 5.04889 | failed:   43
batch:      20430 | loss: 5.11518 | failed:   43
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      20440 | loss: 5.16145 | failed:   45
batch:      20450 | loss: 5.12778 | failed:   45
batch:      20460 | loss: 5.15274 | failed:   45
batch:      20470 | loss: 5.16555 | failed:   45
batch:      20480 | loss: 5.16930 | failed:   45
batch:      20490 | loss: 5.14475 | failed:   45
batch:      20500 | loss: 5.12584 | failed:   45
batch:      20510 | loss: 5.30263 | failed:   45
batch:      20520 | loss: 5.28043 | failed:   45
batch:      20530 | loss: 5.23754 | failed:   45
batch:      20540 | loss: 5.18708 | failed:   45
batch:      20550 | loss: 5.17662 | failed:   45
batch:      20560 | loss: 5.17036 | failed:   45
batch:      20570 | loss: 5.24333 | failed:   45
batch:      20580 | loss: 5.21697 | failed:   45
batch:      20590 | loss: 5.21436 | failed:   45
batch:      20600 | loss: 5.36401 | failed:   45
batch:      20610 | loss: 5.19665 | failed:   45
batch:      20620 | loss: 5.24067 | failed:   45
batch:      20630 | loss: 5.25921 | failed:   45
batch:      20640 | loss: 5.18286 | failed:   45
batch:      20650 | loss: 5.22552 | failed:   45
batch:      20660 | loss: 5.01490 | failed:   45
batch:      20670 | loss: 5.26844 | failed:   45
batch:      20680 | loss: 5.26913 | failed:   45
batch:      20690 | loss: 5.08650 | failed:   45
batch:      20700 | loss: 5.26078 | failed:   45
batch:      20710 | loss: 5.21849 | failed:   45
batch:      20720 | loss: 5.23699 | failed:   45
batch:      20730 | loss: 5.19765 | failed:   45
batch:      20740 | loss: 5.21351 | failed:   45
batch:      20750 | loss: 5.21283 | failed:   45
batch:      20760 | loss: 5.12735 | failed:   45
batch:      20770 | loss: 5.15957 | failed:   45
batch:      20780 | loss: 5.16909 | failed:   45
batch:      20790 | loss: 5.16350 | failed:   45
batch:      20800 | loss: 5.19773 | failed:   45
batch:      20810 | loss: 5.14673 | failed:   45
batch:      20820 | loss: 5.16298 | failed:   45
batch:      20830 | loss: 5.15003 | failed:   45
batch:      20840 | loss: 5.13765 | failed:   45
batch:      20850 | loss: 5.27675 | failed:   45
batch:      20860 | loss: 5.26985 | failed:   45
batch:      20870 | loss: 5.22509 | failed:   45
batch:      20880 | loss: 5.24724 | failed:   45
batch:      20890 | loss: 5.25399 | failed:   45
batch:      20900 | loss: 5.11480 | failed:   45
batch:      20910 | loss: 5.20007 | failed:   45
batch:      20920 | loss: 5.19548 | failed:   45
batch:      20930 | loss: 5.22360 | failed:   45
batch:      20940 | loss: 5.27362 | failed:   45
batch:      20950 | loss: 5.18808 | failed:   45
batch:      20960 | loss: 5.00239 | failed:   45
batch:      20970 | loss: 5.12176 | failed:   45
batch:      20980 | loss: 5.09199 | failed:   45
batch:      20990 | loss: 5.19986 | failed:   45
batch:      21000 | loss: 5.14738 | failed:   45
batch:      21010 | loss: 5.10938 | failed:   45
batch:      21020 | loss: 5.15618 | failed:   45
batch:      21030 | loss: 5.19747 | failed:   45
batch:      21040 | loss: 5.19037 | failed:   45
batch:      21050 | loss: 5.22663 | failed:   45
batch:      21060 | loss: 5.13990 | failed:   45
batch:      21070 | loss: 5.16321 | failed:   45
batch:      21080 | loss: 5.14565 | failed:   45
batch:      21090 | loss: 5.17536 | failed:   45
batch:      21100 | loss: 5.08062 | failed:   45
batch:      21110 | loss: 5.15555 | failed:   45
batch:      21120 | loss: 5.24079 | failed:   45
batch:      21130 | loss: 5.13645 | failed:   45
batch:      21140 | loss: 5.16330 | failed:   45
batch:      21150 | loss: 5.17888 | failed:   45
batch:      21160 | loss: 4.88816 | failed:   45
batch:      21170 | loss: 5.21313 | failed:   45
batch:      21180 | loss: 5.13007 | failed:   45
batch:      21190 | loss: 5.12349 | failed:   45
batch:      21200 | loss: 4.88421 | failed:   45
batch:      21210 | loss: 4.96848 | failed:   45
batch:      21220 | loss: 4.94223 | failed:   45
batch:      21230 | loss: 5.36136 | failed:   45
batch:      21240 | loss: 5.24025 | failed:   45
batch:      21250 | loss: 5.23771 | failed:   45
batch:      21260 | loss: 5.20328 | failed:   45
batch:      21270 | loss: 5.18348 | failed:   45
batch:      21280 | loss: 5.05720 | failed:   45
batch:      21290 | loss: 5.10551 | failed:   45
batch:      21300 | loss: 5.18543 | failed:   45
batch:      21310 | loss: 5.04765 | failed:   45
batch:      21320 | loss: 5.17601 | failed:   45
batch:      21330 | loss: 5.20493 | failed:   45
batch:      21340 | loss: 5.23753 | failed:   45
batch:      21350 | loss: 5.15672 | failed:   45
batch:      21360 | loss: 5.18936 | failed:   45
batch:      21370 | loss: 5.11470 | failed:   45
batch:      21380 | loss: 5.27824 | failed:   45
batch:      21390 | loss: 5.14401 | failed:   45
batch:      21400 | loss: 5.19425 | failed:   45
batch:      21410 | loss: 5.17912 | failed:   45
batch:      21420 | loss: 5.18193 | failed:   45
batch:      21430 | loss: 5.21535 | failed:   45
batch:      21440 | loss: 5.15226 | failed:   45
batch:      21450 | loss: 5.21692 | failed:   45
batch:      21460 | loss: 5.13896 | failed:   45
batch:      21470 | loss: 5.25906 | failed:   45
batch:      21480 | loss: 5.15502 | failed:   45
batch:      21490 | loss: 5.17142 | failed:   45
batch:      21500 | loss: 5.19330 | failed:   45
batch:      21510 | loss: 5.16539 | failed:   45
batch:      21520 | loss: 5.19062 | failed:   45
batch:      21530 | loss: 5.23581 | failed:   45
batch:      21540 | loss: 5.14643 | failed:   45
batch:      21550 | loss: 5.15031 | failed:   45
batch:      21560 | loss: 5.19938 | failed:   45
batch:      21570 | loss: 5.21718 | failed:   45
batch:      21580 | loss: 5.18944 | failed:   45
batch:      21590 | loss: 5.09460 | failed:   45
batch:      21600 | loss: 5.08070 | failed:   45
batch:      21610 | loss: 5.20283 | failed:   45
batch:      21620 | loss: 5.14285 | failed:   45
batch:      21630 | loss: 5.14685 | failed:   45
batch:      21640 | loss: 5.30031 | failed:   45
batch:      21650 | loss: 5.30665 | failed:   45
batch:      21660 | loss: 5.26570 | failed:   45
batch:      21670 | loss: 5.18222 | failed:   45
batch:      21680 | loss: 5.22006 | failed:   45
batch:      21690 | loss: 5.26599 | failed:   45
batch:      21700 | loss: 5.16735 | failed:   45
batch:      21710 | loss: 5.22435 | failed:   45
batch:      21720 | loss: 5.16612 | failed:   45
batch:      21730 | loss: 5.16686 | failed:   45
batch:      21740 | loss: 5.18672 | failed:   45
batch:      21750 | loss: 5.14120 | failed:   45
batch:      21760 | loss: 5.00426 | failed:   45
batch:      21770 | loss: 5.19939 | failed:   45
batch:      21780 | loss: 5.37386 | failed:   45
batch:      21790 | loss: 5.16846 | failed:   45
batch:      21800 | loss: 5.21821 | failed:   45
batch:      21810 | loss: 5.16336 | failed:   45
batch:      21820 | loss: 5.19475 | failed:   45
batch:      21830 | loss: 5.22196 | failed:   45
batch:      21840 | loss: 5.22177 | failed:   45
batch:      21850 | loss: 5.17627 | failed:   45
batch:      21860 | loss: 5.22488 | failed:   45
batch:      21870 | loss: 5.19091 | failed:   45
batch:      21880 | loss: 5.08378 | failed:   45
batch:      21890 | loss: 5.25954 | failed:   45
batch:      21900 | loss: 5.14388 | failed:   45
batch:      21910 | loss: 5.23359 | failed:   45
batch:      21920 | loss: 5.25267 | failed:   45
batch:      21930 | loss: 5.17442 | failed:   45
batch:      21940 | loss: 5.05051 | failed:   45
batch:      21950 | loss: 5.24619 | failed:   45
batch:      21960 | loss: 5.22691 | failed:   45
batch:      21970 | loss: 5.22518 | failed:   45
batch:      21980 | loss: 5.21160 | failed:   45
batch:      21990 | loss: 4.88466 | failed:   45
batch:      22000 | loss: 5.05857 | failed:   45
batch:      22010 | loss: 5.05894 | failed:   45
batch:      22020 | loss: 5.05447 | failed:   45
batch:      22030 | loss: 5.11586 | failed:   45
batch:      22040 | loss: 5.03257 | failed:   45
batch:      22050 | loss: 5.03021 | failed:   45
batch:      22060 | loss: 5.14696 | failed:   45
batch:      22070 | loss: 5.09567 | failed:   45
batch:      22080 | loss: 5.20167 | failed:   45
batch:      22090 | loss: 5.19228 | failed:   45
batch:      22100 | loss: 5.23344 | failed:   45
batch:      22110 | loss: 5.24887 | failed:   45
batch:      22120 | loss: 5.14799 | failed:   45
batch:      22130 | loss: 5.25385 | failed:   45
batch:      22140 | loss: 5.17434 | failed:   45
batch:      22150 | loss: 5.20219 | failed:   45
batch:      22160 | loss: 5.06323 | failed:   45
batch:      22170 | loss: 5.31922 | failed:   45
batch:      22180 | loss: 5.29248 | failed:   45
batch:      22190 | loss: 5.12676 | failed:   45
batch:      22200 | loss: 5.20574 | failed:   45
batch:      22210 | loss: 5.11439 | failed:   45
batch:      22220 | loss: 5.08877 | failed:   45
batch:      22230 | loss: 5.20968 | failed:   45
batch:      22240 | loss: 5.29000 | failed:   45
batch:      22250 | loss: 5.13707 | failed:   45
batch:      22260 | loss: 5.23191 | failed:   45
batch:      22270 | loss: 5.27384 | failed:   45
batch:      22280 | loss: 4.98440 | failed:   45
batch:      22290 | loss: 5.27997 | failed:   45
batch:      22300 | loss: 5.23327 | failed:   45
batch:      22310 | loss: 5.11475 | failed:   45
batch:      22320 | loss: 5.24764 | failed:   45
batch:      22330 | loss: 5.06083 | failed:   45
batch:      22340 | loss: 5.08862 | failed:   45
batch:      22350 | loss: 5.18434 | failed:   45
batch:      22360 | loss: 5.21803 | failed:   45
batch:      22370 | loss: 5.20442 | failed:   45
batch:      22380 | loss: 5.15107 | failed:   45
batch:      22390 | loss: 5.08972 | failed:   45
batch:      22400 | loss: 5.23166 | failed:   45
batch:      22410 | loss: 5.22243 | failed:   45
batch:      22420 | loss: 4.95781 | failed:   45
batch:      22430 | loss: 5.23812 | failed:   45
batch:      22440 | loss: 5.19249 | failed:   45
batch:      22450 | loss: 5.23615 | failed:   45
batch:      22460 | loss: 5.12215 | failed:   45
batch:      22470 | loss: 5.21434 | failed:   45
batch:      22480 | loss: 5.22194 | failed:   45
batch:      22490 | loss: 5.31198 | failed:   45
batch:      22500 | loss: 5.03728 | failed:   45
batch:      22510 | loss: 5.21375 | failed:   45
batch:      22520 | loss: 5.18737 | failed:   45
batch:      22530 | loss: 5.17959 | failed:   45
batch:      22540 | loss: 4.86223 | failed:   45
batch:      22550 | loss: 4.97104 | failed:   45
batch:      22560 | loss: 5.35187 | failed:   45
batch:      22570 | loss: 5.26865 | failed:   45
batch:      22580 | loss: 5.24094 | failed:   45
batch:      22590 | loss: 5.19057 | failed:   45
batch:      22600 | loss: 5.17198 | failed:   45
batch:      22610 | loss: 5.14190 | failed:   45
batch:      22620 | loss: 5.20141 | failed:   45
batch:      22630 | loss: 5.02292 | failed:   45
batch:      22640 | loss: 5.16753 | failed:   45
batch:      22650 | loss: 5.12843 | failed:   45
batch:      22660 | loss: 5.14861 | failed:   45
batch:      22670 | loss: 5.14619 | failed:   45
batch:      22680 | loss: 5.11872 | failed:   45
batch:      22690 | loss: 5.16602 | failed:   45
batch:      22700 | loss: 5.10926 | failed:   45
batch:      22710 | loss: 4.98005 | failed:   45
batch:      22720 | loss: 5.12277 | failed:   45
batch:      22730 | loss: 5.09049 | failed:   45
batch:      22740 | loss: 5.05024 | failed:   45
batch:      22750 | loss: 5.31602 | failed:   45
batch:      22760 | loss: 5.22086 | failed:   45
batch:      22770 | loss: 5.01813 | failed:   45
batch:      22780 | loss: 5.24110 | failed:   45
batch:      22790 | loss: 5.23178 | failed:   45
batch:      22800 | loss: 5.16141 | failed:   45
batch:      22810 | loss: 5.16845 | failed:   45
batch:      22820 | loss: 5.17074 | failed:   45
batch:      22830 | loss: 5.18706 | failed:   45
batch:      22840 | loss: 5.23860 | failed:   45
batch:      22850 | loss: 5.17143 | failed:   45
batch:      22860 | loss: 5.24706 | failed:   45
batch:      22870 | loss: 5.17123 | failed:   45
batch:      22880 | loss: 5.18628 | failed:   45
batch:      22890 | loss: 5.21390 | failed:   45
batch:      22900 | loss: 5.10502 | failed:   45
batch:      22910 | loss: 5.12098 | failed:   45
batch:      22920 | loss: 5.14756 | failed:   45
batch:      22930 | loss: 5.13945 | failed:   45
batch:      22940 | loss: 5.26813 | failed:   45
batch:      22950 | loss: 5.21542 | failed:   45
batch:      22960 | loss: 5.23529 | failed:   45
batch:      22970 | loss: 5.20047 | failed:   45
batch:      22980 | loss: 5.23837 | failed:   45
batch:      22990 | loss: 5.23045 | failed:   45
batch:      23000 | loss: 5.16570 | failed:   45
batch:      23010 | loss: 5.06646 | failed:   45
batch:      23020 | loss: 5.23632 | failed:   45
batch:      23030 | loss: 5.24229 | failed:   45
batch:      23040 | loss: 5.26282 | failed:   45
batch:      23050 | loss: 5.15850 | failed:   45
batch:      23060 | loss: 5.17725 | failed:   45
batch:      23070 | loss: 5.19521 | failed:   45
batch:      23080 | loss: 5.25944 | failed:   45
batch:      23090 | loss: 5.05271 | failed:   45
batch:      23100 | loss: 5.15427 | failed:   45
batch:      23110 | loss: 5.24530 | failed:   45
batch:      23120 | loss: 5.24687 | failed:   45
batch:      23130 | loss: 5.18848 | failed:   45
batch:      23140 | loss: 5.25274 | failed:   45
batch:      23150 | loss: 5.19349 | failed:   45
batch:      23160 | loss: 5.13731 | failed:   45
batch:      23170 | loss: 5.15775 | failed:   45
batch:      23180 | loss: 5.23027 | failed:   45
batch:      23190 | loss: 5.28286 | failed:   45
batch:      23200 | loss: 5.20112 | failed:   45
batch:      23210 | loss: 5.06295 | failed:   45
batch:      23220 | loss: 5.17409 | failed:   45
batch:      23230 | loss: 5.20378 | failed:   45
batch:      23240 | loss: 5.16873 | failed:   45
batch:      23250 | loss: 5.14913 | failed:   45
batch:      23260 | loss: 5.11782 | failed:   45
batch:      23270 | loss: 5.11542 | failed:   45
batch:      23280 | loss: 5.15932 | failed:   45
batch:      23290 | loss: 5.22011 | failed:   45
batch:      23300 | loss: 5.08997 | failed:   45
batch:      23310 | loss: 5.10097 | failed:   45
batch:      23320 | loss: 5.08793 | failed:   45
batch:      23330 | loss: 5.16854 | failed:   45
batch:      23340 | loss: 5.25483 | failed:   45
batch:      23350 | loss: 5.17635 | failed:   45
batch:      23360 | loss: 5.14767 | failed:   45
batch:      23370 | loss: 5.28689 | failed:   45
batch:      23380 | loss: 5.22967 | failed:   45
batch:      23390 | loss: 5.18914 | failed:   45
batch:      23400 | loss: 5.16524 | failed:   45
batch:      23410 | loss: 5.52437 | failed:   45
batch:      23420 | loss: 5.32708 | failed:   45
batch:      23430 | loss: 5.09497 | failed:   45
batch:      23440 | loss: 5.20914 | failed:   45
batch:      23450 | loss: 5.18876 | failed:   45
batch:      23460 | loss: 5.01186 | failed:   45
batch:      23470 | loss: 5.25785 | failed:   45
batch:      23480 | loss: 5.08764 | failed:   45
batch:      23490 | loss: 5.13301 | failed:   45
batch:      23500 | loss: 5.19739 | failed:   45
batch:      23510 | loss: 5.17312 | failed:   45
batch:      23520 | loss: 5.21204 | failed:   45
batch:      23530 | loss: 5.00854 | failed:   45
batch:      23540 | loss: 5.20600 | failed:   45
batch:      23550 | loss: 5.22589 | failed:   45
batch:      23560 | loss: 5.11403 | failed:   45
batch:      23570 | loss: 5.10049 | failed:   45
batch:      23580 | loss: 4.73532 | failed:   45
batch:      23590 | loss: 5.20136 | failed:   45
batch:      23600 | loss: 5.20783 | failed:   45
batch:      23610 | loss: 5.16830 | failed:   45
batch:      23620 | loss: 5.24746 | failed:   45
batch:      23630 | loss: 5.15169 | failed:   45
batch:      23640 | loss: 5.23819 | failed:   45
batch:      23650 | loss: 5.01566 | failed:   45
batch:      23660 | loss: 5.15072 | failed:   45
batch:      23670 | loss: 5.29214 | failed:   45
batch:      23680 | loss: 5.01799 | failed:   45
batch:      23690 | loss: 5.02825 | failed:   45
batch:      23700 | loss: 5.05624 | failed:   45
batch:      23710 | loss: 5.11427 | failed:   45
batch:      23720 | loss: 5.14056 | failed:   45
batch:      23730 | loss: 5.24111 | failed:   45
batch:      23740 | loss: 5.21382 | failed:   45
batch:      23750 | loss: 5.24783 | failed:   45
batch:      23760 | loss: 5.21026 | failed:   45
batch:      23770 | loss: 5.17596 | failed:   45
batch:      23780 | loss: 5.26347 | failed:   45
batch:      23790 | loss: 5.27383 | failed:   45
batch:      23800 | loss: 5.28654 | failed:   45
batch:      23810 | loss: 5.28346 | failed:   45
batch:      23820 | loss: 5.20796 | failed:   45
batch:      23830 | loss: 5.21037 | failed:   45
batch:      23840 | loss: 5.23233 | failed:   45
batch:      23850 | loss: 5.13216 | failed:   45
batch:      23860 | loss: 5.29158 | failed:   45
batch:      23870 | loss: 5.24916 | failed:   45
batch:      23880 | loss: 5.18607 | failed:   45
batch:      23890 | loss: 5.12815 | failed:   45
batch:      23900 | loss: 5.18613 | failed:   45
batch:      23910 | loss: 5.20889 | failed:   45
batch:      23920 | loss: 5.19780 | failed:   45
batch:      23930 | loss: 5.19786 | failed:   45
batch:      23940 | loss: 5.17844 | failed:   45
batch:      23950 | loss: 4.98320 | failed:   45
batch:      23960 | loss: 5.23566 | failed:   45
batch:      23970 | loss: 5.11423 | failed:   45
batch:      23980 | loss: 5.17372 | failed:   45
batch:      23990 | loss: 5.19292 | failed:   45
batch:      24000 | loss: 5.13839 | failed:   45
batch:      24010 | loss: 5.21610 | failed:   45
batch:      24020 | loss: 5.20667 | failed:   45
batch:      24030 | loss: 5.08403 | failed:   45
batch:      24040 | loss: 5.20920 | failed:   45
batch:      24050 | loss: 5.17418 | failed:   45
batch:      24060 | loss: 5.20374 | failed:   45
batch:      24070 | loss: 5.20313 | failed:   45
batch:      24080 | loss: 5.06772 | failed:   45
batch:      24090 | loss: 5.27159 | failed:   45
batch:      24100 | loss: 5.23753 | failed:   45
batch:      24110 | loss: 5.16988 | failed:   45
batch:      24120 | loss: 5.13890 | failed:   45
batch:      24130 | loss: 5.13605 | failed:   45
batch:      24140 | loss: 5.17528 | failed:   45
batch:      24150 | loss: 5.09518 | failed:   45
batch:      24160 | loss: 5.04422 | failed:   45
batch:      24170 | loss: 5.26435 | failed:   45
batch:      24180 | loss: 5.24256 | failed:   45
batch:      24190 | loss: 5.15752 | failed:   45
batch:      24200 | loss: 5.19066 | failed:   45
batch:      24210 | loss: 5.11077 | failed:   45
batch:      24220 | loss: 5.08616 | failed:   45
batch:      24230 | loss: 5.08592 | failed:   45
batch:      24240 | loss: 5.13001 | failed:   45
batch:      24250 | loss: 5.11663 | failed:   45
batch:      24260 | loss: 5.20983 | failed:   45
batch:      24270 | loss: 5.30233 | failed:   45
batch:      24280 | loss: 5.14021 | failed:   45
batch:      24290 | loss: 5.17154 | failed:   45
batch:      24300 | loss: 5.25306 | failed:   45
batch:      24310 | loss: 5.15898 | failed:   45
batch:      24320 | loss: 5.17794 | failed:   45
batch:      24330 | loss: 5.19451 | failed:   45
batch:      24340 | loss: 5.23874 | failed:   45
batch:      24350 | loss: 5.23920 | failed:   45
batch:      24360 | loss: 5.23315 | failed:   45
batch:      24370 | loss: 5.17370 | failed:   45
batch:      24380 | loss: 5.11340 | failed:   45
batch:      24390 | loss: 5.19642 | failed:   45
batch:      24400 | loss: 5.21559 | failed:   45
batch:      24410 | loss: 5.17709 | failed:   45
batch:      24420 | loss: 5.21759 | failed:   45
batch:      24430 | loss: 5.23835 | failed:   45
batch:      24440 | loss: 5.21269 | failed:   45
batch:      24450 | loss: 5.18982 | failed:   45
batch:      24460 | loss: 5.27309 | failed:   45
batch:      24470 | loss: 5.26340 | failed:   45
batch:      24480 | loss: 5.18321 | failed:   45
batch:      24490 | loss: 5.04156 | failed:   45
batch:      24500 | loss: 5.19743 | failed:   45
batch:      24510 | loss: 5.12415 | failed:   45
batch:      24520 | loss: 5.17627 | failed:   45
batch:      24530 | loss: 5.17482 | failed:   45
batch:      24540 | loss: 5.08382 | failed:   45
batch:      24550 | loss: 5.24512 | failed:   45
batch:      24560 | loss: 5.24160 | failed:   45
batch:      24570 | loss: 5.19194 | failed:   45
batch:      24580 | loss: 5.19273 | failed:   45
batch:      24590 | loss: 5.23060 | failed:   45
batch:      24600 | loss: 5.22748 | failed:   45
batch:      24610 | loss: 5.21932 | failed:   45
batch:      24620 | loss: 5.22789 | failed:   45
batch:      24630 | loss: 5.20726 | failed:   45
batch:      24640 | loss: 5.19700 | failed:   45
batch:      24650 | loss: 5.26182 | failed:   45
batch:      24660 | loss: 5.34318 | failed:   45
batch:      24670 | loss: 5.11874 | failed:   45
batch:      24680 | loss: 5.13275 | failed:   45
batch:      24690 | loss: 5.06220 | failed:   45
batch:      24700 | loss: 5.01417 | failed:   45
batch:      24710 | loss: 5.21220 | failed:   45
batch:      24720 | loss: 5.22383 | failed:   45
batch:      24730 | loss: 5.21141 | failed:   45
batch:      24740 | loss: 5.25703 | failed:   45
batch:      24750 | loss: 5.17244 | failed:   45
batch:      24760 | loss: 5.27476 | failed:   45
batch:      24770 | loss: 5.22822 | failed:   45
batch:      24780 | loss: 5.19567 | failed:   45
batch:      24790 | loss: 5.22710 | failed:   45
batch:      24800 | loss: 5.19430 | failed:   45
batch:      24810 | loss: 5.17867 | failed:   45
batch:      24820 | loss: 5.07713 | failed:   45
batch:      24830 | loss: 5.21627 | failed:   45
batch:      24840 | loss: 5.17791 | failed:   45
batch:      24850 | loss: 5.20551 | failed:   45
batch:      24860 | loss: 5.22789 | failed:   45
batch:      24870 | loss: 5.20655 | failed:   45
batch:      24880 | loss: 5.23137 | failed:   45
batch:      24890 | loss: 5.21911 | failed:   45
batch:      24900 | loss: 5.09985 | failed:   45
batch:      24910 | loss: 5.19957 | failed:   45
batch:      24920 | loss: 5.08525 | failed:   45
batch:      24930 | loss: 5.17540 | failed:   45
batch:      24940 | loss: 5.11726 | failed:   45
batch:      24950 | loss: 5.08349 | failed:   45
batch:      24960 | loss: 5.21279 | failed:   45
batch:      24970 | loss: 5.23073 | failed:   45
batch:      24980 | loss: 5.26180 | failed:   45
batch:      24990 | loss: 5.10565 | failed:   45
batch:      25000 | loss: 5.09680 | failed:   45
batch:      25010 | loss: 5.08356 | failed:   45
batch:      25020 | loss: 5.19302 | failed:   45
batch:      25030 | loss: 5.25730 | failed:   45
batch:      25040 | loss: 5.16501 | failed:   45
batch:      25050 | loss: 5.23277 | failed:   45
batch:      25060 | loss: 5.22686 | failed:   45
batch:      25070 | loss: 4.87986 | failed:   45
batch:      25080 | loss: 5.38312 | failed:   45
batch:      25090 | loss: 5.08883 | failed:   45
batch:      25100 | loss: 5.25302 | failed:   45
batch:      25110 | loss: 5.19910 | failed:   45
batch:      25120 | loss: 5.22568 | failed:   45
batch:      25130 | loss: 5.24592 | failed:   45
batch:      25140 | loss: 5.17841 | failed:   45
batch:      25150 | loss: 5.03300 | failed:   45
batch:      25160 | loss: 5.11155 | failed:   45
batch:      25170 | loss: 5.17111 | failed:   45
batch:      25180 | loss: 5.18523 | failed:   45
batch:      25190 | loss: 5.16007 | failed:   45
batch:      25200 | loss: 5.25667 | failed:   45
batch:      25210 | loss: 5.10344 | failed:   45
batch:      25220 | loss: 5.14265 | failed:   45
batch:      25230 | loss: 4.96851 | failed:   45
batch:      25240 | loss: 5.12766 | failed:   45
batch:      25250 | loss: 5.18415 | failed:   45
batch:      25260 | loss: 5.24274 | failed:   45
batch:      25270 | loss: 5.09961 | failed:   45
batch:      25280 | loss: 5.26717 | failed:   45
batch:      25290 | loss: 4.97539 | failed:   45
batch:      25300 | loss: 5.14448 | failed:   45
batch:      25310 | loss: 5.12373 | failed:   45
batch:      25320 | loss: 5.19657 | failed:   45
batch:      25330 | loss: 5.13165 | failed:   45
batch:      25340 | loss: 5.09943 | failed:   45
batch:      25350 | loss: 5.11957 | failed:   45
batch:      25360 | loss: 5.07707 | failed:   45
batch:      25370 | loss: 5.24883 | failed:   45
batch:      25380 | loss: 5.20543 | failed:   45
batch:      25390 | loss: 5.17405 | failed:   45
batch:      25400 | loss: 5.24671 | failed:   45
batch:      25410 | loss: 5.25912 | failed:   45
batch:      25420 | loss: 5.20026 | failed:   45
batch:      25430 | loss: 5.23523 | failed:   45
batch:      25440 | loss: 5.19137 | failed:   45
batch:      25450 | loss: 5.29291 | failed:   45
batch:      25460 | loss: 5.20023 | failed:   45
batch:      25470 | loss: 5.22706 | failed:   45
batch:      25480 | loss: 5.25552 | failed:   45
batch:      25490 | loss: 5.21862 | failed:   45
batch:      25500 | loss: 5.19627 | failed:   45
batch:      25510 | loss: 5.18688 | failed:   45
batch:      25520 | loss: 5.17536 | failed:   45
batch:      25530 | loss: 5.24160 | failed:   45
batch:      25540 | loss: 5.16522 | failed:   45
batch:      25550 | loss: 4.89652 | failed:   45
batch:      25560 | loss: 5.07264 | failed:   45
batch:      25570 | loss: 5.00746 | failed:   45
batch:      25580 | loss: 5.10122 | failed:   45
batch:      25590 | loss: 5.11494 | failed:   45
batch:      25600 | loss: 5.16036 | failed:   45
batch:      25610 | loss: 5.10084 | failed:   45
batch:      25620 | loss: 5.23623 | failed:   45
batch:      25630 | loss: 5.16526 | failed:   45
batch:      25640 | loss: 5.17312 | failed:   45
batch:      25650 | loss: 5.29870 | failed:   45
batch:      25660 | loss: 5.20165 | failed:   45
batch:      25670 | loss: 5.42109 | failed:   45
batch:      25680 | loss: 5.17892 | failed:   45
batch:      25690 | loss: 5.12872 | failed:   45
batch:      25700 | loss: 5.17370 | failed:   45
batch:      25710 | loss: 5.21757 | failed:   45
batch:      25720 | loss: 5.22037 | failed:   45
batch:      25730 | loss: 5.17599 | failed:   45
batch:      25740 | loss: 5.01108 | failed:   45
batch:      25750 | loss: 5.23421 | failed:   45
batch:      25760 | loss: 5.09108 | failed:   45
batch:      25770 | loss: 5.15290 | failed:   45
batch:      25780 | loss: 5.24542 | failed:   45
batch:      25790 | loss: 5.28615 | failed:   45
batch:      25800 | loss: 5.12229 | failed:   45
batch:      25810 | loss: 5.25364 | failed:   45
batch:      25820 | loss: 5.23060 | failed:   45
batch:      25830 | loss: 5.27871 | failed:   45
batch:      25840 | loss: 5.12540 | failed:   45
batch:      25850 | loss: 5.20944 | failed:   45
batch:      25860 | loss: 5.19002 | failed:   45
batch:      25870 | loss: 5.26203 | failed:   45
batch:      25880 | loss: 5.11274 | failed:   45
batch:      25890 | loss: 5.22087 | failed:   45
batch:      25900 | loss: 5.28433 | failed:   45
batch:      25910 | loss: 5.19885 | failed:   45
batch:      25920 | loss: 5.13534 | failed:   45
batch:      25930 | loss: 5.22714 | failed:   45
batch:      25940 | loss: 5.00730 | failed:   45
batch:      25950 | loss: 5.22643 | failed:   45
batch:      25960 | loss: 5.08412 | failed:   45
batch:      25970 | loss: 5.02151 | failed:   45
batch:      25980 | loss: 5.11125 | failed:   45
batch:      25990 | loss: 5.27073 | failed:   45
batch:      26000 | loss: 5.28191 | failed:   45
batch:      26010 | loss: 5.20130 | failed:   45
batch:      26020 | loss: 5.19674 | failed:   45
batch:      26030 | loss: 5.24240 | failed:   45
batch:      26040 | loss: 5.16930 | failed:   45
batch:      26050 | loss: 5.20553 | failed:   45
batch:      26060 | loss: 5.23379 | failed:   45
batch:      26070 | loss: 5.17852 | failed:   45
batch:      26080 | loss: 5.25022 | failed:   45
batch:      26090 | loss: 4.98747 | failed:   45
batch:      26100 | loss: 5.12495 | failed:   45
batch:      26110 | loss: 5.15744 | failed:   45
batch:      26120 | loss: 5.05239 | failed:   45
batch:      26130 | loss: 5.10885 | failed:   45
batch:      26140 | loss: 4.95141 | failed:   45
batch:      26150 | loss: 5.14411 | failed:   45
batch:      26160 | loss: 5.15339 | failed:   45
batch:      26170 | loss: 5.14669 | failed:   45
batch:      26180 | loss: 5.10015 | failed:   45
batch:      26190 | loss: 5.10479 | failed:   45
batch:      26200 | loss: 5.12503 | failed:   45
batch:      26210 | loss: 5.04842 | failed:   45
batch:      26220 | loss: 5.26490 | failed:   45
batch:      26230 | loss: 5.19408 | failed:   45
batch:      26240 | loss: 5.16168 | failed:   45
batch:      26250 | loss: 5.07072 | failed:   45
batch:      26260 | loss: 5.20989 | failed:   45
batch:      26270 | loss: 5.28410 | failed:   45
batch:      26280 | loss: 5.20213 | failed:   45
batch:      26290 | loss: 5.23573 | failed:   45
batch:      26300 | loss: 5.17644 | failed:   45
batch:      26310 | loss: 5.22800 | failed:   45
batch:      26320 | loss: 5.23509 | failed:   45
batch:      26330 | loss: 5.24355 | failed:   45
batch:      26340 | loss: 5.14381 | failed:   45
batch:      26350 | loss: 5.21344 | failed:   45
batch:      26360 | loss: 5.20324 | failed:   45
batch:      26370 | loss: 5.05962 | failed:   45
batch:      26380 | loss: 5.11354 | failed:   45
batch:      26390 | loss: 4.95410 | failed:   45
batch:      26400 | loss: 5.07195 | failed:   45
batch:      26410 | loss: 5.16688 | failed:   45
batch:      26420 | loss: 5.21374 | failed:   45
batch:      26430 | loss: 5.20891 | failed:   45
batch:      26440 | loss: 5.15558 | failed:   45
batch:      26450 | loss: 5.07763 | failed:   45
batch:      26460 | loss: 5.10948 | failed:   45
batch:      26470 | loss: 5.08173 | failed:   45
batch:      26480 | loss: 5.07726 | failed:   45
batch:      26490 | loss: 4.97413 | failed:   45
batch:      26500 | loss: 5.21103 | failed:   45
batch:      26510 | loss: 5.22087 | failed:   45
batch:      26520 | loss: 5.21060 | failed:   45
batch:      26530 | loss: 5.20936 | failed:   45
batch:      26540 | loss: 5.23454 | failed:   45
batch:      26550 | loss: 5.09054 | failed:   45
batch:      26560 | loss: 5.18574 | failed:   45
batch:      26570 | loss: 5.25952 | failed:   45
batch:      26580 | loss: 5.25615 | failed:   45
batch:      26590 | loss: 5.18525 | failed:   45
batch:      26600 | loss: 5.28006 | failed:   45
batch:      26610 | loss: 5.15160 | failed:   45
batch:      26620 | loss: 5.08688 | failed:   45
batch:      26630 | loss: 5.07709 | failed:   45
batch:      26640 | loss: 5.25675 | failed:   45
batch:      26650 | loss: 5.18345 | failed:   45
batch:      26660 | loss: 5.13060 | failed:   45
batch:      26670 | loss: 5.11223 | failed:   45
batch:      26680 | loss: 5.09457 | failed:   45
batch:      26690 | loss: 4.44018 | failed:   45
batch:      26700 | loss: 5.23415 | failed:   45
batch:      26710 | loss: 5.11940 | failed:   45
batch:      26720 | loss: 5.15048 | failed:   45
batch:      26730 | loss: 5.14141 | failed:   45
batch:      26740 | loss: 5.08810 | failed:   45
batch:      26750 | loss: 5.14214 | failed:   45
batch:      26760 | loss: 5.16597 | failed:   45
batch:      26770 | loss: 5.22841 | failed:   45
batch:      26780 | loss: 5.14208 | failed:   45
batch:      26790 | loss: 5.21578 | failed:   45
batch:      26800 | loss: 5.14456 | failed:   45
batch:      26810 | loss: 4.98273 | failed:   45
batch:      26820 | loss: 5.06573 | failed:   45
batch:      26830 | loss: 5.21107 | failed:   45
batch:      26840 | loss: 5.18084 | failed:   45
batch:      26850 | loss: 5.17332 | failed:   45
batch:      26860 | loss: 5.18500 | failed:   45
batch:      26870 | loss: 5.21032 | failed:   45
batch:      26880 | loss: 5.22258 | failed:   45
batch:      26890 | loss: 5.18970 | failed:   45
batch:      26900 | loss: 5.19124 | failed:   45
batch:      26910 | loss: 5.12695 | failed:   45
batch:      26920 | loss: 5.12502 | failed:   45
batch:      26930 | loss: 5.09669 | failed:   45
batch:      26940 | loss: 5.10349 | failed:   45
batch:      26950 | loss: 5.19677 | failed:   45
batch:      26960 | loss: 5.23219 | failed:   45
batch:      26970 | loss: 5.20855 | failed:   45
batch:      26980 | loss: 5.18343 | failed:   45
batch:      26990 | loss: 5.23390 | failed:   45
batch:      27000 | loss: 5.24213 | failed:   45
batch:      27010 | loss: 5.17098 | failed:   45
batch:      27020 | loss: 5.20773 | failed:   45
batch:      27030 | loss: 5.20598 | failed:   45
batch:      27040 | loss: 5.10537 | failed:   45
batch:      27050 | loss: 5.13754 | failed:   45
batch:      27060 | loss: 5.07581 | failed:   45
batch:      27070 | loss: 5.12815 | failed:   45
batch:      27080 | loss: 5.15160 | failed:   45
batch:      27090 | loss: 5.17234 | failed:   45
batch:      27100 | loss: 5.18836 | failed:   45
batch:      27110 | loss: 5.00623 | failed:   45
batch:      27120 | loss: 5.09687 | failed:   45
batch:      27130 | loss: 5.29887 | failed:   45
batch:      27140 | loss: 5.21067 | failed:   45
batch:      27150 | loss: 5.22293 | failed:   45
batch:      27160 | loss: 5.27628 | failed:   45
batch:      27170 | loss: 5.18023 | failed:   45
batch:      27180 | loss: 5.23880 | failed:   45
batch:      27190 | loss: 5.25935 | failed:   45
batch:      27200 | loss: 5.20832 | failed:   45
batch:      27210 | loss: 5.24864 | failed:   45
batch:      27220 | loss: 5.22481 | failed:   45
batch:      27230 | loss: 5.02405 | failed:   45
batch:      27240 | loss: 5.04852 | failed:   45
batch:      27250 | loss: 5.24408 | failed:   45
batch:      27260 | loss: 5.17703 | failed:   45
batch:      27270 | loss: 5.31307 | failed:   45
batch:      27280 | loss: 5.24717 | failed:   45
batch:      27290 | loss: 5.14725 | failed:   45
batch:      27300 | loss: 5.16756 | failed:   45
batch:      27310 | loss: 5.22066 | failed:   45
batch:      27320 | loss: 5.21017 | failed:   45
batch:      27330 | loss: 5.19930 | failed:   45
batch:      27340 | loss: 5.10756 | failed:   45
batch:      27350 | loss: 5.18591 | failed:   45
batch:      27360 | loss: 5.27725 | failed:   45
batch:      27370 | loss: 5.24885 | failed:   45
batch:      27380 | loss: 5.20516 | failed:   45
batch:      27390 | loss: 5.15391 | failed:   45
batch:      27400 | loss: 5.21027 | failed:   45
batch:      27410 | loss: 5.15129 | failed:   45
batch:      27420 | loss: 5.21593 | failed:   45
batch:      27430 | loss: 5.17258 | failed:   45
batch:      27440 | loss: 5.07532 | failed:   45
batch:      27450 | loss: 4.99122 | failed:   45
batch:      27460 | loss: 5.13757 | failed:   45
batch:      27470 | loss: 5.18314 | failed:   45
batch:      27480 | loss: 5.04181 | failed:   45
batch:      27490 | loss: 5.23707 | failed:   45
batch:      27500 | loss: 5.17405 | failed:   45
batch:      27510 | loss: 5.04095 | failed:   45
batch:      27520 | loss: 5.23639 | failed:   45
batch:      27530 | loss: 5.22918 | failed:   45
batch:      27540 | loss: 5.23563 | failed:   45
batch:      27550 | loss: 5.09573 | failed:   45
batch:      27560 | loss: 5.13814 | failed:   45
batch:      27570 | loss: 5.15729 | failed:   45
batch:      27580 | loss: 5.26401 | failed:   45
batch:      27590 | loss: 5.20582 | failed:   45
batch:      27600 | loss: 5.06985 | failed:   45
batch:      27610 | loss: 5.17443 | failed:   45
batch:      27620 | loss: 5.23652 | failed:   45
batch:      27630 | loss: 5.19264 | failed:   45
batch:      27640 | loss: 5.18387 | failed:   45
batch:      27650 | loss: 5.16085 | failed:   45
batch:      27660 | loss: 5.25541 | failed:   45
batch:      27670 | loss: 5.23098 | failed:   45
batch:      27680 | loss: 5.20888 | failed:   45
batch:      27690 | loss: 5.14507 | failed:   45
batch:      27700 | loss: 5.11986 | failed:   45
batch:      27710 | loss: 5.17764 | failed:   45
batch:      27720 | loss: 5.19889 | failed:   45
batch:      27730 | loss: 5.01525 | failed:   45
batch:      27740 | loss: 5.17379 | failed:   45
batch:      27750 | loss: 5.13828 | failed:   45
batch:      27760 | loss: 5.27189 | failed:   45
batch:      27770 | loss: 5.12328 | failed:   45
batch:      27780 | loss: 5.16640 | failed:   45
batch:      27790 | loss: 5.18778 | failed:   45
batch:      27800 | loss: 5.25362 | failed:   45
batch:      27810 | loss: 5.25689 | failed:   45
batch:      27820 | loss: 5.23364 | failed:   45
batch:      27830 | loss: 5.26647 | failed:   45
batch:      27840 | loss: 5.24951 | failed:   45
batch:      27850 | loss: 5.18106 | failed:   45
batch:      27860 | loss: 5.18709 | failed:   45
batch:      27870 | loss: 5.17459 | failed:   45
batch:      27880 | loss: 5.13432 | failed:   45
batch:      27890 | loss: 5.12042 | failed:   45
batch:      27900 | loss: 5.16761 | failed:   45
batch:      27910 | loss: 5.23957 | failed:   45
batch:      27920 | loss: 5.24784 | failed:   45
batch:      27930 | loss: 5.19336 | failed:   45
batch:      27940 | loss: 5.24213 | failed:   45
batch:      27950 | loss: 5.23942 | failed:   45
batch:      27960 | loss: 5.18320 | failed:   45
batch:      27970 | loss: 5.05739 | failed:   45
batch:      27980 | loss: 5.19151 | failed:   45
batch:      27990 | loss: 5.26292 | failed:   45
batch:      28000 | loss: 5.23352 | failed:   45
batch:      28010 | loss: 5.16021 | failed:   45
batch:      28020 | loss: 5.23449 | failed:   45
batch:      28030 | loss: 5.17016 | failed:   45
batch:      28040 | loss: 4.92734 | failed:   45
batch:      28050 | loss: 5.23737 | failed:   45
batch:      28060 | loss: 4.85457 | failed:   45
batch:      28070 | loss: 5.22459 | failed:   45
batch:      28080 | loss: 5.21052 | failed:   45
batch:      28090 | loss: 5.09708 | failed:   45
batch:      28100 | loss: 5.21305 | failed:   45
batch:      28110 | loss: 5.21819 | failed:   45
batch:      28120 | loss: 5.19411 | failed:   45
batch:      28130 | loss: 5.24821 | failed:   45
batch:      28140 | loss: 5.22682 | failed:   45
batch:      28150 | loss: 5.10996 | failed:   45
batch:      28160 | loss: 5.21476 | failed:   45
batch:      28170 | loss: 5.21391 | failed:   45
batch:      28180 | loss: 5.13901 | failed:   45
batch:      28190 | loss: 5.18191 | failed:   45
batch:      28200 | loss: 4.93267 | failed:   45
batch:      28210 | loss: 5.21720 | failed:   45
batch:      28220 | loss: 5.14351 | failed:   45
batch:      28230 | loss: 5.19329 | failed:   45
batch:      28240 | loss: 5.07676 | failed:   45
batch:      28250 | loss: 5.27433 | failed:   45
batch:      28260 | loss: 5.12120 | failed:   45
batch:      28270 | loss: 5.07599 | failed:   45
batch:      28280 | loss: 5.36688 | failed:   45
batch:      28290 | loss: 5.27030 | failed:   45
batch:      28300 | loss: 5.22207 | failed:   45
batch:      28310 | loss: 5.08350 | failed:   45
batch:      28320 | loss: 5.20787 | failed:   45
batch:      28330 | loss: 5.25393 | failed:   45
batch:      28340 | loss: 5.23116 | failed:   45
batch:      28350 | loss: 5.11247 | failed:   45
batch:      28360 | loss: 5.18356 | failed:   45
batch:      28370 | loss: 5.05065 | failed:   45
batch:      28380 | loss: 5.09287 | failed:   45
batch:      28390 | loss: 5.14373 | failed:   45
batch:      28400 | loss: 5.22635 | failed:   45
batch:      28410 | loss: 5.16797 | failed:   45
batch:      28420 | loss: 5.11240 | failed:   45
batch:      28430 | loss: 5.33375 | failed:   45
batch:      28440 | loss: 5.26595 | failed:   45
batch:      28450 | loss: 5.05523 | failed:   45
batch:      28460 | loss: 5.05101 | failed:   45
batch:      28470 | loss: 5.08394 | failed:   45
batch:      28480 | loss: 5.16573 | failed:   45
batch:      28490 | loss: 5.14341 | failed:   45
batch:      28500 | loss: 4.93212 | failed:   45
batch:      28510 | loss: 5.14706 | failed:   45
batch:      28520 | loss: 5.13294 | failed:   45
batch:      28530 | loss: 5.16129 | failed:   45
batch:      28540 | loss: 5.06179 | failed:   45
batch:      28550 | loss: 5.04696 | failed:   45
batch:      28560 | loss: 5.26821 | failed:   45
batch:      28570 | loss: 5.23933 | failed:   45
batch:      28580 | loss: 5.21626 | failed:   45
batch:      28590 | loss: 5.14835 | failed:   45
batch:      28600 | loss: 4.96973 | failed:   45
batch:      28610 | loss: 4.99457 | failed:   45
batch:      28620 | loss: 5.28824 | failed:   45
batch:      28630 | loss: 5.24565 | failed:   45
batch:      28640 | loss: 5.15549 | failed:   45
batch:      28650 | loss: 5.03145 | failed:   45
batch:      28660 | loss: 5.23240 | failed:   45
batch:      28670 | loss: 5.22927 | failed:   45
batch:      28680 | loss: 5.19347 | failed:   45
batch:      28690 | loss: 5.20199 | failed:   45
batch:      28700 | loss: 5.16510 | failed:   45
batch:      28710 | loss: 5.21122 | failed:   45
batch:      28720 | loss: 5.22060 | failed:   45
batch:      28730 | loss: 5.15613 | failed:   45
batch:      28740 | loss: 5.19266 | failed:   45
batch:      28750 | loss: 5.09550 | failed:   45
batch:      28760 | loss: 5.18070 | failed:   45
batch:      28770 | loss: 5.07344 | failed:   45
batch:      28780 | loss: 5.18905 | failed:   45
batch:      28790 | loss: 5.14649 | failed:   45
batch:      28800 | loss: 5.16711 | failed:   45
batch:      28810 | loss: 5.25719 | failed:   45
batch:      28820 | loss: 5.21371 | failed:   45
batch:      28830 | loss: 5.21641 | failed:   45
batch:      28840 | loss: 5.09011 | failed:   45
batch:      28850 | loss: 5.09371 | failed:   45
batch:      28860 | loss: 5.27457 | failed:   45
batch:      28870 | loss: 4.97584 | failed:   45
batch:      28880 | loss: 5.18370 | failed:   45
batch:      28890 | loss: 5.17826 | failed:   45
batch:      28900 | loss: 5.09782 | failed:   45
batch:      28910 | loss: 5.25265 | failed:   45
batch:      28920 | loss: 5.12770 | failed:   45
batch:      28930 | loss: 5.10871 | failed:   45
batch:      28940 | loss: 5.11446 | failed:   45
batch:      28950 | loss: 5.14016 | failed:   45
batch:      28960 | loss: 5.19587 | failed:   45
batch:      28970 | loss: 5.28361 | failed:   45
batch:      28980 | loss: 5.17247 | failed:   45
batch:      28990 | loss: 4.99551 | failed:   45
batch:      29000 | loss: 5.19138 | failed:   45
batch:      29010 | loss: 5.17455 | failed:   45
batch:      29020 | loss: 5.19138 | failed:   45
batch:      29030 | loss: 5.20670 | failed:   45
batch:      29040 | loss: 5.08917 | failed:   45
batch:      29050 | loss: 5.14089 | failed:   45
batch:      29060 | loss: 5.29245 | failed:   45
batch:      29070 | loss: 5.23524 | failed:   45
batch:      29080 | loss: 5.15147 | failed:   45
batch:      29090 | loss: 5.16558 | failed:   45
batch:      29100 | loss: 5.12804 | failed:   45
batch:      29110 | loss: 5.03416 | failed:   45
batch:      29120 | loss: 5.11046 | failed:   45
batch:      29130 | loss: 4.97844 | failed:   45
batch:      29140 | loss: 5.11476 | failed:   45
batch:      29150 | loss: 5.19118 | failed:   45
batch:      29160 | loss: 5.04897 | failed:   45
batch:      29170 | loss: 5.23157 | failed:   45
batch:      29180 | loss: 5.00255 | failed:   45
batch:      29190 | loss: 5.30057 | failed:   45
batch:      29200 | loss: 5.24836 | failed:   45
batch:      29210 | loss: 5.29061 | failed:   45
batch:      29220 | loss: 5.22018 | failed:   45
batch:      29230 | loss: 5.18793 | failed:   45
batch:      29240 | loss: 5.15046 | failed:   45
batch:      29250 | loss: 5.22078 | failed:   45
batch:      29260 | loss: 5.16977 | failed:   45
batch:      29270 | loss: 5.10696 | failed:   45
batch:      29280 | loss: 5.25661 | failed:   45
batch:      29290 | loss: 5.20673 | failed:   45
batch:      29300 | loss: 5.17468 | failed:   45
batch:      29310 | loss: 5.26788 | failed:   45
batch:      29320 | loss: 5.21995 | failed:   45
batch:      29330 | loss: 5.17807 | failed:   45
batch:      29340 | loss: 5.39837 | failed:   45
batch:      29350 | loss: 5.17681 | failed:   45
batch:      29360 | loss: 5.22300 | failed:   45
batch:      29370 | loss: 5.23598 | failed:   45
batch:      29380 | loss: 5.06062 | failed:   45
batch:      29390 | loss: 5.07529 | failed:   45
batch:      29400 | loss: 5.18310 | failed:   45
batch:      29410 | loss: 5.22075 | failed:   45
batch:      29420 | loss: 5.18170 | failed:   45
batch:      29430 | loss: 5.15914 | failed:   45
batch:      29440 | loss: 5.22846 | failed:   45
batch:      29450 | loss: 5.19741 | failed:   45
batch:      29460 | loss: 5.14014 | failed:   45
batch:      29470 | loss: 5.19487 | failed:   45
batch:      29480 | loss: 5.18620 | failed:   45
batch:      29490 | loss: 5.16289 | failed:   45
batch:      29500 | loss: 5.18715 | failed:   45
batch:      29510 | loss: 5.19096 | failed:   45
batch:      29520 | loss: 5.15726 | failed:   45
batch:      29530 | loss: 5.26612 | failed:   45
batch:      29540 | loss: 5.12518 | failed:   45
batch:      29550 | loss: 5.17317 | failed:   45
batch:      29560 | loss: 5.12566 | failed:   45
batch:      29570 | loss: 5.11351 | failed:   45
batch:      29580 | loss: 5.21008 | failed:   45
batch:      29590 | loss: 5.12729 | failed:   45
batch:      29600 | loss: 5.16464 | failed:   45
batch:      29610 | loss: 5.25034 | failed:   45
batch:      29620 | loss: 5.13530 | failed:   45
batch:      29630 | loss: 5.12244 | failed:   45
batch:      29640 | loss: 5.27199 | failed:   45
batch:      29650 | loss: 5.06153 | failed:   45
batch:      29660 | loss: 5.14411 | failed:   45
batch:      29670 | loss: 5.23889 | failed:   45
batch:      29680 | loss: 5.15681 | failed:   45
batch:      29690 | loss: 5.16310 | failed:   45
batch:      29700 | loss: 5.23627 | failed:   45
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      29720 | loss: 5.13375 | failed:   48
batch:      29730 | loss: 5.17110 | failed:   48
batch:      29740 | loss: 5.14262 | failed:   48
batch:      29750 | loss: 5.13196 | failed:   48
batch:      29760 | loss: 5.17415 | failed:   48
batch:      29770 | loss: 5.10019 | failed:   48
batch:      29780 | loss: 5.15247 | failed:   48
batch:      29790 | loss: 5.23868 | failed:   48
batch:      29800 | loss: 5.16977 | failed:   48
batch:      29810 | loss: 5.19873 | failed:   48
batch:      29820 | loss: 5.16755 | failed:   48
batch:      29830 | loss: 5.32379 | failed:   48
batch:      29840 | loss: 5.21008 | failed:   48
batch:      29850 | loss: 5.08267 | failed:   48
batch:      29860 | loss: 5.16073 | failed:   48
batch:      29870 | loss: 4.92003 | failed:   48
batch:      29880 | loss: 5.09072 | failed:   48
batch:      29890 | loss: 5.22500 | failed:   48
batch:      29900 | loss: 5.17785 | failed:   48
batch:      29910 | loss: 5.18023 | failed:   48
batch:      29920 | loss: 5.29133 | failed:   48
batch:      29930 | loss: 5.20393 | failed:   48
batch:      29940 | loss: 4.92833 | failed:   48
batch:      29950 | loss: 5.13099 | failed:   48
batch:      29960 | loss: 5.24006 | failed:   48
batch:      29970 | loss: 5.16332 | failed:   48
batch:      29980 | loss: 5.09547 | failed:   48
batch:      29990 | loss: 5.09646 | failed:   48
batch:      30000 | loss: 5.06993 | failed:   48
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      30010 | loss: 5.08253 | failed:   48
batch:      30020 | loss: 5.27525 | failed:   48
batch:      30030 | loss: 5.01893 | failed:   48
batch:      30040 | loss: 5.26491 | failed:   48
batch:      30050 | loss: 5.16272 | failed:   48
batch:      30060 | loss: 5.16406 | failed:   48
batch:      30070 | loss: 5.12666 | failed:   48
batch:      30080 | loss: 5.16050 | failed:   48
batch:      30090 | loss: 5.17535 | failed:   48
batch:      30100 | loss: 5.15345 | failed:   48
batch:      30110 | loss: 5.32865 | failed:   48
batch:      30120 | loss: 5.14207 | failed:   48
batch:      30130 | loss: 5.17018 | failed:   48
batch:      30140 | loss: 5.15524 | failed:   48
batch:      30150 | loss: 5.21708 | failed:   48
batch:      30160 | loss: 5.20729 | failed:   48
batch:      30170 | loss: 5.17278 | failed:   48
batch:      30180 | loss: 4.96628 | failed:   48
batch:      30190 | loss: 4.90917 | failed:   48
batch:      30200 | loss: 5.12938 | failed:   48
batch:      30210 | loss: 5.21242 | failed:   48
batch:      30220 | loss: 5.22038 | failed:   48
batch:      30230 | loss: 5.20333 | failed:   48
batch:      30240 | loss: 5.20489 | failed:   48
batch:      30250 | loss: 5.23883 | failed:   48
batch:      30260 | loss: 5.15014 | failed:   48
batch:      30270 | loss: 5.15620 | failed:   48
batch:      30280 | loss: 5.07673 | failed:   48
batch:      30290 | loss: 5.10075 | failed:   48
batch:      30300 | loss: 5.16261 | failed:   48
batch:      30310 | loss: 4.99593 | failed:   48
batch:      30320 | loss: 5.19640 | failed:   48
batch:      30330 | loss: 5.19657 | failed:   48
batch:      30340 | loss: 5.03145 | failed:   48
batch:      30350 | loss: 4.85872 | failed:   48
batch:      30360 | loss: 5.17883 | failed:   48
batch:      30370 | loss: 5.21009 | failed:   48
batch:      30380 | loss: 5.20526 | failed:   48
batch:      30390 | loss: 5.26293 | failed:   48
batch:      30400 | loss: 5.17741 | failed:   48
batch:      30410 | loss: 5.12024 | failed:   48
batch:      30420 | loss: 5.14078 | failed:   48
batch:      30430 | loss: 5.22790 | failed:   48
batch:      30440 | loss: 5.14426 | failed:   48
batch:      30450 | loss: 5.18343 | failed:   48
batch:      30460 | loss: 5.26787 | failed:   48
batch:      30470 | loss: 5.12188 | failed:   48
batch:      30480 | loss: 5.12645 | failed:   48
batch:      30490 | loss: 5.18557 | failed:   48
batch:      30500 | loss: 5.23945 | failed:   48
batch:      30510 | loss: 5.05792 | failed:   48
batch:      30520 | loss: 5.11473 | failed:   48
batch:      30530 | loss: 4.99700 | failed:   48
batch:      30540 | loss: 5.14726 | failed:   48
batch:      30550 | loss: 5.08237 | failed:   48
batch:      30560 | loss: 5.24575 | failed:   48
batch:      30570 | loss: 5.24644 | failed:   48
batch:      30580 | loss: 5.17888 | failed:   48
batch:      30590 | loss: 5.12010 | failed:   48
batch:      30600 | loss: 5.25257 | failed:   48
batch:      30610 | loss: 5.11729 | failed:   48
batch:      30620 | loss: 5.18672 | failed:   48
batch:      30630 | loss: 5.20912 | failed:   48
batch:      30640 | loss: 5.28068 | failed:   48
batch:      30650 | loss: 5.29600 | failed:   48
batch:      30660 | loss: 5.25560 | failed:   48
batch:      30670 | loss: 5.19384 | failed:   48
batch:      30680 | loss: 5.18940 | failed:   48
batch:      30690 | loss: 5.16879 | failed:   48
batch:      30700 | loss: 5.21652 | failed:   48
batch:      30710 | loss: 5.17101 | failed:   48
batch:      30720 | loss: 5.23706 | failed:   48
batch:      30730 | loss: 5.25513 | failed:   48
batch:      30740 | loss: 5.23864 | failed:   48
batch:      30750 | loss: 5.15454 | failed:   48
batch:      30760 | loss: 5.14079 | failed:   48
batch:      30770 | loss: 4.98980 | failed:   48
batch:      30780 | loss: 5.14549 | failed:   48
batch:      30790 | loss: 5.15863 | failed:   48
batch:      30800 | loss: 5.17652 | failed:   48
batch:      30810 | loss: 5.12154 | failed:   48
batch:      30820 | loss: 5.16999 | failed:   48
batch:      30830 | loss: 5.17126 | failed:   48
batch:      30840 | loss: 5.10855 | failed:   48
batch:      30850 | loss: 5.12016 | failed:   48
batch:      30860 | loss: 5.11006 | failed:   48
batch:      30870 | loss: 5.10945 | failed:   48
batch:      30880 | loss: 5.22590 | failed:   48
batch:      30890 | loss: 5.17999 | failed:   48
batch:      30900 | loss: 5.18685 | failed:   48
batch:      30910 | loss: 5.05323 | failed:   48
batch:      30920 | loss: 5.22318 | failed:   48
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      30950 | loss: 5.25567 | failed:   60
batch:      30960 | loss: 5.37726 | failed:   60
batch:      30970 | loss: 5.19756 | failed:   60
batch:      30980 | loss: 5.22592 | failed:   60
batch:      30990 | loss: 5.16158 | failed:   60
batch:      31000 | loss: 5.11018 | failed:   60
batch:      31010 | loss: 5.13208 | failed:   60
batch:      31020 | loss: 5.14049 | failed:   60
batch:      31030 | loss: 5.31427 | failed:   60
batch:      31040 | loss: 5.17744 | failed:   60
batch:      31050 | loss: 5.24179 | failed:   60
batch:      31060 | loss: 5.14108 | failed:   60
batch:      31070 | loss: 5.17261 | failed:   60
batch:      31080 | loss: 5.23114 | failed:   60
batch:      31090 | loss: 5.22940 | failed:   60
batch:      31100 | loss: 5.21062 | failed:   60
batch:      31110 | loss: 5.09814 | failed:   60
batch:      31120 | loss: 5.19654 | failed:   60
batch:      31130 | loss: 5.13956 | failed:   60
batch:      31140 | loss: 5.16844 | failed:   60
batch:      31150 | loss: 5.08197 | failed:   60
batch:      31160 | loss: 5.24146 | failed:   60
batch:      31170 | loss: 5.23737 | failed:   60
batch:      31180 | loss: 5.23510 | failed:   60
batch:      31190 | loss: 5.12111 | failed:   60
batch:      31200 | loss: 5.11084 | failed:   60
batch:      31210 | loss: 5.24419 | failed:   60
batch:      31220 | loss: 5.15732 | failed:   60
batch:      31230 | loss: 5.13699 | failed:   60
batch:      31240 | loss: 5.14882 | failed:   60
batch:      31250 | loss: 5.18224 | failed:   60
batch:      31260 | loss: 5.25799 | failed:   60
batch:      31270 | loss: 5.07703 | failed:   60
batch:      31280 | loss: 5.19718 | failed:   60
batch:      31290 | loss: 5.14594 | failed:   60
batch:      31300 | loss: 5.08131 | failed:   60
batch:      31310 | loss: 5.26045 | failed:   60
batch:      31320 | loss: 5.13127 | failed:   60
batch:      31330 | loss: 5.15975 | failed:   60
batch:      31340 | loss: 5.14546 | failed:   60
batch:      31350 | loss: 5.22311 | failed:   60
batch:      31360 | loss: 5.26197 | failed:   60
batch:      31370 | loss: 5.20782 | failed:   60
batch:      31380 | loss: 5.20866 | failed:   60
batch:      31390 | loss: 5.01205 | failed:   60
batch:      31400 | loss: 4.97482 | failed:   60
batch:      31410 | loss: 5.16359 | failed:   60
batch:      31420 | loss: 5.04996 | failed:   60
batch:      31430 | loss: 5.39161 | failed:   60
batch:      31440 | loss: 5.10382 | failed:   60
batch:      31450 | loss: 5.10745 | failed:   60
batch:      31460 | loss: 5.18408 | failed:   60
batch:      31470 | loss: 5.23606 | failed:   60
batch:      31480 | loss: 5.20669 | failed:   60
batch:      31490 | loss: 5.04482 | failed:   60
batch:      31500 | loss: 5.11520 | failed:   60
batch:      31510 | loss: 5.00482 | failed:   60
batch:      31520 | loss: 5.25214 | failed:   60
batch:      31530 | loss: 5.12419 | failed:   60
batch:      31540 | loss: 5.21929 | failed:   60
batch:      31550 | loss: 4.95736 | failed:   60
batch:      31560 | loss: 5.13575 | failed:   60
batch:      31570 | loss: 5.27289 | failed:   60
batch:      31580 | loss: 5.19119 | failed:   60
batch:      31590 | loss: 5.12299 | failed:   60
batch:      31600 | loss: 5.18878 | failed:   60
batch:      31610 | loss: 5.30383 | failed:   60
batch:      31620 | loss: 5.24226 | failed:   60
batch:      31630 | loss: 5.19827 | failed:   60
batch:      31640 | loss: 5.19619 | failed:   60
batch:      31650 | loss: 5.15964 | failed:   60
batch:      31660 | loss: 5.11487 | failed:   60
batch:      31670 | loss: 5.03647 | failed:   60
batch:      31680 | loss: 5.08892 | failed:   60
batch:      31690 | loss: 5.19966 | failed:   60
batch:      31700 | loss: 4.97701 | failed:   60
batch:      31710 | loss: 5.18178 | failed:   60
batch:      31720 | loss: 5.18964 | failed:   60
batch:      31730 | loss: 5.12182 | failed:   60
batch:      31740 | loss: 5.15644 | failed:   60
batch:      31750 | loss: 5.09580 | failed:   60
batch:      31760 | loss: 5.12098 | failed:   60
batch:      31770 | loss: 5.09357 | failed:   60
batch:      31780 | loss: 5.25522 | failed:   60
batch:      31790 | loss: 5.13655 | failed:   60
batch:      31800 | loss: 5.24443 | failed:   60
batch:      31810 | loss: 5.26532 | failed:   60
batch:      31820 | loss: 5.22236 | failed:   60
batch:      31830 | loss: 4.67403 | failed:   60
batch:      31840 | loss: 5.18356 | failed:   60
batch:      31850 | loss: 4.96055 | failed:   60
batch:      31860 | loss: 5.18610 | failed:   60
batch:      31870 | loss: 5.18113 | failed:   60
batch:      31880 | loss: 5.12891 | failed:   60
batch:      31890 | loss: 5.03432 | failed:   60
batch:      31900 | loss: 5.16928 | failed:   60
batch:      31910 | loss: 5.27811 | failed:   60
batch:      31920 | loss: 5.21943 | failed:   60
batch:      31930 | loss: 5.22053 | failed:   60
batch:      31940 | loss: 5.20677 | failed:   60
batch:      31950 | loss: 5.20709 | failed:   60
batch:      31960 | loss: 5.05999 | failed:   60
batch:      31970 | loss: 4.89971 | failed:   60
batch:      31980 | loss: 5.19888 | failed:   60
batch:      31990 | loss: 5.21454 | failed:   60
batch:      32000 | loss: 5.17422 | failed:   60
batch:      32010 | loss: 5.04098 | failed:   60
batch:      32020 | loss: 5.16551 | failed:   60
batch:      32030 | loss: 5.15838 | failed:   60
batch:      32040 | loss: 5.05855 | failed:   60
batch:      32050 | loss: 5.07794 | failed:   60
batch:      32060 | loss: 5.10631 | failed:   60
batch:      32070 | loss: 5.20449 | failed:   60
batch:      32080 | loss: 5.18098 | failed:   60
batch:      32090 | loss: 5.07264 | failed:   60
batch:      32100 | loss: 5.11962 | failed:   60
batch:      32110 | loss: 5.13546 | failed:   60
batch:      32120 | loss: 5.11460 | failed:   60
batch:      32130 | loss: 5.11863 | failed:   60
batch:      32140 | loss: 5.17954 | failed:   60
batch:      32150 | loss: 5.18097 | failed:   60
batch:      32160 | loss: 5.14201 | failed:   60
batch:      32170 | loss: 5.07250 | failed:   60
batch:      32180 | loss: 5.14046 | failed:   60
batch:      32190 | loss: 5.33289 | failed:   60
batch:      32200 | loss: 5.24799 | failed:   60
batch:      32210 | loss: 5.20986 | failed:   60
batch:      32220 | loss: 5.13521 | failed:   60
batch:      32230 | loss: 5.17462 | failed:   60
batch:      32240 | loss: 5.09731 | failed:   60
batch:      32250 | loss: 5.17903 | failed:   60
batch:      32260 | loss: 5.19019 | failed:   60
batch:      32270 | loss: 5.19745 | failed:   60
batch:      32280 | loss: 4.87721 | failed:   60
batch:      32290 | loss: 5.20862 | failed:   60
batch:      32300 | loss: 5.19696 | failed:   60
batch:      32310 | loss: 5.15727 | failed:   60
batch:      32320 | loss: 5.17793 | failed:   60
batch:      32330 | loss: 5.12567 | failed:   60
batch:      32340 | loss: 4.99978 | failed:   60
batch:      32350 | loss: 4.98253 | failed:   60
batch:      32360 | loss: 4.89022 | failed:   60
batch:      32370 | loss: 4.79841 | failed:   60
batch:      32380 | loss: 4.89708 | failed:   60
batch:      32390 | loss: 4.63804 | failed:   60
batch:      32400 | loss: 4.33572 | failed:   60
batch:      32410 | loss: 4.41294 | failed:   60
batch:      32420 | loss: 4.60834 | failed:   60
batch:      32430 | loss: 4.99892 | failed:   60
batch:      32440 | loss: 5.07873 | failed:   60
batch:      32450 | loss: 5.16342 | failed:   60
batch:      32460 | loss: 5.30207 | failed:   60
batch:      32470 | loss: 5.21896 | failed:   60
batch:      32480 | loss: 5.30154 | failed:   60
batch:      32490 | loss: 5.29474 | failed:   60
batch:      32500 | loss: 5.25365 | failed:   60
batch:      32510 | loss: 5.04227 | failed:   60
batch:      32520 | loss: 5.23979 | failed:   60
batch:      32530 | loss: 5.23959 | failed:   60
batch:      32540 | loss: 5.22502 | failed:   60
batch:      32550 | loss: 5.19860 | failed:   60
batch:      32560 | loss: 5.12380 | failed:   60
batch:      32570 | loss: 5.00814 | failed:   60
batch:      32580 | loss: 5.11031 | failed:   60
batch:      32590 | loss: 5.01481 | failed:   60
batch:      32600 | loss: 5.12519 | failed:   60
batch:      32610 | loss: 5.11992 | failed:   60
batch:      32620 | loss: 5.11793 | failed:   60
batch:      32630 | loss: 5.10960 | failed:   60
batch:      32640 | loss: 5.15659 | failed:   60
batch:      32650 | loss: 5.04974 | failed:   60
batch:      32660 | loss: 5.11156 | failed:   60
batch:      32670 | loss: 5.03421 | failed:   60
batch:      32680 | loss: 4.93639 | failed:   60
batch:      32690 | loss: 4.91143 | failed:   60
batch:      32700 | loss: 5.20094 | failed:   60
batch:      32710 | loss: 5.17562 | failed:   60
batch:      32720 | loss: 5.09735 | failed:   60
batch:      32730 | loss: 5.21832 | failed:   60
batch:      32740 | loss: 5.19908 | failed:   60
batch:      32750 | loss: 5.30649 | failed:   60
batch:      32760 | loss: 5.26177 | failed:   60
batch:      32770 | loss: 5.28911 | failed:   60
batch:      32780 | loss: 5.23283 | failed:   60
batch:      32790 | loss: 5.25616 | failed:   60
batch:      32800 | loss: 5.28859 | failed:   60
batch:      32810 | loss: 5.20276 | failed:   60
batch:      32820 | loss: 5.06458 | failed:   60
batch:      32830 | loss: 5.10791 | failed:   60
batch:      32840 | loss: 5.15630 | failed:   60
batch:      32850 | loss: 5.08589 | failed:   60
batch:      32860 | loss: 5.29791 | failed:   60
batch:      32870 | loss: 5.22547 | failed:   60
batch:      32880 | loss: 5.21075 | failed:   60
batch:      32890 | loss: 5.36254 | failed:   60
batch:      32900 | loss: 5.18951 | failed:   60
batch:      32910 | loss: 5.19097 | failed:   60
batch:      32920 | loss: 5.30111 | failed:   60
batch:      32930 | loss: 5.14710 | failed:   60
batch:      32940 | loss: 5.15218 | failed:   60
batch:      32950 | loss: 5.07630 | failed:   60
batch:      32960 | loss: 5.10413 | failed:   60
batch:      32970 | loss: 5.15871 | failed:   60
batch:      32980 | loss: 5.11990 | failed:   60
batch:      32990 | loss: 5.20490 | failed:   60
batch:      33000 | loss: 5.08178 | failed:   60
batch:      33010 | loss: 4.94580 | failed:   60
batch:      33020 | loss: 5.21398 | failed:   60
batch:      33030 | loss: 5.23134 | failed:   60
batch:      33040 | loss: 5.19855 | failed:   60
batch:      33050 | loss: 5.18691 | failed:   60
batch:      33060 | loss: 5.08721 | failed:   60
batch:      33070 | loss: 5.16166 | failed:   60
batch:      33080 | loss: 4.92087 | failed:   60
batch:      33090 | loss: 4.74265 | failed:   60
batch:      33100 | loss: 5.04641 | failed:   60
batch:      33110 | loss: 5.26713 | failed:   60
batch:      33120 | loss: 5.31000 | failed:   60
batch:      33130 | loss: 5.05577 | failed:   60
batch:      33140 | loss: 5.09006 | failed:   60
batch:      33150 | loss: 5.22413 | failed:   60
batch:      33160 | loss: 5.18539 | failed:   60
batch:      33170 | loss: 5.22168 | failed:   60
batch:      33180 | loss: 5.25776 | failed:   60
batch:      33190 | loss: 5.20671 | failed:   60
batch:      33200 | loss: 5.24691 | failed:   60
batch:      33210 | loss: 5.15433 | failed:   60
batch:      33220 | loss: 5.07073 | failed:   60
batch:      33230 | loss: 5.19025 | failed:   60
batch:      33240 | loss: 5.23527 | failed:   60
batch:      33250 | loss: 5.19476 | failed:   60
batch:      33260 | loss: 5.19293 | failed:   60
batch:      33270 | loss: 5.21027 | failed:   60
batch:      33280 | loss: 5.22390 | failed:   60
batch:      33290 | loss: 5.20550 | failed:   60
batch:      33300 | loss: 4.85076 | failed:   60
batch:      33310 | loss: 5.21267 | failed:   60
batch:      33320 | loss: 5.21186 | failed:   60
batch:      33330 | loss: 5.16753 | failed:   60
batch:      33340 | loss: 5.13243 | failed:   60
batch:      33350 | loss: 5.13070 | failed:   60
batch:      33360 | loss: 5.12541 | failed:   60
batch:      33370 | loss: 5.05730 | failed:   60
batch:      33380 | loss: 5.20693 | failed:   60
batch:      33390 | loss: 5.13391 | failed:   60
batch:      33400 | loss: 5.19700 | failed:   60
batch:      33410 | loss: 5.15918 | failed:   60
batch:      33420 | loss: 5.12836 | failed:   60
batch:      33430 | loss: 5.09278 | failed:   60
batch:      33440 | loss: 5.25206 | failed:   60
batch:      33450 | loss: 5.20893 | failed:   60
batch:      33460 | loss: 5.20067 | failed:   60
batch:      33470 | loss: 5.19040 | failed:   60
batch:      33480 | loss: 5.21561 | failed:   60
batch:      33490 | loss: 5.15982 | failed:   60
batch:      33500 | loss: 5.25439 | failed:   60
batch:      33510 | loss: 5.23262 | failed:   60
batch:      33520 | loss: 5.21547 | failed:   60
batch:      33530 | loss: 5.02005 | failed:   60
batch:      33540 | loss: 5.25561 | failed:   60
batch:      33550 | loss: 5.22645 | failed:   60
batch:      33560 | loss: 5.19388 | failed:   60
batch:      33570 | loss: 5.26346 | failed:   60
batch:      33580 | loss: 5.19841 | failed:   60
batch:      33590 | loss: 5.17431 | failed:   60
batch:      33600 | loss: 5.09031 | failed:   60
batch:      33610 | loss: 5.20650 | failed:   60
batch:      33620 | loss: 5.15453 | failed:   60
batch:      33630 | loss: 5.20546 | failed:   60
batch:      33640 | loss: 4.85783 | failed:   60
batch:      33650 | loss: 5.10614 | failed:   60
batch:      33660 | loss: 5.26733 | failed:   60
batch:      33670 | loss: 5.10257 | failed:   60
batch:      33680 | loss: 5.28399 | failed:   60
batch:      33690 | loss: 5.21848 | failed:   60
batch:      33700 | loss: 5.15375 | failed:   60
batch:      33710 | loss: 5.05287 | failed:   60
batch:      33720 | loss: 5.10861 | failed:   60
batch:      33730 | loss: 4.98442 | failed:   60
batch:      33740 | loss: 5.04365 | failed:   60
batch:      33750 | loss: 4.94465 | failed:   60
batch:      33760 | loss: 5.10066 | failed:   60
batch:      33770 | loss: 5.02415 | failed:   60
batch:      33780 | loss: 4.96213 | failed:   60
batch:      33790 | loss: 5.18824 | failed:   60
batch:      33800 | loss: 5.17521 | failed:   60
batch:      33810 | loss: 5.14185 | failed:   60
batch:      33820 | loss: 5.13759 | failed:   60
batch:      33830 | loss: 5.34069 | failed:   60
batch:      33840 | loss: 5.31273 | failed:   60
batch:      33850 | loss: 5.16383 | failed:   60
batch:      33860 | loss: 5.10581 | failed:   60
batch:      33870 | loss: 5.18003 | failed:   60
batch:      33880 | loss: 4.89172 | failed:   60
batch:      33890 | loss: 5.00850 | failed:   60
batch:      33900 | loss: 5.07996 | failed:   60
batch:      33910 | loss: 5.07201 | failed:   60
batch:      33920 | loss: 5.03557 | failed:   60
batch:      33930 | loss: 5.07996 | failed:   60
batch:      33940 | loss: 5.22952 | failed:   60
batch:      33950 | loss: 5.21508 | failed:   60
batch:      33960 | loss: 5.28612 | failed:   60
batch:      33970 | loss: 5.26046 | failed:   60
batch:      33980 | loss: 5.19445 | failed:   60
batch:      33990 | loss: 5.00011 | failed:   60
batch:      34000 | loss: 5.16662 | failed:   60
batch:      34010 | loss: 5.29778 | failed:   60
batch:      34020 | loss: 5.25446 | failed:   60
batch:      34030 | loss: 5.20964 | failed:   60
batch:      34040 | loss: 5.20695 | failed:   60
batch:      34050 | loss: 5.20665 | failed:   60
batch:      34060 | loss: 5.18414 | failed:   60
batch:      34070 | loss: 5.09763 | failed:   60
batch:      34080 | loss: 5.11987 | failed:   60
batch:      34090 | loss: 4.93438 | failed:   60
batch:      34100 | loss: 5.19613 | failed:   60
batch:      34110 | loss: 5.20021 | failed:   60
batch:      34120 | loss: 5.08673 | failed:   60
batch:      34130 | loss: 4.64208 | failed:   60
batch:      34140 | loss: 3.95957 | failed:   60
batch:      34150 | loss: 5.42674 | failed:   60
batch:      34160 | loss: 5.17317 | failed:   60
batch:      34170 | loss: 5.10651 | failed:   60
batch:      34180 | loss: 5.23621 | failed:   60
batch:      34190 | loss: 5.17200 | failed:   60
batch:      34200 | loss: 5.22227 | failed:   60
batch:      34210 | loss: 5.13222 | failed:   60
batch:      34220 | loss: 5.26988 | failed:   60
batch:      34230 | loss: 5.14713 | failed:   60
batch:      34240 | loss: 5.25327 | failed:   60
batch:      34250 | loss: 5.08362 | failed:   60
batch:      34260 | loss: 5.21294 | failed:   60
batch:      34270 | loss: 5.07420 | failed:   60
batch:      34280 | loss: 5.26067 | failed:   60
batch:      34290 | loss: 5.02551 | failed:   60
batch:      34300 | loss: 5.08616 | failed:   60
batch:      34310 | loss: 5.19163 | failed:   60
batch:      34320 | loss: 5.20326 | failed:   60
batch:      34330 | loss: 5.22060 | failed:   60
batch:      34340 | loss: 5.21589 | failed:   60
batch:      34350 | loss: 5.30360 | failed:   60
batch:      34360 | loss: 5.24499 | failed:   60
batch:      34370 | loss: 5.26986 | failed:   60
batch:      34380 | loss: 5.09711 | failed:   60
batch:      34390 | loss: 5.18581 | failed:   60
batch:      34400 | loss: 5.18211 | failed:   60
batch:      34410 | loss: 5.16126 | failed:   60
batch:      34420 | loss: 5.08012 | failed:   60
batch:      34430 | loss: 5.08836 | failed:   60
batch:      34440 | loss: 5.17364 | failed:   60
batch:      34450 | loss: 5.16019 | failed:   60
batch:      34460 | loss: 5.23977 | failed:   60
batch:      34470 | loss: 5.15673 | failed:   60
batch:      34480 | loss: 5.56827 | failed:   60
batch:      34490 | loss: 5.18848 | failed:   60
batch:      34500 | loss: 5.10260 | failed:   60
batch:      34510 | loss: 5.19280 | failed:   60
batch:      34520 | loss: 4.81767 | failed:   60
batch:      34530 | loss: 5.19443 | failed:   60
batch:      34540 | loss: 5.14838 | failed:   60
batch:      34550 | loss: 5.21555 | failed:   60
batch:      34560 | loss: 5.10592 | failed:   60
batch:      34570 | loss: 5.18649 | failed:   60
batch:      34580 | loss: 5.24351 | failed:   60
batch:      34590 | loss: 5.26657 | failed:   60
batch:      34600 | loss: 5.05390 | failed:   60
batch:      34610 | loss: 5.15645 | failed:   60
batch:      34620 | loss: 5.19761 | failed:   60
batch:      34630 | loss: 5.16673 | failed:   60
batch:      34640 | loss: 5.15333 | failed:   60
batch:      34650 | loss: 5.09287 | failed:   60
batch:      34660 | loss: 5.16844 | failed:   60
batch:      34670 | loss: 5.25872 | failed:   60
batch:      34680 | loss: 5.14178 | failed:   60
batch:      34690 | loss: 5.19792 | failed:   60
batch:      34700 | loss: 5.01866 | failed:   60
batch:      34710 | loss: 5.13799 | failed:   60
batch:      34720 | loss: 5.07052 | failed:   60
batch:      34730 | loss: 5.28429 | failed:   60
batch:      34740 | loss: 5.02536 | failed:   60
batch:      34750 | loss: 5.06146 | failed:   60
batch:      34760 | loss: 5.15386 | failed:   60
batch:      34770 | loss: 5.15874 | failed:   60
batch:      34780 | loss: 5.25843 | failed:   60
batch:      34790 | loss: 4.96418 | failed:   60
batch:      34800 | loss: 5.20432 | failed:   60
batch:      34810 | loss: 5.02189 | failed:   60
batch:      34820 | loss: 5.19084 | failed:   60
batch:      34830 | loss: 4.99574 | failed:   60
batch:      34840 | loss: 5.22198 | failed:   60
batch:      34850 | loss: 5.25276 | failed:   60
batch:      34860 | loss: 5.21582 | failed:   60
batch:      34870 | loss: 5.11543 | failed:   60
batch:      34880 | loss: 5.19772 | failed:   60
batch:      34890 | loss: 5.16510 | failed:   60
batch:      34900 | loss: 5.27112 | failed:   60
batch:      34910 | loss: 4.98456 | failed:   60
batch:      34920 | loss: 5.14586 | failed:   60
batch:      34930 | loss: 5.11854 | failed:   60
batch:      34940 | loss: 5.12416 | failed:   60
batch:      34950 | loss: 5.05846 | failed:   60
batch:      34960 | loss: 5.07122 | failed:   60
batch:      34970 | loss: 5.10926 | failed:   60
batch:      34980 | loss: 5.16202 | failed:   60
batch:      34990 | loss: 5.21302 | failed:   60
batch:      35000 | loss: 5.23981 | failed:   60
batch:      35010 | loss: 5.21291 | failed:   60
batch:      35020 | loss: 5.18385 | failed:   60
batch:      35030 | loss: 5.07425 | failed:   60
batch:      35040 | loss: 5.16852 | failed:   60
batch:      35050 | loss: 5.21279 | failed:   60
batch:      35060 | loss: 5.04168 | failed:   60
batch:      35070 | loss: 5.25999 | failed:   60
batch:      35080 | loss: 5.19735 | failed:   60
batch:      35090 | loss: 5.18090 | failed:   60
batch:      35100 | loss: 5.13062 | failed:   60
batch:      35110 | loss: 5.12071 | failed:   60
batch:      35120 | loss: 5.17962 | failed:   60
batch:      35130 | loss: 5.19370 | failed:   60
batch:      35140 | loss: 5.23042 | failed:   60
batch:      35150 | loss: 5.00232 | failed:   60
batch:      35160 | loss: 5.15929 | failed:   60
batch:      35170 | loss: 5.21410 | failed:   60
batch:      35180 | loss: 5.08326 | failed:   60
batch:      35190 | loss: 5.13486 | failed:   60
batch:      35200 | loss: 5.14085 | failed:   60
batch:      35210 | loss: 5.09462 | failed:   60
batch:      35220 | loss: 5.17919 | failed:   60
batch:      35230 | loss: 5.19122 | failed:   60
batch:      35240 | loss: 5.13326 | failed:   60
batch:      35250 | loss: 5.11402 | failed:   60
batch:      35260 | loss: 5.17369 | failed:   60
batch:      35270 | loss: 5.22961 | failed:   60
batch:      35280 | loss: 5.16476 | failed:   60
batch:      35290 | loss: 5.12586 | failed:   60
batch:      35300 | loss: 5.11148 | failed:   60
batch:      35310 | loss: 5.01472 | failed:   60
batch:      35320 | loss: 5.27389 | failed:   60
batch:      35330 | loss: 5.14757 | failed:   60
batch:      35340 | loss: 5.10552 | failed:   60
batch:      35350 | loss: 5.13319 | failed:   60
batch:      35360 | loss: 5.17443 | failed:   60
batch:      35370 | loss: 5.16996 | failed:   60
batch:      35380 | loss: 5.22154 | failed:   60
batch:      35390 | loss: 5.13147 | failed:   60
batch:      35400 | loss: 5.21329 | failed:   60
batch:      35410 | loss: 5.17101 | failed:   60
batch:      35420 | loss: 5.25524 | failed:   60
batch:      35430 | loss: 5.23076 | failed:   60
batch:      35440 | loss: 5.21057 | failed:   60
batch:      35450 | loss: 5.14155 | failed:   60
batch:      35460 | loss: 5.05873 | failed:   60
batch:      35470 | loss: 5.25674 | failed:   60
batch:      35480 | loss: 5.15277 | failed:   60
batch:      35490 | loss: 5.15136 | failed:   60
batch:      35500 | loss: 4.90223 | failed:   60
batch:      35510 | loss: 5.19774 | failed:   60
batch:      35520 | loss: 5.22643 | failed:   60
batch:      35530 | loss: 5.05096 | failed:   60
batch:      35540 | loss: 5.10579 | failed:   60
batch:      35550 | loss: 5.14222 | failed:   60
batch:      35560 | loss: 4.87615 | failed:   60
batch:      35570 | loss: 5.03074 | failed:   60
batch:      35580 | loss: 5.20334 | failed:   60
batch:      35590 | loss: 5.13392 | failed:   60
batch:      35600 | loss: 5.19546 | failed:   60
batch:      35610 | loss: 5.25353 | failed:   60
batch:      35620 | loss: 5.21454 | failed:   60
batch:      35630 | loss: 5.07221 | failed:   60
batch:      35640 | loss: 5.17902 | failed:   60
batch:      35650 | loss: 5.08554 | failed:   60
batch:      35660 | loss: 5.15739 | failed:   60
batch:      35670 | loss: 5.18262 | failed:   60
batch:      35680 | loss: 5.27399 | failed:   60
batch:      35690 | loss: 5.21438 | failed:   60
batch:      35700 | loss: 5.14936 | failed:   60
batch:      35710 | loss: 5.08214 | failed:   60
batch:      35720 | loss: 5.18743 | failed:   60
batch:      35730 | loss: 5.20699 | failed:   60
batch:      35740 | loss: 5.23470 | failed:   60
batch:      35750 | loss: 4.99085 | failed:   60
batch:      35760 | loss: 5.17646 | failed:   60
batch:      35770 | loss: 5.16995 | failed:   60
batch:      35780 | loss: 5.09866 | failed:   60
batch:      35790 | loss: 5.17122 | failed:   60
batch:      35800 | loss: 5.03646 | failed:   60
batch:      35810 | loss: 5.14543 | failed:   60
batch:      35820 | loss: 5.10134 | failed:   60
batch:      35830 | loss: 5.16585 | failed:   60
batch:      35840 | loss: 5.24683 | failed:   60
batch:      35850 | loss: 5.19892 | failed:   60
batch:      35860 | loss: 5.20403 | failed:   60
batch:      35870 | loss: 5.21648 | failed:   60
batch:      35880 | loss: 5.17573 | failed:   60
batch:      35890 | loss: 5.22333 | failed:   60
batch:      35900 | loss: 5.19234 | failed:   60
batch:      35910 | loss: 5.12396 | failed:   60
batch:      35920 | loss: 5.17193 | failed:   60
batch:      35930 | loss: 5.20476 | failed:   60
batch:      35940 | loss: 5.16392 | failed:   60
batch:      35950 | loss: 5.19359 | failed:   60
batch:      35960 | loss: 5.17297 | failed:   60
batch:      35970 | loss: 5.15855 | failed:   60
batch:      35980 | loss: 5.21537 | failed:   60
batch:      35990 | loss: 5.17889 | failed:   60
batch:      36000 | loss: 5.14351 | failed:   60
batch:      36010 | loss: 5.19294 | failed:   60
batch:      36020 | loss: 5.16546 | failed:   60
batch:      36030 | loss: 5.15010 | failed:   60
batch:      36040 | loss: 5.26548 | failed:   60
batch:      36050 | loss: 5.13534 | failed:   60
batch:      36060 | loss: 5.10076 | failed:   60
batch:      36070 | loss: 5.05386 | failed:   60
batch:      36080 | loss: 5.10702 | failed:   60
batch:      36090 | loss: 5.07559 | failed:   60
batch:      36100 | loss: 5.16379 | failed:   60
batch:      36110 | loss: 5.12130 | failed:   60
batch:      36120 | loss: 5.28673 | failed:   60
batch:      36130 | loss: 5.13270 | failed:   60
batch:      36140 | loss: 5.21556 | failed:   60
batch:      36150 | loss: 4.91262 | failed:   60
batch:      36160 | loss: 5.24478 | failed:   60
batch:      36170 | loss: 5.24272 | failed:   60
batch:      36180 | loss: 5.23148 | failed:   60
batch:      36190 | loss: 5.29342 | failed:   60
batch:      36200 | loss: 5.20806 | failed:   60
batch:      36210 | loss: 5.20063 | failed:   60
batch:      36220 | loss: 5.26740 | failed:   60
batch:      36230 | loss: 5.07425 | failed:   60
batch:      36240 | loss: 5.27887 | failed:   60
batch:      36250 | loss: 5.26218 | failed:   60
batch:      36260 | loss: 5.22212 | failed:   60
batch:      36270 | loss: 5.19120 | failed:   60
batch:      36280 | loss: 5.19808 | failed:   60
batch:      36290 | loss: 5.25708 | failed:   60
batch:      36300 | loss: 5.06930 | failed:   60
batch:      36310 | loss: 5.08552 | failed:   60
batch:      36320 | loss: 5.10998 | failed:   60
batch:      36330 | loss: 5.11752 | failed:   60
batch:      36340 | loss: 5.24386 | failed:   60
batch:      36350 | loss: 5.14344 | failed:   60
batch:      36360 | loss: 5.06547 | failed:   60
batch:      36370 | loss: 5.13976 | failed:   60
batch:      36380 | loss: 5.10483 | failed:   60
batch:      36390 | loss: 5.12161 | failed:   60
batch:      36400 | loss: 5.15953 | failed:   60
batch:      36410 | loss: 5.19264 | failed:   60
batch:      36420 | loss: 4.99941 | failed:   60
batch:      36430 | loss: 5.02008 | failed:   60
batch:      36440 | loss: 5.35855 | failed:   60
batch:      36450 | loss: 5.24661 | failed:   60
batch:      36460 | loss: 5.19837 | failed:   60
batch:      36470 | loss: 5.22599 | failed:   60
batch:      36480 | loss: 5.09115 | failed:   60
batch:      36490 | loss: 5.17855 | failed:   60
batch:      36500 | loss: 5.11816 | failed:   60
batch:      36510 | loss: 5.09772 | failed:   60
batch:      36520 | loss: 5.27713 | failed:   60
batch:      36530 | loss: 5.09326 | failed:   60
batch:      36540 | loss: 5.03313 | failed:   60
batch:      36550 | loss: 5.23964 | failed:   60
batch:      36560 | loss: 5.05877 | failed:   60
batch:      36570 | loss: 5.16607 | failed:   60
batch:      36580 | loss: 5.06040 | failed:   60
batch:      36590 | loss: 5.22152 | failed:   60
batch:      36600 | loss: 5.26819 | failed:   60
batch:      36610 | loss: 5.27762 | failed:   60
batch:      36620 | loss: 5.17402 | failed:   60
batch:      36630 | loss: 5.16757 | failed:   60
batch:      36640 | loss: 5.29609 | failed:   60
batch:      36650 | loss: 5.14739 | failed:   60
batch:      36660 | loss: 5.14731 | failed:   60
batch:      36670 | loss: 5.13111 | failed:   60
batch:      36680 | loss: 5.17563 | failed:   60
batch:      36690 | loss: 5.15466 | failed:   60
batch:      36700 | loss: 5.24443 | failed:   60
batch:      36710 | loss: 5.16347 | failed:   60
batch:      36720 | loss: 5.22136 | failed:   60
batch:      36730 | loss: 5.14570 | failed:   60
batch:      36740 | loss: 5.15685 | failed:   60
batch:      36750 | loss: 5.20522 | failed:   60
batch:      36760 | loss: 5.10990 | failed:   60
batch:      36770 | loss: 5.13758 | failed:   60
batch:      36780 | loss: 5.14020 | failed:   60
batch:      36790 | loss: 5.17920 | failed:   60
batch:      36800 | loss: 5.19324 | failed:   60
batch:      36810 | loss: 5.27426 | failed:   60
batch:      36820 | loss: 5.17931 | failed:   60
batch:      36830 | loss: 5.15872 | failed:   60
batch:      36840 | loss: 5.10890 | failed:   60
batch:      36850 | loss: 5.17544 | failed:   60
batch:      36860 | loss: 5.14455 | failed:   60
batch:      36870 | loss: 5.08394 | failed:   60
batch:      36880 | loss: 5.09441 | failed:   60
batch:      36890 | loss: 5.14606 | failed:   60
batch:      36900 | loss: 4.88732 | failed:   60
batch:      36910 | loss: 5.13413 | failed:   60
batch:      36920 | loss: 4.98004 | failed:   60
batch:      36930 | loss: 4.98643 | failed:   60
batch:      36940 | loss: 5.29769 | failed:   60
batch:      36950 | loss: 5.04935 | failed:   60
batch:      36960 | loss: 5.18423 | failed:   60
batch:      36970 | loss: 4.91835 | failed:   60
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      36980 | loss: 5.22902 | failed:   63
batch:      36990 | loss: 5.21411 | failed:   63
batch:      37000 | loss: 5.15415 | failed:   63
batch:      37010 | loss: 5.15786 | failed:   63
batch:      37020 | loss: 5.06355 | failed:   63
batch:      37030 | loss: 5.08360 | failed:   63
batch:      37040 | loss: 5.06481 | failed:   63
batch:      37050 | loss: 5.03913 | failed:   63
batch:      37060 | loss: 5.25128 | failed:   63
batch:      37070 | loss: 5.25571 | failed:   63
batch:      37080 | loss: 5.16239 | failed:   63
batch:      37090 | loss: 5.06478 | failed:   63
batch:      37100 | loss: 4.99725 | failed:   63
batch:      37110 | loss: 5.06919 | failed:   63
batch:      37120 | loss: 5.16297 | failed:   63
batch:      37130 | loss: 5.13925 | failed:   63
batch:      37140 | loss: 5.19318 | failed:   63
batch:      37150 | loss: 5.18688 | failed:   63
batch:      37160 | loss: 5.33073 | failed:   63
batch:      37170 | loss: 5.15492 | failed:   63
batch:      37180 | loss: 5.24751 | failed:   63
batch:      37190 | loss: 5.23387 | failed:   63
batch:      37200 | loss: 5.20947 | failed:   63
batch:      37210 | loss: 5.20265 | failed:   63
batch:      37220 | loss: 5.19427 | failed:   63
batch:      37230 | loss: 5.15032 | failed:   63
batch:      37240 | loss: 5.17923 | failed:   63
batch:      37250 | loss: 5.23847 | failed:   63
batch:      37260 | loss: 5.25155 | failed:   63
batch:      37270 | loss: 5.09699 | failed:   63
batch:      37280 | loss: 5.07863 | failed:   63
batch:      37290 | loss: 5.09903 | failed:   63
batch:      37300 | loss: 5.18474 | failed:   63
batch:      37310 | loss: 5.20928 | failed:   63
batch:      37320 | loss: 5.15639 | failed:   63
batch:      37330 | loss: 5.20807 | failed:   63
batch:      37340 | loss: 5.19264 | failed:   63
batch:      37350 | loss: 5.18905 | failed:   63
batch:      37360 | loss: 5.09457 | failed:   63
batch:      37370 | loss: 5.08513 | failed:   63
batch:      37380 | loss: 5.18961 | failed:   63
batch:      37390 | loss: 4.96618 | failed:   63
batch:      37400 | loss: 5.15686 | failed:   63
batch:      37410 | loss: 5.12142 | failed:   63
batch:      37420 | loss: 5.13828 | failed:   63
batch:      37430 | loss: 5.00647 | failed:   63
batch:      37440 | loss: 5.23978 | failed:   63
batch:      37450 | loss: 5.13971 | failed:   63
batch:      37460 | loss: 5.29292 | failed:   63
batch:      37470 | loss: 5.18865 | failed:   63
batch:      37480 | loss: 5.03200 | failed:   63
batch:      37490 | loss: 5.16990 | failed:   63
batch:      37500 | loss: 5.19087 | failed:   63
batch:      37510 | loss: 5.18497 | failed:   63
batch:      37520 | loss: 5.31514 | failed:   63
batch:      37530 | loss: 5.14547 | failed:   63
batch:      37540 | loss: 5.23426 | failed:   63
batch:      37550 | loss: 5.14869 | failed:   63
batch:      37560 | loss: 5.13684 | failed:   63
batch:      37570 | loss: 5.12684 | failed:   63
batch:      37580 | loss: 5.23033 | failed:   63
batch:      37590 | loss: 5.18329 | failed:   63
batch:      37600 | loss: 5.16917 | failed:   63
batch:      37610 | loss: 5.29413 | failed:   63
batch:      37620 | loss: 5.19573 | failed:   63
batch:      37630 | loss: 5.09986 | failed:   63
batch:      37640 | loss: 5.18766 | failed:   63
batch:      37650 | loss: 5.00544 | failed:   63
batch:      37660 | loss: 5.22067 | failed:   63
batch:      37670 | loss: 5.11083 | failed:   63
batch:      37680 | loss: 5.19863 | failed:   63
batch:      37690 | loss: 5.12817 | failed:   63
batch:      37700 | loss: 5.17668 | failed:   63
batch:      37710 | loss: 5.18479 | failed:   63
batch:      37720 | loss: 5.12933 | failed:   63
batch:      37730 | loss: 5.19502 | failed:   63
batch:      37740 | loss: 5.20013 | failed:   63
batch:      37750 | loss: 5.19657 | failed:   63
batch:      37760 | loss: 5.12051 | failed:   63
batch:      37770 | loss: 5.14401 | failed:   63
batch:      37780 | loss: 5.22585 | failed:   63
batch:      37790 | loss: 5.19930 | failed:   63
batch:      37800 | loss: 4.99973 | failed:   63
batch:      37810 | loss: 5.05281 | failed:   63
batch:      37820 | loss: 5.11807 | failed:   63
batch:      37830 | loss: 5.10121 | failed:   63
batch:      37840 | loss: 5.02194 | failed:   63
batch:      37850 | loss: 5.30089 | failed:   63
batch:      37860 | loss: 4.80598 | failed:   63
batch:      37870 | loss: 4.99751 | failed:   63
batch:      37880 | loss: 5.22203 | failed:   63
batch:      37890 | loss: 5.16675 | failed:   63
batch:      37900 | loss: 5.22335 | failed:   63
batch:      37910 | loss: 5.17310 | failed:   63
batch:      37920 | loss: 5.03456 | failed:   63
batch:      37930 | loss: 5.26334 | failed:   63
batch:      37940 | loss: 5.23779 | failed:   63
batch:      37950 | loss: 5.27161 | failed:   63
batch:      37960 | loss: 5.23562 | failed:   63
batch:      37970 | loss: 5.17739 | failed:   63
batch:      37980 | loss: 5.18335 | failed:   63
batch:      37990 | loss: 5.18578 | failed:   63
batch:      38000 | loss: 5.19151 | failed:   63
batch:      38010 | loss: 5.09518 | failed:   63
batch:      38020 | loss: 5.13123 | failed:   63
batch:      38030 | loss: 5.13460 | failed:   63
batch:      38040 | loss: 5.14332 | failed:   63
batch:      38050 | loss: 5.12667 | failed:   63
batch:      38060 | loss: 5.07465 | failed:   63
batch:      38070 | loss: 5.20560 | failed:   63
batch:      38080 | loss: 5.11540 | failed:   63
batch:      38090 | loss: 5.19946 | failed:   63
batch:      38100 | loss: 5.18457 | failed:   63
batch:      38110 | loss: 5.38548 | failed:   63
batch:      38120 | loss: 5.15547 | failed:   63
batch:      38130 | loss: 5.23714 | failed:   63
batch:      38140 | loss: 5.16127 | failed:   63
batch:      38150 | loss: 5.14776 | failed:   63
batch:      38160 | loss: 5.16641 | failed:   63
batch:      38170 | loss: 5.21232 | failed:   63
batch:      38180 | loss: 5.28512 | failed:   63
batch:      38190 | loss: 5.21526 | failed:   63
batch:      38200 | loss: 5.13637 | failed:   63
batch:      38210 | loss: 5.21133 | failed:   63
batch:      38220 | loss: 5.16279 | failed:   63
batch:      38230 | loss: 5.13144 | failed:   63
batch:      38240 | loss: 4.93406 | failed:   63
batch:      38250 | loss: 5.13599 | failed:   63
batch:      38260 | loss: 5.02469 | failed:   63
batch:      38270 | loss: 5.04055 | failed:   63
batch:      38280 | loss: 5.21163 | failed:   63
batch:      38290 | loss: 5.21546 | failed:   63
batch:      38300 | loss: 5.22564 | failed:   63
batch:      38310 | loss: 5.10204 | failed:   63
batch:      38320 | loss: 5.33184 | failed:   63
batch:      38330 | loss: 5.27853 | failed:   63
batch:      38340 | loss: 5.24673 | failed:   63
batch:      38350 | loss: 5.23554 | failed:   63
batch:      38360 | loss: 5.20969 | failed:   63
batch:      38370 | loss: 5.19733 | failed:   63
batch:      38380 | loss: 4.94524 | failed:   63
batch:      38390 | loss: 5.12127 | failed:   63
batch:      38400 | loss: 5.21480 | failed:   63
batch:      38410 | loss: 5.21592 | failed:   63
batch:      38420 | loss: 5.16706 | failed:   63
batch:      38430 | loss: 5.21317 | failed:   63
batch:      38440 | loss: 5.22510 | failed:   63
batch:      38450 | loss: 5.10132 | failed:   63
batch:      38460 | loss: 5.23181 | failed:   63
batch:      38470 | loss: 5.23400 | failed:   63
batch:      38480 | loss: 5.22938 | failed:   63
batch:      38490 | loss: 5.18095 | failed:   63
batch:      38500 | loss: 5.12420 | failed:   63
batch:      38510 | loss: 5.08109 | failed:   63
batch:      38520 | loss: 5.25763 | failed:   63
batch:      38530 | loss: 5.20937 | failed:   63
batch:      38540 | loss: 5.17661 | failed:   63
batch:      38550 | loss: 5.02114 | failed:   63
batch:      38560 | loss: 5.14935 | failed:   63
batch:      38570 | loss: 5.17378 | failed:   63
batch:      38580 | loss: 5.15003 | failed:   63
batch:      38590 | loss: 5.12254 | failed:   63
batch:      38600 | loss: 5.14131 | failed:   63
batch:      38610 | loss: 5.06456 | failed:   63
batch:      38620 | loss: 5.23809 | failed:   63
batch:      38630 | loss: 5.19262 | failed:   63
batch:      38640 | loss: 5.22337 | failed:   63
batch:      38650 | loss: 5.14477 | failed:   63
batch:      38660 | loss: 5.25089 | failed:   63
batch:      38670 | loss: 5.14594 | failed:   63
batch:      38680 | loss: 4.80052 | failed:   63
batch:      38690 | loss: 5.24317 | failed:   63
batch:      38700 | loss: 5.04803 | failed:   63
batch:      38710 | loss: 5.02846 | failed:   63
batch:      38720 | loss: 5.15583 | failed:   63
batch:      38730 | loss: 5.29569 | failed:   63
batch:      38740 | loss: 5.10116 | failed:   63
batch:      38750 | loss: 5.17581 | failed:   63
batch:      38760 | loss: 5.20835 | failed:   63
batch:      38770 | loss: 5.04168 | failed:   63
batch:      38780 | loss: 5.07050 | failed:   63
batch:      38790 | loss: 5.09685 | failed:   63
batch:      38800 | loss: 5.23534 | failed:   63
batch:      38810 | loss: 5.22060 | failed:   63
batch:      38820 | loss: 5.09376 | failed:   63
batch:      38830 | loss: 5.14807 | failed:   63
batch:      38840 | loss: 5.10042 | failed:   63
batch:      38850 | loss: 5.12091 | failed:   63
batch:      38860 | loss: 5.17622 | failed:   63
batch:      38870 | loss: 4.91625 | failed:   63
batch:      38880 | loss: 5.17983 | failed:   63
batch:      38890 | loss: 4.96980 | failed:   63
batch:      38900 | loss: 5.05726 | failed:   63
batch:      38910 | loss: 5.20224 | failed:   63
batch:      38920 | loss: 5.17303 | failed:   63
batch:      38930 | loss: 5.15929 | failed:   63
batch:      38940 | loss: 5.22063 | failed:   63
batch:      38950 | loss: 5.24137 | failed:   63
batch:      38960 | loss: 5.14830 | failed:   63
batch:      38970 | loss: 5.18673 | failed:   63
batch:      38980 | loss: 5.22240 | failed:   63
batch:      38990 | loss: 5.08675 | failed:   63
batch:      39000 | loss: 5.10766 | failed:   63
batch:      39010 | loss: 5.17289 | failed:   63
batch:      39020 | loss: 5.15193 | failed:   63
batch:      39030 | loss: 5.15539 | failed:   63
batch:      39040 | loss: 5.22350 | failed:   63
batch:      39050 | loss: 5.26109 | failed:   63
batch:      39060 | loss: 5.14646 | failed:   63
batch:      39070 | loss: 5.20177 | failed:   63
batch:      39080 | loss: 5.11565 | failed:   63
batch:      39090 | loss: 5.21413 | failed:   63
batch:      39100 | loss: 5.23436 | failed:   63
batch:      39110 | loss: 5.06812 | failed:   63
batch:      39120 | loss: 5.16286 | failed:   63
batch:      39130 | loss: 5.11276 | failed:   63
batch:      39140 | loss: 5.13061 | failed:   63
batch:      39150 | loss: 5.17540 | failed:   63
batch:      39160 | loss: 5.08244 | failed:   63
batch:      39170 | loss: 5.17424 | failed:   63
batch:      39180 | loss: 5.14674 | failed:   63
batch:      39190 | loss: 5.29647 | failed:   63
batch:      39200 | loss: 5.10550 | failed:   63
batch:      39210 | loss: 5.15602 | failed:   63
batch:      39220 | loss: 5.05869 | failed:   63
batch:      39230 | loss: 5.09530 | failed:   63
batch:      39240 | loss: 5.10400 | failed:   63
batch:      39250 | loss: 5.19604 | failed:   63
batch:      39260 | loss: 5.17119 | failed:   63
batch:      39270 | loss: 5.14470 | failed:   63
batch:      39280 | loss: 5.18906 | failed:   63
batch:      39290 | loss: 5.20623 | failed:   63
batch:      39300 | loss: 5.24096 | failed:   63
batch:      39310 | loss: 5.20403 | failed:   63
batch:      39320 | loss: 5.16613 | failed:   63
batch:      39330 | loss: 5.18071 | failed:   63
batch:      39340 | loss: 5.11105 | failed:   63
batch:      39350 | loss: 5.11661 | failed:   63
batch:      39360 | loss: 5.10408 | failed:   63
batch:      39370 | loss: 5.22587 | failed:   63
batch:      39380 | loss: 5.17430 | failed:   63
batch:      39390 | loss: 5.20204 | failed:   63
batch:      39400 | loss: 4.83684 | failed:   63
batch:      39410 | loss: 5.20653 | failed:   63
batch:      39420 | loss: 5.20470 | failed:   63
batch:      39430 | loss: 5.23602 | failed:   63
batch:      39440 | loss: 5.26713 | failed:   63
batch:      39450 | loss: 4.98434 | failed:   63
batch:      39460 | loss: 4.98920 | failed:   63
batch:      39470 | loss: 4.94269 | failed:   63
batch:      39480 | loss: 4.82312 | failed:   63
batch:      39490 | loss: 4.48748 | failed:   63
batch:      39500 | loss: 4.58858 | failed:   63
batch:      39510 | loss: 4.50588 | failed:   63
batch:      39520 | loss: 5.20883 | failed:   63
batch:      39530 | loss: 5.20215 | failed:   63
batch:      39540 | loss: 5.28060 | failed:   63
batch:      39550 | loss: 5.23994 | failed:   63
batch:      39560 | loss: 5.14321 | failed:   63
batch:      39570 | loss: 5.11758 | failed:   63
batch:      39580 | loss: 5.09429 | failed:   63
batch:      39590 | loss: 5.14724 | failed:   63
batch:      39600 | loss: 5.26365 | failed:   63
batch:      39610 | loss: 5.21411 | failed:   63
batch:      39620 | loss: 5.25506 | failed:   63
batch:      39630 | loss: 5.21436 | failed:   63
batch:      39640 | loss: 5.13959 | failed:   63
batch:      39650 | loss: 5.22714 | failed:   63
batch:      39660 | loss: 5.06354 | failed:   63
batch:      39670 | loss: 5.18468 | failed:   63
batch:      39680 | loss: 4.94114 | failed:   63
batch:      39690 | loss: 5.18725 | failed:   63
batch:      39700 | loss: 5.16705 | failed:   63
batch:      39710 | loss: 5.13660 | failed:   63
batch:      39720 | loss: 5.19236 | failed:   63
batch:      39730 | loss: 5.23792 | failed:   63
batch:      39740 | loss: 5.20811 | failed:   63
batch:      39750 | loss: 4.95239 | failed:   63
batch:      39760 | loss: 5.21691 | failed:   63
batch:      39770 | loss: 4.90269 | failed:   63
batch:      39780 | loss: 4.82654 | failed:   63
batch:      39790 | loss: 5.20369 | failed:   63
batch:      39800 | loss: 5.24279 | failed:   63
batch:      39810 | loss: 5.14059 | failed:   63
batch:      39820 | loss: 5.15505 | failed:   63
batch:      39830 | loss: 5.24784 | failed:   63
batch:      39840 | loss: 5.19620 | failed:   63
batch:      39850 | loss: 5.09183 | failed:   63
batch:      39860 | loss: 5.22772 | failed:   63
batch:      39870 | loss: 5.18433 | failed:   63
batch:      39880 | loss: 5.19265 | failed:   63
batch:      39890 | loss: 5.12120 | failed:   63
batch:      39900 | loss: 5.17902 | failed:   63
batch:      39910 | loss: 5.20260 | failed:   63
batch:      39920 | loss: 5.20792 | failed:   63
batch:      39930 | loss: 4.82163 | failed:   63
batch:      39940 | loss: 5.19230 | failed:   63
batch:      39950 | loss: 5.07498 | failed:   63
batch:      39960 | loss: 5.11555 | failed:   63
batch:      39970 | loss: 5.13427 | failed:   63
batch:      39980 | loss: 5.17552 | failed:   63
batch:      39990 | loss: 5.23545 | failed:   63
batch:      40000 | loss: 5.20621 | failed:   63
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      40010 | loss: 5.06792 | failed:   63
batch:      40020 | loss: 5.14469 | failed:   63
batch:      40030 | loss: 5.10523 | failed:   63
batch:      40040 | loss: 5.16548 | failed:   63
batch:      40050 | loss: 5.23309 | failed:   63
batch:      40060 | loss: 5.04712 | failed:   63
batch:      40070 | loss: 5.18810 | failed:   63
batch:      40080 | loss: 5.03647 | failed:   63
batch:      40090 | loss: 4.54632 | failed:   63
batch:      40100 | loss: 4.38558 | failed:   63
batch:      40110 | loss: 5.30502 | failed:   63
batch:      40120 | loss: 5.21272 | failed:   63
batch:      40130 | loss: 5.20031 | failed:   63
batch:      40140 | loss: 5.23888 | failed:   63
batch:      40150 | loss: 5.22691 | failed:   63
batch:      40160 | loss: 5.23104 | failed:   63
batch:      40170 | loss: 5.22610 | failed:   63
batch:      40180 | loss: 5.19043 | failed:   63
batch:      40190 | loss: 5.22367 | failed:   63
batch:      40200 | loss: 5.19613 | failed:   63
batch:      40210 | loss: 5.21435 | failed:   63
batch:      40220 | loss: 5.18079 | failed:   63
batch:      40230 | loss: 5.22475 | failed:   63
batch:      40240 | loss: 5.05456 | failed:   63
batch:      40250 | loss: 5.17583 | failed:   63
batch:      40260 | loss: 5.18924 | failed:   63
batch:      40270 | loss: 5.26582 | failed:   63
batch:      40280 | loss: 5.22623 | failed:   63
batch:      40290 | loss: 5.23788 | failed:   63
batch:      40300 | loss: 5.03659 | failed:   63
batch:      40310 | loss: 5.04277 | failed:   63
batch:      40320 | loss: 5.17939 | failed:   63
batch:      40330 | loss: 5.17177 | failed:   63
batch:      40340 | loss: 5.25565 | failed:   63
batch:      40350 | loss: 5.17893 | failed:   63
batch:      40360 | loss: 5.20749 | failed:   63
batch:      40370 | loss: 5.19286 | failed:   63
batch:      40380 | loss: 5.19943 | failed:   63
batch:      40390 | loss: 5.16827 | failed:   63
batch:      40400 | loss: 5.23525 | failed:   63
batch:      40410 | loss: 5.20529 | failed:   63
batch:      40420 | loss: 5.10198 | failed:   63
batch:      40430 | loss: 5.15145 | failed:   63
batch:      40440 | loss: 5.15170 | failed:   63
batch:      40450 | loss: 5.18455 | failed:   63
batch:      40460 | loss: 5.26316 | failed:   63
batch:      40470 | loss: 5.23860 | failed:   63
batch:      40480 | loss: 5.20599 | failed:   63
batch:      40490 | loss: 5.23110 | failed:   63
batch:      40500 | loss: 5.28333 | failed:   63
batch:      40510 | loss: 5.14525 | failed:   63
batch:      40520 | loss: 5.10166 | failed:   63
batch:      40530 | loss: 5.13524 | failed:   63
batch:      40540 | loss: 5.06571 | failed:   63
batch:      40550 | loss: 5.09376 | failed:   63
batch:      40560 | loss: 5.15364 | failed:   63
batch:      40570 | loss: 5.08901 | failed:   63
batch:      40580 | loss: 5.18283 | failed:   63
batch:      40590 | loss: 5.07131 | failed:   63
batch:      40600 | loss: 5.07364 | failed:   63
batch:      40610 | loss: 5.15331 | failed:   63
batch:      40620 | loss: 5.21637 | failed:   63
batch:      40630 | loss: 5.31707 | failed:   63
batch:      40640 | loss: 5.24020 | failed:   63
batch:      40650 | loss: 5.19357 | failed:   63
batch:      40660 | loss: 5.15326 | failed:   63
batch:      40670 | loss: 5.17716 | failed:   63
batch:      40680 | loss: 5.12797 | failed:   63
batch:      40690 | loss: 5.25314 | failed:   63
batch:      40700 | loss: 5.29140 | failed:   63
batch:      40710 | loss: 5.23521 | failed:   63
batch:      40720 | loss: 4.98994 | failed:   63
batch:      40730 | loss: 5.31624 | failed:   63
batch:      40740 | loss: 5.17747 | failed:   63
batch:      40750 | loss: 4.99484 | failed:   63
batch:      40760 | loss: 5.03990 | failed:   63
batch:      40770 | loss: 5.19827 | failed:   63
batch:      40780 | loss: 5.23086 | failed:   63
batch:      40790 | loss: 5.25791 | failed:   63
batch:      40800 | loss: 5.08085 | failed:   63
batch:      40810 | loss: 5.10886 | failed:   63
batch:      40820 | loss: 5.26822 | failed:   63
batch:      40830 | loss: 5.16181 | failed:   63
batch:      40840 | loss: 5.14506 | failed:   63
batch:      40850 | loss: 5.26681 | failed:   63
batch:      40860 | loss: 5.23724 | failed:   63
batch:      40870 | loss: 5.09258 | failed:   63
batch:      40880 | loss: 5.19690 | failed:   63
batch:      40890 | loss: 5.15711 | failed:   63
batch:      40900 | loss: 5.19583 | failed:   63
batch:      40910 | loss: 5.10247 | failed:   63
batch:      40920 | loss: 5.21473 | failed:   63
batch:      40930 | loss: 5.19300 | failed:   63
batch:      40940 | loss: 5.13511 | failed:   63
batch:      40950 | loss: 5.06165 | failed:   63
batch:      40960 | loss: 5.13642 | failed:   63
batch:      40970 | loss: 5.21724 | failed:   63
batch:      40980 | loss: 5.28184 | failed:   63
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      41000 | loss: 5.21460 | failed:   64
batch:      41010 | loss: 5.09715 | failed:   64
batch:      41020 | loss: 5.19691 | failed:   64
batch:      41030 | loss: 5.23775 | failed:   64
batch:      41040 | loss: 5.20932 | failed:   64
batch:      41050 | loss: 5.56071 | failed:   64
batch:      41060 | loss: 5.22253 | failed:   64
batch:      41070 | loss: 5.16053 | failed:   64
batch:      41080 | loss: 5.13407 | failed:   64
batch:      41090 | loss: 5.05777 | failed:   64
batch:      41100 | loss: 5.22694 | failed:   64
batch:      41110 | loss: 5.23505 | failed:   64
batch:      41120 | loss: 5.22907 | failed:   64
batch:      41130 | loss: 5.26569 | failed:   64
batch:      41140 | loss: 5.25133 | failed:   64
batch:      41150 | loss: 5.21927 | failed:   64
batch:      41160 | loss: 5.13134 | failed:   64
batch:      41170 | loss: 5.24845 | failed:   64
batch:      41180 | loss: 5.11144 | failed:   64
batch:      41190 | loss: 5.26271 | failed:   64
batch:      41200 | loss: 5.23660 | failed:   64
batch:      41210 | loss: 5.08771 | failed:   64
batch:      41220 | loss: 5.13562 | failed:   64
batch:      41230 | loss: 5.07823 | failed:   64
batch:      41240 | loss: 5.18517 | failed:   64
batch:      41250 | loss: 5.22282 | failed:   64
batch:      41260 | loss: 5.25877 | failed:   64
batch:      41270 | loss: 5.20114 | failed:   64
batch:      41280 | loss: 5.21732 | failed:   64
batch:      41290 | loss: 5.15583 | failed:   64
batch:      41300 | loss: 5.19755 | failed:   64
batch:      41310 | loss: 5.14236 | failed:   64
batch:      41320 | loss: 5.17831 | failed:   64
batch:      41330 | loss: 5.15522 | failed:   64
batch:      41340 | loss: 5.06070 | failed:   64
batch:      41350 | loss: 5.13990 | failed:   64
batch:      41360 | loss: 5.12439 | failed:   64
batch:      41370 | loss: 5.08503 | failed:   64
batch:      41380 | loss: 5.09477 | failed:   64
batch:      41390 | loss: 5.07315 | failed:   64
batch:      41400 | loss: 5.17247 | failed:   64
batch:      41410 | loss: 5.01441 | failed:   64
batch:      41420 | loss: 5.25937 | failed:   64
batch:      41430 | loss: 4.37858 | failed:   64
batch:      41440 | loss: 5.12843 | failed:   64
batch:      41450 | loss: 5.04426 | failed:   64
batch:      41460 | loss: 5.27389 | failed:   64
batch:      41470 | loss: 5.23157 | failed:   64
batch:      41480 | loss: 5.25055 | failed:   64
batch:      41490 | loss: 5.25438 | failed:   64
batch:      41500 | loss: 5.24788 | failed:   64
batch:      41510 | loss: 5.24290 | failed:   64
batch:      41520 | loss: 5.03396 | failed:   64
batch:      41530 | loss: 5.10213 | failed:   64
batch:      41540 | loss: 5.21511 | failed:   64
batch:      41550 | loss: 5.16780 | failed:   64
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      41560 | loss: 5.19800 | failed:   65
batch:      41570 | loss: 5.13085 | failed:   65
batch:      41580 | loss: 5.06132 | failed:   65
batch:      41590 | loss: 5.16203 | failed:   65
batch:      41600 | loss: 5.15433 | failed:   65
batch:      41610 | loss: 5.20300 | failed:   65
batch:      41620 | loss: 5.19599 | failed:   65
batch:      41630 | loss: 5.19710 | failed:   65
batch:      41640 | loss: 5.20226 | failed:   65
batch:      41650 | loss: 5.26647 | failed:   65
batch:      41660 | loss: 5.21731 | failed:   65
batch:      41670 | loss: 5.18307 | failed:   65
batch:      41680 | loss: 5.17674 | failed:   65
batch:      41690 | loss: 5.08027 | failed:   65
batch:      41700 | loss: 5.19062 | failed:   65
batch:      41710 | loss: 5.14276 | failed:   65
batch:      41720 | loss: 5.24296 | failed:   65
batch:      41730 | loss: 5.12482 | failed:   65
batch:      41740 | loss: 5.19672 | failed:   65
batch:      41750 | loss: 5.17548 | failed:   65
batch:      41760 | loss: 5.23109 | failed:   65
batch:      41770 | loss: 4.97687 | failed:   65
batch:      41780 | loss: 5.24262 | failed:   65
batch:      41790 | loss: 5.08143 | failed:   65
batch:      41800 | loss: 4.94577 | failed:   65
batch:      41810 | loss: 5.26798 | failed:   65
batch:      41820 | loss: 5.28112 | failed:   65
batch:      41830 | loss: 5.09731 | failed:   65
batch:      41840 | loss: 5.16811 | failed:   65
batch:      41850 | loss: 4.93710 | failed:   65
batch:      41860 | loss: 5.13464 | failed:   65
batch:      41870 | loss: 4.97916 | failed:   65
batch:      41880 | loss: 5.15812 | failed:   65
batch:      41890 | loss: 5.13509 | failed:   65
batch:      41900 | loss: 5.20364 | failed:   65
batch:      41910 | loss: 5.29001 | failed:   65
batch:      41920 | loss: 5.16466 | failed:   65
batch:      41930 | loss: 5.04182 | failed:   65
batch:      41940 | loss: 5.12952 | failed:   65
batch:      41950 | loss: 5.18255 | failed:   65
batch:      41960 | loss: 5.18027 | failed:   65
batch:      41970 | loss: 5.21179 | failed:   65
batch:      41980 | loss: 5.26418 | failed:   65
batch:      41990 | loss: 5.21976 | failed:   65
batch:      42000 | loss: 5.17866 | failed:   65
batch:      42010 | loss: 5.15574 | failed:   65
batch:      42020 | loss: 5.23541 | failed:   65
batch:      42030 | loss: 5.13866 | failed:   65
batch:      42040 | loss: 5.16986 | failed:   65
batch:      42050 | loss: 5.18322 | failed:   65
batch:      42060 | loss: 5.18270 | failed:   65
batch:      42070 | loss: 5.09281 | failed:   65
batch:      42080 | loss: 5.18941 | failed:   65
batch:      42090 | loss: 5.15050 | failed:   65
batch:      42100 | loss: 5.19312 | failed:   65
batch:      42110 | loss: 5.06322 | failed:   65
batch:      42120 | loss: 5.13378 | failed:   65
batch:      42130 | loss: 4.86170 | failed:   65
batch:      42140 | loss: 5.01167 | failed:   65
batch:      42150 | loss: 5.06868 | failed:   65
batch:      42160 | loss: 5.28537 | failed:   65
batch:      42170 | loss: 5.12384 | failed:   65
batch:      42180 | loss: 5.09579 | failed:   65
batch:      42190 | loss: 5.15693 | failed:   65
batch:      42200 | loss: 5.04383 | failed:   65
batch:      42210 | loss: 5.00839 | failed:   65
batch:      42220 | loss: 4.96719 | failed:   65
batch:      42230 | loss: 5.03563 | failed:   65
batch:      42240 | loss: 5.27413 | failed:   65
batch:      42250 | loss: 5.23484 | failed:   65
batch:      42260 | loss: 5.04198 | failed:   65
batch:      42270 | loss: 5.17586 | failed:   65
batch:      42280 | loss: 5.25195 | failed:   65
batch:      42290 | loss: 5.16562 | failed:   65
batch:      42300 | loss: 5.12501 | failed:   65
batch:      42310 | loss: 5.20326 | failed:   65
batch:      42320 | loss: 5.09890 | failed:   65
batch:      42330 | loss: 5.22143 | failed:   65
batch:      42340 | loss: 5.07233 | failed:   65
batch:      42350 | loss: 5.13083 | failed:   65
batch:      42360 | loss: 5.33744 | failed:   65
batch:      42370 | loss: 5.17969 | failed:   65
batch:      42380 | loss: 5.15064 | failed:   65
batch:      42390 | loss: 5.09135 | failed:   65
batch:      42400 | loss: 5.02597 | failed:   65
batch:      42410 | loss: 5.16595 | failed:   65
batch:      42420 | loss: 5.10433 | failed:   65
batch:      42430 | loss: 5.26185 | failed:   65
batch:      42440 | loss: 5.03513 | failed:   65
batch:      42450 | loss: 5.16644 | failed:   65
batch:      42460 | loss: 5.25673 | failed:   65
batch:      42470 | loss: 5.11623 | failed:   65
batch:      42480 | loss: 5.13864 | failed:   65
batch:      42490 | loss: 5.17891 | failed:   65
batch:      42500 | loss: 5.13001 | failed:   65
batch:      42510 | loss: 5.28954 | failed:   65
batch:      42520 | loss: 5.17409 | failed:   65
batch:      42530 | loss: 5.22222 | failed:   65
batch:      42540 | loss: 5.08999 | failed:   65
batch:      42550 | loss: 5.11442 | failed:   65
batch:      42560 | loss: 5.09390 | failed:   65
batch:      42570 | loss: 5.08591 | failed:   65
batch:      42580 | loss: 5.21671 | failed:   65
batch:      42590 | loss: 5.08143 | failed:   65
batch:      42600 | loss: 5.13872 | failed:   65
batch:      42610 | loss: 5.24337 | failed:   65
batch:      42620 | loss: 5.16030 | failed:   65
batch:      42630 | loss: 5.28848 | failed:   65
batch:      42640 | loss: 5.17598 | failed:   65
batch:      42650 | loss: 5.15327 | failed:   65
batch:      42660 | loss: 5.17163 | failed:   65
batch:      42670 | loss: 5.15086 | failed:   65
batch:      42680 | loss: 5.18722 | failed:   65
batch:      42690 | loss: 5.22205 | failed:   65
batch:      42700 | loss: 5.17917 | failed:   65
batch:      42710 | loss: 5.26304 | failed:   65
batch:      42720 | loss: 5.22295 | failed:   65
batch:      42730 | loss: 5.11633 | failed:   65
batch:      42740 | loss: 5.04330 | failed:   65
batch:      42750 | loss: 4.94311 | failed:   65
batch:      42760 | loss: 5.18941 | failed:   65
batch:      42770 | loss: 5.16820 | failed:   65
batch:      42780 | loss: 5.19640 | failed:   65
batch:      42790 | loss: 5.11173 | failed:   65
batch:      42800 | loss: 5.14902 | failed:   65
batch:      42810 | loss: 5.15483 | failed:   65
batch:      42820 | loss: 5.15946 | failed:   65
batch:      42830 | loss: 5.16617 | failed:   65
batch:      42840 | loss: 5.22066 | failed:   65
batch:      42850 | loss: 5.11178 | failed:   65
batch:      42860 | loss: 5.11947 | failed:   65
batch:      42870 | loss: 5.13527 | failed:   65
batch:      42880 | loss: 5.14910 | failed:   65
batch:      42890 | loss: 5.06854 | failed:   65
batch:      42900 | loss: 5.22806 | failed:   65
batch:      42910 | loss: 5.22242 | failed:   65
batch:      42920 | loss: 5.28529 | failed:   65
batch:      42930 | loss: 5.23162 | failed:   65
batch:      42940 | loss: 5.10355 | failed:   65
batch:      42950 | loss: 5.05304 | failed:   65
batch:      42960 | loss: 5.13012 | failed:   65
batch:      42970 | loss: 5.25664 | failed:   65
batch:      42980 | loss: 5.04605 | failed:   65
batch:      42990 | loss: 5.09073 | failed:   65
batch:      43000 | loss: 5.21426 | failed:   65
batch:      43010 | loss: 5.22073 | failed:   65
batch:      43020 | loss: 5.22931 | failed:   65
batch:      43030 | loss: 5.29015 | failed:   65
batch:      43040 | loss: 5.10196 | failed:   65
batch:      43050 | loss: 5.14338 | failed:   65
batch:      43060 | loss: 4.51235 | failed:   65
batch:      43070 | loss: 4.66512 | failed:   65
batch:      43080 | loss: 4.44745 | failed:   65
batch:      43090 | loss: 4.35520 | failed:   65
batch:      43100 | loss: 4.47512 | failed:   65
batch:      43110 | loss: 4.34413 | failed:   65
batch:      43120 | loss: 4.30890 | failed:   65
batch:      43130 | loss: 4.59850 | failed:   65
batch:      43140 | loss: 4.83169 | failed:   65
batch:      43150 | loss: 5.21341 | failed:   65
batch:      43160 | loss: 5.23283 | failed:   65
batch:      43170 | loss: 5.20705 | failed:   65
batch:      43180 | loss: 5.14854 | failed:   65
batch:      43190 | loss: 5.02368 | failed:   65
batch:      43200 | loss: 5.14171 | failed:   65
batch:      43210 | loss: 5.12718 | failed:   65
batch:      43220 | loss: 5.20809 | failed:   65
batch:      43230 | loss: 5.14386 | failed:   65
batch:      43240 | loss: 5.10247 | failed:   65
batch:      43250 | loss: 5.24260 | failed:   65
batch:      43260 | loss: 5.10442 | failed:   65
batch:      43270 | loss: 5.18686 | failed:   65
batch:      43280 | loss: 5.17903 | failed:   65
batch:      43290 | loss: 5.13218 | failed:   65
batch:      43300 | loss: 5.14446 | failed:   65
batch:      43310 | loss: 5.10115 | failed:   65
batch:      43320 | loss: 5.21420 | failed:   65
batch:      43330 | loss: 5.34382 | failed:   65
batch:      43340 | loss: 5.16094 | failed:   65
batch:      43350 | loss: 5.23589 | failed:   65
batch:      43360 | loss: 5.24695 | failed:   65
batch:      43370 | loss: 5.13663 | failed:   65
batch:      43380 | loss: 5.14775 | failed:   65
batch:      43390 | loss: 5.14626 | failed:   65
batch:      43400 | loss: 5.06310 | failed:   65
batch:      43410 | loss: 5.22928 | failed:   65
batch:      43420 | loss: 5.21052 | failed:   65
batch:      43430 | loss: 5.21779 | failed:   65
batch:      43440 | loss: 4.94821 | failed:   65
batch:      43450 | loss: 5.24025 | failed:   65
batch:      43460 | loss: 5.13645 | failed:   65
batch:      43470 | loss: 5.14142 | failed:   65
batch:      43480 | loss: 5.29272 | failed:   65
batch:      43490 | loss: 5.20850 | failed:   65
batch:      43500 | loss: 5.16192 | failed:   65
batch:      43510 | loss: 5.14162 | failed:   65
batch:      43520 | loss: 5.13040 | failed:   65
batch:      43530 | loss: 5.21859 | failed:   65
batch:      43540 | loss: 5.17596 | failed:   65
batch:      43550 | loss: 4.99530 | failed:   65
batch:      43560 | loss: 5.21421 | failed:   65
batch:      43570 | loss: 5.17774 | failed:   65
batch:      43580 | loss: 5.11105 | failed:   65
batch:      43590 | loss: 5.26529 | failed:   65
batch:      43600 | loss: 4.99714 | failed:   65
batch:      43610 | loss: 5.24712 | failed:   65
batch:      43620 | loss: 5.06794 | failed:   65
batch:      43630 | loss: 5.13054 | failed:   65
batch:      43640 | loss: 5.25917 | failed:   65
batch:      43650 | loss: 5.14931 | failed:   65
batch:      43660 | loss: 5.19450 | failed:   65
batch:      43670 | loss: 5.22541 | failed:   65
batch:      43680 | loss: 5.09155 | failed:   65
batch:      43690 | loss: 5.17671 | failed:   65
batch:      43700 | loss: 5.15805 | failed:   65
batch:      43710 | loss: 5.13651 | failed:   65
batch:      43720 | loss: 5.15230 | failed:   65
batch:      43730 | loss: 5.23928 | failed:   65
batch:      43740 | loss: 5.19388 | failed:   65
batch:      43750 | loss: 5.16464 | failed:   65
batch:      43760 | loss: 5.16269 | failed:   65
batch:      43770 | loss: 5.11681 | failed:   65
batch:      43780 | loss: 5.07538 | failed:   65
batch:      43790 | loss: 5.16367 | failed:   65
batch:      43800 | loss: 5.14499 | failed:   65
batch:      43810 | loss: 5.07573 | failed:   65
batch:      43820 | loss: 5.25649 | failed:   65
batch:      43830 | loss: 5.14729 | failed:   65
batch:      43840 | loss: 5.27967 | failed:   65
batch:      43850 | loss: 5.22116 | failed:   65
batch:      43860 | loss: 5.01061 | failed:   65
batch:      43870 | loss: 5.11212 | failed:   65
batch:      43880 | loss: 5.16000 | failed:   65
batch:      43890 | loss: 5.23549 | failed:   65
batch:      43900 | loss: 5.22738 | failed:   65
batch:      43910 | loss: 5.04457 | failed:   65
batch:      43920 | loss: 5.21037 | failed:   65
batch:      43930 | loss: 5.21120 | failed:   65
batch:      43940 | loss: 5.14516 | failed:   65
batch:      43950 | loss: 5.22949 | failed:   65
batch:      43960 | loss: 5.17632 | failed:   65
batch:      43970 | loss: 5.11926 | failed:   65
batch:      43980 | loss: 5.30620 | failed:   65
batch:      43990 | loss: 5.13081 | failed:   65
batch:      44000 | loss: 5.14378 | failed:   65
batch:      44010 | loss: 5.12113 | failed:   65
batch:      44020 | loss: 5.11442 | failed:   65
batch:      44030 | loss: 5.21648 | failed:   65
batch:      44040 | loss: 5.19522 | failed:   65
batch:      44050 | loss: 5.14969 | failed:   65
batch:      44060 | loss: 5.20210 | failed:   65
batch:      44070 | loss: 5.21353 | failed:   65
batch:      44080 | loss: 5.16099 | failed:   65
batch:      44090 | loss: 5.16059 | failed:   65
batch:      44100 | loss: 5.20674 | failed:   65
batch:      44110 | loss: 5.19536 | failed:   65
batch:      44120 | loss: 5.22310 | failed:   65
batch:      44130 | loss: 5.16024 | failed:   65
batch:      44140 | loss: 5.04195 | failed:   65
batch:      44150 | loss: 5.16750 | failed:   65
batch:      44160 | loss: 5.24889 | failed:   65
batch:      44170 | loss: 5.22101 | failed:   65
batch:      44180 | loss: 5.19675 | failed:   65
batch:      44190 | loss: 5.19688 | failed:   65
batch:      44200 | loss: 5.19319 | failed:   65
batch:      44210 | loss: 5.16350 | failed:   65
batch:      44220 | loss: 5.17933 | failed:   65
batch:      44230 | loss: 5.22464 | failed:   65
batch:      44240 | loss: 5.18120 | failed:   65
batch:      44250 | loss: 5.27616 | failed:   65
batch:      44260 | loss: 5.16066 | failed:   65
batch:      44270 | loss: 5.17098 | failed:   65
batch:      44280 | loss: 5.08108 | failed:   65
batch:      44290 | loss: 5.08246 | failed:   65
batch:      44300 | loss: 5.10737 | failed:   65
batch:      44310 | loss: 5.01812 | failed:   65
batch:      44320 | loss: 4.96420 | failed:   65
batch:      44330 | loss: 5.22610 | failed:   65
batch:      44340 | loss: 5.25093 | failed:   65
batch:      44350 | loss: 5.23701 | failed:   65
batch:      44360 | loss: 5.23937 | failed:   65
batch:      44370 | loss: 5.16215 | failed:   65
batch:      44380 | loss: 5.09443 | failed:   65
batch:      44390 | loss: 5.21879 | failed:   65
batch:      44400 | loss: 5.06468 | failed:   65
batch:      44410 | loss: 5.19021 | failed:   65
batch:      44420 | loss: 5.18117 | failed:   65
batch:      44430 | loss: 5.06372 | failed:   65
batch:      44440 | loss: 5.16365 | failed:   65
batch:      44450 | loss: 5.06020 | failed:   65
batch:      44460 | loss: 4.98537 | failed:   65
batch:      44470 | loss: 4.95900 | failed:   65
batch:      44480 | loss: 4.86507 | failed:   65
batch:      44490 | loss: 5.13978 | failed:   65
batch:      44500 | loss: 5.17833 | failed:   65
batch:      44510 | loss: 4.90240 | failed:   65
batch:      44520 | loss: 5.15357 | failed:   65
batch:      44530 | loss: 5.22037 | failed:   65
batch:      44540 | loss: 5.20037 | failed:   65
batch:      44550 | loss: 5.20861 | failed:   65
batch:      44560 | loss: 5.07420 | failed:   65
batch:      44570 | loss: 5.02991 | failed:   65
batch:      44580 | loss: 5.04657 | failed:   65
batch:      44590 | loss: 5.24358 | failed:   65
batch:      44600 | loss: 5.15676 | failed:   65
batch:      44610 | loss: 5.03153 | failed:   65
batch:      44620 | loss: 5.12235 | failed:   65
batch:      44630 | loss: 5.07311 | failed:   65
batch:      44640 | loss: 5.12537 | failed:   65
batch:      44650 | loss: 5.17351 | failed:   65
batch:      44660 | loss: 5.19596 | failed:   65
batch:      44670 | loss: 5.06634 | failed:   65
batch:      44680 | loss: 5.20523 | failed:   65
batch:      44690 | loss: 5.22476 | failed:   65
batch:      44700 | loss: 5.13838 | failed:   65
batch:      44710 | loss: 5.06895 | failed:   65
batch:      44720 | loss: 5.11307 | failed:   65
batch:      44730 | loss: 5.09391 | failed:   65
batch:      44740 | loss: 5.21786 | failed:   65
batch:      44750 | loss: 5.19598 | failed:   65
batch:      44760 | loss: 5.12824 | failed:   65
batch:      44770 | loss: 5.08170 | failed:   65
batch:      44780 | loss: 5.14806 | failed:   65
batch:      44790 | loss: 5.13194 | failed:   65
batch:      44800 | loss: 5.13874 | failed:   65
batch:      44810 | loss: 4.99353 | failed:   65
batch:      44820 | loss: 5.17099 | failed:   65
batch:      44830 | loss: 5.25770 | failed:   65
batch:      44840 | loss: 5.14970 | failed:   65
batch:      44850 | loss: 5.08795 | failed:   65
batch:      44860 | loss: 5.00056 | failed:   65
batch:      44870 | loss: 5.19340 | failed:   65
batch:      44880 | loss: 5.06041 | failed:   65
batch:      44890 | loss: 5.08229 | failed:   65
batch:      44900 | loss: 5.31056 | failed:   65
batch:      44910 | loss: 5.24401 | failed:   65
batch:      44920 | loss: 5.14910 | failed:   65
batch:      44930 | loss: 4.99301 | failed:   65
batch:      44940 | loss: 5.20226 | failed:   65
batch:      44950 | loss: 5.22156 | failed:   65
batch:      44960 | loss: 5.08937 | failed:   65
batch:      44970 | loss: 5.18388 | failed:   65
batch:      44980 | loss: 5.00276 | failed:   65
batch:      44990 | loss: 5.19542 | failed:   65
batch:      45000 | loss: 5.20113 | failed:   65
batch:      45010 | loss: 5.18998 | failed:   65
batch:      45020 | loss: 5.23893 | failed:   65
batch:      45030 | loss: 5.02763 | failed:   65
batch:      45040 | loss: 5.22437 | failed:   65
batch:      45050 | loss: 5.22401 | failed:   65
batch:      45060 | loss: 5.15459 | failed:   65
batch:      45070 | loss: 5.11485 | failed:   65
batch:      45080 | loss: 5.23254 | failed:   65
batch:      45090 | loss: 5.14435 | failed:   65
batch:      45100 | loss: 5.09048 | failed:   65
batch:      45110 | loss: 5.11016 | failed:   65
batch:      45120 | loss: 5.08888 | failed:   65
batch:      45130 | loss: 4.99262 | failed:   65
batch:      45140 | loss: 5.18860 | failed:   65
batch:      45150 | loss: 5.08804 | failed:   65
batch:      45160 | loss: 5.11828 | failed:   65
batch:      45170 | loss: 5.07312 | failed:   65
batch:      45180 | loss: 4.89698 | failed:   65
batch:      45190 | loss: 5.12841 | failed:   65
batch:      45200 | loss: 5.03666 | failed:   65
batch:      45210 | loss: 5.10280 | failed:   65
batch:      45220 | loss: 5.12364 | failed:   65
batch:      45230 | loss: 5.22542 | failed:   65
batch:      45240 | loss: 5.00060 | failed:   65
batch:      45250 | loss: 5.22093 | failed:   65
batch:      45260 | loss: 5.17458 | failed:   65
batch:      45270 | loss: 5.15563 | failed:   65
batch:      45280 | loss: 5.20792 | failed:   65
batch:      45290 | loss: 5.10311 | failed:   65
batch:      45300 | loss: 5.07811 | failed:   65
batch:      45310 | loss: 5.18241 | failed:   65
batch:      45320 | loss: 5.21077 | failed:   65
batch:      45330 | loss: 5.14807 | failed:   65
batch:      45340 | loss: 5.21349 | failed:   65
batch:      45350 | loss: 5.19865 | failed:   65
batch:      45360 | loss: 5.29275 | failed:   65
batch:      45370 | loss: 5.18319 | failed:   65
batch:      45380 | loss: 5.01609 | failed:   65
batch:      45390 | loss: 5.08533 | failed:   65
batch:      45400 | loss: 5.29639 | failed:   65
batch:      45410 | loss: 4.98020 | failed:   65
batch:      45420 | loss: 5.22490 | failed:   65
batch:      45430 | loss: 5.12825 | failed:   65
batch:      45440 | loss: 5.13945 | failed:   65
batch:      45450 | loss: 5.09228 | failed:   65
batch:      45460 | loss: 5.17318 | failed:   65
batch:      45470 | loss: 5.16922 | failed:   65
batch:      45480 | loss: 5.22343 | failed:   65
batch:      45490 | loss: 5.19491 | failed:   65
batch:      45500 | loss: 5.15252 | failed:   65
batch:      45510 | loss: 5.23005 | failed:   65
batch:      45520 | loss: 5.21629 | failed:   65
batch:      45530 | loss: 5.11840 | failed:   65
batch:      45540 | loss: 5.11243 | failed:   65
batch:      45550 | loss: 4.96865 | failed:   65
batch:      45560 | loss: 5.04073 | failed:   65
batch:      45570 | loss: 5.18929 | failed:   65
batch:      45580 | loss: 5.17688 | failed:   65
batch:      45590 | loss: 5.19864 | failed:   65
batch:      45600 | loss: 4.97461 | failed:   65
batch:      45610 | loss: 5.16097 | failed:   65
batch:      45620 | loss: 5.13019 | failed:   65
batch:      45630 | loss: 5.06736 | failed:   65
batch:      45640 | loss: 5.11142 | failed:   65
batch:      45650 | loss: 5.04494 | failed:   65
batch:      45660 | loss: 5.24076 | failed:   65
batch:      45670 | loss: 5.19203 | failed:   65
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      45690 | loss: 5.19580 | failed:   67
batch:      45700 | loss: 5.24946 | failed:   67
batch:      45710 | loss: 5.12026 | failed:   67
batch:      45720 | loss: 5.09918 | failed:   67
batch:      45730 | loss: 5.12963 | failed:   67
batch:      45740 | loss: 5.29232 | failed:   67
batch:      45750 | loss: 5.11101 | failed:   67
batch:      45760 | loss: 5.11068 | failed:   67
batch:      45770 | loss: 4.98688 | failed:   67
batch:      45780 | loss: 5.09532 | failed:   67
batch:      45790 | loss: 5.01416 | failed:   67
batch:      45800 | loss: 5.27412 | failed:   67
batch:      45810 | loss: 5.14979 | failed:   67
batch:      45820 | loss: 5.17788 | failed:   67
batch:      45830 | loss: 5.11601 | failed:   67
batch:      45840 | loss: 5.04834 | failed:   67
batch:      45850 | loss: 5.02253 | failed:   67
batch:      45860 | loss: 5.31246 | failed:   67
batch:      45870 | loss: 5.30675 | failed:   67
batch:      45880 | loss: 5.20651 | failed:   67
batch:      45890 | loss: 5.18179 | failed:   67
batch:      45900 | loss: 5.13295 | failed:   67
batch:      45910 | loss: 5.06690 | failed:   67
batch:      45920 | loss: 5.13660 | failed:   67
batch:      45930 | loss: 5.07183 | failed:   67
batch:      45940 | loss: 5.02753 | failed:   67
batch:      45950 | loss: 5.05352 | failed:   67
batch:      45960 | loss: 5.20826 | failed:   67
batch:      45970 | loss: 5.12983 | failed:   67
batch:      45980 | loss: 5.08654 | failed:   67
batch:      45990 | loss: 5.09365 | failed:   67
batch:      46000 | loss: 5.23372 | failed:   67
batch:      46010 | loss: 5.15862 | failed:   67
batch:      46020 | loss: 5.01488 | failed:   67
batch:      46030 | loss: 4.83872 | failed:   67
batch:      46040 | loss: 4.77952 | failed:   67
batch:      46050 | loss: 4.76938 | failed:   67
batch:      46060 | loss: 5.24266 | failed:   67
batch:      46070 | loss: 5.19282 | failed:   67
batch:      46080 | loss: 5.23051 | failed:   67
batch:      46090 | loss: 5.10160 | failed:   67
batch:      46100 | loss: 5.28684 | failed:   67
batch:      46110 | loss: 5.18577 | failed:   67
batch:      46120 | loss: 5.14349 | failed:   67
batch:      46130 | loss: 5.10346 | failed:   67
batch:      46140 | loss: 5.13968 | failed:   67
batch:      46150 | loss: 4.89547 | failed:   67
batch:      46160 | loss: 5.01557 | failed:   67
batch:      46170 | loss: 5.24002 | failed:   67
batch:      46180 | loss: 5.27532 | failed:   67
batch:      46190 | loss: 5.17868 | failed:   67
batch:      46200 | loss: 5.01617 | failed:   67
batch:      46210 | loss: 5.11311 | failed:   67
batch:      46220 | loss: 5.17936 | failed:   67
batch:      46230 | loss: 5.08099 | failed:   67
batch:      46240 | loss: 5.16227 | failed:   67
batch:      46250 | loss: 5.16800 | failed:   67
batch:      46260 | loss: 5.16751 | failed:   67
batch:      46270 | loss: 5.12655 | failed:   67
batch:      46280 | loss: 5.30795 | failed:   67
batch:      46290 | loss: 5.19362 | failed:   67
batch:      46300 | loss: 4.93533 | failed:   67
batch:      46310 | loss: 5.21597 | failed:   67
batch:      46320 | loss: 5.16481 | failed:   67
batch:      46330 | loss: 5.23788 | failed:   67
batch:      46340 | loss: 5.08653 | failed:   67
batch:      46350 | loss: 5.16206 | failed:   67
batch:      46360 | loss: 5.16981 | failed:   67
batch:      46370 | loss: 5.09481 | failed:   67
batch:      46380 | loss: 5.19043 | failed:   67
batch:      46390 | loss: 5.21207 | failed:   67
batch:      46400 | loss: 5.12643 | failed:   67
batch:      46410 | loss: 5.04995 | failed:   67
batch:      46420 | loss: 5.07252 | failed:   67
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      46430 | loss: 4.84913 | failed:   68
batch:      46440 | loss: 5.07341 | failed:   68
batch:      46450 | loss: 5.25451 | failed:   68
batch:      46460 | loss: 5.15002 | failed:   68
batch:      46470 | loss: 5.06169 | failed:   68
batch:      46480 | loss: 5.22019 | failed:   68
batch:      46490 | loss: 5.03863 | failed:   68
batch:      46500 | loss: 5.11949 | failed:   68
batch:      46510 | loss: 5.16730 | failed:   68
batch:      46520 | loss: 5.14489 | failed:   68
batch:      46530 | loss: 5.23430 | failed:   68
batch:      46540 | loss: 5.13000 | failed:   68
batch:      46550 | loss: 5.15858 | failed:   68
batch:      46560 | loss: 5.12778 | failed:   68
batch:      46570 | loss: 5.21361 | failed:   68
batch:      46580 | loss: 5.30524 | failed:   68
batch:      46590 | loss: 5.15411 | failed:   68
batch:      46600 | loss: 5.13378 | failed:   68
batch:      46610 | loss: 4.97497 | failed:   68
batch:      46620 | loss: 5.35599 | failed:   68
batch:      46630 | loss: 5.21742 | failed:   68
batch:      46640 | loss: 5.11766 | failed:   68
batch:      46650 | loss: 5.15429 | failed:   68
batch:      46660 | loss: 4.96758 | failed:   68
batch:      46670 | loss: 5.16758 | failed:   68
batch:      46680 | loss: 5.15672 | failed:   68
batch:      46690 | loss: 5.23287 | failed:   68
batch:      46700 | loss: 5.08546 | failed:   68
batch:      46710 | loss: 5.15216 | failed:   68
batch:      46720 | loss: 5.16176 | failed:   68
batch:      46730 | loss: 5.17203 | failed:   68
batch:      46740 | loss: 4.92071 | failed:   68
batch:      46750 | loss: 5.14113 | failed:   68
batch:      46760 | loss: 5.19281 | failed:   68
batch:      46770 | loss: 5.27058 | failed:   68
batch:      46780 | loss: 5.18238 | failed:   68
batch:      46790 | loss: 5.20201 | failed:   68
batch:      46800 | loss: 5.17389 | failed:   68
batch:      46810 | loss: 5.18934 | failed:   68
batch:      46820 | loss: 5.09023 | failed:   68
batch:      46830 | loss: 5.06906 | failed:   68
batch:      46840 | loss: 5.15466 | failed:   68
batch:      46850 | loss: 5.36460 | failed:   68
batch:      46860 | loss: 5.10026 | failed:   68
batch:      46870 | loss: 5.10999 | failed:   68
batch:      46880 | loss: 5.13305 | failed:   68
batch:      46890 | loss: 5.20537 | failed:   68
batch:      46900 | loss: 5.15889 | failed:   68
batch:      46910 | loss: 5.15211 | failed:   68
batch:      46920 | loss: 5.12696 | failed:   68
batch:      46930 | loss: 5.13548 | failed:   68
batch:      46940 | loss: 5.12164 | failed:   68
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      46950 | loss: 5.31166 | failed:   69
batch:      46960 | loss: 5.26291 | failed:   69
batch:      46970 | loss: 5.28423 | failed:   69
batch:      46980 | loss: 5.24927 | failed:   69
batch:      46990 | loss: 5.27542 | failed:   69
batch:      47000 | loss: 5.25413 | failed:   69
batch:      47010 | loss: 5.26314 | failed:   69
batch:      47020 | loss: 5.23308 | failed:   69
batch:      47030 | loss: 5.23226 | failed:   69
batch:      47040 | loss: 5.12186 | failed:   69
batch:      47050 | loss: 5.25776 | failed:   69
batch:      47060 | loss: 5.13229 | failed:   69
batch:      47070 | loss: 4.85375 | failed:   69
batch:      47080 | loss: 5.08865 | failed:   69
batch:      47090 | loss: 5.06128 | failed:   69
batch:      47100 | loss: 5.24205 | failed:   69
batch:      47110 | loss: 5.09154 | failed:   69
batch:      47120 | loss: 5.08603 | failed:   69
batch:      47130 | loss: 5.15171 | failed:   69
batch:      47140 | loss: 5.16252 | failed:   69
batch:      47150 | loss: 5.20514 | failed:   69
batch:      47160 | loss: 5.06237 | failed:   69
batch:      47170 | loss: 5.08527 | failed:   69
batch:      47180 | loss: 5.07058 | failed:   69
batch:      47190 | loss: 4.99727 | failed:   69
batch:      47200 | loss: 5.11570 | failed:   69
batch:      47210 | loss: 5.20485 | failed:   69
batch:      47220 | loss: 5.14967 | failed:   69
batch:      47230 | loss: 5.13256 | failed:   69
batch:      47240 | loss: 5.18423 | failed:   69
batch:      47250 | loss: 5.14916 | failed:   69
batch:      47260 | loss: 5.15383 | failed:   69
batch:      47270 | loss: 5.10534 | failed:   69
batch:      47280 | loss: 5.06129 | failed:   69
batch:      47290 | loss: 5.15116 | failed:   69
batch:      47300 | loss: 5.15575 | failed:   69
batch:      47310 | loss: 5.30675 | failed:   69
batch:      47320 | loss: 5.22361 | failed:   69
batch:      47330 | loss: 5.14189 | failed:   69
batch:      47340 | loss: 5.11182 | failed:   69
batch:      47350 | loss: 5.23560 | failed:   69
batch:      47360 | loss: 5.18439 | failed:   69
batch:      47370 | loss: 5.02648 | failed:   69
batch:      47380 | loss: 5.11274 | failed:   69
batch:      47390 | loss: 4.94679 | failed:   69
batch:      47400 | loss: 4.98140 | failed:   69
batch:      47410 | loss: 5.21341 | failed:   69
batch:      47420 | loss: 5.17496 | failed:   69
batch:      47430 | loss: 5.11094 | failed:   69
batch:      47440 | loss: 5.14027 | failed:   69
batch:      47450 | loss: 5.05691 | failed:   69
batch:      47460 | loss: 5.04603 | failed:   69
batch:      47470 | loss: 5.07850 | failed:   69
batch:      47480 | loss: 5.15586 | failed:   69
batch:      47490 | loss: 5.03271 | failed:   69
batch:      47500 | loss: 5.26333 | failed:   69
batch:      47510 | loss: 5.20154 | failed:   69
batch:      47520 | loss: 5.00771 | failed:   69
batch:      47530 | loss: 4.88605 | failed:   69
batch:      47540 | loss: 5.09228 | failed:   69
batch:      47550 | loss: 5.26353 | failed:   69
batch:      47560 | loss: 5.19839 | failed:   69
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      47570 | loss: 5.19772 | failed:   73
batch:      47580 | loss: 5.16435 | failed:   73
batch:      47590 | loss: 5.24620 | failed:   73
batch:      47600 | loss: 5.26190 | failed:   73
batch:      47610 | loss: 5.19609 | failed:   73
batch:      47620 | loss: 5.17598 | failed:   73
batch:      47630 | loss: 5.02844 | failed:   73
batch:      47640 | loss: 5.24446 | failed:   73
batch:      47650 | loss: 5.14598 | failed:   73
batch:      47660 | loss: 5.13155 | failed:   73
batch:      47670 | loss: 5.12617 | failed:   73
batch:      47680 | loss: 5.18308 | failed:   73
batch:      47690 | loss: 5.04917 | failed:   73
batch:      47700 | loss: 5.18587 | failed:   73
batch:      47710 | loss: 5.17750 | failed:   73
batch:      47720 | loss: 5.22081 | failed:   73
batch:      47730 | loss: 5.17838 | failed:   73
batch:      47740 | loss: 5.23045 | failed:   73
batch:      47750 | loss: 5.14529 | failed:   73
batch:      47760 | loss: 5.10603 | failed:   73
batch:      47770 | loss: 4.99978 | failed:   73
batch:      47780 | loss: 5.23180 | failed:   73
batch:      47790 | loss: 5.25515 | failed:   73
batch:      47800 | loss: 5.21650 | failed:   73
batch:      47810 | loss: 5.21832 | failed:   73
batch:      47820 | loss: 5.22294 | failed:   73
batch:      47830 | loss: 5.01298 | failed:   73
batch:      47840 | loss: 5.08068 | failed:   73
batch:      47850 | loss: 5.19700 | failed:   73
batch:      47860 | loss: 5.17884 | failed:   73
batch:      47870 | loss: 5.10070 | failed:   73
batch:      47880 | loss: 4.74023 | failed:   73
batch:      47890 | loss: 4.48160 | failed:   73
batch:      47900 | loss: 4.38058 | failed:   73
batch:      47910 | loss: 4.04169 | failed:   73
batch:      47920 | loss: 4.11932 | failed:   73
batch:      47930 | loss: 5.11476 | failed:   73
batch:      47940 | loss: 5.32774 | failed:   73
batch:      47950 | loss: 5.25803 | failed:   73
batch:      47960 | loss: 5.15249 | failed:   73
batch:      47970 | loss: 5.18414 | failed:   73
batch:      47980 | loss: 5.19142 | failed:   73
batch:      47990 | loss: 5.11623 | failed:   73
batch:      48000 | loss: 5.24197 | failed:   73
batch:      48010 | loss: 5.14094 | failed:   73
batch:      48020 | loss: 5.23915 | failed:   73
batch:      48030 | loss: 5.19976 | failed:   73
batch:      48040 | loss: 4.44544 | failed:   73
batch:      48050 | loss: 5.11477 | failed:   73
batch:      48060 | loss: 5.16797 | failed:   73
batch:      48070 | loss: 5.23927 | failed:   73
batch:      48080 | loss: 5.10218 | failed:   73
batch:      48090 | loss: 5.23172 | failed:   73
batch:      48100 | loss: 5.16383 | failed:   73
batch:      48110 | loss: 5.14434 | failed:   73
batch:      48120 | loss: 5.11899 | failed:   73
batch:      48130 | loss: 5.08424 | failed:   73
batch:      48140 | loss: 5.13888 | failed:   73
batch:      48150 | loss: 5.23453 | failed:   73
batch:      48160 | loss: 5.20786 | failed:   73
batch:      48170 | loss: 5.08956 | failed:   73
batch:      48180 | loss: 5.17582 | failed:   73
batch:      48190 | loss: 5.20720 | failed:   73
batch:      48200 | loss: 5.10039 | failed:   73
batch:      48210 | loss: 5.10357 | failed:   73
batch:      48220 | loss: 5.01133 | failed:   73
batch:      48230 | loss: 5.27220 | failed:   73
batch:      48240 | loss: 5.25316 | failed:   73
batch:      48250 | loss: 5.20179 | failed:   73
batch:      48260 | loss: 5.23193 | failed:   73
batch:      48270 | loss: 5.10927 | failed:   73
batch:      48280 | loss: 5.23809 | failed:   73
batch:      48290 | loss: 5.15680 | failed:   73
batch:      48300 | loss: 5.17508 | failed:   73
batch:      48310 | loss: 5.17282 | failed:   73
batch:      48320 | loss: 5.14759 | failed:   73
batch:      48330 | loss: 5.13080 | failed:   73
batch:      48340 | loss: 5.07588 | failed:   73
batch:      48350 | loss: 5.20014 | failed:   73
batch:      48360 | loss: 5.11680 | failed:   73
batch:      48370 | loss: 5.26175 | failed:   73
batch:      48380 | loss: 5.19442 | failed:   73
batch:      48390 | loss: 4.88374 | failed:   73
batch:      48400 | loss: 5.10207 | failed:   73
batch:      48410 | loss: 5.02964 | failed:   73
batch:      48420 | loss: 5.08650 | failed:   73
batch:      48430 | loss: 5.24763 | failed:   73
batch:      48440 | loss: 5.26286 | failed:   73
batch:      48450 | loss: 5.17872 | failed:   73
batch:      48460 | loss: 5.06820 | failed:   73
batch:      48470 | loss: 5.14753 | failed:   73
batch:      48480 | loss: 5.15510 | failed:   73
batch:      48490 | loss: 5.21438 | failed:   73
batch:      48500 | loss: 5.08722 | failed:   73
batch:      48510 | loss: 4.97228 | failed:   73
batch:      48520 | loss: 5.25979 | failed:   73
batch:      48530 | loss: 5.08218 | failed:   73
batch:      48540 | loss: 5.18229 | failed:   73
batch:      48550 | loss: 5.14234 | failed:   73
batch:      48560 | loss: 5.19449 | failed:   73
batch:      48570 | loss: 4.93203 | failed:   73
batch:      48580 | loss: 4.98096 | failed:   73
batch:      48590 | loss: 5.03071 | failed:   73
batch:      48600 | loss: 5.11405 | failed:   73
batch:      48610 | loss: 5.02426 | failed:   73
batch:      48620 | loss: 5.22138 | failed:   73
batch:      48630 | loss: 5.20729 | failed:   73
batch:      48640 | loss: 5.25504 | failed:   73
batch:      48650 | loss: 5.23906 | failed:   73
batch:      48660 | loss: 5.14897 | failed:   73
batch:      48670 | loss: 5.21643 | failed:   73
batch:      48680 | loss: 4.93946 | failed:   73
batch:      48690 | loss: 5.27289 | failed:   73
batch:      48700 | loss: 5.21795 | failed:   73
batch:      48710 | loss: 5.21898 | failed:   73
batch:      48720 | loss: 4.81042 | failed:   73
batch:      48730 | loss: 5.18385 | failed:   73
batch:      48740 | loss: 5.13509 | failed:   73
batch:      48750 | loss: 5.06952 | failed:   73
batch:      48760 | loss: 5.11182 | failed:   73
batch:      48770 | loss: 5.21815 | failed:   73
batch:      48780 | loss: 5.02717 | failed:   73
batch:      48790 | loss: 5.21432 | failed:   73
batch:      48800 | loss: 5.05470 | failed:   73
batch:      48810 | loss: 5.11377 | failed:   73
batch:      48820 | loss: 5.16218 | failed:   73
batch:      48830 | loss: 5.19800 | failed:   73
batch:      48840 | loss: 5.19290 | failed:   73
batch:      48850 | loss: 5.10048 | failed:   73
batch:      48860 | loss: 5.06364 | failed:   73
batch:      48870 | loss: 5.25619 | failed:   73
batch:      48880 | loss: 4.99094 | failed:   73
batch:      48890 | loss: 5.22099 | failed:   73
batch:      48900 | loss: 5.17943 | failed:   73
batch:      48910 | loss: 5.09081 | failed:   73
batch:      48920 | loss: 5.16800 | failed:   73
batch:      48930 | loss: 5.18006 | failed:   73
batch:      48940 | loss: 5.10415 | failed:   73
batch:      48950 | loss: 5.20636 | failed:   73
batch:      48960 | loss: 5.11743 | failed:   73
batch:      48970 | loss: 5.17714 | failed:   73
batch:      48980 | loss: 5.06342 | failed:   73
batch:      48990 | loss: 5.25041 | failed:   73
batch:      49000 | loss: 5.03094 | failed:   73
batch:      49010 | loss: 5.13114 | failed:   73
batch:      49020 | loss: 5.17925 | failed:   73
batch:      49030 | loss: 5.12542 | failed:   73
batch:      49040 | loss: 5.11916 | failed:   73
batch:      49050 | loss: 5.15393 | failed:   73
batch:      49060 | loss: 5.11843 | failed:   73
batch:      49070 | loss: 5.14771 | failed:   73
batch:      49080 | loss: 5.29088 | failed:   73
batch:      49090 | loss: 5.23657 | failed:   73
batch:      49100 | loss: 5.20043 | failed:   73
batch:      49110 | loss: 5.06830 | failed:   73
batch:      49120 | loss: 5.05917 | failed:   73
batch:      49130 | loss: 5.12343 | failed:   73
batch:      49140 | loss: 5.18949 | failed:   73
batch:      49150 | loss: 5.01491 | failed:   73
batch:      49160 | loss: 5.01840 | failed:   73
batch:      49170 | loss: 5.04993 | failed:   73
batch:      49180 | loss: 5.09136 | failed:   73
batch:      49190 | loss: 4.99069 | failed:   73
batch:      49200 | loss: 5.09074 | failed:   73
batch:      49210 | loss: 5.14024 | failed:   73
batch:      49220 | loss: 4.92025 | failed:   73
batch:      49230 | loss: 5.10959 | failed:   73
batch:      49240 | loss: 5.15533 | failed:   73
batch:      49250 | loss: 5.11178 | failed:   73
batch:      49260 | loss: 4.94822 | failed:   73
batch:      49270 | loss: 5.06610 | failed:   73
batch:      49280 | loss: 5.12681 | failed:   73
batch:      49290 | loss: 5.10360 | failed:   73
batch:      49300 | loss: 5.12811 | failed:   73
batch:      49310 | loss: 5.20336 | failed:   73
batch:      49320 | loss: 5.18364 | failed:   73
batch:      49330 | loss: 5.16586 | failed:   73
batch:      49340 | loss: 5.22140 | failed:   73
batch:      49350 | loss: 5.06450 | failed:   73
batch:      49360 | loss: 5.05721 | failed:   73
batch:      49370 | loss: 5.07483 | failed:   73
batch:      49380 | loss: 5.06013 | failed:   73
batch:      49390 | loss: 5.04985 | failed:   73
batch:      49400 | loss: 4.97328 | failed:   73
batch:      49410 | loss: 5.13174 | failed:   73
batch:      49420 | loss: 5.12069 | failed:   73
batch:      49430 | loss: 5.03563 | failed:   73
batch:      49440 | loss: 5.12379 | failed:   73
batch:      49450 | loss: 5.19044 | failed:   73
batch:      49460 | loss: 5.05393 | failed:   73
batch:      49470 | loss: 5.04823 | failed:   73
batch:      49480 | loss: 5.12218 | failed:   73
batch:      49490 | loss: 5.13850 | failed:   73
batch:      49500 | loss: 5.10808 | failed:   73
batch:      49510 | loss: 5.12347 | failed:   73
batch:      49520 | loss: 5.05061 | failed:   73
batch:      49530 | loss: 5.10348 | failed:   73
batch:      49540 | loss: 5.11828 | failed:   73
batch:      49550 | loss: 5.18661 | failed:   73
batch:      49560 | loss: 5.22915 | failed:   73
batch:      49570 | loss: 5.10226 | failed:   73
batch:      49580 | loss: 5.26241 | failed:   73
batch:      49590 | loss: 5.24189 | failed:   73
batch:      49600 | loss: 5.19659 | failed:   73
batch:      49610 | loss: 5.18010 | failed:   73
batch:      49620 | loss: 5.01696 | failed:   73
batch:      49630 | loss: 5.09912 | failed:   73
batch:      49640 | loss: 5.10810 | failed:   73
batch:      49650 | loss: 5.01628 | failed:   73
batch:      49660 | loss: 5.16529 | failed:   73
batch:      49670 | loss: 5.12012 | failed:   73
batch:      49680 | loss: 5.14293 | failed:   73
batch:      49690 | loss: 5.04083 | failed:   73
batch:      49700 | loss: 5.22447 | failed:   73
batch:      49710 | loss: 5.23418 | failed:   73
batch:      49720 | loss: 5.21349 | failed:   73
batch:      49730 | loss: 5.16708 | failed:   73
batch:      49740 | loss: 4.89851 | failed:   73
batch:      49750 | loss: 5.12527 | failed:   73
batch:      49760 | loss: 5.07319 | failed:   73
batch:      49770 | loss: 5.16966 | failed:   73
batch:      49780 | loss: 5.22775 | failed:   73
batch:      49790 | loss: 5.19940 | failed:   73
batch:      49800 | loss: 5.09320 | failed:   73
batch:      49810 | loss: 5.10867 | failed:   73
batch:      49820 | loss: 5.24354 | failed:   73
batch:      49830 | loss: 5.17118 | failed:   73
batch:      49840 | loss: 5.10342 | failed:   73
batch:      49850 | loss: 5.23562 | failed:   73
batch:      49860 | loss: 5.24906 | failed:   73
batch:      49870 | loss: 5.19094 | failed:   73
batch:      49880 | loss: 5.24478 | failed:   73
batch:      49890 | loss: 5.16575 | failed:   73
batch:      49900 | loss: 4.94844 | failed:   73
batch:      49910 | loss: 5.12636 | failed:   73
batch:      49920 | loss: 5.13739 | failed:   73
batch:      49930 | loss: 5.17663 | failed:   73
batch:      49940 | loss: 5.22621 | failed:   73
batch:      49950 | loss: 5.25483 | failed:   73
batch:      49960 | loss: 5.23140 | failed:   73
batch:      49970 | loss: 5.15806 | failed:   73
batch:      49980 | loss: 5.09230 | failed:   73
batch:      49990 | loss: 5.12495 | failed:   73
batch:      50000 | loss: 4.58051 | failed:   73
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      50010 | loss: 5.21885 | failed:   73
batch:      50020 | loss: 5.01362 | failed:   73
batch:      50030 | loss: 5.21456 | failed:   73
batch:      50040 | loss: 5.12469 | failed:   73
batch:      50050 | loss: 5.12207 | failed:   73
batch:      50060 | loss: 5.15385 | failed:   73
batch:      50070 | loss: 5.12989 | failed:   73
batch:      50080 | loss: 3.89303 | failed:   73
batch:      50090 | loss: 5.25675 | failed:   73
batch:      50100 | loss: 5.06249 | failed:   73
batch:      50110 | loss: 5.18946 | failed:   73
batch:      50120 | loss: 5.19319 | failed:   73
batch:      50130 | loss: 5.18372 | failed:   73
batch:      50140 | loss: 5.21616 | failed:   73
batch:      50150 | loss: 5.16135 | failed:   73
batch:      50160 | loss: 5.17319 | failed:   73
batch:      50170 | loss: 5.17178 | failed:   73
batch:      50180 | loss: 5.00311 | failed:   73
batch:      50190 | loss: 5.11371 | failed:   73
batch:      50200 | loss: 5.12299 | failed:   73
batch:      50210 | loss: 5.05496 | failed:   73
batch:      50220 | loss: 5.10384 | failed:   73
batch:      50230 | loss: 5.14044 | failed:   73
batch:      50240 | loss: 5.23236 | failed:   73
batch:      50250 | loss: 4.97534 | failed:   73
batch:      50260 | loss: 5.14405 | failed:   73
batch:      50270 | loss: 5.24270 | failed:   73
batch:      50280 | loss: 5.19640 | failed:   73
batch:      50290 | loss: 5.12096 | failed:   73
batch:      50300 | loss: 5.16067 | failed:   73
batch:      50310 | loss: 5.12443 | failed:   73
batch:      50320 | loss: 5.21229 | failed:   73
batch:      50330 | loss: 5.12980 | failed:   73
batch:      50340 | loss: 5.20740 | failed:   73
batch:      50350 | loss: 5.12240 | failed:   73
batch:      50360 | loss: 5.05911 | failed:   73
batch:      50370 | loss: 5.10038 | failed:   73
batch:      50380 | loss: 5.18602 | failed:   73
batch:      50390 | loss: 5.01637 | failed:   73
batch:      50400 | loss: 4.99627 | failed:   73
batch:      50410 | loss: 5.15541 | failed:   73
batch:      50420 | loss: 5.19197 | failed:   73
batch:      50430 | loss: 5.03497 | failed:   73
batch:      50440 | loss: 5.25271 | failed:   73
batch:      50450 | loss: 5.28088 | failed:   73
batch:      50460 | loss: 5.22770 | failed:   73
batch:      50470 | loss: 5.20813 | failed:   73
batch:      50480 | loss: 5.01479 | failed:   73
batch:      50490 | loss: 4.92046 | failed:   73
batch:      50500 | loss: 5.14355 | failed:   73
batch:      50510 | loss: 4.96715 | failed:   73
batch:      50520 | loss: 5.05951 | failed:   73
batch:      50530 | loss: 5.20124 | failed:   73
batch:      50540 | loss: 5.15596 | failed:   73
batch:      50550 | loss: 5.21483 | failed:   73
batch:      50560 | loss: 5.20779 | failed:   73
batch:      50570 | loss: 5.09923 | failed:   73
batch:      50580 | loss: 5.11687 | failed:   73
batch:      50590 | loss: 4.90479 | failed:   73
batch:      50600 | loss: 5.16305 | failed:   73
batch:      50610 | loss: 5.23719 | failed:   73
batch:      50620 | loss: 5.17660 | failed:   73
batch:      50630 | loss: 4.91927 | failed:   73
batch:      50640 | loss: 5.18240 | failed:   73
batch:      50650 | loss: 5.25861 | failed:   73
batch:      50660 | loss: 5.19266 | failed:   73
batch:      50670 | loss: 5.00875 | failed:   73
batch:      50680 | loss: 5.08499 | failed:   73
batch:      50690 | loss: 5.11356 | failed:   73
batch:      50700 | loss: 4.96901 | failed:   73
batch:      50710 | loss: 5.17996 | failed:   73
batch:      50720 | loss: 5.14828 | failed:   73
batch:      50730 | loss: 5.03758 | failed:   73
batch:      50740 | loss: 5.19969 | failed:   73
batch:      50750 | loss: 5.13084 | failed:   73
batch:      50760 | loss: 5.07910 | failed:   73
batch:      50770 | loss: 4.94218 | failed:   73
batch:      50780 | loss: 5.07026 | failed:   73
batch:      50790 | loss: 5.15131 | failed:   73
batch:      50800 | loss: 5.01319 | failed:   73
batch:      50810 | loss: 5.18474 | failed:   73
batch:      50820 | loss: 5.14265 | failed:   73
batch:      50830 | loss: 5.19728 | failed:   73
batch:      50840 | loss: 5.05648 | failed:   73
batch:      50850 | loss: 5.07116 | failed:   73
batch:      50860 | loss: 5.15445 | failed:   73
batch:      50870 | loss: 5.21275 | failed:   73
batch:      50880 | loss: 5.22582 | failed:   73
batch:      50890 | loss: 5.20143 | failed:   73
batch:      50900 | loss: 5.13482 | failed:   73
batch:      50910 | loss: 5.07920 | failed:   73
batch:      50920 | loss: 5.26889 | failed:   73
batch:      50930 | loss: 5.18893 | failed:   73
batch:      50940 | loss: 5.16460 | failed:   73
batch:      50950 | loss: 5.18164 | failed:   73
batch:      50960 | loss: 5.19153 | failed:   73
batch:      50970 | loss: 5.23307 | failed:   73
batch:      50980 | loss: 4.93408 | failed:   73
batch:      50990 | loss: 5.15593 | failed:   73
batch:      51000 | loss: 5.11018 | failed:   73
batch:      51010 | loss: 5.01930 | failed:   73
batch:      51020 | loss: 5.15241 | failed:   73
batch:      51030 | loss: 5.22480 | failed:   73
batch:      51040 | loss: 5.23529 | failed:   73
batch:      51050 | loss: 5.13673 | failed:   73
batch:      51060 | loss: 5.15372 | failed:   73
batch:      51070 | loss: 5.17900 | failed:   73
batch:      51080 | loss: 5.09503 | failed:   73
batch:      51090 | loss: 5.16949 | failed:   73
batch:      51100 | loss: 5.27519 | failed:   73
batch:      51110 | loss: 5.18156 | failed:   73
batch:      51120 | loss: 4.99156 | failed:   73
batch:      51130 | loss: 5.10574 | failed:   73
batch:      51140 | loss: 5.02794 | failed:   73
batch:      51150 | loss: 5.11236 | failed:   73
batch:      51160 | loss: 5.19774 | failed:   73
batch:      51170 | loss: 5.17883 | failed:   73
batch:      51180 | loss: 5.15248 | failed:   73
batch:      51190 | loss: 5.19289 | failed:   73
batch:      51200 | loss: 5.07009 | failed:   73
batch:      51210 | loss: 4.86547 | failed:   73
batch:      51220 | loss: 5.09970 | failed:   73
batch:      51230 | loss: 5.13722 | failed:   73
batch:      51240 | loss: 5.19864 | failed:   73
batch:      51250 | loss: 5.12960 | failed:   73
batch:      51260 | loss: 5.27972 | failed:   73
batch:      51270 | loss: 5.16112 | failed:   73
batch:      51280 | loss: 5.20231 | failed:   73
batch:      51290 | loss: 5.13206 | failed:   73
batch:      51300 | loss: 5.17905 | failed:   73
batch:      51310 | loss: 5.25435 | failed:   73
batch:      51320 | loss: 5.24186 | failed:   73
batch:      51330 | loss: 5.13208 | failed:   73
batch:      51340 | loss: 5.10384 | failed:   73
batch:      51350 | loss: 4.95180 | failed:   73
batch:      51360 | loss: 5.15458 | failed:   73
batch:      51370 | loss: 5.12136 | failed:   73
batch:      51380 | loss: 5.22450 | failed:   73
batch:      51390 | loss: 5.15900 | failed:   73
batch:      51400 | loss: 5.16587 | failed:   73
batch:      51410 | loss: 5.15393 | failed:   73
batch:      51420 | loss: 5.19659 | failed:   73
batch:      51430 | loss: 5.15429 | failed:   73
batch:      51440 | loss: 5.21729 | failed:   73
batch:      51450 | loss: 5.02447 | failed:   73
batch:      51460 | loss: 4.90456 | failed:   73
batch:      51470 | loss: 4.86866 | failed:   73
batch:      51480 | loss: 5.18748 | failed:   73
batch:      51490 | loss: 5.22432 | failed:   73
batch:      51500 | loss: 5.20776 | failed:   73
batch:      51510 | loss: 5.09995 | failed:   73
batch:      51520 | loss: 5.14937 | failed:   73
batch:      51530 | loss: 5.07410 | failed:   73
batch:      51540 | loss: 5.15687 | failed:   73
batch:      51550 | loss: 5.16422 | failed:   73
batch:      51560 | loss: 5.09890 | failed:   73
batch:      51570 | loss: 5.22907 | failed:   73
batch:      51580 | loss: 5.12140 | failed:   73
batch:      51590 | loss: 5.18236 | failed:   73
batch:      51600 | loss: 5.08929 | failed:   73
batch:      51610 | loss: 5.17960 | failed:   73
batch:      51620 | loss: 5.04568 | failed:   73
batch:      51630 | loss: 5.26331 | failed:   73
batch:      51640 | loss: 5.20166 | failed:   73
batch:      51650 | loss: 5.22503 | failed:   73
batch:      51660 | loss: 5.21330 | failed:   73
batch:      51670 | loss: 5.23511 | failed:   73
batch:      51680 | loss: 5.02503 | failed:   73
batch:      51690 | loss: 5.17604 | failed:   73
batch:      51700 | loss: 5.17299 | failed:   73
batch:      51710 | loss: 5.08459 | failed:   73
batch:      51720 | loss: 5.10304 | failed:   73
batch:      51730 | loss: 5.15156 | failed:   73
batch:      51740 | loss: 5.13768 | failed:   73
batch:      51750 | loss: 5.18517 | failed:   73
batch:      51760 | loss: 5.01491 | failed:   73
batch:      51770 | loss: 5.23270 | failed:   73
batch:      51780 | loss: 5.20625 | failed:   73
batch:      51790 | loss: 5.05049 | failed:   73
batch:      51800 | loss: 5.16998 | failed:   73
batch:      51810 | loss: 5.15298 | failed:   73
batch:      51820 | loss: 5.24510 | failed:   73
batch:      51830 | loss: 5.22151 | failed:   73
batch:      51840 | loss: 5.05981 | failed:   73
batch:      51850 | loss: 5.25304 | failed:   73
batch:      51860 | loss: 5.24917 | failed:   73
batch:      51870 | loss: 5.04481 | failed:   73
batch:      51880 | loss: 5.14708 | failed:   73
batch:      51890 | loss: 5.20905 | failed:   73
batch:      51900 | loss: 5.23689 | failed:   73
batch:      51910 | loss: 5.22060 | failed:   73
batch:      51920 | loss: 5.25381 | failed:   73
batch:      51930 | loss: 5.19596 | failed:   73
batch:      51940 | loss: 5.20142 | failed:   73
batch:      51950 | loss: 5.13797 | failed:   73
batch:      51960 | loss: 4.94916 | failed:   73
batch:      51970 | loss: 5.25904 | failed:   73
batch:      51980 | loss: 5.14126 | failed:   73
batch:      51990 | loss: 5.11129 | failed:   73
batch:      52000 | loss: 5.15901 | failed:   73
batch:      52010 | loss: 5.17445 | failed:   73
batch:      52020 | loss: 5.13001 | failed:   73
batch:      52030 | loss: 5.10599 | failed:   73
batch:      52040 | loss: 5.08551 | failed:   73
batch:      52050 | loss: 5.19951 | failed:   73
batch:      52060 | loss: 5.19931 | failed:   73
batch:      52070 | loss: 4.89549 | failed:   73
batch:      52080 | loss: 5.16669 | failed:   73
batch:      52090 | loss: 5.27568 | failed:   73
batch:      52100 | loss: 5.27522 | failed:   73
batch:      52110 | loss: 5.19976 | failed:   73
batch:      52120 | loss: 5.13382 | failed:   73
batch:      52130 | loss: 5.18133 | failed:   73
batch:      52140 | loss: 5.05809 | failed:   73
batch:      52150 | loss: 5.11573 | failed:   73
batch:      52160 | loss: 5.15053 | failed:   73
batch:      52170 | loss: 5.08808 | failed:   73
batch:      52180 | loss: 5.18838 | failed:   73
batch:      52190 | loss: 5.10084 | failed:   73
batch:      52200 | loss: 5.19536 | failed:   73
batch:      52210 | loss: 5.15313 | failed:   73
batch:      52220 | loss: 5.16582 | failed:   73
batch:      52230 | loss: 5.09996 | failed:   73
batch:      52240 | loss: 5.18429 | failed:   73
batch:      52250 | loss: 5.21990 | failed:   73
batch:      52260 | loss: 4.72362 | failed:   73
batch:      52270 | loss: 5.09909 | failed:   73
batch:      52280 | loss: 5.20468 | failed:   73
batch:      52290 | loss: 5.06613 | failed:   73
batch:      52300 | loss: 5.06440 | failed:   73
batch:      52310 | loss: 5.15489 | failed:   73
batch:      52320 | loss: 5.19190 | failed:   73
batch:      52330 | loss: 5.17678 | failed:   73
batch:      52340 | loss: 5.08725 | failed:   73
batch:      52350 | loss: 5.16599 | failed:   73
batch:      52360 | loss: 5.03707 | failed:   73
batch:      52370 | loss: 4.93160 | failed:   73
batch:      52380 | loss: 5.07290 | failed:   73
batch:      52390 | loss: 5.19219 | failed:   73
batch:      52400 | loss: 5.01999 | failed:   73
batch:      52410 | loss: 5.13154 | failed:   73
batch:      52420 | loss: 5.19562 | failed:   73
batch:      52430 | loss: 5.17391 | failed:   73
batch:      52440 | loss: 5.03237 | failed:   73
batch:      52450 | loss: 5.01225 | failed:   73
batch:      52460 | loss: 5.24817 | failed:   73
batch:      52470 | loss: 5.23125 | failed:   73
batch:      52480 | loss: 5.23215 | failed:   73
batch:      52490 | loss: 4.96949 | failed:   73
batch:      52500 | loss: 4.75962 | failed:   73
batch:      52510 | loss: 4.72674 | failed:   73
batch:      52520 | loss: 4.61842 | failed:   73
batch:      52530 | loss: 4.49987 | failed:   73
batch:      52540 | loss: 4.44197 | failed:   73
batch:      52550 | loss: 4.43514 | failed:   73
batch:      52560 | loss: 4.38826 | failed:   73
batch:      52570 | loss: 4.20587 | failed:   73
batch:      52580 | loss: 4.21184 | failed:   73
batch:      52590 | loss: 4.26094 | failed:   73
batch:      52600 | loss: 4.64437 | failed:   73
batch:      52610 | loss: 4.16287 | failed:   73
batch:      52620 | loss: 4.23758 | failed:   73
batch:      52630 | loss: 4.08630 | failed:   73
batch:      52640 | loss: 4.08884 | failed:   73
batch:      52650 | loss: 5.32063 | failed:   73
batch:      52660 | loss: 5.16700 | failed:   73
batch:      52670 | loss: 5.09401 | failed:   73
batch:      52680 | loss: 5.08293 | failed:   73
batch:      52690 | loss: 5.09107 | failed:   73
batch:      52700 | loss: 5.02143 | failed:   73
batch:      52710 | loss: 4.91969 | failed:   73
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      52720 | loss: 4.99396 | failed:   75
batch:      52730 | loss: 5.08325 | failed:   75
batch:      52740 | loss: 5.19941 | failed:   75
batch:      52750 | loss: 5.06981 | failed:   75
batch:      52760 | loss: 5.10454 | failed:   75
batch:      52770 | loss: 5.11136 | failed:   75
batch:      52780 | loss: 5.28498 | failed:   75
batch:      52790 | loss: 5.20804 | failed:   75
batch:      52800 | loss: 4.98265 | failed:   75
batch:      52810 | loss: 5.20764 | failed:   75
batch:      52820 | loss: 5.11300 | failed:   75
batch:      52830 | loss: 5.23408 | failed:   75
batch:      52840 | loss: 5.18408 | failed:   75
batch:      52850 | loss: 5.18770 | failed:   75
batch:      52860 | loss: 5.12249 | failed:   75
batch:      52870 | loss: 5.29748 | failed:   75
batch:      52880 | loss: 5.13540 | failed:   75
batch:      52890 | loss: 5.00156 | failed:   75
batch:      52900 | loss: 5.11086 | failed:   75
batch:      52910 | loss: 5.08306 | failed:   75
batch:      52920 | loss: 5.26962 | failed:   75
batch:      52930 | loss: 5.27039 | failed:   75
batch:      52940 | loss: 5.04909 | failed:   75
batch:      52950 | loss: 5.24034 | failed:   75
batch:      52960 | loss: 5.19690 | failed:   75
batch:      52970 | loss: 5.17938 | failed:   75
batch:      52980 | loss: 5.08581 | failed:   75
batch:      52990 | loss: 5.13699 | failed:   75
batch:      53000 | loss: 5.09673 | failed:   75
batch:      53010 | loss: 5.10546 | failed:   75
batch:      53020 | loss: 5.05797 | failed:   75
batch:      53030 | loss: 5.08556 | failed:   75
batch:      53040 | loss: 5.16374 | failed:   75
batch:      53050 | loss: 5.20217 | failed:   75
batch:      53060 | loss: 5.25475 | failed:   75
batch:      53070 | loss: 5.20894 | failed:   75
batch:      53080 | loss: 5.05571 | failed:   75
batch:      53090 | loss: 5.10282 | failed:   75
batch:      53100 | loss: 5.25811 | failed:   75
batch:      53110 | loss: 5.15350 | failed:   75
batch:      53120 | loss: 5.12524 | failed:   75
batch:      53130 | loss: 5.29577 | failed:   75
batch:      53140 | loss: 5.17422 | failed:   75
batch:      53150 | loss: 5.07139 | failed:   75
batch:      53160 | loss: 4.91344 | failed:   75
batch:      53170 | loss: 5.20896 | failed:   75
batch:      53180 | loss: 5.19045 | failed:   75
batch:      53190 | loss: 5.15355 | failed:   75
batch:      53200 | loss: 5.09069 | failed:   75
batch:      53210 | loss: 5.13673 | failed:   75
batch:      53220 | loss: 5.25779 | failed:   75
batch:      53230 | loss: 5.26469 | failed:   75
batch:      53240 | loss: 5.21508 | failed:   75
batch:      53250 | loss: 5.16156 | failed:   75
batch:      53260 | loss: 5.18839 | failed:   75
batch:      53270 | loss: 5.25143 | failed:   75
batch:      53280 | loss: 5.18814 | failed:   75
batch:      53290 | loss: 5.19539 | failed:   75
batch:      53300 | loss: 5.22736 | failed:   75
batch:      53310 | loss: 5.17857 | failed:   75
batch:      53320 | loss: 5.04909 | failed:   75
batch:      53330 | loss: 5.19389 | failed:   75
batch:      53340 | loss: 5.05774 | failed:   75
batch:      53350 | loss: 5.19684 | failed:   75
batch:      53360 | loss: 5.14895 | failed:   75
batch:      53370 | loss: 5.16140 | failed:   75
batch:      53380 | loss: 5.06368 | failed:   75
batch:      53390 | loss: 5.17355 | failed:   75
batch:      53400 | loss: 5.17386 | failed:   75
batch:      53410 | loss: 5.06976 | failed:   75
batch:      53420 | loss: 5.17132 | failed:   75
batch:      53430 | loss: 5.14555 | failed:   75
batch:      53440 | loss: 5.25018 | failed:   75
batch:      53450 | loss: 5.13262 | failed:   75
batch:      53460 | loss: 5.11427 | failed:   75
batch:      53470 | loss: 5.14483 | failed:   75
batch:      53480 | loss: 5.24969 | failed:   75
batch:      53490 | loss: 5.28636 | failed:   75
batch:      53500 | loss: 5.28897 | failed:   75
batch:      53510 | loss: 5.20698 | failed:   75
batch:      53520 | loss: 5.15882 | failed:   75
batch:      53530 | loss: 5.13575 | failed:   75
batch:      53540 | loss: 5.17454 | failed:   75
batch:      53550 | loss: 5.17120 | failed:   75
batch:      53560 | loss: 5.15585 | failed:   75
batch:      53570 | loss: 5.23040 | failed:   75
batch:      53580 | loss: 4.86777 | failed:   75
batch:      53590 | loss: 5.13091 | failed:   75
batch:      53600 | loss: 5.09671 | failed:   75
batch:      53610 | loss: 5.11252 | failed:   75
batch:      53620 | loss: 5.30256 | failed:   75
batch:      53630 | loss: 5.09877 | failed:   75
batch:      53640 | loss: 5.19800 | failed:   75
batch:      53650 | loss: 5.05545 | failed:   75
batch:      53660 | loss: 5.15418 | failed:   75
batch:      53670 | loss: 5.20565 | failed:   75
batch:      53680 | loss: 5.17405 | failed:   75
batch:      53690 | loss: 5.14892 | failed:   75
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      53700 | loss: 5.23825 | failed:   76
batch:      53710 | loss: 5.18175 | failed:   76
batch:      53720 | loss: 5.20870 | failed:   76
batch:      53730 | loss: 5.20714 | failed:   76
batch:      53740 | loss: 5.19559 | failed:   76
batch:      53750 | loss: 5.22900 | failed:   76
batch:      53760 | loss: 5.24637 | failed:   76
batch:      53770 | loss: 5.14289 | failed:   76
batch:      53780 | loss: 5.25971 | failed:   76
batch:      53790 | loss: 5.04066 | failed:   76
batch:      53800 | loss: 5.25102 | failed:   76
batch:      53810 | loss: 5.18616 | failed:   76
batch:      53820 | loss: 5.23936 | failed:   76
batch:      53830 | loss: 5.15182 | failed:   76
batch:      53840 | loss: 5.21196 | failed:   76
batch:      53850 | loss: 5.15165 | failed:   76
batch:      53860 | loss: 5.04441 | failed:   76
batch:      53870 | loss: 5.16342 | failed:   76
batch:      53880 | loss: 5.24188 | failed:   76
batch:      53890 | loss: 5.12909 | failed:   76
batch:      53900 | loss: 5.24375 | failed:   76
batch:      53910 | loss: 5.16268 | failed:   76
batch:      53920 | loss: 5.18904 | failed:   76
batch:      53930 | loss: 5.18888 | failed:   76
batch:      53940 | loss: 5.21388 | failed:   76
batch:      53950 | loss: 5.10473 | failed:   76
batch:      53960 | loss: 5.12740 | failed:   76
batch:      53970 | loss: 5.18056 | failed:   76
batch:      53980 | loss: 5.27281 | failed:   76
batch:      53990 | loss: 5.03996 | failed:   76
batch:      54000 | loss: 5.10752 | failed:   76
batch:      54010 | loss: 5.25541 | failed:   76
batch:      54020 | loss: 5.25741 | failed:   76
batch:      54030 | loss: 5.19908 | failed:   76
batch:      54040 | loss: 5.21732 | failed:   76
batch:      54050 | loss: 5.18123 | failed:   76
batch:      54060 | loss: 5.18412 | failed:   76
batch:      54070 | loss: 5.12193 | failed:   76
batch:      54080 | loss: 5.16265 | failed:   76
batch:      54090 | loss: 5.27930 | failed:   76
batch:      54100 | loss: 5.13389 | failed:   76
batch:      54110 | loss: 5.12116 | failed:   76
batch:      54120 | loss: 5.12607 | failed:   76
batch:      54130 | loss: 5.07643 | failed:   76
batch:      54140 | loss: 5.08702 | failed:   76
batch:      54150 | loss: 5.03561 | failed:   76
batch:      54160 | loss: 4.92934 | failed:   76
batch:      54170 | loss: 5.18016 | failed:   76
batch:      54180 | loss: 5.16507 | failed:   76
batch:      54190 | loss: 5.09497 | failed:   76
batch:      54200 | loss: 5.22980 | failed:   76
batch:      54210 | loss: 5.14340 | failed:   76
batch:      54220 | loss: 5.17027 | failed:   76
batch:      54230 | loss: 5.23469 | failed:   76
batch:      54240 | loss: 5.09409 | failed:   76
batch:      54250 | loss: 5.14890 | failed:   76
batch:      54260 | loss: 5.10879 | failed:   76
batch:      54270 | loss: 5.12610 | failed:   76
batch:      54280 | loss: 5.24458 | failed:   76
batch:      54290 | loss: 5.07157 | failed:   76
batch:      54300 | loss: 5.09822 | failed:   76
batch:      54310 | loss: 5.14202 | failed:   76
batch:      54320 | loss: 5.12664 | failed:   76
batch:      54330 | loss: 5.04448 | failed:   76
batch:      54340 | loss: 5.21456 | failed:   76
batch:      54350 | loss: 5.24096 | failed:   76
batch:      54360 | loss: 5.19524 | failed:   76
batch:      54370 | loss: 5.15168 | failed:   76
batch:      54380 | loss: 5.05989 | failed:   76
batch:      54390 | loss: 5.15316 | failed:   76
batch:      54400 | loss: 5.20025 | failed:   76
batch:      54410 | loss: 5.26674 | failed:   76
batch:      54420 | loss: 5.02627 | failed:   76
batch:      54430 | loss: 5.03625 | failed:   76
batch:      54440 | loss: 5.23059 | failed:   76
batch:      54450 | loss: 5.11617 | failed:   76
batch:      54460 | loss: 5.05408 | failed:   76
batch:      54470 | loss: 5.26588 | failed:   76
batch:      54480 | loss: 5.19617 | failed:   76
batch:      54490 | loss: 5.12716 | failed:   76
batch:      54500 | loss: 5.15191 | failed:   76
batch:      54510 | loss: 5.21604 | failed:   76
batch:      54520 | loss: 5.05823 | failed:   76
batch:      54530 | loss: 5.12248 | failed:   76
batch:      54540 | loss: 5.09900 | failed:   76
batch:      54550 | loss: 5.16785 | failed:   76
batch:      54560 | loss: 5.15379 | failed:   76
batch:      54570 | loss: 5.25537 | failed:   76
batch:      54580 | loss: 5.23179 | failed:   76
batch:      54590 | loss: 5.06540 | failed:   76
batch:      54600 | loss: 5.11611 | failed:   76
batch:      54610 | loss: 5.27689 | failed:   76
batch:      54620 | loss: 4.96425 | failed:   76
batch:      54630 | loss: 5.13200 | failed:   76
batch:      54640 | loss: 5.14609 | failed:   76
batch:      54650 | loss: 5.11849 | failed:   76
batch:      54660 | loss: 5.13087 | failed:   76
batch:      54670 | loss: 5.06392 | failed:   76
batch:      54680 | loss: 4.92791 | failed:   76
batch:      54690 | loss: 5.20123 | failed:   76
batch:      54700 | loss: 5.04073 | failed:   76
batch:      54710 | loss: 5.22390 | failed:   76
batch:      54720 | loss: 5.13886 | failed:   76
batch:      54730 | loss: 5.17371 | failed:   76
batch:      54740 | loss: 5.23305 | failed:   76
batch:      54750 | loss: 5.08071 | failed:   76
batch:      54760 | loss: 5.15387 | failed:   76
batch:      54770 | loss: 5.19081 | failed:   76
batch:      54780 | loss: 5.21098 | failed:   76
batch:      54790 | loss: 4.97024 | failed:   76
batch:      54800 | loss: 5.13948 | failed:   76
batch:      54810 | loss: 5.12956 | failed:   76
batch:      54820 | loss: 5.18123 | failed:   76
batch:      54830 | loss: 5.13278 | failed:   76
batch:      54840 | loss: 5.18886 | failed:   76
batch:      54850 | loss: 5.09469 | failed:   76
batch:      54860 | loss: 5.18987 | failed:   76
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      54870 | loss: 5.16373 | failed:   77
batch:      54880 | loss: 5.21965 | failed:   77
batch:      54890 | loss: 5.23397 | failed:   77
batch:      54900 | loss: 5.13478 | failed:   77
batch:      54910 | loss: 5.25613 | failed:   77
batch:      54920 | loss: 5.11366 | failed:   77
batch:      54930 | loss: 5.21199 | failed:   77
batch:      54940 | loss: 5.11184 | failed:   77
batch:      54950 | loss: 5.10179 | failed:   77
batch:      54960 | loss: 5.14674 | failed:   77
batch:      54970 | loss: 5.20891 | failed:   77
batch:      54980 | loss: 5.17722 | failed:   77
batch:      54990 | loss: 4.96878 | failed:   77
batch:      55000 | loss: 5.11143 | failed:   77
batch:      55010 | loss: 5.11499 | failed:   77
batch:      55020 | loss: 5.13853 | failed:   77
batch:      55030 | loss: 5.14453 | failed:   77
batch:      55040 | loss: 5.14592 | failed:   77
batch:      55050 | loss: 5.15900 | failed:   77
batch:      55060 | loss: 5.11167 | failed:   77
batch:      55070 | loss: 5.06846 | failed:   77
batch:      55080 | loss: 5.01550 | failed:   77
batch:      55090 | loss: 5.16348 | failed:   77
batch:      55100 | loss: 5.17268 | failed:   77
batch:      55110 | loss: 5.19946 | failed:   77
batch:      55120 | loss: 5.14356 | failed:   77
batch:      55130 | loss: 5.17462 | failed:   77
batch:      55140 | loss: 5.11353 | failed:   77
batch:      55150 | loss: 5.19957 | failed:   77
batch:      55160 | loss: 5.17624 | failed:   77
batch:      55170 | loss: 5.20760 | failed:   77
batch:      55180 | loss: 5.14153 | failed:   77
batch:      55190 | loss: 4.95504 | failed:   77
batch:      55200 | loss: 5.20281 | failed:   77
batch:      55210 | loss: 5.12335 | failed:   77
batch:      55220 | loss: 5.09733 | failed:   77
batch:      55230 | loss: 5.14246 | failed:   77
batch:      55240 | loss: 5.23215 | failed:   77
batch:      55250 | loss: 5.17673 | failed:   77
batch:      55260 | loss: 5.16404 | failed:   77
batch:      55270 | loss: 5.10377 | failed:   77
batch:      55280 | loss: 5.21773 | failed:   77
batch:      55290 | loss: 5.12076 | failed:   77
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      55300 | loss: 5.21442 | failed:   78
batch:      55310 | loss: 5.14956 | failed:   78
batch:      55320 | loss: 5.18822 | failed:   78
batch:      55330 | loss: 5.06550 | failed:   78
batch:      55340 | loss: 5.20793 | failed:   78
batch:      55350 | loss: 5.13732 | failed:   78
batch:      55360 | loss: 5.06446 | failed:   78
batch:      55370 | loss: 5.18344 | failed:   78
batch:      55380 | loss: 5.18521 | failed:   78
batch:      55390 | loss: 5.11114 | failed:   78
batch:      55400 | loss: 5.18805 | failed:   78
batch:      55410 | loss: 5.25276 | failed:   78
batch:      55420 | loss: 5.14878 | failed:   78
batch:      55430 | loss: 5.09342 | failed:   78
batch:      55440 | loss: 5.24269 | failed:   78
batch:      55450 | loss: 5.15069 | failed:   78
batch:      55460 | loss: 5.11877 | failed:   78
batch:      55470 | loss: 5.16641 | failed:   78
batch:      55480 | loss: 5.12369 | failed:   78
batch:      55490 | loss: 5.19022 | failed:   78
batch:      55500 | loss: 5.08213 | failed:   78
batch:      55510 | loss: 5.20290 | failed:   78
batch:      55520 | loss: 5.24912 | failed:   78
batch:      55530 | loss: 5.24997 | failed:   78
batch:      55540 | loss: 5.09115 | failed:   78
batch:      55550 | loss: 5.17435 | failed:   78
batch:      55560 | loss: 5.17823 | failed:   78
batch:      55570 | loss: 5.14114 | failed:   78
batch:      55580 | loss: 5.15379 | failed:   78
batch:      55590 | loss: 5.16619 | failed:   78
batch:      55600 | loss: 5.15514 | failed:   78
batch:      55610 | loss: 4.90695 | failed:   78
batch:      55620 | loss: 5.24019 | failed:   78
batch:      55630 | loss: 4.90440 | failed:   78
batch:      55640 | loss: 5.10873 | failed:   78
batch:      55650 | loss: 5.15677 | failed:   78
batch:      55660 | loss: 5.24211 | failed:   78
batch:      55670 | loss: 4.92337 | failed:   78
batch:      55680 | loss: 5.26948 | failed:   78
batch:      55690 | loss: 5.26666 | failed:   78
batch:      55700 | loss: 5.16312 | failed:   78
batch:      55710 | loss: 4.91287 | failed:   78
batch:      55720 | loss: 5.17645 | failed:   78
batch:      55730 | loss: 5.12605 | failed:   78
batch:      55740 | loss: 5.22878 | failed:   78
batch:      55750 | loss: 5.28486 | failed:   78
batch:      55760 | loss: 5.22480 | failed:   78
batch:      55770 | loss: 5.04427 | failed:   78
batch:      55780 | loss: 5.07735 | failed:   78
batch:      55790 | loss: 4.59200 | failed:   78
batch:      55800 | loss: 5.15706 | failed:   78
batch:      55810 | loss: 5.20478 | failed:   78
batch:      55820 | loss: 5.23583 | failed:   78
batch:      55830 | loss: 5.18159 | failed:   78
batch:      55840 | loss: 5.09013 | failed:   78
batch:      55850 | loss: 4.96028 | failed:   78
batch:      55860 | loss: 4.93803 | failed:   78
batch:      55870 | loss: 5.16486 | failed:   78
batch:      55880 | loss: 5.28473 | failed:   78
batch:      55890 | loss: 5.14688 | failed:   78
batch:      55900 | loss: 5.13128 | failed:   78
batch:      55910 | loss: 5.12172 | failed:   78
batch:      55920 | loss: 5.23122 | failed:   78
batch:      55930 | loss: 5.31368 | failed:   78
batch:      55940 | loss: 5.27644 | failed:   78
batch:      55950 | loss: 5.24092 | failed:   78
batch:      55960 | loss: 5.19805 | failed:   78
batch:      55970 | loss: 5.04139 | failed:   78
batch:      55980 | loss: 5.20120 | failed:   78
batch:      55990 | loss: 5.11862 | failed:   78
batch:      56000 | loss: 5.21981 | failed:   78
batch:      56010 | loss: 5.21264 | failed:   78
batch:      56020 | loss: 5.21338 | failed:   78
batch:      56030 | loss: 5.19920 | failed:   78
batch:      56040 | loss: 5.19573 | failed:   78
batch:      56050 | loss: 5.22580 | failed:   78
batch:      56060 | loss: 5.18047 | failed:   78
batch:      56070 | loss: 5.20154 | failed:   78
batch:      56080 | loss: 5.22567 | failed:   78
batch:      56090 | loss: 5.31108 | failed:   78
batch:      56100 | loss: 5.16270 | failed:   78
batch:      56110 | loss: 5.08294 | failed:   78
batch:      56120 | loss: 5.16453 | failed:   78
batch:      56130 | loss: 5.14350 | failed:   78
batch:      56140 | loss: 5.13933 | failed:   78
batch:      56150 | loss: 5.06669 | failed:   78
batch:      56160 | loss: 4.12320 | failed:   78
batch:      56170 | loss: 5.23011 | failed:   78
batch:      56180 | loss: 5.08621 | failed:   78
batch:      56190 | loss: 5.10313 | failed:   78
batch:      56200 | loss: 5.20783 | failed:   78
batch:      56210 | loss: 5.18713 | failed:   78
batch:      56220 | loss: 5.20241 | failed:   78
batch:      56230 | loss: 5.20452 | failed:   78
batch:      56240 | loss: 5.21421 | failed:   78
batch:      56250 | loss: 5.23925 | failed:   78
batch:      56260 | loss: 5.15406 | failed:   78
batch:      56270 | loss: 5.17753 | failed:   78
batch:      56280 | loss: 5.13231 | failed:   78
batch:      56290 | loss: 5.20566 | failed:   78
batch:      56300 | loss: 5.21335 | failed:   78
batch:      56310 | loss: 5.16923 | failed:   78
batch:      56320 | loss: 5.15750 | failed:   78
batch:      56330 | loss: 5.22291 | failed:   78
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56350 | loss: 5.15329 | failed:   85
batch:      56360 | loss: 5.13613 | failed:   85
batch:      56370 | loss: 5.06876 | failed:   85
batch:      56380 | loss: 5.06349 | failed:   85
batch:      56390 | loss: 5.08182 | failed:   85
batch:      56400 | loss: 5.12637 | failed:   85
batch:      56410 | loss: 5.07351 | failed:   85
batch:      56420 | loss: 4.31995 | failed:   85
batch:      56430 | loss: 5.18473 | failed:   85
batch:      56440 | loss: 5.13363 | failed:   85
batch:      56450 | loss: 5.02012 | failed:   85
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56470 | loss: 5.12303 | failed:   93
batch:      56480 | loss: 5.21593 | failed:   93
batch:      56490 | loss: 5.16096 | failed:   93
batch:      56500 | loss: 5.15944 | failed:   93
batch:      56510 | loss: 5.15289 | failed:   93
batch:      56520 | loss: 5.14797 | failed:   93
batch:      56530 | loss: 5.16356 | failed:   93
batch:      56540 | loss: 5.14029 | failed:   93
batch:      56550 | loss: 5.19087 | failed:   93
batch:      56560 | loss: 5.04402 | failed:   93
batch:      56570 | loss: 5.05032 | failed:   93
batch:      56580 | loss: 5.14617 | failed:   93
batch:      56590 | loss: 5.15005 | failed:   93
batch:      56600 | loss: 5.02522 | failed:   93
batch:      56610 | loss: 5.26266 | failed:   93
batch:      56620 | loss: 5.26251 | failed:   93
batch:      56630 | loss: 5.15438 | failed:   93
batch:      56640 | loss: 5.28656 | failed:   93
batch:      56650 | loss: 5.18938 | failed:   93
batch:      56660 | loss: 4.99532 | failed:   93
batch:      56670 | loss: 5.10182 | failed:   93
batch:      56680 | loss: 5.19461 | failed:   93
batch:      56690 | loss: 5.17808 | failed:   93
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      56700 | loss: 5.15202 | failed:   94
batch:      56710 | loss: 5.14935 | failed:   94
batch:      56720 | loss: 5.18091 | failed:   94
batch:      56730 | loss: 5.12641 | failed:   94
batch:      56740 | loss: 5.02442 | failed:   94
batch:      56750 | loss: 5.15718 | failed:   94
batch:      56760 | loss: 4.96662 | failed:   94
batch:      56770 | loss: 5.10041 | failed:   94
batch:      56780 | loss: 5.17938 | failed:   94
batch:      56790 | loss: 5.12842 | failed:   94
batch:      56800 | loss: 5.24641 | failed:   94
batch:      56810 | loss: 5.05661 | failed:   94
batch:      56820 | loss: 5.02702 | failed:   94
batch:      56830 | loss: 5.12233 | failed:   94
batch:      56840 | loss: 5.28079 | failed:   94
batch:      56850 | loss: 5.05018 | failed:   94
batch:      56860 | loss: 5.19525 | failed:   94
batch:      56870 | loss: 5.16224 | failed:   94
batch:      56880 | loss: 5.01217 | failed:   94
batch:      56890 | loss: 5.21877 | failed:   94
batch:      56900 | loss: 5.17691 | failed:   94
batch:      56910 | loss: 5.12356 | failed:   94
batch:      56920 | loss: 5.18939 | failed:   94
batch:      56930 | loss: 5.20510 | failed:   94
batch:      56940 | loss: 5.03220 | failed:   94
batch:      56950 | loss: 5.09876 | failed:   94
batch:      56960 | loss: 5.15494 | failed:   94
batch:      56970 | loss: 5.11542 | failed:   94
batch:      56980 | loss: 5.03861 | failed:   94
batch:      56990 | loss: 5.26177 | failed:   94
batch:      57000 | loss: 5.12216 | failed:   94
batch:      57010 | loss: 5.10247 | failed:   94
batch:      57020 | loss: 5.29815 | failed:   94
batch:      57030 | loss: 5.24163 | failed:   94
batch:      57040 | loss: 5.08718 | failed:   94
batch:      57050 | loss: 5.21825 | failed:   94
batch:      57060 | loss: 5.19737 | failed:   94
batch:      57070 | loss: 5.13333 | failed:   94
batch:      57080 | loss: 5.14201 | failed:   94
batch:      57090 | loss: 5.13452 | failed:   94
batch:      57100 | loss: 5.09193 | failed:   94
batch:      57110 | loss: 5.17583 | failed:   94
batch:      57120 | loss: 5.05460 | failed:   94
batch:      57130 | loss: 5.07384 | failed:   94
batch:      57140 | loss: 5.04039 | failed:   94
batch:      57150 | loss: 5.12820 | failed:   94
batch:      57160 | loss: 5.25867 | failed:   94
batch:      57170 | loss: 5.21344 | failed:   94
batch:      57180 | loss: 5.12336 | failed:   94
batch:      57190 | loss: 5.15933 | failed:   94
batch:      57200 | loss: 4.95611 | failed:   94
batch:      57210 | loss: 5.19588 | failed:   94
batch:      57220 | loss: 5.04136 | failed:   94
batch:      57230 | loss: 5.24020 | failed:   94
batch:      57240 | loss: 5.13050 | failed:   94
batch:      57250 | loss: 5.17600 | failed:   94
batch:      57260 | loss: 5.15111 | failed:   94
batch:      57270 | loss: 4.96876 | failed:   94
batch:      57280 | loss: 4.67600 | failed:   94
batch:      57290 | loss: 4.53798 | failed:   94
batch:      57300 | loss: 4.62373 | failed:   94
batch:      57310 | loss: 5.18126 | failed:   94
batch:      57320 | loss: 4.99970 | failed:   94
batch:      57330 | loss: 5.10354 | failed:   94
batch:      57340 | loss: 4.99168 | failed:   94
batch:      57350 | loss: 5.16351 | failed:   94
batch:      57360 | loss: 5.12444 | failed:   94
batch:      57370 | loss: 4.86824 | failed:   94
batch:      57380 | loss: 5.15378 | failed:   94
batch:      57390 | loss: 5.18020 | failed:   94
batch:      57400 | loss: 5.15629 | failed:   94
batch:      57410 | loss: 4.98093 | failed:   94
batch:      57420 | loss: 5.16790 | failed:   94
batch:      57430 | loss: 4.87620 | failed:   94
batch:      57440 | loss: 5.23773 | failed:   94
batch:      57450 | loss: 5.30175 | failed:   94
batch:      57460 | loss: 5.17929 | failed:   94
batch:      57470 | loss: 5.10579 | failed:   94
batch:      57480 | loss: 4.90860 | failed:   94
batch:      57490 | loss: 5.26478 | failed:   94
batch:      57500 | loss: 5.15383 | failed:   94
batch:      57510 | loss: 5.12693 | failed:   94
batch:      57520 | loss: 5.13229 | failed:   94
batch:      57530 | loss: 5.22927 | failed:   94
batch:      57540 | loss: 5.11421 | failed:   94
batch:      57550 | loss: 5.17327 | failed:   94
batch:      57560 | loss: 5.17055 | failed:   94
batch:      57570 | loss: 5.22921 | failed:   94
batch:      57580 | loss: 5.19815 | failed:   94
batch:      57590 | loss: 5.12082 | failed:   94
batch:      57600 | loss: 5.12557 | failed:   94
batch:      57610 | loss: 5.12968 | failed:   94
batch:      57620 | loss: 5.11018 | failed:   94
batch:      57630 | loss: 5.15627 | failed:   94
batch:      57640 | loss: 5.15777 | failed:   94
batch:      57650 | loss: 5.20698 | failed:   94
batch:      57660 | loss: 5.04734 | failed:   94
batch:      57670 | loss: 5.09098 | failed:   94
batch:      57680 | loss: 5.15595 | failed:   94
batch:      57690 | loss: 5.19970 | failed:   94
batch:      57700 | loss: 5.13618 | failed:   94
batch:      57710 | loss: 5.16473 | failed:   94
batch:      57720 | loss: 5.11150 | failed:   94
batch:      57730 | loss: 5.12365 | failed:   94
batch:      57740 | loss: 5.23334 | failed:   94
batch:      57750 | loss: 5.23895 | failed:   94
batch:      57760 | loss: 5.24513 | failed:   94
batch:      57770 | loss: 5.23242 | failed:   94
batch:      57780 | loss: 5.22112 | failed:   94
batch:      57790 | loss: 5.12585 | failed:   94
batch:      57800 | loss: 5.16682 | failed:   94
batch:      57810 | loss: 5.15559 | failed:   94
batch:      57820 | loss: 5.06183 | failed:   94
batch:      57830 | loss: 4.55489 | failed:   94
batch:      57840 | loss: 4.33701 | failed:   94
batch:      57850 | loss: 4.21149 | failed:   94
batch:      57860 | loss: 4.17506 | failed:   94
batch:      57870 | loss: 3.88230 | failed:   94
batch:      57880 | loss: 4.04122 | failed:   94
batch:      57890 | loss: 4.01322 | failed:   94
batch:      57900 | loss: 5.17948 | failed:   94
batch:      57910 | loss: 5.04439 | failed:   94
batch:      57920 | loss: 5.28216 | failed:   94
batch:      57930 | loss: 5.28439 | failed:   94
batch:      57940 | loss: 5.25974 | failed:   94
batch:      57950 | loss: 5.14847 | failed:   94
batch:      57960 | loss: 5.03298 | failed:   94
batch:      57970 | loss: 5.22332 | failed:   94
batch:      57980 | loss: 5.12987 | failed:   94
batch:      57990 | loss: 5.11625 | failed:   94
batch:      58000 | loss: 5.05036 | failed:   94
batch:      58010 | loss: 4.95932 | failed:   94
batch:      58020 | loss: 4.95905 | failed:   94
batch:      58030 | loss: 5.06073 | failed:   94
batch:      58040 | loss: 5.16497 | failed:   94
batch:      58050 | loss: 5.08753 | failed:   94
batch:      58060 | loss: 5.01345 | failed:   94
batch:      58070 | loss: 5.09661 | failed:   94
batch:      58080 | loss: 5.23152 | failed:   94
batch:      58090 | loss: 4.90951 | failed:   94
batch:      58100 | loss: 5.22627 | failed:   94
batch:      58110 | loss: 5.20560 | failed:   94
batch:      58120 | loss: 5.07695 | failed:   94
batch:      58130 | loss: 5.13760 | failed:   94
batch:      58140 | loss: 5.08326 | failed:   94
batch:      58150 | loss: 5.06989 | failed:   94
batch:      58160 | loss: 5.05160 | failed:   94
batch:      58170 | loss: 5.03968 | failed:   94
batch:      58180 | loss: 5.11162 | failed:   94
batch:      58190 | loss: 5.12544 | failed:   94
batch:      58200 | loss: 5.20865 | failed:   94
batch:      58210 | loss: 5.14533 | failed:   94
batch:      58220 | loss: 5.18205 | failed:   94
batch:      58230 | loss: 5.05112 | failed:   94
batch:      58240 | loss: 5.18776 | failed:   94
batch:      58250 | loss: 5.19191 | failed:   94
batch:      58260 | loss: 5.01859 | failed:   94
batch:      58270 | loss: 5.20731 | failed:   94
batch:      58280 | loss: 5.27411 | failed:   94
batch:      58290 | loss: 5.08368 | failed:   94
batch:      58300 | loss: 5.23906 | failed:   94
batch:      58310 | loss: 5.16218 | failed:   94
batch:      58320 | loss: 5.20119 | failed:   94
batch:      58330 | loss: 5.11769 | failed:   94
batch:      58340 | loss: 5.16237 | failed:   94
batch:      58350 | loss: 5.08072 | failed:   94
batch:      58360 | loss: 5.23948 | failed:   94
batch:      58370 | loss: 5.25792 | failed:   94
batch:      58380 | loss: 5.24667 | failed:   94
batch:      58390 | loss: 5.25004 | failed:   94
batch:      58400 | loss: 5.10577 | failed:   94
batch:      58410 | loss: 5.02532 | failed:   94
batch:      58420 | loss: 5.04527 | failed:   94
batch:      58430 | loss: 5.20006 | failed:   94
batch:      58440 | loss: 5.17038 | failed:   94
batch:      58450 | loss: 5.12490 | failed:   94
batch:      58460 | loss: 5.13862 | failed:   94
batch:      58470 | loss: 5.09642 | failed:   94
batch:      58480 | loss: 5.11955 | failed:   94
batch:      58490 | loss: 4.82412 | failed:   94
batch:      58500 | loss: 5.10479 | failed:   94
batch:      58510 | loss: 5.02214 | failed:   94
batch:      58520 | loss: 5.30976 | failed:   94
batch:      58530 | loss: 5.27053 | failed:   94
batch:      58540 | loss: 5.14965 | failed:   94
batch:      58550 | loss: 4.82783 | failed:   94
batch:      58560 | loss: 5.21412 | failed:   94
batch:      58570 | loss: 5.24666 | failed:   94
batch:      58580 | loss: 5.24306 | failed:   94
batch:      58590 | loss: 5.15725 | failed:   94
batch:      58600 | loss: 5.06007 | failed:   94
batch:      58610 | loss: 5.00531 | failed:   94
batch:      58620 | loss: 4.73045 | failed:   94
batch:      58630 | loss: 5.15890 | failed:   94
batch:      58640 | loss: 5.17260 | failed:   94
batch:      58650 | loss: 4.91966 | failed:   94
batch:      58660 | loss: 5.11121 | failed:   94
batch:      58670 | loss: 5.09433 | failed:   94
batch:      58680 | loss: 5.32683 | failed:   94
batch:      58690 | loss: 5.22173 | failed:   94
batch:      58700 | loss: 5.22981 | failed:   94
batch:      58710 | loss: 5.26493 | failed:   94
batch:      58720 | loss: 5.21825 | failed:   94
batch:      58730 | loss: 5.18131 | failed:   94
batch:      58740 | loss: 5.20927 | failed:   94
batch:      58750 | loss: 5.13722 | failed:   94
batch:      58760 | loss: 5.11292 | failed:   94
batch:      58770 | loss: 5.17287 | failed:   94
batch:      58780 | loss: 5.19405 | failed:   94
batch:      58790 | loss: 5.15205 | failed:   94
batch:      58800 | loss: 4.85598 | failed:   94
batch:      58810 | loss: 5.14079 | failed:   94
batch:      58820 | loss: 4.98552 | failed:   94
batch:      58830 | loss: 5.05647 | failed:   94
batch:      58840 | loss: 5.11642 | failed:   94
batch:      58850 | loss: 5.25519 | failed:   94
batch:      58860 | loss: 5.26440 | failed:   94
batch:      58870 | loss: 5.24084 | failed:   94
batch:      58880 | loss: 5.07416 | failed:   94
batch:      58890 | loss: 5.24122 | failed:   94
batch:      58900 | loss: 5.11364 | failed:   94
batch:      58910 | loss: 4.97181 | failed:   94
batch:      58920 | loss: 5.11510 | failed:   94
batch:      58930 | loss: 5.12132 | failed:   94
batch:      58940 | loss: 5.19138 | failed:   94
batch:      58950 | loss: 5.17064 | failed:   94
batch:      58960 | loss: 5.25075 | failed:   94
batch:      58970 | loss: 5.18777 | failed:   94
batch:      58980 | loss: 5.12468 | failed:   94
batch:      58990 | loss: 4.99862 | failed:   94
batch:      59000 | loss: 5.19747 | failed:   94
batch:      59010 | loss: 5.12943 | failed:   94
batch:      59020 | loss: 5.08799 | failed:   94
batch:      59030 | loss: 5.22244 | failed:   94
batch:      59040 | loss: 5.17618 | failed:   94
batch:      59050 | loss: 5.17529 | failed:   94
batch:      59060 | loss: 5.19052 | failed:   94
batch:      59070 | loss: 5.22599 | failed:   94
batch:      59080 | loss: 5.05836 | failed:   94
batch:      59090 | loss: 5.29382 | failed:   94
batch:      59100 | loss: 5.12107 | failed:   94
batch:      59110 | loss: 5.10298 | failed:   94
batch:      59120 | loss: 5.15414 | failed:   94
batch:      59130 | loss: 5.15587 | failed:   94
batch:      59140 | loss: 5.06382 | failed:   94
batch:      59150 | loss: 5.02431 | failed:   94
batch:      59160 | loss: 5.09736 | failed:   94
batch:      59170 | loss: 5.13516 | failed:   94
batch:      59180 | loss: 5.17226 | failed:   94
batch:      59190 | loss: 5.27428 | failed:   94
batch:      59200 | loss: 5.16875 | failed:   94
batch:      59210 | loss: 5.01923 | failed:   94
batch:      59220 | loss: 5.09466 | failed:   94
batch:      59230 | loss: 5.16759 | failed:   94
batch:      59240 | loss: 5.19272 | failed:   94
batch:      59250 | loss: 5.19722 | failed:   94
batch:      59260 | loss: 5.19090 | failed:   94
batch:      59270 | loss: 5.23399 | failed:   94
batch:      59280 | loss: 5.05387 | failed:   94
batch:      59290 | loss: 5.15032 | failed:   94
batch:      59300 | loss: 5.20476 | failed:   94
batch:      59310 | loss: 5.35583 | failed:   94
batch:      59320 | loss: 5.30092 | failed:   94
batch:      59330 | loss: 5.23042 | failed:   94
batch:      59340 | loss: 5.28366 | failed:   94
batch:      59350 | loss: 5.29194 | failed:   94
batch:      59360 | loss: 5.19285 | failed:   94
batch:      59370 | loss: 5.06518 | failed:   94
batch:      59380 | loss: 5.20556 | failed:   94
batch:      59390 | loss: 5.23812 | failed:   94
batch:      59400 | loss: 5.00111 | failed:   94
batch:      59410 | loss: 5.21950 | failed:   94
batch:      59420 | loss: 4.96754 | failed:   94
batch:      59430 | loss: 5.10957 | failed:   94
batch:      59440 | loss: 5.12574 | failed:   94
batch:      59450 | loss: 5.26934 | failed:   94
batch:      59460 | loss: 5.24034 | failed:   94
batch:      59470 | loss: 5.12679 | failed:   94
batch:      59480 | loss: 5.17749 | failed:   94
batch:      59490 | loss: 5.13007 | failed:   94
batch:      59500 | loss: 5.11861 | failed:   94
batch:      59510 | loss: 5.21009 | failed:   94
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      59520 | loss: 5.16171 | failed:   95
batch:      59530 | loss: 5.18425 | failed:   95
batch:      59540 | loss: 5.18323 | failed:   95
batch:      59550 | loss: 5.21294 | failed:   95
batch:      59560 | loss: 5.16567 | failed:   95
batch:      59570 | loss: 5.17683 | failed:   95
batch:      59580 | loss: 5.13193 | failed:   95
batch:      59590 | loss: 5.16855 | failed:   95
batch:      59600 | loss: 5.17704 | failed:   95
batch:      59610 | loss: 5.19947 | failed:   95
batch:      59620 | loss: 4.97251 | failed:   95
batch:      59630 | loss: 5.17301 | failed:   95
batch:      59640 | loss: 5.20001 | failed:   95
batch:      59650 | loss: 5.14842 | failed:   95
batch:      59660 | loss: 5.17971 | failed:   95
batch:      59670 | loss: 5.09560 | failed:   95
batch:      59680 | loss: 5.27698 | failed:   95
batch:      59690 | loss: 5.25473 | failed:   95
batch:      59700 | loss: 5.08328 | failed:   95
batch:      59710 | loss: 5.15952 | failed:   95
batch:      59720 | loss: 5.18600 | failed:   95
batch:      59730 | loss: 5.14311 | failed:   95
batch:      59740 | loss: 5.32061 | failed:   95
batch:      59750 | loss: 5.27171 | failed:   95
batch:      59760 | loss: 5.02243 | failed:   95
batch:      59770 | loss: 5.14776 | failed:   95
batch:      59780 | loss: 5.12609 | failed:   95
batch:      59790 | loss: 5.20463 | failed:   95
batch:      59800 | loss: 5.04797 | failed:   95
batch:      59810 | loss: 5.00215 | failed:   95
batch:      59820 | loss: 5.34409 | failed:   95
batch:      59830 | loss: 5.10322 | failed:   95
batch:      59840 | loss: 5.11927 | failed:   95
batch:      59850 | loss: 4.96948 | failed:   95
batch:      59860 | loss: 5.09429 | failed:   95
batch:      59870 | loss: 5.06542 | failed:   95
batch:      59880 | loss: 5.14089 | failed:   95
batch:      59890 | loss: 5.07757 | failed:   95
batch:      59900 | loss: 5.24898 | failed:   95
batch:      59910 | loss: 5.20340 | failed:   95
batch:      59920 | loss: 5.26069 | failed:   95
batch:      59930 | loss: 5.15124 | failed:   95
batch:      59940 | loss: 5.08897 | failed:   95
batch:      59950 | loss: 5.09853 | failed:   95
batch:      59960 | loss: 5.21091 | failed:   95
batch:      59970 | loss: 5.24856 | failed:   95
batch:      59980 | loss: 5.14797 | failed:   95
batch:      59990 | loss: 5.14215 | failed:   95
batch:      60000 | loss: 5.06556 | failed:   95
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      60010 | loss: 5.06243 | failed:   95
batch:      60020 | loss: 5.09414 | failed:   95
batch:      60030 | loss: 4.86498 | failed:   95
batch:      60040 | loss: 5.15945 | failed:   95
batch:      60050 | loss: 5.21156 | failed:   95
batch:      60060 | loss: 5.14853 | failed:   95
batch:      60070 | loss: 5.04930 | failed:   95
batch:      60080 | loss: 5.14714 | failed:   95
batch:      60090 | loss: 5.18736 | failed:   95
batch:      60100 | loss: 5.21278 | failed:   95
batch:      60110 | loss: 5.16674 | failed:   95
batch:      60120 | loss: 5.16459 | failed:   95
batch:      60130 | loss: 5.23491 | failed:   95
batch:      60140 | loss: 5.18968 | failed:   95
batch:      60150 | loss: 5.11785 | failed:   95
batch:      60160 | loss: 5.19155 | failed:   95
batch:      60170 | loss: 5.11659 | failed:   95
batch:      60180 | loss: 5.09367 | failed:   95
batch:      60190 | loss: 5.21555 | failed:   95
batch:      60200 | loss: 5.03370 | failed:   95
batch:      60210 | loss: 5.08536 | failed:   95
batch:      60220 | loss: 5.28716 | failed:   95
batch:      60230 | loss: 5.15398 | failed:   95
batch:      60240 | loss: 5.39712 | failed:   95
batch:      60250 | loss: 5.03089 | failed:   95
batch:      60260 | loss: 5.09324 | failed:   95
batch:      60270 | loss: 5.23053 | failed:   95
batch:      60280 | loss: 5.17091 | failed:   95
batch:      60290 | loss: 5.13697 | failed:   95
batch:      60300 | loss: 5.22383 | failed:   95
batch:      60310 | loss: 4.88850 | failed:   95
batch:      60320 | loss: 5.21493 | failed:   95
batch:      60330 | loss: 5.11987 | failed:   95
batch:      60340 | loss: 5.15115 | failed:   95
batch:      60350 | loss: 5.14365 | failed:   95
batch:      60360 | loss: 5.23136 | failed:   95
batch:      60370 | loss: 5.13656 | failed:   95
batch:      60380 | loss: 5.15778 | failed:   95
batch:      60390 | loss: 5.24994 | failed:   95
batch:      60400 | loss: 5.23264 | failed:   95
batch:      60410 | loss: 5.10440 | failed:   95
batch:      60420 | loss: 5.23758 | failed:   95
batch:      60430 | loss: 5.26756 | failed:   95
batch:      60440 | loss: 5.13740 | failed:   95
batch:      60450 | loss: 4.97815 | failed:   95
batch:      60460 | loss: 5.18081 | failed:   95
batch:      60470 | loss: 5.23364 | failed:   95
batch:      60480 | loss: 5.20177 | failed:   95
batch:      60490 | loss: 5.20867 | failed:   95
batch:      60500 | loss: 5.21600 | failed:   95
batch:      60510 | loss: 5.18116 | failed:   95
batch:      60520 | loss: 5.19092 | failed:   95
batch:      60530 | loss: 5.17021 | failed:   95
batch:      60540 | loss: 5.15394 | failed:   95
batch:      60550 | loss: 5.20827 | failed:   95
batch:      60560 | loss: 5.28888 | failed:   95
batch:      60570 | loss: 5.12659 | failed:   95
batch:      60580 | loss: 4.96323 | failed:   95
batch:      60590 | loss: 5.20902 | failed:   95
batch:      60600 | loss: 5.24615 | failed:   95
batch:      60610 | loss: 5.17569 | failed:   95
batch:      60620 | loss: 5.12301 | failed:   95
batch:      60630 | loss: 5.17795 | failed:   95
batch:      60640 | loss: 5.15903 | failed:   95
batch:      60650 | loss: 5.18843 | failed:   95
batch:      60660 | loss: 5.15737 | failed:   95
batch:      60670 | loss: 5.09805 | failed:   95
batch:      60680 | loss: 5.16123 | failed:   95
batch:      60690 | loss: 5.18768 | failed:   95
batch:      60700 | loss: 5.09879 | failed:   95
batch:      60710 | loss: 5.19295 | failed:   95
batch:      60720 | loss: 5.12629 | failed:   95
batch:      60730 | loss: 5.13969 | failed:   95
batch:      60740 | loss: 5.09321 | failed:   95
batch:      60750 | loss: 5.18362 | failed:   95
batch:      60760 | loss: 5.15388 | failed:   95
batch:      60770 | loss: 5.29939 | failed:   95
batch:      60780 | loss: 5.12470 | failed:   95
batch:      60790 | loss: 5.16180 | failed:   95
batch:      60800 | loss: 5.21315 | failed:   95
batch:      60810 | loss: 5.18734 | failed:   95
batch:      60820 | loss: 5.24670 | failed:   95
batch:      60830 | loss: 5.21917 | failed:   95
batch:      60840 | loss: 5.10378 | failed:   95
batch:      60850 | loss: 5.16396 | failed:   95
batch:      60860 | loss: 5.12965 | failed:   95
batch:      60870 | loss: 5.21645 | failed:   95
batch:      60880 | loss: 5.25089 | failed:   95
batch:      60890 | loss: 5.09479 | failed:   95
batch:      60900 | loss: 5.14489 | failed:   95
batch:      60910 | loss: 5.13724 | failed:   95
batch:      60920 | loss: 5.22707 | failed:   95
batch:      60930 | loss: 5.02916 | failed:   95
batch:      60940 | loss: 5.14864 | failed:   95
batch:      60950 | loss: 5.14345 | failed:   95
batch:      60960 | loss: 5.22281 | failed:   95
batch:      60970 | loss: 5.19133 | failed:   95
batch:      60980 | loss: 5.15991 | failed:   95
batch:      60990 | loss: 5.18415 | failed:   95
batch:      61000 | loss: 5.12416 | failed:   95
batch:      61010 | loss: 4.89282 | failed:   95
batch:      61020 | loss: 5.15924 | failed:   95
batch:      61030 | loss: 5.04444 | failed:   95
batch:      61040 | loss: 5.05091 | failed:   95
batch:      61050 | loss: 5.15301 | failed:   95
batch:      61060 | loss: 5.03355 | failed:   95
batch:      61070 | loss: 5.17647 | failed:   95
batch:      61080 | loss: 5.22691 | failed:   95
batch:      61090 | loss: 5.15873 | failed:   95
batch:      61100 | loss: 5.16805 | failed:   95
batch:      61110 | loss: 5.04013 | failed:   95
batch:      61120 | loss: 5.04698 | failed:   95
batch:      61130 | loss: 4.99684 | failed:   95
batch:      61140 | loss: 5.26844 | failed:   95
batch:      61150 | loss: 5.13491 | failed:   95
batch:      61160 | loss: 5.24324 | failed:   95
batch:      61170 | loss: 5.09736 | failed:   95
batch:      61180 | loss: 5.27156 | failed:   95
batch:      61190 | loss: 5.11075 | failed:   95
batch:      61200 | loss: 5.24476 | failed:   95
batch:      61210 | loss: 4.98653 | failed:   95
batch:      61220 | loss: 5.17657 | failed:   95
batch:      61230 | loss: 5.03631 | failed:   95
batch:      61240 | loss: 5.09466 | failed:   95
batch:      61250 | loss: 5.17914 | failed:   95
batch:      61260 | loss: 5.17758 | failed:   95
batch:      61270 | loss: 5.05140 | failed:   95
batch:      61280 | loss: 5.25605 | failed:   95
batch:      61290 | loss: 5.20879 | failed:   95
batch:      61300 | loss: 5.16864 | failed:   95
batch:      61310 | loss: 5.10768 | failed:   95
batch:      61320 | loss: 5.16651 | failed:   95
batch:      61330 | loss: 5.14947 | failed:   95
batch:      61340 | loss: 5.19096 | failed:   95
batch:      61350 | loss: 5.21506 | failed:   95
batch:      61360 | loss: 5.21337 | failed:   95
batch:      61370 | loss: 5.15502 | failed:   95
batch:      61380 | loss: 5.10478 | failed:   95
batch:      61390 | loss: 5.10689 | failed:   95
batch:      61400 | loss: 5.16730 | failed:   95
batch:      61410 | loss: 5.14892 | failed:   95
batch:      61420 | loss: 5.17421 | failed:   95
batch:      61430 | loss: 5.08893 | failed:   95
batch:      61440 | loss: 5.26479 | failed:   95
batch:      61450 | loss: 5.16347 | failed:   95
batch:      61460 | loss: 5.16176 | failed:   95
batch:      61470 | loss: 5.12371 | failed:   95
batch:      61480 | loss: 5.04943 | failed:   95
batch:      61490 | loss: 5.16864 | failed:   95
batch:      61500 | loss: 5.22575 | failed:   95
batch:      61510 | loss: 5.19014 | failed:   95
batch:      61520 | loss: 5.14229 | failed:   95
batch:      61530 | loss: 5.10707 | failed:   95
batch:      61540 | loss: 5.12846 | failed:   95
batch:      61550 | loss: 5.23218 | failed:   95
batch:      61560 | loss: 5.20793 | failed:   95
batch:      61570 | loss: 5.19464 | failed:   95
batch:      61580 | loss: 5.17382 | failed:   95
batch:      61590 | loss: 5.17933 | failed:   95
batch:      61600 | loss: 5.14384 | failed:   95
batch:      61610 | loss: 5.14774 | failed:   95
batch:      61620 | loss: 4.90127 | failed:   95
batch:      61630 | loss: 4.98078 | failed:   95
batch:      61640 | loss: 5.23202 | failed:   95
batch:      61650 | loss: 5.02207 | failed:   95
batch:      61660 | loss: 5.07829 | failed:   95
batch:      61670 | loss: 5.06056 | failed:   95
batch:      61680 | loss: 5.09463 | failed:   95
batch:      61690 | loss: 5.16629 | failed:   95
batch:      61700 | loss: 5.32703 | failed:   95
batch:      61710 | loss: 5.13558 | failed:   95
batch:      61720 | loss: 5.23419 | failed:   95
batch:      61730 | loss: 5.17118 | failed:   95
batch:      61740 | loss: 5.19086 | failed:   95
batch:      61750 | loss: 5.11227 | failed:   95
batch:      61760 | loss: 5.20141 | failed:   95
batch:      61770 | loss: 5.09536 | failed:   95
batch:      61780 | loss: 5.03653 | failed:   95
batch:      61790 | loss: 5.18065 | failed:   95
batch:      61800 | loss: 5.09570 | failed:   95
batch:      61810 | loss: 5.11482 | failed:   95
batch:      61820 | loss: 5.01143 | failed:   95
batch:      61830 | loss: 5.10540 | failed:   95
batch:      61840 | loss: 5.23678 | failed:   95
batch:      61850 | loss: 5.16415 | failed:   95
batch:      61860 | loss: 5.10456 | failed:   95
batch:      61870 | loss: 5.22122 | failed:   95
batch:      61880 | loss: 5.17964 | failed:   95
batch:      61890 | loss: 5.04430 | failed:   95
batch:      61900 | loss: 5.02035 | failed:   95
batch:      61910 | loss: 5.16660 | failed:   95
batch:      61920 | loss: 5.00041 | failed:   95
batch:      61930 | loss: 5.10234 | failed:   95
batch:      61940 | loss: 5.10303 | failed:   95
batch:      61950 | loss: 5.08217 | failed:   95
batch:      61960 | loss: 5.22828 | failed:   95
batch:      61970 | loss: 5.15089 | failed:   95
batch:      61980 | loss: 4.98191 | failed:   95
batch:      61990 | loss: 5.20382 | failed:   95
batch:      62000 | loss: 5.08036 | failed:   95
batch:      62010 | loss: 5.17239 | failed:   95
batch:      62020 | loss: 5.11353 | failed:   95
batch:      62030 | loss: 5.08051 | failed:   95
batch:      62040 | loss: 5.12662 | failed:   95
batch:      62050 | loss: 5.14833 | failed:   95
batch:      62060 | loss: 5.14893 | failed:   95
batch:      62070 | loss: 5.17898 | failed:   95
batch:      62080 | loss: 5.04924 | failed:   95
batch:      62090 | loss: 5.21411 | failed:   95
batch:      62100 | loss: 5.22955 | failed:   95
batch:      62110 | loss: 5.23571 | failed:   95
batch:      62120 | loss: 5.24362 | failed:   95
batch:      62130 | loss: 5.02031 | failed:   95
batch:      62140 | loss: 5.24879 | failed:   95
batch:      62150 | loss: 5.15258 | failed:   95
batch:      62160 | loss: 5.08468 | failed:   95
batch:      62170 | loss: 5.13073 | failed:   95
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      62180 | loss: 5.21243 | failed:   96
batch:      62190 | loss: 5.14704 | failed:   96
batch:      62200 | loss: 5.17645 | failed:   96
batch:      62210 | loss: 5.20459 | failed:   96
batch:      62220 | loss: 5.16692 | failed:   96
batch:      62230 | loss: 5.18874 | failed:   96
batch:      62240 | loss: 5.12929 | failed:   96
batch:      62250 | loss: 5.15456 | failed:   96
batch:      62260 | loss: 5.11104 | failed:   96
batch:      62270 | loss: 5.11389 | failed:   96
batch:      62280 | loss: 5.26304 | failed:   96
batch:      62290 | loss: 5.26273 | failed:   96
batch:      62300 | loss: 5.01043 | failed:   96
batch:      62310 | loss: 4.99056 | failed:   96
batch:      62320 | loss: 4.97033 | failed:   96
batch:      62330 | loss: 5.20832 | failed:   96
batch:      62340 | loss: 5.17246 | failed:   96
batch:      62350 | loss: 5.19978 | failed:   96
batch:      62360 | loss: 5.07788 | failed:   96
batch:      62370 | loss: 5.05403 | failed:   96
batch:      62380 | loss: 5.15185 | failed:   96
batch:      62390 | loss: 5.13784 | failed:   96
batch:      62400 | loss: 5.24835 | failed:   96
batch:      62410 | loss: 5.21289 | failed:   96
batch:      62420 | loss: 5.21904 | failed:   96
batch:      62430 | loss: 5.23761 | failed:   96
batch:      62440 | loss: 5.18991 | failed:   96
batch:      62450 | loss: 5.13956 | failed:   96
batch:      62460 | loss: 5.14276 | failed:   96
batch:      62470 | loss: 5.13188 | failed:   96
batch:      62480 | loss: 5.04513 | failed:   96
batch:      62490 | loss: 5.13154 | failed:   96
batch:      62500 | loss: 5.13820 | failed:   96
batch:      62510 | loss: 5.09431 | failed:   96
batch:      62520 | loss: 4.96701 | failed:   96
batch:      62530 | loss: 5.19893 | failed:   96
batch:      62540 | loss: 4.77072 | failed:   96
batch:      62550 | loss: 5.18926 | failed:   96
batch:      62560 | loss: 5.14633 | failed:   96
batch:      62570 | loss: 5.05893 | failed:   96
batch:      62580 | loss: 5.24219 | failed:   96
batch:      62590 | loss: 5.16032 | failed:   96
batch:      62600 | loss: 5.22129 | failed:   96
batch:      62610 | loss: 5.22865 | failed:   96
batch:      62620 | loss: 5.11990 | failed:   96
batch:      62630 | loss: 5.11975 | failed:   96
batch:      62640 | loss: 5.19803 | failed:   96
batch:      62650 | loss: 5.12148 | failed:   96
batch:      62660 | loss: 5.22327 | failed:   96
batch:      62670 | loss: 5.17515 | failed:   96
batch:      62680 | loss: 5.01694 | failed:   96
batch:      62690 | loss: 5.12595 | failed:   96
batch:      62700 | loss: 5.08598 | failed:   96
batch:      62710 | loss: 5.10130 | failed:   96
batch:      62720 | loss: 5.07095 | failed:   96
batch:      62730 | loss: 5.12197 | failed:   96
batch:      62740 | loss: 5.03962 | failed:   96
batch:      62750 | loss: 4.94503 | failed:   96
batch:      62760 | loss: 5.15719 | failed:   96
batch:      62770 | loss: 5.15719 | failed:   96
batch:      62780 | loss: 5.24469 | failed:   96
batch:      62790 | loss: 5.21327 | failed:   96
batch:      62800 | loss: 5.22350 | failed:   96
batch:      62810 | loss: 5.25992 | failed:   96
batch:      62820 | loss: 5.22773 | failed:   96
batch:      62830 | loss: 5.12497 | failed:   96
batch:      62840 | loss: 5.09179 | failed:   96
batch:      62850 | loss: 5.15527 | failed:   96
batch:      62860 | loss: 5.18506 | failed:   96
batch:      62870 | loss: 5.16092 | failed:   96
batch:      62880 | loss: 5.13256 | failed:   96
batch:      62890 | loss: 5.16491 | failed:   96
batch:      62900 | loss: 5.22989 | failed:   96
batch:      62910 | loss: 5.17788 | failed:   96
batch:      62920 | loss: 5.16921 | failed:   96
batch:      62930 | loss: 5.07498 | failed:   96
batch:      62940 | loss: 5.19675 | failed:   96
batch:      62950 | loss: 5.18851 | failed:   96
batch:      62960 | loss: 5.00124 | failed:   96
batch:      62970 | loss: 5.29841 | failed:   96
batch:      62980 | loss: 5.26243 | failed:   96
batch:      62990 | loss: 5.23206 | failed:   96
batch:      63000 | loss: 5.12618 | failed:   96
batch:      63010 | loss: 5.21388 | failed:   96
batch:      63020 | loss: 5.16507 | failed:   96
batch:      63030 | loss: 5.01056 | failed:   96
batch:      63040 | loss: 5.19125 | failed:   96
batch:      63050 | loss: 5.19946 | failed:   96
batch:      63060 | loss: 5.14192 | failed:   96
batch:      63070 | loss: 5.08915 | failed:   96
batch:      63080 | loss: 5.03642 | failed:   96
batch:      63090 | loss: 5.15673 | failed:   96
batch:      63100 | loss: 5.09157 | failed:   96
batch:      63110 | loss: 5.23474 | failed:   96
batch:      63120 | loss: 5.18844 | failed:   96
batch:      63130 | loss: 5.21332 | failed:   96
batch:      63140 | loss: 5.18557 | failed:   96
batch:      63150 | loss: 5.12150 | failed:   96
batch:      63160 | loss: 5.10352 | failed:   96
batch:      63170 | loss: 4.99744 | failed:   96
batch:      63180 | loss: 5.15634 | failed:   96
batch:      63190 | loss: 5.20929 | failed:   96
batch:      63200 | loss: 5.20872 | failed:   96
batch:      63210 | loss: 5.17526 | failed:   96
batch:      63220 | loss: 5.18899 | failed:   96
batch:      63230 | loss: 5.11057 | failed:   96
batch:      63240 | loss: 5.06941 | failed:   96
batch:      63250 | loss: 5.11464 | failed:   96
batch:      63260 | loss: 5.12473 | failed:   96
batch:      63270 | loss: 5.19558 | failed:   96
batch:      63280 | loss: 5.22368 | failed:   96
batch:      63290 | loss: 5.12597 | failed:   96
batch:      63300 | loss: 5.13048 | failed:   96
batch:      63310 | loss: 5.17867 | failed:   96
batch:      63320 | loss: 5.20830 | failed:   96
batch:      63330 | loss: 5.24524 | failed:   96
batch:      63340 | loss: 5.12316 | failed:   96
batch:      63350 | loss: 5.12494 | failed:   96
batch:      63360 | loss: 4.92322 | failed:   96
batch:      63370 | loss: 5.17991 | failed:   96
batch:      63380 | loss: 5.28222 | failed:   96
batch:      63390 | loss: 5.20780 | failed:   96
batch:      63400 | loss: 5.10171 | failed:   96
batch:      63410 | loss: 5.31370 | failed:   96
batch:      63420 | loss: 5.25901 | failed:   96
batch:      63430 | loss: 5.24583 | failed:   96
batch:      63440 | loss: 5.20354 | failed:   96
batch:      63450 | loss: 5.19217 | failed:   96
batch:      63460 | loss: 5.29115 | failed:   96
batch:      63470 | loss: 5.17917 | failed:   96
batch:      63480 | loss: 5.13940 | failed:   96
batch:      63490 | loss: 5.11816 | failed:   96
batch:      63500 | loss: 5.24788 | failed:   96
batch:      63510 | loss: 5.10975 | failed:   96
batch:      63520 | loss: 5.07462 | failed:   96
batch:      63530 | loss: 4.99474 | failed:   96
batch:      63540 | loss: 5.24248 | failed:   96
batch:      63550 | loss: 5.16854 | failed:   96
batch:      63560 | loss: 5.18282 | failed:   96
batch:      63570 | loss: 5.26197 | failed:   96
batch:      63580 | loss: 5.21269 | failed:   96
batch:      63590 | loss: 5.12040 | failed:   96
batch:      63600 | loss: 5.20183 | failed:   96
batch:      63610 | loss: 4.96790 | failed:   96
batch:      63620 | loss: 5.15430 | failed:   96
batch:      63630 | loss: 4.91844 | failed:   96
batch:      63640 | loss: 5.03480 | failed:   96
batch:      63650 | loss: 5.15665 | failed:   96
batch:      63660 | loss: 5.18273 | failed:   96
batch:      63670 | loss: 5.17929 | failed:   96
batch:      63680 | loss: 5.05364 | failed:   96
batch:      63690 | loss: 5.04884 | failed:   96
batch:      63700 | loss: 5.01552 | failed:   96
batch:      63710 | loss: 5.09688 | failed:   96
batch:      63720 | loss: 4.99188 | failed:   96
batch:      63730 | loss: 4.96048 | failed:   96
batch:      63740 | loss: 4.96941 | failed:   96
batch:      63750 | loss: 5.21317 | failed:   96
batch:      63760 | loss: 5.20561 | failed:   96
batch:      63770 | loss: 5.14570 | failed:   96
batch:      63780 | loss: 5.13943 | failed:   96
batch:      63790 | loss: 5.08431 | failed:   96
batch:      63800 | loss: 5.01436 | failed:   96
batch:      63810 | loss: 5.10818 | failed:   96
batch:      63820 | loss: 5.07543 | failed:   96
batch:      63830 | loss: 5.10462 | failed:   96
batch:      63840 | loss: 4.98366 | failed:   96
batch:      63850 | loss: 5.19936 | failed:   96
batch:      63860 | loss: 5.14568 | failed:   96
batch:      63870 | loss: 5.18163 | failed:   96
batch:      63880 | loss: 5.03878 | failed:   96
batch:      63890 | loss: 4.98230 | failed:   96
batch:      63900 | loss: 5.06993 | failed:   96
batch:      63910 | loss: 5.09145 | failed:   96
batch:      63920 | loss: 5.10959 | failed:   96
batch:      63930 | loss: 4.88467 | failed:   96
batch:      63940 | loss: 5.17045 | failed:   96
batch:      63950 | loss: 5.05706 | failed:   96
batch:      63960 | loss: 5.03699 | failed:   96
batch:      63970 | loss: 5.24675 | failed:   96
batch:      63980 | loss: 5.15662 | failed:   96
batch:      63990 | loss: 5.17225 | failed:   96
batch:      64000 | loss: 5.15784 | failed:   96
batch:      64010 | loss: 5.06431 | failed:   96
batch:      64020 | loss: 5.21039 | failed:   96
batch:      64030 | loss: 5.17521 | failed:   96
batch:      64040 | loss: 5.36199 | failed:   96
batch:      64050 | loss: 5.21025 | failed:   96
batch:      64060 | loss: 5.14510 | failed:   96
batch:      64070 | loss: 5.05774 | failed:   96
batch:      64080 | loss: 5.17482 | failed:   96
batch:      64090 | loss: 5.14022 | failed:   96
batch:      64100 | loss: 5.15064 | failed:   96
batch:      64110 | loss: 5.21295 | failed:   96
batch:      64120 | loss: 5.18106 | failed:   96
batch:      64130 | loss: 5.03018 | failed:   96
batch:      64140 | loss: 4.96414 | failed:   96
batch:      64150 | loss: 5.13836 | failed:   96
batch:      64160 | loss: 5.15410 | failed:   96
batch:      64170 | loss: 5.08746 | failed:   96
batch:      64180 | loss: 5.09000 | failed:   96
batch:      64190 | loss: 5.17548 | failed:   96
batch:      64200 | loss: 5.12772 | failed:   96
batch:      64210 | loss: 5.15383 | failed:   96
batch:      64220 | loss: 5.14910 | failed:   96
batch:      64230 | loss: 5.18634 | failed:   96
batch:      64240 | loss: 5.25857 | failed:   96
batch:      64250 | loss: 5.13428 | failed:   96
batch:      64260 | loss: 5.23442 | failed:   96
batch:      64270 | loss: 5.12263 | failed:   96
batch:      64280 | loss: 5.24924 | failed:   96
batch:      64290 | loss: 5.20720 | failed:   96
batch:      64300 | loss: 5.17235 | failed:   96
batch:      64310 | loss: 5.15954 | failed:   96
batch:      64320 | loss: 5.17070 | failed:   96
batch:      64330 | loss: 5.18608 | failed:   96
batch:      64340 | loss: 4.94295 | failed:   96
batch:      64350 | loss: 5.19172 | failed:   96
batch:      64360 | loss: 5.08140 | failed:   96
batch:      64370 | loss: 5.26885 | failed:   96
batch:      64380 | loss: 5.19374 | failed:   96
batch:      64390 | loss: 5.12420 | failed:   96
batch:      64400 | loss: 5.17779 | failed:   96
batch:      64410 | loss: 5.22286 | failed:   96
batch:      64420 | loss: 5.20075 | failed:   96
batch:      64430 | loss: 5.16181 | failed:   96
batch:      64440 | loss: 5.17679 | failed:   96
batch:      64450 | loss: 5.16235 | failed:   96
batch:      64460 | loss: 5.23341 | failed:   96
batch:      64470 | loss: 5.04501 | failed:   96
batch:      64480 | loss: 4.94653 | failed:   96
batch:      64490 | loss: 4.91364 | failed:   96
batch:      64500 | loss: 4.89635 | failed:   96
batch:      64510 | loss: 5.17552 | failed:   96
batch:      64520 | loss: 5.22442 | failed:   96
batch:      64530 | loss: 5.22461 | failed:   96
batch:      64540 | loss: 5.17781 | failed:   96
batch:      64550 | loss: 5.17757 | failed:   96
batch:      64560 | loss: 5.20250 | failed:   96
batch:      64570 | loss: 5.16734 | failed:   96
batch:      64580 | loss: 5.17108 | failed:   96
batch:      64590 | loss: 5.06595 | failed:   96
batch:      64600 | loss: 5.30172 | failed:   96
batch:      64610 | loss: 5.19894 | failed:   96
batch:      64620 | loss: 5.15412 | failed:   96
batch:      64630 | loss: 5.03218 | failed:   96
batch:      64640 | loss: 5.12214 | failed:   96
batch:      64650 | loss: 5.20230 | failed:   96
batch:      64660 | loss: 5.22004 | failed:   96
batch:      64670 | loss: 5.10457 | failed:   96
batch:      64680 | loss: 5.17474 | failed:   96
batch:      64690 | loss: 5.24994 | failed:   96
batch:      64700 | loss: 4.98444 | failed:   96
batch:      64710 | loss: 5.25482 | failed:   96
batch:      64720 | loss: 5.21189 | failed:   96
batch:      64730 | loss: 5.11730 | failed:   96
batch:      64740 | loss: 5.09271 | failed:   96
batch:      64750 | loss: 5.07354 | failed:   96
batch:      64760 | loss: 5.18762 | failed:   96
batch:      64770 | loss: 5.23146 | failed:   96
batch:      64780 | loss: 5.14967 | failed:   96
batch:      64790 | loss: 5.21973 | failed:   96
batch:      64800 | loss: 5.21998 | failed:   96
batch:      64810 | loss: 5.26528 | failed:   96
batch:      64820 | loss: 5.21232 | failed:   96
batch:      64830 | loss: 5.23599 | failed:   96
batch:      64840 | loss: 5.08055 | failed:   96
batch:      64850 | loss: 4.93239 | failed:   96
batch:      64860 | loss: 5.13928 | failed:   96
batch:      64870 | loss: 5.19362 | failed:   96
batch:      64880 | loss: 5.10514 | failed:   96
batch:      64890 | loss: 5.09947 | failed:   96
batch:      64900 | loss: 5.11938 | failed:   96
batch:      64910 | loss: 5.03423 | failed:   96
batch:      64920 | loss: 5.04258 | failed:   96
batch:      64930 | loss: 4.69390 | failed:   96
batch:      64940 | loss: 5.17630 | failed:   96
batch:      64950 | loss: 5.02208 | failed:   96
batch:      64960 | loss: 5.18745 | failed:   96
batch:      64970 | loss: 5.17364 | failed:   96
batch:      64980 | loss: 5.11534 | failed:   96
batch:      64990 | loss: 4.98584 | failed:   96
batch:      65000 | loss: 5.14740 | failed:   96
batch:      65010 | loss: 5.25833 | failed:   96
batch:      65020 | loss: 5.16387 | failed:   96
batch:      65030 | loss: 5.14345 | failed:   96
batch:      65040 | loss: 5.11871 | failed:   96
batch:      65050 | loss: 5.12844 | failed:   96
batch:      65060 | loss: 5.23195 | failed:   96
batch:      65070 | loss: 5.17540 | failed:   96
batch:      65080 | loss: 5.14135 | failed:   96
batch:      65090 | loss: 5.19099 | failed:   96
batch:      65100 | loss: 5.09946 | failed:   96
batch:      65110 | loss: 5.11261 | failed:   96
batch:      65120 | loss: 5.01996 | failed:   96
batch:      65130 | loss: 5.16332 | failed:   96
batch:      65140 | loss: 5.12577 | failed:   96
batch:      65150 | loss: 5.17194 | failed:   96
batch:      65160 | loss: 5.12484 | failed:   96
batch:      65170 | loss: 5.05071 | failed:   96
batch:      65180 | loss: 5.11084 | failed:   96
batch:      65190 | loss: 5.17968 | failed:   96
batch:      65200 | loss: 5.08040 | failed:   96
batch:      65210 | loss: 5.12296 | failed:   96
batch:      65220 | loss: 5.11721 | failed:   96
batch:      65230 | loss: 5.07937 | failed:   96
batch:      65240 | loss: 5.10524 | failed:   96
batch:      65250 | loss: 5.14031 | failed:   96
batch:      65260 | loss: 5.27598 | failed:   96
batch:      65270 | loss: 5.00706 | failed:   96
batch:      65280 | loss: 5.04725 | failed:   96
batch:      65290 | loss: 5.18365 | failed:   96
batch:      65300 | loss: 5.10851 | failed:   96
batch:      65310 | loss: 5.12300 | failed:   96
batch:      65320 | loss: 4.94127 | failed:   96
batch:      65330 | loss: 5.08408 | failed:   96
batch:      65340 | loss: 5.10985 | failed:   96
batch:      65350 | loss: 5.18229 | failed:   96
batch:      65360 | loss: 5.15644 | failed:   96
batch:      65370 | loss: 5.24129 | failed:   96
batch:      65380 | loss: 5.21699 | failed:   96
batch:      65390 | loss: 5.00856 | failed:   96
batch:      65400 | loss: 5.17328 | failed:   96
batch:      65410 | loss: 5.10832 | failed:   96
batch:      65420 | loss: 5.02801 | failed:   96
batch:      65430 | loss: 5.10228 | failed:   96
batch:      65440 | loss: 5.23727 | failed:   96
batch:      65450 | loss: 5.11575 | failed:   96
batch:      65460 | loss: 5.04073 | failed:   96
batch:      65470 | loss: 5.17697 | failed:   96
batch:      65480 | loss: 5.15751 | failed:   96
batch:      65490 | loss: 5.00072 | failed:   96
batch:      65500 | loss: 4.89126 | failed:   96
batch:      65510 | loss: 5.41683 | failed:   96
batch:      65520 | loss: 5.19195 | failed:   96
batch:      65530 | loss: 5.17686 | failed:   96
batch:      65540 | loss: 5.19108 | failed:   96
batch:      65550 | loss: 5.19437 | failed:   96
batch:      65560 | loss: 5.15561 | failed:   96
batch:      65570 | loss: 5.18372 | failed:   96
batch:      65580 | loss: 5.05249 | failed:   96
batch:      65590 | loss: 5.14422 | failed:   96
batch:      65600 | loss: 5.12053 | failed:   96
batch:      65610 | loss: 5.15595 | failed:   96
batch:      65620 | loss: 5.28616 | failed:   96
batch:      65630 | loss: 5.21184 | failed:   96
batch:      65640 | loss: 5.24650 | failed:   96
batch:      65650 | loss: 5.12261 | failed:   96
batch:      65660 | loss: 5.22684 | failed:   96
batch:      65670 | loss: 5.21489 | failed:   96
batch:      65680 | loss: 5.12004 | failed:   96
batch:      65690 | loss: 4.97880 | failed:   96
batch:      65700 | loss: 5.22333 | failed:   96
batch:      65710 | loss: 5.15617 | failed:   96
batch:      65720 | loss: 5.26132 | failed:   96
batch:      65730 | loss: 5.21056 | failed:   96
batch:      65740 | loss: 5.15876 | failed:   96
batch:      65750 | loss: 5.22681 | failed:   96
batch:      65760 | loss: 5.20793 | failed:   96
batch:      65770 | loss: 5.08233 | failed:   96
batch:      65780 | loss: 5.22190 | failed:   96
batch:      65790 | loss: 5.10953 | failed:   96
batch:      65800 | loss: 4.97775 | failed:   96
batch:      65810 | loss: 5.17081 | failed:   96
batch:      65820 | loss: 5.14181 | failed:   96
batch:      65830 | loss: 5.14742 | failed:   96
batch:      65840 | loss: 5.16227 | failed:   96
batch:      65850 | loss: 5.19266 | failed:   96
batch:      65860 | loss: 5.20535 | failed:   96
batch:      65870 | loss: 5.15039 | failed:   96
batch:      65880 | loss: 5.15122 | failed:   96
batch:      65890 | loss: 5.14113 | failed:   96
batch:      65900 | loss: 5.12440 | failed:   96
batch:      65910 | loss: 5.00070 | failed:   96
batch:      65920 | loss: 5.00482 | failed:   96
batch:      65930 | loss: 5.01289 | failed:   96
batch:      65940 | loss: 5.11710 | failed:   96
batch:      65950 | loss: 5.18332 | failed:   96
batch:      65960 | loss: 5.18036 | failed:   96
batch:      65970 | loss: 5.23917 | failed:   96
batch:      65980 | loss: 5.18107 | failed:   96
batch:      65990 | loss: 5.40767 | failed:   96
batch:      66000 | loss: 5.25613 | failed:   96
batch:      66010 | loss: 5.24893 | failed:   96
batch:      66020 | loss: 5.10328 | failed:   96
batch:      66030 | loss: 5.16471 | failed:   96
batch:      66040 | loss: 5.19394 | failed:   96
batch:      66050 | loss: 5.13086 | failed:   96
batch:      66060 | loss: 5.17146 | failed:   96
batch:      66070 | loss: 5.19791 | failed:   96
batch:      66080 | loss: 5.04086 | failed:   96
batch:      66090 | loss: 5.10609 | failed:   96
batch:      66100 | loss: 5.20558 | failed:   96
batch:      66110 | loss: 5.14019 | failed:   96
batch:      66120 | loss: 5.24998 | failed:   96
batch:      66130 | loss: 5.19153 | failed:   96
batch:      66140 | loss: 5.19549 | failed:   96
batch:      66150 | loss: 5.18260 | failed:   96
batch:      66160 | loss: 5.12951 | failed:   96
batch:      66170 | loss: 5.09684 | failed:   96
batch:      66180 | loss: 5.20290 | failed:   96
batch:      66190 | loss: 5.15886 | failed:   96
batch:      66200 | loss: 5.14109 | failed:   96
batch:      66210 | loss: 5.01233 | failed:   96
batch:      66220 | loss: 5.16191 | failed:   96
batch:      66230 | loss: 5.03802 | failed:   96
batch:      66240 | loss: 5.12747 | failed:   96
batch:      66250 | loss: 5.14890 | failed:   96
batch:      66260 | loss: 5.20475 | failed:   96
batch:      66270 | loss: 5.17401 | failed:   96
batch:      66280 | loss: 5.16039 | failed:   96
batch:      66290 | loss: 5.06597 | failed:   96
batch:      66300 | loss: 5.16070 | failed:   96
batch:      66310 | loss: 4.84155 | failed:   96
batch:      66320 | loss: 5.23110 | failed:   96
batch:      66330 | loss: 5.02822 | failed:   96
batch:      66340 | loss: 5.17990 | failed:   96
batch:      66350 | loss: 5.11982 | failed:   96
batch:      66360 | loss: 5.18655 | failed:   96
batch:      66370 | loss: 5.20524 | failed:   96
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      66380 | loss: 5.22210 | failed:   97
batch:      66390 | loss: 5.23291 | failed:   97
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      66400 | loss: 5.11989 | failed:   98
batch:      66410 | loss: 5.21026 | failed:   98
batch:      66420 | loss: 5.15797 | failed:   98
batch:      66430 | loss: 5.10241 | failed:   98
batch:      66440 | loss: 5.09436 | failed:   98
batch:      66450 | loss: 5.08988 | failed:   98
batch:      66460 | loss: 5.19481 | failed:   98
batch:      66470 | loss: 5.13018 | failed:   98
batch:      66480 | loss: 5.14397 | failed:   98
batch:      66490 | loss: 4.84877 | failed:   98
batch:      66500 | loss: 5.08563 | failed:   98
batch:      66510 | loss: 5.01097 | failed:   98
batch:      66520 | loss: 5.17912 | failed:   98
batch:      66530 | loss: 5.11633 | failed:   98
batch:      66540 | loss: 5.33038 | failed:   98
batch:      66550 | loss: 5.07570 | failed:   98
batch:      66560 | loss: 4.94918 | failed:   98
batch:      66570 | loss: 5.08799 | failed:   98
batch:      66580 | loss: 5.17265 | failed:   98
batch:      66590 | loss: 5.03328 | failed:   98
batch:      66600 | loss: 5.03632 | failed:   98
batch:      66610 | loss: 4.97723 | failed:   98
batch:      66620 | loss: 4.95896 | failed:   98
batch:      66630 | loss: 4.76726 | failed:   98
batch:      66640 | loss: 5.17562 | failed:   98
batch:      66650 | loss: 5.05691 | failed:   98
batch:      66660 | loss: 5.24648 | failed:   98
batch:      66670 | loss: 5.08302 | failed:   98
batch:      66680 | loss: 5.05306 | failed:   98
batch:      66690 | loss: 5.15910 | failed:   98
batch:      66700 | loss: 5.16408 | failed:   98
batch:      66710 | loss: 5.19293 | failed:   98
batch:      66720 | loss: 5.06112 | failed:   98
batch:      66730 | loss: 5.04885 | failed:   98
batch:      66740 | loss: 5.05799 | failed:   98
batch:      66750 | loss: 5.22657 | failed:   98
batch:      66760 | loss: 5.22483 | failed:   98
batch:      66770 | loss: 5.26355 | failed:   98
batch:      66780 | loss: 5.25257 | failed:   98
batch:      66790 | loss: 5.18120 | failed:   98
batch:      66800 | loss: 5.19359 | failed:   98
batch:      66810 | loss: 5.26106 | failed:   98
batch:      66820 | loss: 5.10321 | failed:   98
batch:      66830 | loss: 5.07004 | failed:   98
batch:      66840 | loss: 5.10167 | failed:   98
batch:      66850 | loss: 4.77985 | failed:   98
batch:      66860 | loss: 5.07257 | failed:   98
batch:      66870 | loss: 5.20400 | failed:   98
batch:      66880 | loss: 5.18604 | failed:   98
batch:      66890 | loss: 5.06725 | failed:   98
batch:      66900 | loss: 5.17512 | failed:   98
batch:      66910 | loss: 5.09328 | failed:   98
batch:      66920 | loss: 5.16705 | failed:   98
batch:      66930 | loss: 5.21701 | failed:   98
batch:      66940 | loss: 5.17488 | failed:   98
batch:      66950 | loss: 5.18168 | failed:   98
batch:      66960 | loss: 5.14524 | failed:   98
batch:      66970 | loss: 5.25052 | failed:   98
batch:      66980 | loss: 5.25464 | failed:   98
batch:      66990 | loss: 5.16980 | failed:   98
batch:      67000 | loss: 5.03692 | failed:   98
batch:      67010 | loss: 5.09740 | failed:   98
batch:      67020 | loss: 5.06144 | failed:   98
batch:      67030 | loss: 5.19998 | failed:   98
batch:      67040 | loss: 5.08179 | failed:   98
batch:      67050 | loss: 5.10214 | failed:   98
batch:      67060 | loss: 5.17255 | failed:   98
batch:      67070 | loss: 5.12851 | failed:   98
batch:      67080 | loss: 5.20651 | failed:   98
batch:      67090 | loss: 5.25586 | failed:   98
batch:      67100 | loss: 5.26794 | failed:   98
batch:      67110 | loss: 5.04051 | failed:   98
batch:      67120 | loss: 5.16102 | failed:   98
batch:      67130 | loss: 5.10667 | failed:   98
batch:      67140 | loss: 5.10358 | failed:   98
batch:      67150 | loss: 5.23827 | failed:   98
batch:      67160 | loss: 5.06035 | failed:   98
batch:      67170 | loss: 5.22183 | failed:   98
batch:      67180 | loss: 5.24425 | failed:   98
batch:      67190 | loss: 5.15657 | failed:   98
batch:      67200 | loss: 5.14164 | failed:   98
batch:      67210 | loss: 5.20219 | failed:   98
batch:      67220 | loss: 5.07437 | failed:   98
batch:      67230 | loss: 5.21469 | failed:   98
batch:      67240 | loss: 5.12257 | failed:   98
batch:      67250 | loss: 4.84566 | failed:   98
batch:      67260 | loss: 5.03976 | failed:   98
batch:      67270 | loss: 5.08631 | failed:   98
batch:      67280 | loss: 5.13521 | failed:   98
batch:      67290 | loss: 5.25985 | failed:   98
batch:      67300 | loss: 5.09818 | failed:   98
batch:      67310 | loss: 5.00317 | failed:   98
batch:      67320 | loss: 5.13284 | failed:   98
batch:      67330 | loss: 5.06420 | failed:   98
batch:      67340 | loss: 5.09678 | failed:   98
batch:      67350 | loss: 5.17776 | failed:   98
batch:      67360 | loss: 5.19777 | failed:   98
batch:      67370 | loss: 4.88848 | failed:   98
batch:      67380 | loss: 5.16546 | failed:   98
batch:      67390 | loss: 5.22842 | failed:   98
batch:      67400 | loss: 5.21960 | failed:   98
batch:      67410 | loss: 5.17459 | failed:   98
batch:      67420 | loss: 4.90565 | failed:   98
batch:      67430 | loss: 5.07683 | failed:   98
batch:      67440 | loss: 5.13232 | failed:   98
batch:      67450 | loss: 5.21995 | failed:   98
batch:      67460 | loss: 5.18974 | failed:   98
batch:      67470 | loss: 5.29806 | failed:   98
batch:      67480 | loss: 5.12537 | failed:   98
batch:      67490 | loss: 5.04249 | failed:   98
batch:      67500 | loss: 5.07997 | failed:   98
batch:      67510 | loss: 5.17719 | failed:   98
batch:      67520 | loss: 5.10671 | failed:   98
batch:      67530 | loss: 5.20825 | failed:   98
batch:      67540 | loss: 5.10002 | failed:   98
batch:      67550 | loss: 5.12740 | failed:   98
batch:      67560 | loss: 5.18129 | failed:   98
batch:      67570 | loss: 5.19444 | failed:   98
batch:      67580 | loss: 5.16440 | failed:   98
batch:      67590 | loss: 5.23993 | failed:   98
batch:      67600 | loss: 5.16900 | failed:   98
batch:      67610 | loss: 5.10509 | failed:   98
batch:      67620 | loss: 5.30172 | failed:   98
batch:      67630 | loss: 5.14499 | failed:   98
batch:      67640 | loss: 5.23462 | failed:   98
batch:      67650 | loss: 5.15194 | failed:   98
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      67670 | loss: 5.18440 | failed:  100
batch:      67680 | loss: 5.19006 | failed:  100
batch:      67690 | loss: 5.13277 | failed:  100
batch:      67700 | loss: 5.03466 | failed:  100
batch:      67710 | loss: 5.16375 | failed:  100
batch:      67720 | loss: 4.85870 | failed:  100
batch:      67730 | loss: 5.09259 | failed:  100
batch:      67740 | loss: 5.15015 | failed:  100
batch:      67750 | loss: 5.12755 | failed:  100
batch:      67760 | loss: 5.24634 | failed:  100
batch:      67770 | loss: 5.27392 | failed:  100
batch:      67780 | loss: 5.17839 | failed:  100
batch:      67790 | loss: 5.19509 | failed:  100
batch:      67800 | loss: 5.15550 | failed:  100
batch:      67810 | loss: 5.00146 | failed:  100
batch:      67820 | loss: 5.16109 | failed:  100
batch:      67830 | loss: 5.26257 | failed:  100
batch:      67840 | loss: 5.23323 | failed:  100
batch:      67850 | loss: 5.05786 | failed:  100
batch:      67860 | loss: 5.07082 | failed:  100
batch:      67870 | loss: 5.25650 | failed:  100
batch:      67880 | loss: 5.11280 | failed:  100
batch:      67890 | loss: 5.21562 | failed:  100
batch:      67900 | loss: 5.38678 | failed:  100
batch:      67910 | loss: 5.31602 | failed:  100
batch:      67920 | loss: 5.14391 | failed:  100
batch:      67930 | loss: 5.22149 | failed:  100
batch:      67940 | loss: 5.12969 | failed:  100
batch:      67950 | loss: 5.14765 | failed:  100
batch:      67960 | loss: 5.05495 | failed:  100
batch:      67970 | loss: 5.08726 | failed:  100
batch:      67980 | loss: 5.03079 | failed:  100
batch:      67990 | loss: 4.92811 | failed:  100
batch:      68000 | loss: 5.06062 | failed:  100
batch:      68010 | loss: 5.11887 | failed:  100
batch:      68020 | loss: 5.15692 | failed:  100
batch:      68030 | loss: 5.19862 | failed:  100
batch:      68040 | loss: 5.33763 | failed:  100
batch:      68050 | loss: 5.15022 | failed:  100
batch:      68060 | loss: 5.13078 | failed:  100
batch:      68070 | loss: 5.19387 | failed:  100
batch:      68080 | loss: 5.14285 | failed:  100
batch:      68090 | loss: 5.17425 | failed:  100
batch:      68100 | loss: 5.16893 | failed:  100
batch:      68110 | loss: 5.07624 | failed:  100
batch:      68120 | loss: 4.95325 | failed:  100
batch:      68130 | loss: 5.19364 | failed:  100
batch:      68140 | loss: 5.06077 | failed:  100
batch:      68150 | loss: 5.04066 | failed:  100
batch:      68160 | loss: 5.12578 | failed:  100
batch:      68170 | loss: 5.07897 | failed:  100
batch:      68180 | loss: 5.11349 | failed:  100
batch:      68190 | loss: 5.05674 | failed:  100
batch:      68200 | loss: 5.07041 | failed:  100
batch:      68210 | loss: 5.13058 | failed:  100
batch:      68220 | loss: 5.06327 | failed:  100
batch:      68230 | loss: 5.04491 | failed:  100
batch:      68240 | loss: 5.29819 | failed:  100
batch:      68250 | loss: 4.92826 | failed:  100
batch:      68260 | loss: 5.13831 | failed:  100
batch:      68270 | loss: 5.12607 | failed:  100
batch:      68280 | loss: 5.18958 | failed:  100
batch:      68290 | loss: 5.24018 | failed:  100
batch:      68300 | loss: 5.17467 | failed:  100
batch:      68310 | loss: 5.20427 | failed:  100
batch:      68320 | loss: 5.27658 | failed:  100
batch:      68330 | loss: 5.19597 | failed:  100
batch:      68340 | loss: 5.01061 | failed:  100
batch:      68350 | loss: 5.13853 | failed:  100
batch:      68360 | loss: 4.95616 | failed:  100
batch:      68370 | loss: 5.11187 | failed:  100
batch:      68380 | loss: 5.14122 | failed:  100
batch:      68390 | loss: 5.20428 | failed:  100
batch:      68400 | loss: 5.13538 | failed:  100
batch:      68410 | loss: 5.11379 | failed:  100
batch:      68420 | loss: 5.11228 | failed:  100
batch:      68430 | loss: 5.08197 | failed:  100
batch:      68440 | loss: 5.20191 | failed:  100
batch:      68450 | loss: 5.12507 | failed:  100
batch:      68460 | loss: 5.05148 | failed:  100
batch:      68470 | loss: 5.04129 | failed:  100
batch:      68480 | loss: 5.06819 | failed:  100
batch:      68490 | loss: 5.09200 | failed:  100
batch:      68500 | loss: 4.96102 | failed:  100
batch:      68510 | loss: 5.10913 | failed:  100
batch:      68520 | loss: 5.12529 | failed:  100
batch:      68530 | loss: 5.19117 | failed:  100
batch:      68540 | loss: 5.14295 | failed:  100
batch:      68550 | loss: 5.20655 | failed:  100
batch:      68560 | loss: 5.05662 | failed:  100
batch:      68570 | loss: 5.19751 | failed:  100
batch:      68580 | loss: 5.20461 | failed:  100
batch:      68590 | loss: 5.14509 | failed:  100
batch:      68600 | loss: 5.16246 | failed:  100
batch:      68610 | loss: 5.17723 | failed:  100
batch:      68620 | loss: 5.10886 | failed:  100
batch:      68630 | loss: 5.18349 | failed:  100
batch:      68640 | loss: 5.09290 | failed:  100
batch:      68650 | loss: 5.13402 | failed:  100
batch:      68660 | loss: 5.17772 | failed:  100
batch:      68670 | loss: 5.12712 | failed:  100
batch:      68680 | loss: 5.12398 | failed:  100
batch:      68690 | loss: 5.13786 | failed:  100
batch:      68700 | loss: 5.09241 | failed:  100
batch:      68710 | loss: 4.99405 | failed:  100
batch:      68720 | loss: 5.15067 | failed:  100
batch:      68730 | loss: 5.20464 | failed:  100
batch:      68740 | loss: 4.89254 | failed:  100
batch:      68750 | loss: 5.10496 | failed:  100
batch:      68760 | loss: 5.29436 | failed:  100
batch:      68770 | loss: 5.23562 | failed:  100
batch:      68780 | loss: 5.00212 | failed:  100
batch:      68790 | loss: 4.82405 | failed:  100
batch:      68800 | loss: 5.08002 | failed:  100
batch:      68810 | loss: 5.14453 | failed:  100
batch:      68820 | loss: 5.06934 | failed:  100
batch:      68830 | loss: 5.43679 | failed:  100
batch:      68840 | loss: 5.29214 | failed:  100
batch:      68850 | loss: 5.12427 | failed:  100
batch:      68860 | loss: 5.22069 | failed:  100
batch:      68870 | loss: 5.09467 | failed:  100
batch:      68880 | loss: 5.08794 | failed:  100
batch:      68890 | loss: 5.11766 | failed:  100
batch:      68900 | loss: 5.12742 | failed:  100
batch:      68910 | loss: 4.91239 | failed:  100
batch:      68920 | loss: 5.17954 | failed:  100
batch:      68930 | loss: 5.12904 | failed:  100
batch:      68940 | loss: 5.17817 | failed:  100
batch:      68950 | loss: 5.13611 | failed:  100
batch:      68960 | loss: 5.08710 | failed:  100
batch:      68970 | loss: 4.96642 | failed:  100
batch:      68980 | loss: 5.11605 | failed:  100
batch:      68990 | loss: 4.95024 | failed:  100
batch:      69000 | loss: 5.25427 | failed:  100
batch:      69010 | loss: 5.03351 | failed:  100
batch:      69020 | loss: 5.19515 | failed:  100
batch:      69030 | loss: 5.09726 | failed:  100
batch:      69040 | loss: 5.21690 | failed:  100
batch:      69050 | loss: 5.24927 | failed:  100
batch:      69060 | loss: 5.16051 | failed:  100
batch:      69070 | loss: 5.17963 | failed:  100
batch:      69080 | loss: 5.16065 | failed:  100
batch:      69090 | loss: 5.23473 | failed:  100
batch:      69100 | loss: 5.14882 | failed:  100
batch:      69110 | loss: 5.26415 | failed:  100
batch:      69120 | loss: 5.26859 | failed:  100
batch:      69130 | loss: 5.20181 | failed:  100
batch:      69140 | loss: 5.14623 | failed:  100
batch:      69150 | loss: 5.20015 | failed:  100
batch:      69160 | loss: 5.14744 | failed:  100
batch:      69170 | loss: 5.25736 | failed:  100
batch:      69180 | loss: 5.26498 | failed:  100
batch:      69190 | loss: 5.28933 | failed:  100
batch:      69200 | loss: 5.13979 | failed:  100
batch:      69210 | loss: 5.16670 | failed:  100
batch:      69220 | loss: 5.10730 | failed:  100
batch:      69230 | loss: 4.89365 | failed:  100
batch:      69240 | loss: 5.27368 | failed:  100
batch:      69250 | loss: 5.17978 | failed:  100
batch:      69260 | loss: 5.12947 | failed:  100
batch:      69270 | loss: 5.18606 | failed:  100
batch:      69280 | loss: 5.11718 | failed:  100
batch:      69290 | loss: 5.09711 | failed:  100
batch:      69300 | loss: 5.27927 | failed:  100
batch:      69310 | loss: 5.25562 | failed:  100
batch:      69320 | loss: 5.28557 | failed:  100
batch:      69330 | loss: 5.18745 | failed:  100
batch:      69340 | loss: 5.10963 | failed:  100
batch:      69350 | loss: 5.10750 | failed:  100
batch:      69360 | loss: 5.13034 | failed:  100
batch:      69370 | loss: 5.14575 | failed:  100
batch:      69380 | loss: 5.19334 | failed:  100
batch:      69390 | loss: 5.13239 | failed:  100
batch:      69400 | loss: 5.27888 | failed:  100
batch:      69410 | loss: 5.17552 | failed:  100
batch:      69420 | loss: 5.17037 | failed:  100
batch:      69430 | loss: 5.04131 | failed:  100
batch:      69440 | loss: 5.12710 | failed:  100
batch:      69450 | loss: 5.01253 | failed:  100
batch:      69460 | loss: 5.23562 | failed:  100
batch:      69470 | loss: 5.18473 | failed:  100
batch:      69480 | loss: 5.09229 | failed:  100
batch:      69490 | loss: 5.03537 | failed:  100
batch:      69500 | loss: 5.17394 | failed:  100
batch:      69510 | loss: 5.14828 | failed:  100
batch:      69520 | loss: 5.28123 | failed:  100
batch:      69530 | loss: 5.18906 | failed:  100
batch:      69540 | loss: 5.22713 | failed:  100
batch:      69550 | loss: 5.00816 | failed:  100
batch:      69560 | loss: 5.02733 | failed:  100
batch:      69570 | loss: 4.92251 | failed:  100
batch:      69580 | loss: 4.21831 | failed:  100
batch:      69590 | loss: 5.20043 | failed:  100
batch:      69600 | loss: 5.20500 | failed:  100
batch:      69610 | loss: 5.18781 | failed:  100
batch:      69620 | loss: 5.13805 | failed:  100
batch:      69630 | loss: 5.20009 | failed:  100
batch:      69640 | loss: 5.25691 | failed:  100
batch:      69650 | loss: 5.26097 | failed:  100
batch:      69660 | loss: 5.12845 | failed:  100
batch:      69670 | loss: 5.08535 | failed:  100
batch:      69680 | loss: 5.13476 | failed:  100
batch:      69690 | loss: 4.89486 | failed:  100
batch:      69700 | loss: 5.05513 | failed:  100
batch:      69710 | loss: 5.16823 | failed:  100
batch:      69720 | loss: 5.16728 | failed:  100
batch:      69730 | loss: 5.08540 | failed:  100
batch:      69740 | loss: 5.10058 | failed:  100
batch:      69750 | loss: 5.19839 | failed:  100
batch:      69760 | loss: 4.84187 | failed:  100
batch:      69770 | loss: 4.98501 | failed:  100
batch:      69780 | loss: 5.03819 | failed:  100
batch:      69790 | loss: 5.14702 | failed:  100
batch:      69800 | loss: 5.01254 | failed:  100
batch:      69810 | loss: 4.95913 | failed:  100
batch:      69820 | loss: 4.95013 | failed:  100
batch:      69830 | loss: 5.17633 | failed:  100
batch:      69840 | loss: 5.16189 | failed:  100
batch:      69850 | loss: 5.15384 | failed:  100
batch:      69860 | loss: 5.02737 | failed:  100
batch:      69870 | loss: 4.96799 | failed:  100
batch:      69880 | loss: 5.19247 | failed:  100
batch:      69890 | loss: 4.94208 | failed:  100
batch:      69900 | loss: 5.22788 | failed:  100
batch:      69910 | loss: 5.07769 | failed:  100
batch:      69920 | loss: 5.21426 | failed:  100
batch:      69930 | loss: 5.14491 | failed:  100
batch:      69940 | loss: 5.07081 | failed:  100
batch:      69950 | loss: 5.23509 | failed:  100
batch:      69960 | loss: 5.18912 | failed:  100
batch:      69970 | loss: 5.20161 | failed:  100
batch:      69980 | loss: 4.96369 | failed:  100
batch:      69990 | loss: 5.21893 | failed:  100
batch:      70000 | loss: 4.87264 | failed:  100
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      70010 | loss: 5.29861 | failed:  100
batch:      70020 | loss: 5.09640 | failed:  100
batch:      70030 | loss: 5.18753 | failed:  100
batch:      70040 | loss: 5.21915 | failed:  100
batch:      70050 | loss: 5.06873 | failed:  100
batch:      70060 | loss: 5.13720 | failed:  100
batch:      70070 | loss: 4.94868 | failed:  100
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      70080 | loss: 5.21210 | failed:  101
batch:      70090 | loss: 5.25257 | failed:  101
batch:      70100 | loss: 5.07952 | failed:  101
batch:      70110 | loss: 5.12008 | failed:  101
batch:      70120 | loss: 5.05887 | failed:  101
batch:      70130 | loss: 5.15302 | failed:  101
batch:      70140 | loss: 5.14984 | failed:  101
batch:      70150 | loss: 5.22135 | failed:  101
batch:      70160 | loss: 5.13709 | failed:  101
batch:      70170 | loss: 5.09360 | failed:  101
batch:      70180 | loss: 5.08457 | failed:  101
batch:      70190 | loss: 5.14018 | failed:  101
batch:      70200 | loss: 5.04145 | failed:  101
batch:      70210 | loss: 5.20666 | failed:  101
batch:      70220 | loss: 5.23834 | failed:  101
batch:      70230 | loss: 5.09152 | failed:  101
batch:      70240 | loss: 4.92452 | failed:  101
batch:      70250 | loss: 5.22278 | failed:  101
batch:      70260 | loss: 5.24253 | failed:  101
batch:      70270 | loss: 5.13606 | failed:  101
batch:      70280 | loss: 5.05828 | failed:  101
batch:      70290 | loss: 5.17602 | failed:  101
batch:      70300 | loss: 5.14740 | failed:  101
batch:      70310 | loss: 5.03012 | failed:  101
batch:      70320 | loss: 5.13031 | failed:  101
batch:      70330 | loss: 5.20176 | failed:  101
batch:      70340 | loss: 5.03159 | failed:  101
batch:      70350 | loss: 5.13733 | failed:  101
batch:      70360 | loss: 5.09384 | failed:  101
batch:      70370 | loss: 5.10807 | failed:  101
batch:      70380 | loss: 5.13327 | failed:  101
batch:      70390 | loss: 5.14304 | failed:  101
batch:      70400 | loss: 5.11804 | failed:  101
batch:      70410 | loss: 5.27263 | failed:  101
batch:      70420 | loss: 4.97333 | failed:  101
batch:      70430 | loss: 5.01262 | failed:  101
batch:      70440 | loss: 5.11285 | failed:  101
batch:      70450 | loss: 5.05559 | failed:  101
batch:      70460 | loss: 5.04993 | failed:  101
batch:      70470 | loss: 4.85714 | failed:  101
batch:      70480 | loss: 5.11815 | failed:  101
batch:      70490 | loss: 5.17765 | failed:  101
batch:      70500 | loss: 5.09123 | failed:  101
batch:      70510 | loss: 4.99547 | failed:  101
batch:      70520 | loss: 5.20658 | failed:  101
batch:      70530 | loss: 5.19214 | failed:  101
batch:      70540 | loss: 5.22686 | failed:  101
batch:      70550 | loss: 5.10217 | failed:  101
batch:      70560 | loss: 5.10241 | failed:  101
batch:      70570 | loss: 5.09216 | failed:  101
batch:      70580 | loss: 5.06945 | failed:  101
batch:      70590 | loss: 5.13024 | failed:  101
batch:      70600 | loss: 5.06416 | failed:  101
batch:      70610 | loss: 5.16583 | failed:  101
batch:      70620 | loss: 5.06048 | failed:  101
batch:      70630 | loss: 5.11132 | failed:  101
batch:      70640 | loss: 5.11994 | failed:  101
batch:      70650 | loss: 5.16547 | failed:  101
batch:      70660 | loss: 4.91207 | failed:  101
batch:      70670 | loss: 5.29750 | failed:  101
batch:      70680 | loss: 5.27447 | failed:  101
batch:      70690 | loss: 5.28727 | failed:  101
batch:      70700 | loss: 5.13664 | failed:  101
batch:      70710 | loss: 5.14876 | failed:  101
batch:      70720 | loss: 5.15523 | failed:  101
batch:      70730 | loss: 5.15437 | failed:  101
batch:      70740 | loss: 5.13072 | failed:  101
batch:      70750 | loss: 5.20656 | failed:  101
batch:      70760 | loss: 5.06911 | failed:  101
batch:      70770 | loss: 5.13148 | failed:  101
batch:      70780 | loss: 5.02303 | failed:  101
batch:      70790 | loss: 4.87469 | failed:  101
batch:      70800 | loss: 5.31112 | failed:  101
batch:      70810 | loss: 5.03211 | failed:  101
batch:      70820 | loss: 5.19873 | failed:  101
batch:      70830 | loss: 5.21112 | failed:  101
batch:      70840 | loss: 5.19335 | failed:  101
batch:      70850 | loss: 5.18876 | failed:  101
batch:      70860 | loss: 5.17652 | failed:  101
batch:      70870 | loss: 5.11867 | failed:  101
batch:      70880 | loss: 5.16303 | failed:  101
batch:      70890 | loss: 5.10820 | failed:  101
batch:      70900 | loss: 5.07620 | failed:  101
batch:      70910 | loss: 5.13817 | failed:  101
batch:      70920 | loss: 5.17370 | failed:  101
batch:      70930 | loss: 5.15770 | failed:  101
batch:      70940 | loss: 5.06482 | failed:  101
batch:      70950 | loss: 5.07511 | failed:  101
batch:      70960 | loss: 5.10451 | failed:  101
batch:      70970 | loss: 5.24296 | failed:  101
batch:      70980 | loss: 5.26456 | failed:  101
batch:      70990 | loss: 5.18957 | failed:  101
batch:      71000 | loss: 5.04763 | failed:  101
batch:      71010 | loss: 5.20204 | failed:  101
batch:      71020 | loss: 5.18672 | failed:  101
batch:      71030 | loss: 5.13350 | failed:  101
batch:      71040 | loss: 5.19944 | failed:  101
batch:      71050 | loss: 5.25792 | failed:  101
batch:      71060 | loss: 5.13038 | failed:  101
batch:      71070 | loss: 5.02160 | failed:  101
batch:      71080 | loss: 5.02022 | failed:  101
batch:      71090 | loss: 5.20141 | failed:  101
batch:      71100 | loss: 5.20398 | failed:  101
batch:      71110 | loss: 5.25272 | failed:  101
batch:      71120 | loss: 5.14037 | failed:  101
batch:      71130 | loss: 5.21418 | failed:  101
batch:      71140 | loss: 5.04911 | failed:  101
batch:      71150 | loss: 5.12849 | failed:  101
batch:      71160 | loss: 5.14067 | failed:  101
batch:      71170 | loss: 5.07098 | failed:  101
batch:      71180 | loss: 5.14681 | failed:  101
batch:      71190 | loss: 5.17466 | failed:  101
batch:      71200 | loss: 5.17346 | failed:  101
batch:      71210 | loss: 5.00830 | failed:  101
batch:      71220 | loss: 5.07652 | failed:  101
batch:      71230 | loss: 5.01719 | failed:  101
batch:      71240 | loss: 5.20542 | failed:  101
batch:      71250 | loss: 5.07104 | failed:  101
batch:      71260 | loss: 5.04363 | failed:  101
batch:      71270 | loss: 4.98917 | failed:  101
batch:      71280 | loss: 5.06506 | failed:  101
batch:      71290 | loss: 5.09513 | failed:  101
batch:      71300 | loss: 5.25307 | failed:  101
batch:      71310 | loss: 5.28047 | failed:  101
batch:      71320 | loss: 5.12954 | failed:  101
batch:      71330 | loss: 5.13087 | failed:  101
batch:      71340 | loss: 5.25403 | failed:  101
batch:      71350 | loss: 5.18378 | failed:  101
batch:      71360 | loss: 5.19140 | failed:  101
batch:      71370 | loss: 5.19886 | failed:  101
batch:      71380 | loss: 5.24540 | failed:  101
batch:      71390 | loss: 5.21259 | failed:  101
batch:      71400 | loss: 5.14963 | failed:  101
batch:      71410 | loss: 5.16172 | failed:  101
batch:      71420 | loss: 5.16616 | failed:  101
batch:      71430 | loss: 5.10575 | failed:  101
batch:      71440 | loss: 5.16602 | failed:  101
batch:      71450 | loss: 4.95093 | failed:  101
batch:      71460 | loss: 4.91122 | failed:  101
batch:      71470 | loss: 5.14786 | failed:  101
batch:      71480 | loss: 5.21980 | failed:  101
batch:      71490 | loss: 5.16983 | failed:  101
batch:      71500 | loss: 5.07676 | failed:  101
batch:      71510 | loss: 5.14637 | failed:  101
batch:      71520 | loss: 5.12255 | failed:  101
batch:      71530 | loss: 4.99304 | failed:  101
batch:      71540 | loss: 5.13551 | failed:  101
batch:      71550 | loss: 5.25023 | failed:  101
batch:      71560 | loss: 5.16584 | failed:  101
batch:      71570 | loss: 5.26359 | failed:  101
batch:      71580 | loss: 5.17425 | failed:  101
batch:      71590 | loss: 5.11637 | failed:  101
batch:      71600 | loss: 5.08780 | failed:  101
batch:      71610 | loss: 5.16361 | failed:  101
batch:      71620 | loss: 5.15440 | failed:  101
batch:      71630 | loss: 5.21632 | failed:  101
batch:      71640 | loss: 5.17486 | failed:  101
batch:      71650 | loss: 5.11309 | failed:  101
batch:      71660 | loss: 5.09847 | failed:  101
batch:      71670 | loss: 5.02764 | failed:  101
batch:      71680 | loss: 5.13960 | failed:  101
batch:      71690 | loss: 5.10275 | failed:  101
batch:      71700 | loss: 5.22103 | failed:  101
batch:      71710 | loss: 5.13652 | failed:  101
batch:      71720 | loss: 5.03977 | failed:  101
batch:      71730 | loss: 5.15331 | failed:  101
batch:      71740 | loss: 5.11804 | failed:  101
batch:      71750 | loss: 5.08091 | failed:  101
batch:      71760 | loss: 5.21076 | failed:  101
batch:      71770 | loss: 5.04109 | failed:  101
batch:      71780 | loss: 5.15077 | failed:  101
batch:      71790 | loss: 5.06815 | failed:  101
batch:      71800 | loss: 5.13448 | failed:  101
batch:      71810 | loss: 5.17605 | failed:  101
batch:      71820 | loss: 5.11853 | failed:  101
batch:      71830 | loss: 5.09928 | failed:  101
batch:      71840 | loss: 5.23913 | failed:  101
batch:      71850 | loss: 5.22744 | failed:  101
batch:      71860 | loss: 5.12819 | failed:  101
batch:      71870 | loss: 5.15651 | failed:  101
batch:      71880 | loss: 5.24041 | failed:  101
batch:      71890 | loss: 5.15702 | failed:  101
batch:      71900 | loss: 5.10003 | failed:  101
batch:      71910 | loss: 5.21561 | failed:  101
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      71930 | loss: 5.20364 | failed:  114
batch:      71940 | loss: 5.02104 | failed:  114
batch:      71950 | loss: 5.07476 | failed:  114
batch:      71960 | loss: 5.13625 | failed:  114
batch:      71970 | loss: 5.20547 | failed:  114
batch:      71980 | loss: 5.23881 | failed:  114
batch:      71990 | loss: 5.19522 | failed:  114
batch:      72000 | loss: 5.20329 | failed:  114
batch:      72010 | loss: 5.13617 | failed:  114
batch:      72020 | loss: 5.34202 | failed:  114
batch:      72030 | loss: 5.36301 | failed:  114
batch:      72040 | loss: 5.31432 | failed:  114
batch:      72050 | loss: 5.31429 | failed:  114
batch:      72060 | loss: 5.20319 | failed:  114
batch:      72070 | loss: 5.19225 | failed:  114
batch:      72080 | loss: 5.32626 | failed:  114
batch:      72090 | loss: 5.17942 | failed:  114
batch:      72100 | loss: 5.12698 | failed:  114
batch:      72110 | loss: 5.25040 | failed:  114
batch:      72120 | loss: 5.18565 | failed:  114
batch:      72130 | loss: 5.10507 | failed:  114
batch:      72140 | loss: 5.09101 | failed:  114
batch:      72150 | loss: 5.06192 | failed:  114
batch:      72160 | loss: 5.23258 | failed:  114
batch:      72170 | loss: 5.12749 | failed:  114
batch:      72180 | loss: 5.24908 | failed:  114
batch:      72190 | loss: 5.14688 | failed:  114
batch:      72200 | loss: 5.18762 | failed:  114
batch:      72210 | loss: 5.11352 | failed:  114
batch:      72220 | loss: 5.23684 | failed:  114
batch:      72230 | loss: 4.88896 | failed:  114
batch:      72240 | loss: 5.19838 | failed:  114
batch:      72250 | loss: 5.10952 | failed:  114
batch:      72260 | loss: 5.13277 | failed:  114
batch:      72270 | loss: 5.13900 | failed:  114
batch:      72280 | loss: 5.03526 | failed:  114
batch:      72290 | loss: 5.13463 | failed:  114
batch:      72300 | loss: 5.17451 | failed:  114
batch:      72310 | loss: 5.11319 | failed:  114
batch:      72320 | loss: 5.16955 | failed:  114
batch:      72330 | loss: 5.07433 | failed:  114
batch:      72340 | loss: 5.00894 | failed:  114
batch:      72350 | loss: 5.09360 | failed:  114
batch:      72360 | loss: 5.17460 | failed:  114
batch:      72370 | loss: 5.09374 | failed:  114
batch:      72380 | loss: 5.15063 | failed:  114
batch:      72390 | loss: 4.97852 | failed:  114
batch:      72400 | loss: 5.07237 | failed:  114
batch:      72410 | loss: 5.13319 | failed:  114
batch:      72420 | loss: 5.18995 | failed:  114
batch:      72430 | loss: 5.08508 | failed:  114
batch:      72440 | loss: 5.10956 | failed:  114
batch:      72450 | loss: 5.07095 | failed:  114
batch:      72460 | loss: 5.14084 | failed:  114
batch:      72470 | loss: 5.18787 | failed:  114
batch:      72480 | loss: 5.12150 | failed:  114
batch:      72490 | loss: 5.08048 | failed:  114
batch:      72500 | loss: 4.97412 | failed:  114
batch:      72510 | loss: 5.08838 | failed:  114
batch:      72520 | loss: 5.25192 | failed:  114
batch:      72530 | loss: 5.04754 | failed:  114
batch:      72540 | loss: 5.19897 | failed:  114
batch:      72550 | loss: 5.01398 | failed:  114
batch:      72560 | loss: 5.19377 | failed:  114
batch:      72570 | loss: 5.22082 | failed:  114
batch:      72580 | loss: 5.12005 | failed:  114
batch:      72590 | loss: 5.15328 | failed:  114
batch:      72600 | loss: 5.09540 | failed:  114
batch:      72610 | loss: 5.08928 | failed:  114
batch:      72620 | loss: 5.27632 | failed:  114
batch:      72630 | loss: 5.08165 | failed:  114
batch:      72640 | loss: 5.03264 | failed:  114
batch:      72650 | loss: 5.18043 | failed:  114
batch:      72660 | loss: 5.07702 | failed:  114
batch:      72670 | loss: 5.22404 | failed:  114
batch:      72680 | loss: 4.94157 | failed:  114
batch:      72690 | loss: 5.08068 | failed:  114
batch:      72700 | loss: 5.09914 | failed:  114
batch:      72710 | loss: 4.92746 | failed:  114
batch:      72720 | loss: 5.17028 | failed:  114
batch:      72730 | loss: 5.13460 | failed:  114
batch:      72740 | loss: 5.13762 | failed:  114
batch:      72750 | loss: 5.11709 | failed:  114
batch:      72760 | loss: 5.21563 | failed:  114
batch:      72770 | loss: 5.17767 | failed:  114
batch:      72780 | loss: 5.26857 | failed:  114
batch:      72790 | loss: 5.12673 | failed:  114
batch:      72800 | loss: 5.16100 | failed:  114
batch:      72810 | loss: 5.25688 | failed:  114
batch:      72820 | loss: 5.16559 | failed:  114
batch:      72830 | loss: 5.11825 | failed:  114
batch:      72840 | loss: 5.18206 | failed:  114
batch:      72850 | loss: 5.15902 | failed:  114
batch:      72860 | loss: 5.25680 | failed:  114
batch:      72870 | loss: 5.26996 | failed:  114
batch:      72880 | loss: 5.14647 | failed:  114
batch:      72890 | loss: 5.10806 | failed:  114
batch:      72900 | loss: 5.07863 | failed:  114
batch:      72910 | loss: 5.17163 | failed:  114
batch:      72920 | loss: 5.25806 | failed:  114
batch:      72930 | loss: 5.05838 | failed:  114
batch:      72940 | loss: 5.22528 | failed:  114
batch:      72950 | loss: 5.00535 | failed:  114
batch:      72960 | loss: 5.25037 | failed:  114
batch:      72970 | loss: 5.08161 | failed:  114
batch:      72980 | loss: 5.25589 | failed:  114
batch:      72990 | loss: 4.81980 | failed:  114
batch:      73000 | loss: 4.91065 | failed:  114
batch:      73010 | loss: 5.06328 | failed:  114
batch:      73020 | loss: 5.18941 | failed:  114
batch:      73030 | loss: 5.08431 | failed:  114
batch:      73040 | loss: 5.12070 | failed:  114
batch:      73050 | loss: 5.16600 | failed:  114
batch:      73060 | loss: 5.20715 | failed:  114
batch:      73070 | loss: 5.19353 | failed:  114
batch:      73080 | loss: 5.20000 | failed:  114
batch:      73090 | loss: 5.07889 | failed:  114
batch:      73100 | loss: 5.21467 | failed:  114
batch:      73110 | loss: 5.18041 | failed:  114
batch:      73120 | loss: 5.26917 | failed:  114
batch:      73130 | loss: 5.21473 | failed:  114
batch:      73140 | loss: 5.17634 | failed:  114
batch:      73150 | loss: 5.23887 | failed:  114
batch:      73160 | loss: 5.05277 | failed:  114
batch:      73170 | loss: 5.22104 | failed:  114
batch:      73180 | loss: 5.14568 | failed:  114
batch:      73190 | loss: 5.14041 | failed:  114
batch:      73200 | loss: 5.16069 | failed:  114
batch:      73210 | loss: 5.07692 | failed:  114
batch:      73220 | loss: 5.14017 | failed:  114
batch:      73230 | loss: 4.96892 | failed:  114
batch:      73240 | loss: 5.25935 | failed:  114
batch:      73250 | loss: 5.21612 | failed:  114
batch:      73260 | loss: 5.23746 | failed:  114
batch:      73270 | loss: 5.17098 | failed:  114
batch:      73280 | loss: 5.16214 | failed:  114
batch:      73290 | loss: 5.19835 | failed:  114
batch:      73300 | loss: 5.18667 | failed:  114
batch:      73310 | loss: 5.18613 | failed:  114
batch:      73320 | loss: 5.19400 | failed:  114
batch:      73330 | loss: 5.19072 | failed:  114
batch:      73340 | loss: 5.03511 | failed:  114
batch:      73350 | loss: 5.05451 | failed:  114
batch:      73360 | loss: 4.93500 | failed:  114
batch:      73370 | loss: 5.18524 | failed:  114
batch:      73380 | loss: 5.24288 | failed:  114
batch:      73390 | loss: 5.19185 | failed:  114
batch:      73400 | loss: 5.04899 | failed:  114
batch:      73410 | loss: 5.09354 | failed:  114
batch:      73420 | loss: 5.14789 | failed:  114
batch:      73430 | loss: 5.00918 | failed:  114
batch:      73440 | loss: 5.11244 | failed:  114
batch:      73450 | loss: 5.07992 | failed:  114
batch:      73460 | loss: 5.28047 | failed:  114
batch:      73470 | loss: 5.14490 | failed:  114
batch:      73480 | loss: 4.96621 | failed:  114
batch:      73490 | loss: 5.20872 | failed:  114
batch:      73500 | loss: 5.03165 | failed:  114
batch:      73510 | loss: 5.15603 | failed:  114
batch:      73520 | loss: 5.05758 | failed:  114
batch:      73530 | loss: 5.06804 | failed:  114
batch:      73540 | loss: 5.24090 | failed:  114
batch:      73550 | loss: 5.26208 | failed:  114
batch:      73560 | loss: 5.16967 | failed:  114
batch:      73570 | loss: 5.19428 | failed:  114
batch:      73580 | loss: 5.12441 | failed:  114
batch:      73590 | loss: 4.98039 | failed:  114
batch:      73600 | loss: 5.10448 | failed:  114
batch:      73610 | loss: 5.18895 | failed:  114
batch:      73620 | loss: 5.00824 | failed:  114
batch:      73630 | loss: 4.81179 | failed:  114
batch:      73640 | loss: 5.13382 | failed:  114
batch:      73650 | loss: 5.08849 | failed:  114
batch:      73660 | loss: 5.26745 | failed:  114
batch:      73670 | loss: 5.04044 | failed:  114
batch:      73680 | loss: 5.15795 | failed:  114
batch:      73690 | loss: 5.18440 | failed:  114
batch:      73700 | loss: 5.25664 | failed:  114
batch:      73710 | loss: 5.05576 | failed:  114
batch:      73720 | loss: 5.11579 | failed:  114
batch:      73730 | loss: 5.17365 | failed:  114
batch:      73740 | loss: 5.14736 | failed:  114
batch:      73750 | loss: 5.16594 | failed:  114
batch:      73760 | loss: 5.22487 | failed:  114
batch:      73770 | loss: 5.19125 | failed:  114
batch:      73780 | loss: 4.99356 | failed:  114
batch:      73790 | loss: 5.02835 | failed:  114
batch:      73800 | loss: 5.17323 | failed:  114
batch:      73810 | loss: 5.04623 | failed:  114
batch:      73820 | loss: 5.13788 | failed:  114
batch:      73830 | loss: 5.16504 | failed:  114
batch:      73840 | loss: 5.16258 | failed:  114
batch:      73850 | loss: 5.19230 | failed:  114
batch:      73860 | loss: 5.09199 | failed:  114
batch:      73870 | loss: 5.08989 | failed:  114
batch:      73880 | loss: 5.05402 | failed:  114
batch:      73890 | loss: 5.03592 | failed:  114
batch:      73900 | loss: 5.20970 | failed:  114
batch:      73910 | loss: 5.25277 | failed:  114
batch:      73920 | loss: 5.07919 | failed:  114
batch:      73930 | loss: 4.95638 | failed:  114
batch:      73940 | loss: 5.17637 | failed:  114
batch:      73950 | loss: 4.99795 | failed:  114
batch:      73960 | loss: 5.05824 | failed:  114
batch:      73970 | loss: 5.21641 | failed:  114
batch:      73980 | loss: 5.16155 | failed:  114
batch:      73990 | loss: 5.05805 | failed:  114
batch:      74000 | loss: 5.11289 | failed:  114
batch:      74010 | loss: 5.25158 | failed:  114
batch:      74020 | loss: 5.12352 | failed:  114
batch:      74030 | loss: 4.96340 | failed:  114
batch:      74040 | loss: 5.21503 | failed:  114
batch:      74050 | loss: 5.13106 | failed:  114
batch:      74060 | loss: 5.11659 | failed:  114
batch:      74070 | loss: 5.03151 | failed:  114
batch:      74080 | loss: 5.21072 | failed:  114
batch:      74090 | loss: 4.89752 | failed:  114
batch:      74100 | loss: 5.23496 | failed:  114
batch:      74110 | loss: 5.23558 | failed:  114
batch:      74120 | loss: 5.16603 | failed:  114
batch:      74130 | loss: 5.16255 | failed:  114
batch:      74140 | loss: 5.19339 | failed:  114
batch:      74150 | loss: 5.04863 | failed:  114
batch:      74160 | loss: 5.06813 | failed:  114
batch:      74170 | loss: 5.22191 | failed:  114
batch:      74180 | loss: 4.99258 | failed:  114
batch:      74190 | loss: 4.98052 | failed:  114
batch:      74200 | loss: 5.14150 | failed:  114
batch:      74210 | loss: 5.11588 | failed:  114
batch:      74220 | loss: 4.99747 | failed:  114
batch:      74230 | loss: 5.05795 | failed:  114
batch:      74240 | loss: 5.00711 | failed:  114
batch:      74250 | loss: 5.16838 | failed:  114
batch:      74260 | loss: 5.14083 | failed:  114
batch:      74270 | loss: 5.18593 | failed:  114
batch:      74280 | loss: 5.11687 | failed:  114
batch:      74290 | loss: 5.05005 | failed:  114
batch:      74300 | loss: 5.21718 | failed:  114
batch:      74310 | loss: 5.15478 | failed:  114
batch:      74320 | loss: 4.97368 | failed:  114
batch:      74330 | loss: 5.09506 | failed:  114
batch:      74340 | loss: 5.11036 | failed:  114
batch:      74350 | loss: 5.11013 | failed:  114
batch:      74360 | loss: 4.96069 | failed:  114
batch:      74370 | loss: 5.24727 | failed:  114
batch:      74380 | loss: 5.22834 | failed:  114
batch:      74390 | loss: 5.02252 | failed:  114
batch:      74400 | loss: 5.07149 | failed:  114
batch:      74410 | loss: 5.26155 | failed:  114
batch:      74420 | loss: 5.25034 | failed:  114
batch:      74430 | loss: 5.19507 | failed:  114
batch:      74440 | loss: 5.12498 | failed:  114
batch:      74450 | loss: 5.13951 | failed:  114
batch:      74460 | loss: 5.17152 | failed:  114
batch:      74470 | loss: 4.83366 | failed:  114
batch:      74480 | loss: 5.18316 | failed:  114
batch:      74490 | loss: 5.20389 | failed:  114
batch:      74500 | loss: 5.15196 | failed:  114
batch:      74510 | loss: 4.85211 | failed:  114
batch:      74520 | loss: 5.11853 | failed:  114
batch:      74530 | loss: 5.09703 | failed:  114
batch:      74540 | loss: 5.09887 | failed:  114
batch:      74550 | loss: 5.23077 | failed:  114
batch:      74560 | loss: 5.25184 | failed:  114
batch:      74570 | loss: 5.06796 | failed:  114
batch:      74580 | loss: 4.89929 | failed:  114
batch:      74590 | loss: 5.25169 | failed:  114
batch:      74600 | loss: 5.16595 | failed:  114
batch:      74610 | loss: 4.69071 | failed:  114
batch:      74620 | loss: 5.23268 | failed:  114
batch:      74630 | loss: 5.06123 | failed:  114
batch:      74640 | loss: 5.17385 | failed:  114
batch:      74650 | loss: 5.07559 | failed:  114
batch:      74660 | loss: 5.07791 | failed:  114
batch:      74670 | loss: 5.14754 | failed:  114
batch:      74680 | loss: 5.14285 | failed:  114
batch:      74690 | loss: 4.99467 | failed:  114
batch:      74700 | loss: 5.04298 | failed:  114
batch:      74710 | loss: 5.08303 | failed:  114
batch:      74720 | loss: 5.33547 | failed:  114
batch:      74730 | loss: 5.17646 | failed:  114
batch:      74740 | loss: 5.17359 | failed:  114
batch:      74750 | loss: 5.30671 | failed:  114
batch:      74760 | loss: 5.21456 | failed:  114
batch:      74770 | loss: 5.21141 | failed:  114
batch:      74780 | loss: 5.17991 | failed:  114
batch:      74790 | loss: 5.10230 | failed:  114
batch:      74800 | loss: 5.09964 | failed:  114
batch:      74810 | loss: 5.19605 | failed:  114
batch:      74820 | loss: 5.46729 | failed:  114
batch:      74830 | loss: 5.13082 | failed:  114
batch:      74840 | loss: 5.08189 | failed:  114
batch:      74850 | loss: 4.98952 | failed:  114
batch:      74860 | loss: 5.23515 | failed:  114
batch:      74870 | loss: 5.22585 | failed:  114
batch:      74880 | loss: 5.18204 | failed:  114
batch:      74890 | loss: 5.17161 | failed:  114
batch:      74900 | loss: 5.21836 | failed:  114
batch:      74910 | loss: 5.16965 | failed:  114
batch:      74920 | loss: 4.96317 | failed:  114
batch:      74930 | loss: 5.03455 | failed:  114
batch:      74940 | loss: 5.18102 | failed:  114
batch:      74950 | loss: 4.94412 | failed:  114
batch:      74960 | loss: 5.09707 | failed:  114
batch:      74970 | loss: 5.11512 | failed:  114
batch:      74980 | loss: 5.11056 | failed:  114
batch:      74990 | loss: 5.06896 | failed:  114
batch:      75000 | loss: 5.17614 | failed:  114
batch:      75010 | loss: 5.07532 | failed:  114
batch:      75020 | loss: 5.10248 | failed:  114
batch:      75030 | loss: 5.05756 | failed:  114
batch:      75040 | loss: 5.09192 | failed:  114
batch:      75050 | loss: 5.10231 | failed:  114
batch:      75060 | loss: 5.01626 | failed:  114
batch:      75070 | loss: 5.28148 | failed:  114
batch:      75080 | loss: 5.13028 | failed:  114
batch:      75090 | loss: 4.65203 | failed:  114
batch:      75100 | loss: 5.17324 | failed:  114
batch:      75110 | loss: 5.09807 | failed:  114
batch:      75120 | loss: 5.03415 | failed:  114
batch:      75130 | loss: 5.12198 | failed:  114
batch:      75140 | loss: 5.07417 | failed:  114
batch:      75150 | loss: 5.07120 | failed:  114
batch:      75160 | loss: 5.11370 | failed:  114
batch:      75170 | loss: 5.10083 | failed:  114
batch:      75180 | loss: 4.96773 | failed:  114
batch:      75190 | loss: 4.97320 | failed:  114
batch:      75200 | loss: 5.19762 | failed:  114
batch:      75210 | loss: 5.04638 | failed:  114
batch:      75220 | loss: 5.10317 | failed:  114
batch:      75230 | loss: 5.19346 | failed:  114
batch:      75240 | loss: 5.15335 | failed:  114
batch:      75250 | loss: 5.25622 | failed:  114
batch:      75260 | loss: 4.96612 | failed:  114
batch:      75270 | loss: 5.07436 | failed:  114
batch:      75280 | loss: 5.20996 | failed:  114
batch:      75290 | loss: 5.08878 | failed:  114
batch:      75300 | loss: 5.21343 | failed:  114
batch:      75310 | loss: 5.37046 | failed:  114
batch:      75320 | loss: 5.13150 | failed:  114
batch:      75330 | loss: 5.17193 | failed:  114
batch:      75340 | loss: 5.12087 | failed:  114
batch:      75350 | loss: 5.17435 | failed:  114
batch:      75360 | loss: 5.18963 | failed:  114
batch:      75370 | loss: 5.22475 | failed:  114
batch:      75380 | loss: 5.15485 | failed:  114
batch:      75390 | loss: 5.10649 | failed:  114
batch:      75400 | loss: 5.20915 | failed:  114
batch:      75410 | loss: 5.19777 | failed:  114
batch:      75420 | loss: 5.17123 | failed:  114
batch:      75430 | loss: 5.15124 | failed:  114
batch:      75440 | loss: 5.18628 | failed:  114
batch:      75450 | loss: 5.16153 | failed:  114
batch:      75460 | loss: 5.13596 | failed:  114
batch:      75470 | loss: 5.09592 | failed:  114
batch:      75480 | loss: 5.11980 | failed:  114
batch:      75490 | loss: 5.18193 | failed:  114
batch:      75500 | loss: 5.14457 | failed:  114
batch:      75510 | loss: 5.16141 | failed:  114
batch:      75520 | loss: 5.07225 | failed:  114
batch:      75530 | loss: 5.12074 | failed:  114
batch:      75540 | loss: 4.96175 | failed:  114
batch:      75550 | loss: 5.00863 | failed:  114
batch:      75560 | loss: 5.04120 | failed:  114
batch:      75570 | loss: 5.15833 | failed:  114
batch:      75580 | loss: 5.16126 | failed:  114
batch:      75590 | loss: 5.17206 | failed:  114
batch:      75600 | loss: 5.17325 | failed:  114
batch:      75610 | loss: 5.20518 | failed:  114
batch:      75620 | loss: 5.16903 | failed:  114
batch:      75630 | loss: 5.10831 | failed:  114
batch:      75640 | loss: 5.26255 | failed:  114
batch:      75650 | loss: 4.89638 | failed:  114
batch:      75660 | loss: 5.04157 | failed:  114
batch:      75670 | loss: 5.20876 | failed:  114
batch:      75680 | loss: 5.20159 | failed:  114
batch:      75690 | loss: 5.15770 | failed:  114
batch:      75700 | loss: 5.18133 | failed:  114
batch:      75710 | loss: 5.18414 | failed:  114
batch:      75720 | loss: 5.17690 | failed:  114
batch:      75730 | loss: 5.15467 | failed:  114
batch:      75740 | loss: 5.16844 | failed:  114
batch:      75750 | loss: 5.14074 | failed:  114
batch:      75760 | loss: 5.19240 | failed:  114
batch:      75770 | loss: 5.19062 | failed:  114
batch:      75780 | loss: 4.97047 | failed:  114
batch:      75790 | loss: 5.20343 | failed:  114
batch:      75800 | loss: 5.16362 | failed:  114
batch:      75810 | loss: 5.19492 | failed:  114
batch:      75820 | loss: 5.05482 | failed:  114
batch:      75830 | loss: 5.22568 | failed:  114
batch:      75840 | loss: 5.04566 | failed:  114
batch:      75850 | loss: 5.22470 | failed:  114
batch:      75860 | loss: 5.18202 | failed:  114
batch:      75870 | loss: 4.83827 | failed:  114
batch:      75880 | loss: 5.20509 | failed:  114
batch:      75890 | loss: 5.13554 | failed:  114
batch:      75900 | loss: 5.16176 | failed:  114
batch:      75910 | loss: 4.91994 | failed:  114
batch:      75920 | loss: 5.10945 | failed:  114
batch:      75930 | loss: 5.21637 | failed:  114
batch:      75940 | loss: 5.05396 | failed:  114
batch:      75950 | loss: 5.01143 | failed:  114
batch:      75960 | loss: 5.11776 | failed:  114
batch:      75970 | loss: 5.21755 | failed:  114
batch:      75980 | loss: 5.23413 | failed:  114
batch:      75990 | loss: 5.17283 | failed:  114
batch:      76000 | loss: 5.11019 | failed:  114
batch:      76010 | loss: 5.29415 | failed:  114
batch:      76020 | loss: 4.83238 | failed:  114
batch:      76030 | loss: 5.12773 | failed:  114
batch:      76040 | loss: 5.20384 | failed:  114
batch:      76050 | loss: 5.15350 | failed:  114
batch:      76060 | loss: 5.18015 | failed:  114
batch:      76070 | loss: 5.17320 | failed:  114
batch:      76080 | loss: 5.09328 | failed:  114
batch:      76090 | loss: 5.00194 | failed:  114
batch:      76100 | loss: 5.16069 | failed:  114
batch:      76110 | loss: 5.20933 | failed:  114
batch:      76120 | loss: 5.24178 | failed:  114
batch:      76130 | loss: 5.21722 | failed:  114
batch:      76140 | loss: 5.09151 | failed:  114
batch:      76150 | loss: 5.20417 | failed:  114
batch:      76160 | loss: 5.19186 | failed:  114
batch:      76170 | loss: 5.23012 | failed:  114
batch:      76180 | loss: 5.23793 | failed:  114
batch:      76190 | loss: 5.23539 | failed:  114
batch:      76200 | loss: 5.04817 | failed:  114
batch:      76210 | loss: 5.14110 | failed:  114
batch:      76220 | loss: 5.05702 | failed:  114
batch:      76230 | loss: 5.05693 | failed:  114
batch:      76240 | loss: 4.94272 | failed:  114
batch:      76250 | loss: 5.14725 | failed:  114
batch:      76260 | loss: 5.10681 | failed:  114
batch:      76270 | loss: 5.09253 | failed:  114
batch:      76280 | loss: 5.05286 | failed:  114
batch:      76290 | loss: 5.19657 | failed:  114
batch:      76300 | loss: 5.15087 | failed:  114
batch:      76310 | loss: 5.07513 | failed:  114
batch:      76320 | loss: 5.20376 | failed:  114
batch:      76330 | loss: 5.18768 | failed:  114
batch:      76340 | loss: 5.24431 | failed:  114
batch:      76350 | loss: 5.15819 | failed:  114
batch:      76360 | loss: 5.00015 | failed:  114
batch:      76370 | loss: 5.23012 | failed:  114
batch:      76380 | loss: 5.22725 | failed:  114
batch:      76390 | loss: 5.17983 | failed:  114
batch:      76400 | loss: 5.07689 | failed:  114
batch:      76410 | loss: 4.96964 | failed:  114
batch:      76420 | loss: 5.18429 | failed:  114
batch:      76430 | loss: 5.20533 | failed:  114
batch:      76440 | loss: 5.19086 | failed:  114
batch:      76450 | loss: 5.12829 | failed:  114
batch:      76460 | loss: 5.19487 | failed:  114
batch:      76470 | loss: 5.12002 | failed:  114
batch:      76480 | loss: 5.22061 | failed:  114
batch:      76490 | loss: 5.29756 | failed:  114
batch:      76500 | loss: 5.12524 | failed:  114
batch:      76510 | loss: 5.05544 | failed:  114
batch:      76520 | loss: 4.44148 | failed:  114
batch:      76530 | loss: 5.18506 | failed:  114
batch:      76540 | loss: 4.86091 | failed:  114
batch:      76550 | loss: 5.24509 | failed:  114
batch:      76560 | loss: 5.22437 | failed:  114
batch:      76570 | loss: 5.26184 | failed:  114
batch:      76580 | loss: 4.97053 | failed:  114
batch:      76590 | loss: 5.06926 | failed:  114
batch:      76600 | loss: 5.12691 | failed:  114
batch:      76610 | loss: 5.25958 | failed:  114
batch:      76620 | loss: 5.16160 | failed:  114
batch:      76630 | loss: 5.16000 | failed:  114
batch:      76640 | loss: 5.07464 | failed:  114
batch:      76650 | loss: 5.22255 | failed:  114
batch:      76660 | loss: 5.09970 | failed:  114
batch:      76670 | loss: 5.10833 | failed:  114
batch:      76680 | loss: 5.05383 | failed:  114
batch:      76690 | loss: 4.96857 | failed:  114
batch:      76700 | loss: 5.05688 | failed:  114
batch:      76710 | loss: 5.16794 | failed:  114
batch:      76720 | loss: 5.13828 | failed:  114
batch:      76730 | loss: 5.20278 | failed:  114
batch:      76740 | loss: 5.16307 | failed:  114
batch:      76750 | loss: 5.20821 | failed:  114
batch:      76760 | loss: 5.17153 | failed:  114
batch:      76770 | loss: 5.19130 | failed:  114
batch:      76780 | loss: 5.12149 | failed:  114
batch:      76790 | loss: 5.13687 | failed:  114
batch:      76800 | loss: 4.92634 | failed:  114
batch:      76810 | loss: 5.21551 | failed:  114
batch:      76820 | loss: 5.08114 | failed:  114
batch:      76830 | loss: 4.98657 | failed:  114
batch:      76840 | loss: 5.16368 | failed:  114
batch:      76850 | loss: 5.24322 | failed:  114
batch:      76860 | loss: 5.15360 | failed:  114
batch:      76870 | loss: 5.13364 | failed:  114
batch:      76880 | loss: 5.19695 | failed:  114
batch:      76890 | loss: 5.19138 | failed:  114
batch:      76900 | loss: 5.20328 | failed:  114
batch:      76910 | loss: 4.96816 | failed:  114
batch:      76920 | loss: 5.02872 | failed:  114
batch:      76930 | loss: 4.72604 | failed:  114
batch:      76940 | loss: 5.16934 | failed:  114
batch:      76950 | loss: 4.98837 | failed:  114
batch:      76960 | loss: 4.89226 | failed:  114
batch:      76970 | loss: 4.82047 | failed:  114
batch:      76980 | loss: 5.30796 | failed:  114
batch:      76990 | loss: 5.26094 | failed:  114
batch:      77000 | loss: 4.98925 | failed:  114
batch:      77010 | loss: 5.06896 | failed:  114
batch:      77020 | loss: 4.99086 | failed:  114
batch:      77030 | loss: 5.13012 | failed:  114
batch:      77040 | loss: 5.13793 | failed:  114
batch:      77050 | loss: 4.93625 | failed:  114
batch:      77060 | loss: 5.11430 | failed:  114
batch:      77070 | loss: 5.21090 | failed:  114
batch:      77080 | loss: 5.04671 | failed:  114
batch:      77090 | loss: 4.83543 | failed:  114
batch:      77100 | loss: 4.97533 | failed:  114
batch:      77110 | loss: 5.16332 | failed:  114
batch:      77120 | loss: 5.14512 | failed:  114
batch:      77130 | loss: 5.12676 | failed:  114
batch:      77140 | loss: 5.13265 | failed:  114
batch:      77150 | loss: 4.99875 | failed:  114
batch:      77160 | loss: 5.09549 | failed:  114
batch:      77170 | loss: 5.18602 | failed:  114
batch:      77180 | loss: 5.06050 | failed:  114
batch:      77190 | loss: 4.82497 | failed:  114
batch:      77200 | loss: 5.12338 | failed:  114
batch:      77210 | loss: 4.99376 | failed:  114
batch:      77220 | loss: 4.98739 | failed:  114
batch:      77230 | loss: 4.92899 | failed:  114
batch:      77240 | loss: 4.82936 | failed:  114
batch:      77250 | loss: 5.20464 | failed:  114
batch:      77260 | loss: 5.14937 | failed:  114
batch:      77270 | loss: 5.20416 | failed:  114
batch:      77280 | loss: 5.06873 | failed:  114
batch:      77290 | loss: 5.19606 | failed:  114
batch:      77300 | loss: 5.28818 | failed:  114
batch:      77310 | loss: 5.26706 | failed:  114
batch:      77320 | loss: 5.15161 | failed:  114
batch:      77330 | loss: 5.22990 | failed:  114
batch:      77340 | loss: 5.19960 | failed:  114
batch:      77350 | loss: 5.23740 | failed:  114
batch:      77360 | loss: 5.10009 | failed:  114
batch:      77370 | loss: 5.19924 | failed:  114
batch:      77380 | loss: 5.27657 | failed:  114
batch:      77390 | loss: 5.16340 | failed:  114
batch:      77400 | loss: 5.15456 | failed:  114
batch:      77410 | loss: 5.20040 | failed:  114
batch:      77420 | loss: 5.18273 | failed:  114
batch:      77430 | loss: 5.18955 | failed:  114
batch:      77440 | loss: 5.15681 | failed:  114
batch:      77450 | loss: 4.97674 | failed:  114
batch:      77460 | loss: 5.15589 | failed:  114
batch:      77470 | loss: 5.14911 | failed:  114
batch:      77480 | loss: 5.18727 | failed:  114
batch:      77490 | loss: 5.06821 | failed:  114
batch:      77500 | loss: 4.86284 | failed:  114
batch:      77510 | loss: 5.14702 | failed:  114
batch:      77520 | loss: 4.98446 | failed:  114
batch:      77530 | loss: 5.11056 | failed:  114
batch:      77540 | loss: 5.15083 | failed:  114
batch:      77550 | loss: 5.09042 | failed:  114
batch:      77560 | loss: 5.10445 | failed:  114
batch:      77570 | loss: 5.25188 | failed:  114
batch:      77580 | loss: 5.18377 | failed:  114
batch:      77590 | loss: 5.09485 | failed:  114
batch:      77600 | loss: 5.22016 | failed:  114
batch:      77610 | loss: 5.20129 | failed:  114
batch:      77620 | loss: 5.14881 | failed:  114
batch:      77630 | loss: 5.12187 | failed:  114
batch:      77640 | loss: 5.01848 | failed:  114
batch:      77650 | loss: 5.01800 | failed:  114
batch:      77660 | loss: 4.98935 | failed:  114
batch:      77670 | loss: 5.14463 | failed:  114
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      77680 | loss: 5.10491 | failed:  115
batch:      77690 | loss: 5.14256 | failed:  115
batch:      77700 | loss: 5.19030 | failed:  115
batch:      77710 | loss: 5.05639 | failed:  115
batch:      77720 | loss: 5.18994 | failed:  115
batch:      77730 | loss: 5.09187 | failed:  115
batch:      77740 | loss: 5.21346 | failed:  115
batch:      77750 | loss: 4.84355 | failed:  115
batch:      77760 | loss: 5.26466 | failed:  115
batch:      77770 | loss: 5.03964 | failed:  115
batch:      77780 | loss: 5.08439 | failed:  115
batch:      77790 | loss: 5.23645 | failed:  115
batch:      77800 | loss: 5.13712 | failed:  115
batch:      77810 | loss: 5.28155 | failed:  115
batch:      77820 | loss: 5.24261 | failed:  115
batch:      77830 | loss: 5.17074 | failed:  115
batch:      77840 | loss: 5.13680 | failed:  115
batch:      77850 | loss: 5.15173 | failed:  115
batch:      77860 | loss: 5.10142 | failed:  115
batch:      77870 | loss: 5.30859 | failed:  115
batch:      77880 | loss: 5.18158 | failed:  115
batch:      77890 | loss: 5.18107 | failed:  115
batch:      77900 | loss: 5.20267 | failed:  115
batch:      77910 | loss: 5.07817 | failed:  115
batch:      77920 | loss: 5.20920 | failed:  115
batch:      77930 | loss: 5.19549 | failed:  115
batch:      77940 | loss: 5.07495 | failed:  115
batch:      77950 | loss: 5.12877 | failed:  115
batch:      77960 | loss: 5.12554 | failed:  115
batch:      77970 | loss: 5.20510 | failed:  115
batch:      77980 | loss: 5.23957 | failed:  115
batch:      77990 | loss: 5.23072 | failed:  115
batch:      78000 | loss: 5.18678 | failed:  115
batch:      78010 | loss: 5.19868 | failed:  115
batch:      78020 | loss: 5.39371 | failed:  115
batch:      78030 | loss: 5.16314 | failed:  115
batch:      78040 | loss: 5.21323 | failed:  115
batch:      78050 | loss: 4.96308 | failed:  115
batch:      78060 | loss: 5.10267 | failed:  115
batch:      78070 | loss: 5.18852 | failed:  115
batch:      78080 | loss: 5.22270 | failed:  115
batch:      78090 | loss: 5.19951 | failed:  115
batch:      78100 | loss: 5.06877 | failed:  115
batch:      78110 | loss: 5.19223 | failed:  115
batch:      78120 | loss: 5.12208 | failed:  115
batch:      78130 | loss: 5.11980 | failed:  115
batch:      78140 | loss: 5.26716 | failed:  115
batch:      78150 | loss: 5.15465 | failed:  115
batch:      78160 | loss: 5.19055 | failed:  115
batch:      78170 | loss: 5.21562 | failed:  115
batch:      78180 | loss: 5.21133 | failed:  115
batch:      78190 | loss: 5.19635 | failed:  115
batch:      78200 | loss: 5.19013 | failed:  115
batch:      78210 | loss: 5.23877 | failed:  115
batch:      78220 | loss: 5.22209 | failed:  115
batch:      78230 | loss: 5.09035 | failed:  115
batch:      78240 | loss: 5.18613 | failed:  115
batch:      78250 | loss: 5.14710 | failed:  115
batch:      78260 | loss: 5.16492 | failed:  115
batch:      78270 | loss: 5.03117 | failed:  115
batch:      78280 | loss: 5.12419 | failed:  115
batch:      78290 | loss: 5.19829 | failed:  115
batch:      78300 | loss: 5.05205 | failed:  115
batch:      78310 | loss: 5.00829 | failed:  115
batch:      78320 | loss: 5.20754 | failed:  115
batch:      78330 | loss: 5.07059 | failed:  115
batch:      78340 | loss: 5.16475 | failed:  115
batch:      78350 | loss: 5.20616 | failed:  115
batch:      78360 | loss: 5.22479 | failed:  115
batch:      78370 | loss: 5.13609 | failed:  115
batch:      78380 | loss: 5.06562 | failed:  115
batch:      78390 | loss: 5.05190 | failed:  115
batch:      78400 | loss: 5.22234 | failed:  115
batch:      78410 | loss: 5.26696 | failed:  115
batch:      78420 | loss: 5.18092 | failed:  115
batch:      78430 | loss: 5.07677 | failed:  115
batch:      78440 | loss: 5.19395 | failed:  115
batch:      78450 | loss: 5.20282 | failed:  115
batch:      78460 | loss: 5.21159 | failed:  115
batch:      78470 | loss: 5.11777 | failed:  115
batch:      78480 | loss: 4.91595 | failed:  115
batch:      78490 | loss: 5.16020 | failed:  115
batch:      78500 | loss: 4.89797 | failed:  115
batch:      78510 | loss: 5.12910 | failed:  115
batch:      78520 | loss: 5.29786 | failed:  115
batch:      78530 | loss: 5.27574 | failed:  115
batch:      78540 | loss: 5.27383 | failed:  115
batch:      78550 | loss: 5.14713 | failed:  115
batch:      78560 | loss: 5.22250 | failed:  115
batch:      78570 | loss: 5.10909 | failed:  115
batch:      78580 | loss: 5.09104 | failed:  115
batch:      78590 | loss: 5.15925 | failed:  115
batch:      78600 | loss: 5.18121 | failed:  115
batch:      78610 | loss: 5.22844 | failed:  115
batch:      78620 | loss: 5.11276 | failed:  115
batch:      78630 | loss: 5.08213 | failed:  115
batch:      78640 | loss: 4.99877 | failed:  115
batch:      78650 | loss: 5.22838 | failed:  115
batch:      78660 | loss: 5.26048 | failed:  115
batch:      78670 | loss: 5.19470 | failed:  115
batch:      78680 | loss: 5.20084 | failed:  115
batch:      78690 | loss: 5.20024 | failed:  115
batch:      78700 | loss: 5.08719 | failed:  115
batch:      78710 | loss: 5.13828 | failed:  115
batch:      78720 | loss: 5.20323 | failed:  115
batch:      78730 | loss: 5.26522 | failed:  115
batch:      78740 | loss: 5.23585 | failed:  115
batch:      78750 | loss: 5.22567 | failed:  115
batch:      78760 | loss: 5.14065 | failed:  115
batch:      78770 | loss: 5.20343 | failed:  115
batch:      78780 | loss: 5.26225 | failed:  115
batch:      78790 | loss: 5.17089 | failed:  115
batch:      78800 | loss: 5.06582 | failed:  115
batch:      78810 | loss: 4.95822 | failed:  115
batch:      78820 | loss: 5.22833 | failed:  115
batch:      78830 | loss: 5.20642 | failed:  115
batch:      78840 | loss: 5.16349 | failed:  115
batch:      78850 | loss: 5.08304 | failed:  115
batch:      78860 | loss: 5.06958 | failed:  115
batch:      78870 | loss: 5.19111 | failed:  115
batch:      78880 | loss: 4.98227 | failed:  115
batch:      78890 | loss: 5.15974 | failed:  115
batch:      78900 | loss: 5.22512 | failed:  115
batch:      78910 | loss: 5.11537 | failed:  115
batch:      78920 | loss: 5.10175 | failed:  115
batch:      78930 | loss: 5.23205 | failed:  115
batch:      78940 | loss: 5.24345 | failed:  115
batch:      78950 | loss: 4.21227 | failed:  115
batch:      78960 | loss: 5.17736 | failed:  115
batch:      78970 | loss: 5.18881 | failed:  115
batch:      78980 | loss: 5.14556 | failed:  115
batch:      78990 | loss: 5.21328 | failed:  115
batch:      79000 | loss: 5.02144 | failed:  115
batch:      79010 | loss: 5.04933 | failed:  115
batch:      79020 | loss: 5.24387 | failed:  115
batch:      79030 | loss: 5.10230 | failed:  115
batch:      79040 | loss: 5.18784 | failed:  115
batch:      79050 | loss: 5.18452 | failed:  115
batch:      79060 | loss: 5.05785 | failed:  115
batch:      79070 | loss: 5.14026 | failed:  115
batch:      79080 | loss: 5.16168 | failed:  115
batch:      79090 | loss: 5.02566 | failed:  115
batch:      79100 | loss: 5.00044 | failed:  115
batch:      79110 | loss: 5.14884 | failed:  115
batch:      79120 | loss: 5.14852 | failed:  115
batch:      79130 | loss: 5.18561 | failed:  115
batch:      79140 | loss: 5.27291 | failed:  115
batch:      79150 | loss: 5.21766 | failed:  115
batch:      79160 | loss: 5.15281 | failed:  115
batch:      79170 | loss: 4.96755 | failed:  115
batch:      79180 | loss: 4.94652 | failed:  115
batch:      79190 | loss: 5.01453 | failed:  115
batch:      79200 | loss: 5.19375 | failed:  115
batch:      79210 | loss: 5.21761 | failed:  115
batch:      79220 | loss: 5.12614 | failed:  115
batch:      79230 | loss: 5.12052 | failed:  115
batch:      79240 | loss: 4.90020 | failed:  115
batch:      79250 | loss: 5.11868 | failed:  115
batch:      79260 | loss: 5.08394 | failed:  115
batch:      79270 | loss: 5.15323 | failed:  115
batch:      79280 | loss: 5.14996 | failed:  115
batch:      79290 | loss: 5.21241 | failed:  115
batch:      79300 | loss: 4.71811 | failed:  115
batch:      79310 | loss: 5.15655 | failed:  115
batch:      79320 | loss: 5.14875 | failed:  115
batch:      79330 | loss: 5.26871 | failed:  115
batch:      79340 | loss: 5.16835 | failed:  115
batch:      79350 | loss: 5.22305 | failed:  115
batch:      79360 | loss: 5.18884 | failed:  115
batch:      79370 | loss: 5.20882 | failed:  115
batch:      79380 | loss: 5.03582 | failed:  115
batch:      79390 | loss: 5.10798 | failed:  115
batch:      79400 | loss: 5.15066 | failed:  115
batch:      79410 | loss: 5.18750 | failed:  115
batch:      79420 | loss: 5.11486 | failed:  115
batch:      79430 | loss: 5.21040 | failed:  115
batch:      79440 | loss: 5.03038 | failed:  115
batch:      79450 | loss: 5.19721 | failed:  115
batch:      79460 | loss: 5.13146 | failed:  115
batch:      79470 | loss: 5.14997 | failed:  115
batch:      79480 | loss: 4.88665 | failed:  115
batch:      79490 | loss: 5.17662 | failed:  115
batch:      79500 | loss: 5.06584 | failed:  115
batch:      79510 | loss: 5.06309 | failed:  115
batch:      79520 | loss: 5.21281 | failed:  115
batch:      79530 | loss: 5.17513 | failed:  115
batch:      79540 | loss: 5.20284 | failed:  115
batch:      79550 | loss: 5.18459 | failed:  115
batch:      79560 | loss: 5.13300 | failed:  115
batch:      79570 | loss: 5.15430 | failed:  115
batch:      79580 | loss: 5.09473 | failed:  115
batch:      79590 | loss: 5.20953 | failed:  115
batch:      79600 | loss: 5.10170 | failed:  115
batch:      79610 | loss: 5.10214 | failed:  115
batch:      79620 | loss: 5.15455 | failed:  115
batch:      79630 | loss: 5.12034 | failed:  115
batch:      79640 | loss: 5.12757 | failed:  115
batch:      79650 | loss: 5.14627 | failed:  115
batch:      79660 | loss: 5.22481 | failed:  115
batch:      79670 | loss: 5.15856 | failed:  115
batch:      79680 | loss: 5.24439 | failed:  115
batch:      79690 | loss: 5.21194 | failed:  115
batch:      79700 | loss: 5.01723 | failed:  115
batch:      79710 | loss: 5.18045 | failed:  115
batch:      79720 | loss: 5.15226 | failed:  115
batch:      79730 | loss: 5.14369 | failed:  115
batch:      79740 | loss: 5.16559 | failed:  115
batch:      79750 | loss: 5.15152 | failed:  115
batch:      79760 | loss: 5.13411 | failed:  115
batch:      79770 | loss: 5.21316 | failed:  115
batch:      79780 | loss: 5.08216 | failed:  115
batch:      79790 | loss: 5.10030 | failed:  115
batch:      79800 | loss: 5.11180 | failed:  115
batch:      79810 | loss: 5.18919 | failed:  115
batch:      79820 | loss: 5.19363 | failed:  115
batch:      79830 | loss: 5.08377 | failed:  115
batch:      79840 | loss: 5.19727 | failed:  115
batch:      79850 | loss: 5.21523 | failed:  115
batch:      79860 | loss: 5.06590 | failed:  115
batch:      79870 | loss: 5.20325 | failed:  115
batch:      79880 | loss: 5.13234 | failed:  115
batch:      79890 | loss: 5.11373 | failed:  115
batch:      79900 | loss: 5.07523 | failed:  115
batch:      79910 | loss: 5.07436 | failed:  115
batch:      79920 | loss: 5.03108 | failed:  115
batch:      79930 | loss: 4.87360 | failed:  115
batch:      79940 | loss: 4.85813 | failed:  115
batch:      79950 | loss: 5.36517 | failed:  115
batch:      79960 | loss: 5.21263 | failed:  115
batch:      79970 | loss: 5.26188 | failed:  115
batch:      79980 | loss: 5.09251 | failed:  115
batch:      79990 | loss: 5.15842 | failed:  115
batch:      80000 | loss: 4.96787 | failed:  115
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      80010 | loss: 4.80815 | failed:  115
batch:      80020 | loss: 5.18036 | failed:  115
batch:      80030 | loss: 4.94852 | failed:  115
batch:      80040 | loss: 5.16506 | failed:  115
batch:      80050 | loss: 5.09220 | failed:  115
batch:      80060 | loss: 5.00271 | failed:  115
batch:      80070 | loss: 5.10065 | failed:  115
batch:      80080 | loss: 5.14385 | failed:  115
batch:      80090 | loss: 4.97864 | failed:  115
batch:      80100 | loss: 5.23845 | failed:  115
batch:      80110 | loss: 5.23106 | failed:  115
batch:      80120 | loss: 5.05424 | failed:  115
batch:      80130 | loss: 5.02096 | failed:  115
batch:      80140 | loss: 5.14489 | failed:  115
batch:      80150 | loss: 5.22865 | failed:  115
batch:      80160 | loss: 5.09590 | failed:  115
batch:      80170 | loss: 5.23339 | failed:  115
batch:      80180 | loss: 5.10715 | failed:  115
batch:      80190 | loss: 5.27383 | failed:  115
batch:      80200 | loss: 5.20281 | failed:  115
batch:      80210 | loss: 5.22568 | failed:  115
batch:      80220 | loss: 5.16019 | failed:  115
batch:      80230 | loss: 5.22643 | failed:  115
batch:      80240 | loss: 5.13021 | failed:  115
batch:      80250 | loss: 5.16233 | failed:  115
batch:      80260 | loss: 5.20624 | failed:  115
batch:      80270 | loss: 5.09912 | failed:  115
batch:      80280 | loss: 5.15445 | failed:  115
batch:      80290 | loss: 5.17021 | failed:  115
batch:      80300 | loss: 5.16419 | failed:  115
batch:      80310 | loss: 5.07722 | failed:  115
batch:      80320 | loss: 5.11558 | failed:  115
batch:      80330 | loss: 5.28756 | failed:  115
batch:      80340 | loss: 5.02788 | failed:  115
batch:      80350 | loss: 4.92078 | failed:  115
batch:      80360 | loss: 5.15391 | failed:  115
batch:      80370 | loss: 5.18559 | failed:  115
batch:      80380 | loss: 5.14024 | failed:  115
batch:      80390 | loss: 4.92443 | failed:  115
batch:      80400 | loss: 5.15849 | failed:  115
batch:      80410 | loss: 5.16970 | failed:  115
batch:      80420 | loss: 5.07894 | failed:  115
batch:      80430 | loss: 5.25250 | failed:  115
batch:      80440 | loss: 5.19181 | failed:  115
batch:      80450 | loss: 5.16500 | failed:  115
batch:      80460 | loss: 5.07241 | failed:  115
batch:      80470 | loss: 5.04040 | failed:  115
batch:      80480 | loss: 5.22474 | failed:  115
batch:      80490 | loss: 5.24007 | failed:  115
batch:      80500 | loss: 5.23660 | failed:  115
batch:      80510 | loss: 5.08448 | failed:  115
batch:      80520 | loss: 5.24613 | failed:  115
batch:      80530 | loss: 5.22217 | failed:  115
batch:      80540 | loss: 5.05069 | failed:  115
batch:      80550 | loss: 5.15616 | failed:  115
batch:      80560 | loss: 5.05603 | failed:  115
batch:      80570 | loss: 5.31302 | failed:  115
batch:      80580 | loss: 5.27460 | failed:  115
batch:      80590 | loss: 5.15178 | failed:  115
batch:      80600 | loss: 5.10149 | failed:  115
batch:      80610 | loss: 5.12003 | failed:  115
batch:      80620 | loss: 4.96418 | failed:  115
batch:      80630 | loss: 5.14238 | failed:  115
batch:      80640 | loss: 5.06346 | failed:  115
batch:      80650 | loss: 5.14710 | failed:  115
batch:      80660 | loss: 5.23651 | failed:  115
batch:      80670 | loss: 5.11524 | failed:  115
batch:      80680 | loss: 5.11922 | failed:  115
batch:      80690 | loss: 5.12279 | failed:  115
batch:      80700 | loss: 5.12838 | failed:  115
batch:      80710 | loss: 5.10131 | failed:  115
batch:      80720 | loss: 5.12814 | failed:  115
batch:      80730 | loss: 5.21063 | failed:  115
batch:      80740 | loss: 4.79765 | failed:  115
batch:      80750 | loss: 5.08158 | failed:  115
batch:      80760 | loss: 5.23093 | failed:  115
batch:      80770 | loss: 5.14189 | failed:  115
batch:      80780 | loss: 5.19908 | failed:  115
batch:      80790 | loss: 5.08388 | failed:  115
batch:      80800 | loss: 5.03706 | failed:  115
batch:      80810 | loss: 5.11966 | failed:  115
batch:      80820 | loss: 5.07860 | failed:  115
batch:      80830 | loss: 5.11418 | failed:  115
batch:      80840 | loss: 5.10312 | failed:  115
batch:      80850 | loss: 5.07682 | failed:  115
batch:      80860 | loss: 5.05070 | failed:  115
batch:      80870 | loss: 5.17373 | failed:  115
batch:      80880 | loss: 5.07192 | failed:  115
batch:      80890 | loss: 5.05789 | failed:  115
batch:      80900 | loss: 5.16719 | failed:  115
batch:      80910 | loss: 5.06374 | failed:  115
batch:      80920 | loss: 4.79941 | failed:  115
batch:      80930 | loss: 5.19340 | failed:  115
batch:      80940 | loss: 5.15278 | failed:  115
batch:      80950 | loss: 5.08426 | failed:  115
batch:      80960 | loss: 5.16218 | failed:  115
batch:      80970 | loss: 5.13964 | failed:  115
batch:      80980 | loss: 5.02004 | failed:  115
batch:      80990 | loss: 5.12891 | failed:  115
batch:      81000 | loss: 5.15296 | failed:  115
batch:      81010 | loss: 5.00054 | failed:  115
batch:      81020 | loss: 5.12850 | failed:  115
batch:      81030 | loss: 5.17874 | failed:  115
batch:      81040 | loss: 5.18453 | failed:  115
batch:      81050 | loss: 5.12551 | failed:  115
batch:      81060 | loss: 5.16110 | failed:  115
batch:      81070 | loss: 5.14254 | failed:  115
batch:      81080 | loss: 5.16074 | failed:  115
batch:      81090 | loss: 5.23996 | failed:  115
batch:      81100 | loss: 5.04867 | failed:  115
batch:      81110 | loss: 5.15207 | failed:  115
batch:      81120 | loss: 5.06529 | failed:  115
batch:      81130 | loss: 5.16032 | failed:  115
batch:      81140 | loss: 5.02703 | failed:  115
batch:      81150 | loss: 5.09813 | failed:  115
batch:      81160 | loss: 5.19616 | failed:  115
batch:      81170 | loss: 5.11687 | failed:  115
batch:      81180 | loss: 5.25320 | failed:  115
batch:      81190 | loss: 5.09743 | failed:  115
batch:      81200 | loss: 4.90514 | failed:  115
batch:      81210 | loss: 5.28178 | failed:  115
batch:      81220 | loss: 5.21186 | failed:  115
batch:      81230 | loss: 5.10008 | failed:  115
batch:      81240 | loss: 5.23779 | failed:  115
batch:      81250 | loss: 5.14958 | failed:  115
batch:      81260 | loss: 4.83490 | failed:  115
batch:      81270 | loss: 5.08619 | failed:  115
batch:      81280 | loss: 4.98070 | failed:  115
batch:      81290 | loss: 4.94987 | failed:  115
batch:      81300 | loss: 5.16573 | failed:  115
batch:      81310 | loss: 5.18646 | failed:  115
batch:      81320 | loss: 5.18669 | failed:  115
batch:      81330 | loss: 5.14656 | failed:  115
batch:      81340 | loss: 5.17739 | failed:  115
batch:      81350 | loss: 5.17421 | failed:  115
batch:      81360 | loss: 5.19956 | failed:  115
batch:      81370 | loss: 5.17659 | failed:  115
batch:      81380 | loss: 5.06224 | failed:  115
batch:      81390 | loss: 5.16377 | failed:  115
batch:      81400 | loss: 5.19968 | failed:  115
batch:      81410 | loss: 5.10004 | failed:  115
batch:      81420 | loss: 5.21990 | failed:  115
batch:      81430 | loss: 5.00421 | failed:  115
batch:      81440 | loss: 5.02982 | failed:  115
batch:      81450 | loss: 5.03274 | failed:  115
batch:      81460 | loss: 5.07768 | failed:  115
batch:      81470 | loss: 5.23804 | failed:  115
batch:      81480 | loss: 5.24086 | failed:  115
batch:      81490 | loss: 5.26059 | failed:  115
batch:      81500 | loss: 5.19322 | failed:  115
batch:      81510 | loss: 5.13586 | failed:  115
batch:      81520 | loss: 5.14392 | failed:  115
batch:      81530 | loss: 5.15668 | failed:  115
batch:      81540 | loss: 5.07127 | failed:  115
batch:      81550 | loss: 5.22373 | failed:  115
batch:      81560 | loss: 5.19882 | failed:  115
batch:      81570 | loss: 5.10380 | failed:  115
batch:      81580 | loss: 5.16765 | failed:  115
batch:      81590 | loss: 5.11519 | failed:  115
batch:      81600 | loss: 5.05159 | failed:  115
batch:      81610 | loss: 5.08836 | failed:  115
batch:      81620 | loss: 5.09986 | failed:  115
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      81650 | loss: 5.08424 | failed:  135
batch:      81660 | loss: 5.13015 | failed:  135
batch:      81670 | loss: 5.29938 | failed:  135
batch:      81680 | loss: 5.19694 | failed:  135
batch:      81690 | loss: 5.15145 | failed:  135
batch:      81700 | loss: 5.18459 | failed:  135
batch:      81710 | loss: 5.13693 | failed:  135
batch:      81720 | loss: 5.10584 | failed:  135
batch:      81730 | loss: 5.22449 | failed:  135
batch:      81740 | loss: 5.25520 | failed:  135
batch:      81750 | loss: 5.21520 | failed:  135
batch:      81760 | loss: 5.22260 | failed:  135
batch:      81770 | loss: 5.23745 | failed:  135
batch:      81780 | loss: 5.05903 | failed:  135
batch:      81790 | loss: 4.97604 | failed:  135
batch:      81800 | loss: 5.11653 | failed:  135
batch:      81810 | loss: 5.19481 | failed:  135
batch:      81820 | loss: 5.21159 | failed:  135
batch:      81830 | loss: 4.94966 | failed:  135
batch:      81840 | loss: 5.21413 | failed:  135
batch:      81850 | loss: 5.13019 | failed:  135
batch:      81860 | loss: 5.11849 | failed:  135
batch:      81870 | loss: 5.25148 | failed:  135
batch:      81880 | loss: 5.20680 | failed:  135
batch:      81890 | loss: 5.26006 | failed:  135
batch:      81900 | loss: 5.18818 | failed:  135
batch:      81910 | loss: 5.13544 | failed:  135
batch:      81920 | loss: 5.21113 | failed:  135
batch:      81930 | loss: 5.14532 | failed:  135
batch:      81940 | loss: 5.16066 | failed:  135
batch:      81950 | loss: 5.13280 | failed:  135
batch:      81960 | loss: 5.12664 | failed:  135
batch:      81970 | loss: 5.09722 | failed:  135
batch:      81980 | loss: 5.27191 | failed:  135
batch:      81990 | loss: 5.27747 | failed:  135
batch:      82000 | loss: 5.21537 | failed:  135
batch:      82010 | loss: 5.26935 | failed:  135
batch:      82020 | loss: 5.21276 | failed:  135
batch:      82030 | loss: 5.08178 | failed:  135
batch:      82040 | loss: 5.10291 | failed:  135
batch:      82050 | loss: 5.05039 | failed:  135
batch:      82060 | loss: 5.04163 | failed:  135
batch:      82070 | loss: 5.01463 | failed:  135
batch:      82080 | loss: 5.21849 | failed:  135
batch:      82090 | loss: 5.15997 | failed:  135
batch:      82100 | loss: 5.10744 | failed:  135
batch:      82110 | loss: 5.15673 | failed:  135
batch:      82120 | loss: 5.17567 | failed:  135
batch:      82130 | loss: 5.11538 | failed:  135
batch:      82140 | loss: 5.19032 | failed:  135
batch:      82150 | loss: 5.21365 | failed:  135
batch:      82160 | loss: 5.25541 | failed:  135
batch:      82170 | loss: 5.18750 | failed:  135
batch:      82180 | loss: 5.13066 | failed:  135
batch:      82190 | loss: 5.14015 | failed:  135
batch:      82200 | loss: 5.11186 | failed:  135
batch:      82210 | loss: 5.09811 | failed:  135
batch:      82220 | loss: 5.00686 | failed:  135
batch:      82230 | loss: 5.06850 | failed:  135
batch:      82240 | loss: 5.22108 | failed:  135
batch:      82250 | loss: 5.19198 | failed:  135
batch:      82260 | loss: 5.09826 | failed:  135
batch:      82270 | loss: 5.22663 | failed:  135
batch:      82280 | loss: 5.22513 | failed:  135
batch:      82290 | loss: 5.20014 | failed:  135
batch:      82300 | loss: 5.13768 | failed:  135
batch:      82310 | loss: 5.20389 | failed:  135
batch:      82320 | loss: 5.15437 | failed:  135
batch:      82330 | loss: 5.01743 | failed:  135
batch:      82340 | loss: 5.18269 | failed:  135
batch:      82350 | loss: 5.19890 | failed:  135
batch:      82360 | loss: 5.12327 | failed:  135
batch:      82370 | loss: 5.05567 | failed:  135
batch:      82380 | loss: 5.02476 | failed:  135
batch:      82390 | loss: 5.07149 | failed:  135
batch:      82400 | loss: 5.04703 | failed:  135
batch:      82410 | loss: 5.04388 | failed:  135
batch:      82420 | loss: 5.16234 | failed:  135
batch:      82430 | loss: 5.08212 | failed:  135
batch:      82440 | loss: 5.17485 | failed:  135
batch:      82450 | loss: 5.15703 | failed:  135
batch:      82460 | loss: 4.87435 | failed:  135
batch:      82470 | loss: 5.20665 | failed:  135
batch:      82480 | loss: 5.23681 | failed:  135
batch:      82490 | loss: 4.94262 | failed:  135
batch:      82500 | loss: 5.10656 | failed:  135
batch:      82510 | loss: 5.13636 | failed:  135
batch:      82520 | loss: 5.22217 | failed:  135
batch:      82530 | loss: 5.14145 | failed:  135
batch:      82540 | loss: 5.17759 | failed:  135
batch:      82550 | loss: 5.08945 | failed:  135
batch:      82560 | loss: 4.69696 | failed:  135
batch:      82570 | loss: 4.99226 | failed:  135
batch:      82580 | loss: 4.93449 | failed:  135
batch:      82590 | loss: 5.17185 | failed:  135
batch:      82600 | loss: 5.02415 | failed:  135
batch:      82610 | loss: 5.07657 | failed:  135
batch:      82620 | loss: 5.00047 | failed:  135
batch:      82630 | loss: 5.10264 | failed:  135
batch:      82640 | loss: 5.18306 | failed:  135
batch:      82650 | loss: 5.24252 | failed:  135
batch:      82660 | loss: 5.23934 | failed:  135
batch:      82670 | loss: 4.84806 | failed:  135
batch:      82680 | loss: 5.17679 | failed:  135
batch:      82690 | loss: 4.90256 | failed:  135
batch:      82700 | loss: 5.20221 | failed:  135
batch:      82710 | loss: 5.10510 | failed:  135
batch:      82720 | loss: 5.04141 | failed:  135
batch:      82730 | loss: 5.07916 | failed:  135
batch:      82740 | loss: 5.10145 | failed:  135
batch:      82750 | loss: 5.15040 | failed:  135
batch:      82760 | loss: 5.17546 | failed:  135
batch:      82770 | loss: 5.25772 | failed:  135
batch:      82780 | loss: 5.05457 | failed:  135
batch:      82790 | loss: 5.12289 | failed:  135
batch:      82800 | loss: 5.19188 | failed:  135
batch:      82810 | loss: 5.16440 | failed:  135
batch:      82820 | loss: 5.12036 | failed:  135
batch:      82830 | loss: 5.14401 | failed:  135
batch:      82840 | loss: 5.17617 | failed:  135
batch:      82850 | loss: 5.07934 | failed:  135
batch:      82860 | loss: 5.10559 | failed:  135
batch:      82870 | loss: 5.15575 | failed:  135
batch:      82880 | loss: 5.13724 | failed:  135
batch:      82890 | loss: 5.08172 | failed:  135
batch:      82900 | loss: 4.97887 | failed:  135
batch:      82910 | loss: 5.08046 | failed:  135
batch:      82920 | loss: 4.99606 | failed:  135
batch:      82930 | loss: 5.15704 | failed:  135
batch:      82940 | loss: 5.01035 | failed:  135
batch:      82950 | loss: 5.14314 | failed:  135
batch:      82960 | loss: 5.04420 | failed:  135
batch:      82970 | loss: 5.36898 | failed:  135
batch:      82980 | loss: 5.04809 | failed:  135
batch:      82990 | loss: 5.12430 | failed:  135
batch:      83000 | loss: 5.17653 | failed:  135
batch:      83010 | loss: 5.07903 | failed:  135
batch:      83020 | loss: 5.25762 | failed:  135
batch:      83030 | loss: 5.18653 | failed:  135
batch:      83040 | loss: 5.24878 | failed:  135
batch:      83050 | loss: 4.87063 | failed:  135
batch:      83060 | loss: 5.23413 | failed:  135
batch:      83070 | loss: 5.12567 | failed:  135
batch:      83080 | loss: 5.19330 | failed:  135
batch:      83090 | loss: 5.26263 | failed:  135
batch:      83100 | loss: 5.26610 | failed:  135
batch:      83110 | loss: 5.23032 | failed:  135
batch:      83120 | loss: 5.17632 | failed:  135
batch:      83130 | loss: 5.18057 | failed:  135
batch:      83140 | loss: 5.08007 | failed:  135
batch:      83150 | loss: 5.08666 | failed:  135
batch:      83160 | loss: 4.98371 | failed:  135
batch:      83170 | loss: 4.81771 | failed:  135
batch:      83180 | loss: 5.02565 | failed:  135
batch:      83190 | loss: 5.05211 | failed:  135
batch:      83200 | loss: 4.88631 | failed:  135
batch:      83210 | loss: 5.22824 | failed:  135
batch:      83220 | loss: 5.14583 | failed:  135
batch:      83230 | loss: 5.21500 | failed:  135
batch:      83240 | loss: 5.13647 | failed:  135
batch:      83250 | loss: 5.05778 | failed:  135
batch:      83260 | loss: 5.13359 | failed:  135
batch:      83270 | loss: 5.10885 | failed:  135
batch:      83280 | loss: 5.18295 | failed:  135
batch:      83290 | loss: 5.16140 | failed:  135
batch:      83300 | loss: 4.96805 | failed:  135
batch:      83310 | loss: 5.03243 | failed:  135
batch:      83320 | loss: 5.12519 | failed:  135
batch:      83330 | loss: 5.12229 | failed:  135
batch:      83340 | loss: 4.93554 | failed:  135
batch:      83350 | loss: 5.17012 | failed:  135
batch:      83360 | loss: 5.18949 | failed:  135
batch:      83370 | loss: 5.16722 | failed:  135
batch:      83380 | loss: 5.13121 | failed:  135
batch:      83390 | loss: 5.07242 | failed:  135
batch:      83400 | loss: 5.14599 | failed:  135
batch:      83410 | loss: 5.08636 | failed:  135
batch:      83420 | loss: 5.26488 | failed:  135
batch:      83430 | loss: 5.30126 | failed:  135
batch:      83440 | loss: 5.00230 | failed:  135
batch:      83450 | loss: 5.14825 | failed:  135
batch:      83460 | loss: 5.13844 | failed:  135
batch:      83470 | loss: 5.15664 | failed:  135
batch:      83480 | loss: 4.90525 | failed:  135
batch:      83490 | loss: 5.14788 | failed:  135
batch:      83500 | loss: 4.93937 | failed:  135
batch:      83510 | loss: 5.19231 | failed:  135
batch:      83520 | loss: 5.05311 | failed:  135
batch:      83530 | loss: 4.91398 | failed:  135
batch:      83540 | loss: 5.20590 | failed:  135
batch:      83550 | loss: 5.20969 | failed:  135
batch:      83560 | loss: 5.15971 | failed:  135
batch:      83570 | loss: 5.21124 | failed:  135
batch:      83580 | loss: 5.04911 | failed:  135
batch:      83590 | loss: 5.12128 | failed:  135
batch:      83600 | loss: 5.05022 | failed:  135
batch:      83610 | loss: 5.04496 | failed:  135
batch:      83620 | loss: 5.21312 | failed:  135
batch:      83630 | loss: 5.21306 | failed:  135
batch:      83640 | loss: 5.14404 | failed:  135
batch:      83650 | loss: 5.04082 | failed:  135
batch:      83660 | loss: 5.14570 | failed:  135
batch:      83670 | loss: 5.19475 | failed:  135
batch:      83680 | loss: 5.21270 | failed:  135
batch:      83690 | loss: 5.21321 | failed:  135
batch:      83700 | loss: 5.19438 | failed:  135
batch:      83710 | loss: 5.11168 | failed:  135
batch:      83720 | loss: 5.30035 | failed:  135
batch:      83730 | loss: 4.85468 | failed:  135
batch:      83740 | loss: 5.13440 | failed:  135
batch:      83750 | loss: 4.85873 | failed:  135
batch:      83760 | loss: 5.03263 | failed:  135
batch:      83770 | loss: 5.14171 | failed:  135
batch:      83780 | loss: 5.18956 | failed:  135
batch:      83790 | loss: 5.16758 | failed:  135
batch:      83800 | loss: 5.11374 | failed:  135
batch:      83810 | loss: 5.16070 | failed:  135
batch:      83820 | loss: 5.21350 | failed:  135
batch:      83830 | loss: 5.23101 | failed:  135
batch:      83840 | loss: 5.25353 | failed:  135
batch:      83850 | loss: 5.23521 | failed:  135
batch:      83860 | loss: 5.01795 | failed:  135
batch:      83870 | loss: 5.16472 | failed:  135
batch:      83880 | loss: 5.14013 | failed:  135
batch:      83890 | loss: 5.16742 | failed:  135
batch:      83900 | loss: 5.11387 | failed:  135
batch:      83910 | loss: 5.30420 | failed:  135
batch:      83920 | loss: 5.21566 | failed:  135
batch:      83930 | loss: 5.19716 | failed:  135
batch:      83940 | loss: 5.22115 | failed:  135
batch:      83950 | loss: 5.13948 | failed:  135
batch:      83960 | loss: 5.23862 | failed:  135
batch:      83970 | loss: 4.97479 | failed:  135
batch:      83980 | loss: 4.99275 | failed:  135
batch:      83990 | loss: 4.93679 | failed:  135
batch:      84000 | loss: 4.74660 | failed:  135
batch:      84010 | loss: 5.00369 | failed:  135
batch:      84020 | loss: 5.21255 | failed:  135
batch:      84030 | loss: 4.96798 | failed:  135
batch:      84040 | loss: 5.25724 | failed:  135
batch:      84050 | loss: 5.23029 | failed:  135
batch:      84060 | loss: 5.28082 | failed:  135
batch:      84070 | loss: 5.18533 | failed:  135
batch:      84080 | loss: 5.10395 | failed:  135
batch:      84090 | loss: 5.16696 | failed:  135
batch:      84100 | loss: 5.15444 | failed:  135
batch:      84110 | loss: 5.19170 | failed:  135
batch:      84120 | loss: 5.04835 | failed:  135
batch:      84130 | loss: 5.11819 | failed:  135
batch:      84140 | loss: 4.97322 | failed:  135
batch:      84150 | loss: 5.01252 | failed:  135
batch:      84160 | loss: 5.16855 | failed:  135
batch:      84170 | loss: 5.21399 | failed:  135
batch:      84180 | loss: 5.21362 | failed:  135
batch:      84190 | loss: 5.15606 | failed:  135
batch:      84200 | loss: 5.28395 | failed:  135
batch:      84210 | loss: 5.14634 | failed:  135
batch:      84220 | loss: 5.12178 | failed:  135
batch:      84230 | loss: 5.24121 | failed:  135
batch:      84240 | loss: 5.22838 | failed:  135
batch:      84250 | loss: 5.09911 | failed:  135
batch:      84260 | loss: 5.23025 | failed:  135
batch:      84270 | loss: 5.17918 | failed:  135
batch:      84280 | loss: 5.08504 | failed:  135
batch:      84290 | loss: 5.04484 | failed:  135
batch:      84300 | loss: 5.20487 | failed:  135
batch:      84310 | loss: 5.24223 | failed:  135
batch:      84320 | loss: 5.08685 | failed:  135
batch:      84330 | loss: 5.15537 | failed:  135
batch:      84340 | loss: 5.10070 | failed:  135
batch:      84350 | loss: 5.11654 | failed:  135
batch:      84360 | loss: 5.05533 | failed:  135
batch:      84370 | loss: 5.03328 | failed:  135
batch:      84380 | loss: 5.04977 | failed:  135
batch:      84390 | loss: 5.18272 | failed:  135
batch:      84400 | loss: 4.96942 | failed:  135
batch:      84410 | loss: 4.47960 | failed:  135
batch:      84420 | loss: 4.91663 | failed:  135
batch:      84430 | loss: 5.03919 | failed:  135
batch:      84440 | loss: 5.17212 | failed:  135
batch:      84450 | loss: 5.14554 | failed:  135
batch:      84460 | loss: 5.20511 | failed:  135
batch:      84470 | loss: 5.22490 | failed:  135
batch:      84480 | loss: 5.17818 | failed:  135
batch:      84490 | loss: 5.16637 | failed:  135
batch:      84500 | loss: 5.10865 | failed:  135
batch:      84510 | loss: 5.08822 | failed:  135
batch:      84520 | loss: 4.86942 | failed:  135
batch:      84530 | loss: 5.15096 | failed:  135
batch:      84540 | loss: 5.03281 | failed:  135
batch:      84550 | loss: 5.04357 | failed:  135
batch:      84560 | loss: 5.10280 | failed:  135
batch:      84570 | loss: 5.12194 | failed:  135
batch:      84580 | loss: 5.21018 | failed:  135
batch:      84590 | loss: 5.17728 | failed:  135
batch:      84600 | loss: 5.12995 | failed:  135
batch:      84610 | loss: 5.22278 | failed:  135
batch:      84620 | loss: 5.10747 | failed:  135
batch:      84630 | loss: 5.09529 | failed:  135
batch:      84640 | loss: 5.17505 | failed:  135
batch:      84650 | loss: 5.13552 | failed:  135
batch:      84660 | loss: 5.12710 | failed:  135
batch:      84670 | loss: 5.03141 | failed:  135
batch:      84680 | loss: 5.12097 | failed:  135
batch:      84690 | loss: 5.15319 | failed:  135
batch:      84700 | loss: 5.11510 | failed:  135
batch:      84710 | loss: 5.23412 | failed:  135
batch:      84720 | loss: 5.18477 | failed:  135
batch:      84730 | loss: 5.12787 | failed:  135
batch:      84740 | loss: 4.87962 | failed:  135
batch:      84750 | loss: 5.25416 | failed:  135
batch:      84760 | loss: 5.24773 | failed:  135
batch:      84770 | loss: 5.19376 | failed:  135
batch:      84780 | loss: 5.04594 | failed:  135
batch:      84790 | loss: 4.96456 | failed:  135
batch:      84800 | loss: 5.13039 | failed:  135
batch:      84810 | loss: 4.91833 | failed:  135
batch:      84820 | loss: 5.11136 | failed:  135
batch:      84830 | loss: 5.03625 | failed:  135
batch:      84840 | loss: 4.99721 | failed:  135
batch:      84850 | loss: 5.08084 | failed:  135
batch:      84860 | loss: 5.11782 | failed:  135
batch:      84870 | loss: 5.12822 | failed:  135
batch:      84880 | loss: 5.10564 | failed:  135
batch:      84890 | loss: 5.06380 | failed:  135
batch:      84900 | loss: 5.06862 | failed:  135
batch:      84910 | loss: 5.27791 | failed:  135
batch:      84920 | loss: 4.95341 | failed:  135
batch:      84930 | loss: 4.99629 | failed:  135
batch:      84940 | loss: 5.08447 | failed:  135
batch:      84950 | loss: 5.14958 | failed:  135
batch:      84960 | loss: 5.11494 | failed:  135
batch:      84970 | loss: 5.26542 | failed:  135
batch:      84980 | loss: 5.18705 | failed:  135
batch:      84990 | loss: 5.11394 | failed:  135
batch:      85000 | loss: 5.11666 | failed:  135
batch:      85010 | loss: 5.16945 | failed:  135
batch:      85020 | loss: 5.13604 | failed:  135
batch:      85030 | loss: 5.09994 | failed:  135
batch:      85040 | loss: 5.19123 | failed:  135
batch:      85050 | loss: 5.20778 | failed:  135
batch:      85060 | loss: 5.20483 | failed:  135
batch:      85070 | loss: 5.15937 | failed:  135
batch:      85080 | loss: 5.08352 | failed:  135
batch:      85090 | loss: 5.13910 | failed:  135
batch:      85100 | loss: 5.24198 | failed:  135
batch:      85110 | loss: 5.13291 | failed:  135
batch:      85120 | loss: 5.14023 | failed:  135
batch:      85130 | loss: 5.18971 | failed:  135
batch:      85140 | loss: 5.09677 | failed:  135
batch:      85150 | loss: 5.25579 | failed:  135
batch:      85160 | loss: 5.20540 | failed:  135
batch:      85170 | loss: 5.10574 | failed:  135
batch:      85180 | loss: 5.05593 | failed:  135
batch:      85190 | loss: 5.15376 | failed:  135
batch:      85200 | loss: 5.08055 | failed:  135
batch:      85210 | loss: 5.24840 | failed:  135
batch:      85220 | loss: 5.20728 | failed:  135
batch:      85230 | loss: 5.21260 | failed:  135
batch:      85240 | loss: 5.17636 | failed:  135
batch:      85250 | loss: 5.22965 | failed:  135
batch:      85260 | loss: 5.16990 | failed:  135
batch:      85270 | loss: 5.09894 | failed:  135
batch:      85280 | loss: 5.10657 | failed:  135
batch:      85290 | loss: 5.08720 | failed:  135
batch:      85300 | loss: 4.71999 | failed:  135
batch:      85310 | loss: 5.09444 | failed:  135
batch:      85320 | loss: 5.19025 | failed:  135
batch:      85330 | loss: 5.14702 | failed:  135
batch:      85340 | loss: 5.19909 | failed:  135
batch:      85350 | loss: 5.04154 | failed:  135
batch:      85360 | loss: 5.18607 | failed:  135
batch:      85370 | loss: 5.18706 | failed:  135
batch:      85380 | loss: 5.14490 | failed:  135
batch:      85390 | loss: 5.02268 | failed:  135
batch:      85400 | loss: 3.78956 | failed:  135
batch:      85410 | loss: 5.08331 | failed:  135
batch:      85420 | loss: 5.13602 | failed:  135
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      85430 | loss: 5.15091 | failed:  137
batch:      85440 | loss: 5.14952 | failed:  137
batch:      85450 | loss: 5.09603 | failed:  137
batch:      85460 | loss: 5.25312 | failed:  137
batch:      85470 | loss: 5.03498 | failed:  137
batch:      85480 | loss: 5.09255 | failed:  137
batch:      85490 | loss: 5.07162 | failed:  137
batch:      85500 | loss: 5.13189 | failed:  137
batch:      85510 | loss: 5.16861 | failed:  137
batch:      85520 | loss: 5.02426 | failed:  137
batch:      85530 | loss: 5.19302 | failed:  137
batch:      85540 | loss: 5.11862 | failed:  137
batch:      85550 | loss: 5.24321 | failed:  137
batch:      85560 | loss: 5.12648 | failed:  137
batch:      85570 | loss: 5.23383 | failed:  137
batch:      85580 | loss: 5.07726 | failed:  137
batch:      85590 | loss: 5.12494 | failed:  137
batch:      85600 | loss: 5.23537 | failed:  137
batch:      85610 | loss: 5.11432 | failed:  137
batch:      85620 | loss: 5.25370 | failed:  137
batch:      85630 | loss: 5.11304 | failed:  137
batch:      85640 | loss: 5.14360 | failed:  137
batch:      85650 | loss: 5.24051 | failed:  137
batch:      85660 | loss: 5.18517 | failed:  137
batch:      85670 | loss: 5.21625 | failed:  137
batch:      85680 | loss: 5.26650 | failed:  137
batch:      85690 | loss: 5.10272 | failed:  137
batch:      85700 | loss: 5.07886 | failed:  137
batch:      85710 | loss: 5.15460 | failed:  137
batch:      85720 | loss: 5.15577 | failed:  137
batch:      85730 | loss: 5.19907 | failed:  137
batch:      85740 | loss: 5.20282 | failed:  137
batch:      85750 | loss: 5.15508 | failed:  137
batch:      85760 | loss: 4.97612 | failed:  137
batch:      85770 | loss: 5.12645 | failed:  137
batch:      85780 | loss: 5.21714 | failed:  137
batch:      85790 | loss: 5.15590 | failed:  137
batch:      85800 | loss: 5.18104 | failed:  137
batch:      85810 | loss: 5.03374 | failed:  137
batch:      85820 | loss: 5.03725 | failed:  137
batch:      85830 | loss: 5.23469 | failed:  137
batch:      85840 | loss: 5.19182 | failed:  137
batch:      85850 | loss: 5.25025 | failed:  137
batch:      85860 | loss: 5.31217 | failed:  137
batch:      85870 | loss: 5.17638 | failed:  137
batch:      85880 | loss: 5.19132 | failed:  137
batch:      85890 | loss: 5.10658 | failed:  137
batch:      85900 | loss: 5.23196 | failed:  137
batch:      85910 | loss: 5.11221 | failed:  137
batch:      85920 | loss: 5.12617 | failed:  137
batch:      85930 | loss: 4.97200 | failed:  137
batch:      85940 | loss: 5.01028 | failed:  137
batch:      85950 | loss: 5.28012 | failed:  137
batch:      85960 | loss: 5.21239 | failed:  137
batch:      85970 | loss: 5.15251 | failed:  137
batch:      85980 | loss: 5.07549 | failed:  137
batch:      85990 | loss: 5.16408 | failed:  137
batch:      86000 | loss: 5.02192 | failed:  137
batch:      86010 | loss: 5.20154 | failed:  137
batch:      86020 | loss: 5.06959 | failed:  137
batch:      86030 | loss: 5.07083 | failed:  137
batch:      86040 | loss: 5.10594 | failed:  137
batch:      86050 | loss: 5.13527 | failed:  137
batch:      86060 | loss: 5.24280 | failed:  137
batch:      86070 | loss: 5.16072 | failed:  137
batch:      86080 | loss: 4.89555 | failed:  137
batch:      86090 | loss: 5.06133 | failed:  137
batch:      86100 | loss: 5.10904 | failed:  137
batch:      86110 | loss: 5.20268 | failed:  137
batch:      86120 | loss: 5.12828 | failed:  137
batch:      86130 | loss: 5.17514 | failed:  137
batch:      86140 | loss: 5.13608 | failed:  137
batch:      86150 | loss: 5.14834 | failed:  137
batch:      86160 | loss: 5.17844 | failed:  137
batch:      86170 | loss: 5.04994 | failed:  137
batch:      86180 | loss: 5.09797 | failed:  137
batch:      86190 | loss: 5.11678 | failed:  137
batch:      86200 | loss: 5.16763 | failed:  137
batch:      86210 | loss: 5.25841 | failed:  137
batch:      86220 | loss: 5.14861 | failed:  137
batch:      86230 | loss: 5.13151 | failed:  137
batch:      86240 | loss: 5.04958 | failed:  137
batch:      86250 | loss: 5.11089 | failed:  137
batch:      86260 | loss: 5.14880 | failed:  137
batch:      86270 | loss: 5.14881 | failed:  137
batch:      86280 | loss: 5.08052 | failed:  137
batch:      86290 | loss: 5.10736 | failed:  137
batch:      86300 | loss: 5.08683 | failed:  137
batch:      86310 | loss: 5.12570 | failed:  137
batch:      86320 | loss: 5.19197 | failed:  137
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      86330 | loss: 5.17608 | failed:  138
batch:      86340 | loss: 5.11576 | failed:  138
batch:      86350 | loss: 5.13386 | failed:  138
batch:      86360 | loss: 5.24429 | failed:  138
batch:      86370 | loss: 5.25519 | failed:  138
batch:      86380 | loss: 5.20570 | failed:  138
batch:      86390 | loss: 5.20235 | failed:  138
batch:      86400 | loss: 5.01726 | failed:  138
batch:      86410 | loss: 5.22651 | failed:  138
batch:      86420 | loss: 5.17896 | failed:  138
batch:      86430 | loss: 5.18335 | failed:  138
batch:      86440 | loss: 5.16672 | failed:  138
batch:      86450 | loss: 5.10686 | failed:  138
batch:      86460 | loss: 5.15215 | failed:  138
batch:      86470 | loss: 5.15672 | failed:  138
batch:      86480 | loss: 5.12493 | failed:  138
batch:      86490 | loss: 5.22382 | failed:  138
batch:      86500 | loss: 4.89850 | failed:  138
batch:      86510 | loss: 5.13544 | failed:  138
batch:      86520 | loss: 5.14963 | failed:  138
batch:      86530 | loss: 5.25247 | failed:  138
batch:      86540 | loss: 5.22411 | failed:  138
batch:      86550 | loss: 5.15687 | failed:  138
batch:      86560 | loss: 5.18028 | failed:  138
batch:      86570 | loss: 5.18368 | failed:  138
batch:      86580 | loss: 4.92923 | failed:  138
batch:      86590 | loss: 4.97826 | failed:  138
batch:      86600 | loss: 5.03521 | failed:  138
batch:      86610 | loss: 4.94461 | failed:  138
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      86620 | loss: 5.15381 | failed:  139
batch:      86630 | loss: 4.94360 | failed:  139
batch:      86640 | loss: 5.00574 | failed:  139
batch:      86650 | loss: 5.19965 | failed:  139
batch:      86660 | loss: 5.22748 | failed:  139
batch:      86670 | loss: 5.23465 | failed:  139
batch:      86680 | loss: 5.14759 | failed:  139
batch:      86690 | loss: 5.17517 | failed:  139
batch:      86700 | loss: 5.18903 | failed:  139
batch:      86710 | loss: 5.15818 | failed:  139
batch:      86720 | loss: 5.04103 | failed:  139
batch:      86730 | loss: 5.15123 | failed:  139
batch:      86740 | loss: 5.08032 | failed:  139
batch:      86750 | loss: 5.26654 | failed:  139
batch:      86760 | loss: 5.02521 | failed:  139
batch:      86770 | loss: 5.00165 | failed:  139
batch:      86780 | loss: 4.74017 | failed:  139
batch:      86790 | loss: 4.80513 | failed:  139
batch:      86800 | loss: 4.88267 | failed:  139
batch:      86810 | loss: 4.88101 | failed:  139
batch:      86820 | loss: 4.84076 | failed:  139
batch:      86830 | loss: 4.63465 | failed:  139
batch:      86840 | loss: 4.78973 | failed:  139
batch:      86850 | loss: 5.08930 | failed:  139
batch:      86860 | loss: 5.22353 | failed:  139
batch:      86870 | loss: 5.26982 | failed:  139
batch:      86880 | loss: 5.17710 | failed:  139
batch:      86890 | loss: 5.07714 | failed:  139
batch:      86900 | loss: 5.13214 | failed:  139
batch:      86910 | loss: 5.18339 | failed:  139
batch:      86920 | loss: 5.17886 | failed:  139
batch:      86930 | loss: 5.10903 | failed:  139
batch:      86940 | loss: 5.22393 | failed:  139
batch:      86950 | loss: 4.92697 | failed:  139
batch:      86960 | loss: 5.17916 | failed:  139
batch:      86970 | loss: 5.17162 | failed:  139
batch:      86980 | loss: 5.18638 | failed:  139
batch:      86990 | loss: 5.19706 | failed:  139
batch:      87000 | loss: 5.18533 | failed:  139
batch:      87010 | loss: 5.08304 | failed:  139
batch:      87020 | loss: 4.75833 | failed:  139
batch:      87030 | loss: 4.72938 | failed:  139
batch:      87040 | loss: 4.78228 | failed:  139
batch:      87050 | loss: 4.68921 | failed:  139
batch:      87060 | loss: 4.92839 | failed:  139
batch:      87070 | loss: 5.18139 | failed:  139
batch:      87080 | loss: 4.84446 | failed:  139
batch:      87090 | loss: 5.16003 | failed:  139
batch:      87100 | loss: 5.17263 | failed:  139
batch:      87110 | loss: 5.16644 | failed:  139
batch:      87120 | loss: 5.21053 | failed:  139
batch:      87130 | loss: 5.19533 | failed:  139
batch:      87140 | loss: 5.12766 | failed:  139
batch:      87150 | loss: 5.22821 | failed:  139
batch:      87160 | loss: 5.08559 | failed:  139
batch:      87170 | loss: 5.14832 | failed:  139
batch:      87180 | loss: 5.29148 | failed:  139
batch:      87190 | loss: 5.21232 | failed:  139
batch:      87200 | loss: 5.17772 | failed:  139
batch:      87210 | loss: 5.22940 | failed:  139
batch:      87220 | loss: 5.15569 | failed:  139
batch:      87230 | loss: 5.19550 | failed:  139
batch:      87240 | loss: 5.18380 | failed:  139
batch:      87250 | loss: 4.92995 | failed:  139
batch:      87260 | loss: 5.14459 | failed:  139
batch:      87270 | loss: 5.26764 | failed:  139
batch:      87280 | loss: 5.15964 | failed:  139
batch:      87290 | loss: 5.07963 | failed:  139
batch:      87300 | loss: 5.03475 | failed:  139
batch:      87310 | loss: 4.99929 | failed:  139
batch:      87320 | loss: 5.01472 | failed:  139
batch:      87330 | loss: 5.14482 | failed:  139
batch:      87340 | loss: 5.15037 | failed:  139
batch:      87350 | loss: 5.18313 | failed:  139
batch:      87360 | loss: 5.10175 | failed:  139
batch:      87370 | loss: 5.04275 | failed:  139
batch:      87380 | loss: 5.14804 | failed:  139
batch:      87390 | loss: 5.25260 | failed:  139
batch:      87400 | loss: 5.23042 | failed:  139
batch:      87410 | loss: 5.28631 | failed:  139
batch:      87420 | loss: 5.20556 | failed:  139
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      87440 | loss: 5.28164 | failed:  148
batch:      87450 | loss: 5.16453 | failed:  148
batch:      87460 | loss: 5.04413 | failed:  148
batch:      87470 | loss: 5.08329 | failed:  148
batch:      87480 | loss: 5.11771 | failed:  148
batch:      87490 | loss: 5.19863 | failed:  148
batch:      87500 | loss: 5.03973 | failed:  148
batch:      87510 | loss: 4.43938 | failed:  148
batch:      87520 | loss: 4.91273 | failed:  148
batch:      87530 | loss: 5.19850 | failed:  148
batch:      87540 | loss: 5.11737 | failed:  148
batch:      87550 | loss: 5.06521 | failed:  148
batch:      87560 | loss: 5.11215 | failed:  148
batch:      87570 | loss: 5.14833 | failed:  148
batch:      87580 | loss: 5.16929 | failed:  148
batch:      87590 | loss: 5.08513 | failed:  148
batch:      87600 | loss: 5.18551 | failed:  148
batch:      87610 | loss: 5.18427 | failed:  148
batch:      87620 | loss: 5.10840 | failed:  148
batch:      87630 | loss: 5.16786 | failed:  148
batch:      87640 | loss: 5.11633 | failed:  148
batch:      87650 | loss: 5.10807 | failed:  148
batch:      87660 | loss: 5.14159 | failed:  148
batch:      87670 | loss: 5.08990 | failed:  148
batch:      87680 | loss: 5.23310 | failed:  148
batch:      87690 | loss: 5.11381 | failed:  148
batch:      87700 | loss: 5.12110 | failed:  148
batch:      87710 | loss: 5.21998 | failed:  148
batch:      87720 | loss: 5.14822 | failed:  148
batch:      87730 | loss: 5.14596 | failed:  148
batch:      87740 | loss: 5.14609 | failed:  148
batch:      87750 | loss: 5.21621 | failed:  148
batch:      87760 | loss: 5.19536 | failed:  148
batch:      87770 | loss: 5.12077 | failed:  148
batch:      87780 | loss: 5.11661 | failed:  148
batch:      87790 | loss: 5.08936 | failed:  148
batch:      87800 | loss: 5.28193 | failed:  148
batch:      87810 | loss: 5.01685 | failed:  148
batch:      87820 | loss: 5.16779 | failed:  148
batch:      87830 | loss: 4.88694 | failed:  148
batch:      87840 | loss: 5.06085 | failed:  148
batch:      87850 | loss: 5.01183 | failed:  148
batch:      87860 | loss: 5.20504 | failed:  148
batch:      87870 | loss: 5.15644 | failed:  148
batch:      87880 | loss: 5.14626 | failed:  148
batch:      87890 | loss: 5.17094 | failed:  148
batch:      87900 | loss: 5.13090 | failed:  148
batch:      87910 | loss: 5.19088 | failed:  148
batch:      87920 | loss: 4.83752 | failed:  148
batch:      87930 | loss: 5.09367 | failed:  148
batch:      87940 | loss: 5.10628 | failed:  148
batch:      87950 | loss: 5.21840 | failed:  148
batch:      87960 | loss: 5.18798 | failed:  148
batch:      87970 | loss: 5.11762 | failed:  148
batch:      87980 | loss: 4.94346 | failed:  148
batch:      87990 | loss: 5.22029 | failed:  148
batch:      88000 | loss: 5.13475 | failed:  148
batch:      88010 | loss: 5.25012 | failed:  148
batch:      88020 | loss: 5.16628 | failed:  148
batch:      88030 | loss: 5.21906 | failed:  148
batch:      88040 | loss: 5.21175 | failed:  148
batch:      88050 | loss: 5.15000 | failed:  148
batch:      88060 | loss: 5.11256 | failed:  148
batch:      88070 | loss: 5.20382 | failed:  148
batch:      88080 | loss: 5.19941 | failed:  148
batch:      88090 | loss: 5.16039 | failed:  148
batch:      88100 | loss: 5.25209 | failed:  148
batch:      88110 | loss: 5.17129 | failed:  148
batch:      88120 | loss: 4.92530 | failed:  148
batch:      88130 | loss: 5.12188 | failed:  148
batch:      88140 | loss: 5.17143 | failed:  148
batch:      88150 | loss: 5.16571 | failed:  148
batch:      88160 | loss: 5.22754 | failed:  148
batch:      88170 | loss: 5.17304 | failed:  148
batch:      88180 | loss: 5.23035 | failed:  148
batch:      88190 | loss: 5.16642 | failed:  148
batch:      88200 | loss: 5.16197 | failed:  148
batch:      88210 | loss: 5.11702 | failed:  148
batch:      88220 | loss: 5.25616 | failed:  148
batch:      88230 | loss: 5.06211 | failed:  148
batch:      88240 | loss: 5.10432 | failed:  148
batch:      88250 | loss: 5.05576 | failed:  148
batch:      88260 | loss: 5.20067 | failed:  148
batch:      88270 | loss: 5.29280 | failed:  148
batch:      88280 | loss: 5.18895 | failed:  148
batch:      88290 | loss: 5.15853 | failed:  148
batch:      88300 | loss: 5.23281 | failed:  148
batch:      88310 | loss: 5.12227 | failed:  148
batch:      88320 | loss: 5.16539 | failed:  148
batch:      88330 | loss: 4.98327 | failed:  148
batch:      88340 | loss: 5.17856 | failed:  148
batch:      88350 | loss: 5.05674 | failed:  148
batch:      88360 | loss: 4.62183 | failed:  148
batch:      88370 | loss: 4.80839 | failed:  148
batch:      88380 | loss: 4.83296 | failed:  148
batch:      88390 | loss: 3.84346 | failed:  148
batch:      88400 | loss: 3.61539 | failed:  148
batch:      88410 | loss: 4.32435 | failed:  148
batch:      88420 | loss: 3.73195 | failed:  148
batch:      88430 | loss: 5.15907 | failed:  148
batch:      88440 | loss: 5.14559 | failed:  148
batch:      88450 | loss: 5.01996 | failed:  148
batch:      88460 | loss: 5.20494 | failed:  148
batch:      88470 | loss: 5.62116 | failed:  148
batch:      88480 | loss: 5.32312 | failed:  148
batch:      88490 | loss: 5.14733 | failed:  148
batch:      88500 | loss: 4.97799 | failed:  148
batch:      88510 | loss: 4.99722 | failed:  148
batch:      88520 | loss: 5.18044 | failed:  148
batch:      88530 | loss: 5.09852 | failed:  148
batch:      88540 | loss: 5.09595 | failed:  148
batch:      88550 | loss: 5.07847 | failed:  148
batch:      88560 | loss: 5.24968 | failed:  148
batch:      88570 | loss: 5.20403 | failed:  148
batch:      88580 | loss: 5.23368 | failed:  148
batch:      88590 | loss: 5.19433 | failed:  148
batch:      88600 | loss: 5.18329 | failed:  148
batch:      88610 | loss: 5.17703 | failed:  148
batch:      88620 | loss: 5.06296 | failed:  148
batch:      88630 | loss: 5.19831 | failed:  148
batch:      88640 | loss: 5.21633 | failed:  148
batch:      88650 | loss: 5.20630 | failed:  148
batch:      88660 | loss: 5.11016 | failed:  148
batch:      88670 | loss: 5.16958 | failed:  148
batch:      88680 | loss: 5.21358 | failed:  148
batch:      88690 | loss: 4.96316 | failed:  148
batch:      88700 | loss: 5.07065 | failed:  148
batch:      88710 | loss: 5.16164 | failed:  148
batch:      88720 | loss: 5.21444 | failed:  148
batch:      88730 | loss: 5.13519 | failed:  148
batch:      88740 | loss: 5.05537 | failed:  148
batch:      88750 | loss: 5.12488 | failed:  148
batch:      88760 | loss: 5.19909 | failed:  148
batch:      88770 | loss: 5.19578 | failed:  148
batch:      88780 | loss: 5.13178 | failed:  148
batch:      88790 | loss: 5.08454 | failed:  148
batch:      88800 | loss: 5.19008 | failed:  148
batch:      88810 | loss: 5.16151 | failed:  148
batch:      88820 | loss: 5.11298 | failed:  148
batch:      88830 | loss: 5.14135 | failed:  148
batch:      88840 | loss: 5.16782 | failed:  148
batch:      88850 | loss: 5.26795 | failed:  148
batch:      88860 | loss: 5.15237 | failed:  148
batch:      88870 | loss: 5.19908 | failed:  148
batch:      88880 | loss: 5.16181 | failed:  148
batch:      88890 | loss: 5.09804 | failed:  148
batch:      88900 | loss: 5.21667 | failed:  148
batch:      88910 | loss: 5.17756 | failed:  148
batch:      88920 | loss: 5.06610 | failed:  148
batch:      88930 | loss: 5.31044 | failed:  148
batch:      88940 | loss: 5.13668 | failed:  148
batch:      88950 | loss: 5.12231 | failed:  148
batch:      88960 | loss: 5.21048 | failed:  148
batch:      88970 | loss: 5.11583 | failed:  148
batch:      88980 | loss: 5.10319 | failed:  148
batch:      88990 | loss: 5.15249 | failed:  148
batch:      89000 | loss: 5.18092 | failed:  148
batch:      89010 | loss: 5.07322 | failed:  148
batch:      89020 | loss: 5.15246 | failed:  148
batch:      89030 | loss: 5.24659 | failed:  148
batch:      89040 | loss: 5.07587 | failed:  148
batch:      89050 | loss: 5.03444 | failed:  148
batch:      89060 | loss: 5.17357 | failed:  148
batch:      89070 | loss: 5.19057 | failed:  148
batch:      89080 | loss: 5.10972 | failed:  148
batch:      89090 | loss: 5.13252 | failed:  148
batch:      89100 | loss: 5.11145 | failed:  148
batch:      89110 | loss: 5.14645 | failed:  148
batch:      89120 | loss: 5.13080 | failed:  148
batch:      89130 | loss: 5.19453 | failed:  148
batch:      89140 | loss: 5.21627 | failed:  148
batch:      89150 | loss: 4.93817 | failed:  148
batch:      89160 | loss: 5.20121 | failed:  148
batch:      89170 | loss: 5.22010 | failed:  148
batch:      89180 | loss: 5.22039 | failed:  148
batch:      89190 | loss: 5.06044 | failed:  148
batch:      89200 | loss: 5.26710 | failed:  148
batch:      89210 | loss: 5.03141 | failed:  148
batch:      89220 | loss: 5.14343 | failed:  148
batch:      89230 | loss: 5.11051 | failed:  148
batch:      89240 | loss: 5.24021 | failed:  148
batch:      89250 | loss: 5.20653 | failed:  148
batch:      89260 | loss: 5.20346 | failed:  148
batch:      89270 | loss: 5.12922 | failed:  148
batch:      89280 | loss: 5.14364 | failed:  148
batch:      89290 | loss: 5.27765 | failed:  148
batch:      89300 | loss: 5.31586 | failed:  148
batch:      89310 | loss: 5.10148 | failed:  148
batch:      89320 | loss: 5.03573 | failed:  148
batch:      89330 | loss: 5.24092 | failed:  148
batch:      89340 | loss: 4.99899 | failed:  148
batch:      89350 | loss: 5.15990 | failed:  148
batch:      89360 | loss: 5.19378 | failed:  148
batch:      89370 | loss: 5.07541 | failed:  148
batch:      89380 | loss: 5.16646 | failed:  148
batch:      89390 | loss: 5.15773 | failed:  148
batch:      89400 | loss: 5.01375 | failed:  148
batch:      89410 | loss: 4.99290 | failed:  148
batch:      89420 | loss: 5.17945 | failed:  148
batch:      89430 | loss: 5.11878 | failed:  148
batch:      89440 | loss: 5.18705 | failed:  148
batch:      89450 | loss: 5.07123 | failed:  148
batch:      89460 | loss: 5.13772 | failed:  148
batch:      89470 | loss: 5.21963 | failed:  148
batch:      89480 | loss: 5.15021 | failed:  148
batch:      89490 | loss: 5.09839 | failed:  148
batch:      89500 | loss: 5.21616 | failed:  148
batch:      89510 | loss: 4.96990 | failed:  148
batch:      89520 | loss: 5.11578 | failed:  148
batch:      89530 | loss: 5.02737 | failed:  148
batch:      89540 | loss: 4.94444 | failed:  148
batch:      89550 | loss: 5.21344 | failed:  148
batch:      89560 | loss: 5.19643 | failed:  148
batch:      89570 | loss: 5.06991 | failed:  148
batch:      89580 | loss: 5.20787 | failed:  148
batch:      89590 | loss: 5.18330 | failed:  148
batch:      89600 | loss: 5.13505 | failed:  148
batch:      89610 | loss: 5.17432 | failed:  148
batch:      89620 | loss: 5.11980 | failed:  148
batch:      89630 | loss: 5.06791 | failed:  148
batch:      89640 | loss: 5.30742 | failed:  148
batch:      89650 | loss: 5.17754 | failed:  148
batch:      89660 | loss: 5.20111 | failed:  148
batch:      89670 | loss: 5.21529 | failed:  148
batch:      89680 | loss: 5.09975 | failed:  148
batch:      89690 | loss: 5.11936 | failed:  148
batch:      89700 | loss: 5.11027 | failed:  148
batch:      89710 | loss: 4.91057 | failed:  148
batch:      89720 | loss: 5.36916 | failed:  148
batch:      89730 | loss: 5.28299 | failed:  148
batch:      89740 | loss: 5.22243 | failed:  148
batch:      89750 | loss: 5.15045 | failed:  148
batch:      89760 | loss: 5.14364 | failed:  148
batch:      89770 | loss: 5.13918 | failed:  148
batch:      89780 | loss: 5.17153 | failed:  148
batch:      89790 | loss: 5.10334 | failed:  148
batch:      89800 | loss: 5.19630 | failed:  148
batch:      89810 | loss: 5.12643 | failed:  148
batch:      89820 | loss: 5.08244 | failed:  148
batch:      89830 | loss: 5.12160 | failed:  148
batch:      89840 | loss: 5.07592 | failed:  148
batch:      89850 | loss: 5.14306 | failed:  148
batch:      89860 | loss: 5.19258 | failed:  148
batch:      89870 | loss: 5.01229 | failed:  148
batch:      89880 | loss: 5.05354 | failed:  148
batch:      89890 | loss: 5.28230 | failed:  148
batch:      89900 | loss: 5.19327 | failed:  148
batch:      89910 | loss: 4.87444 | failed:  148
batch:      89920 | loss: 5.12052 | failed:  148
batch:      89930 | loss: 5.11820 | failed:  148
batch:      89940 | loss: 5.09109 | failed:  148
batch:      89950 | loss: 5.19630 | failed:  148
batch:      89960 | loss: 5.05401 | failed:  148
batch:      89970 | loss: 5.17108 | failed:  148
batch:      89980 | loss: 5.13897 | failed:  148
batch:      89990 | loss: 5.10885 | failed:  148
batch:      90000 | loss: 5.22583 | failed:  148
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:      90010 | loss: 5.20958 | failed:  148
batch:      90020 | loss: 5.03004 | failed:  148
batch:      90030 | loss: 5.13009 | failed:  148
batch:      90040 | loss: 5.00035 | failed:  148
batch:      90050 | loss: 5.02734 | failed:  148
batch:      90060 | loss: 5.08198 | failed:  148
batch:      90070 | loss: 5.19974 | failed:  148
batch:      90080 | loss: 5.23281 | failed:  148
batch:      90090 | loss: 5.28091 | failed:  148
batch:      90100 | loss: 4.89965 | failed:  148
batch:      90110 | loss: 5.21367 | failed:  148
batch:      90120 | loss: 5.32569 | failed:  148
batch:      90130 | loss: 5.21185 | failed:  148
batch:      90140 | loss: 5.03820 | failed:  148
batch:      90150 | loss: 5.25035 | failed:  148
batch:      90160 | loss: 5.16798 | failed:  148
batch:      90170 | loss: 5.25587 | failed:  148
batch:      90180 | loss: 5.21186 | failed:  148
batch:      90190 | loss: 5.21765 | failed:  148
batch:      90200 | loss: 5.10509 | failed:  148
batch:      90210 | loss: 5.14818 | failed:  148
batch:      90220 | loss: 5.09042 | failed:  148
batch:      90230 | loss: 5.00965 | failed:  148
batch:      90240 | loss: 4.99755 | failed:  148
batch:      90250 | loss: 5.27651 | failed:  148
batch:      90260 | loss: 5.10407 | failed:  148
batch:      90270 | loss: 5.20005 | failed:  148
batch:      90280 | loss: 5.19609 | failed:  148
batch:      90290 | loss: 5.17622 | failed:  148
batch:      90300 | loss: 5.05907 | failed:  148
batch:      90310 | loss: 5.05855 | failed:  148
batch:      90320 | loss: 5.06073 | failed:  148
batch:      90330 | loss: 5.14405 | failed:  148
batch:      90340 | loss: 5.21996 | failed:  148
batch:      90350 | loss: 5.18195 | failed:  148
batch:      90360 | loss: 4.86646 | failed:  148
batch:      90370 | loss: 5.19689 | failed:  148
batch:      90380 | loss: 5.18809 | failed:  148
batch:      90390 | loss: 5.15738 | failed:  148
batch:      90400 | loss: 5.14776 | failed:  148
batch:      90410 | loss: 5.20492 | failed:  148
batch:      90420 | loss: 5.17755 | failed:  148
batch:      90430 | loss: 5.12966 | failed:  148
batch:      90440 | loss: 4.99128 | failed:  148
batch:      90450 | loss: 5.02023 | failed:  148
batch:      90460 | loss: 5.03675 | failed:  148
batch:      90470 | loss: 4.93302 | failed:  148
batch:      90480 | loss: 4.98373 | failed:  148
batch:      90490 | loss: 4.91163 | failed:  148
batch:      90500 | loss: 4.91154 | failed:  148
batch:      90510 | loss: 4.98923 | failed:  148
batch:      90520 | loss: 5.06720 | failed:  148
batch:      90530 | loss: 5.17222 | failed:  148
batch:      90540 | loss: 5.10647 | failed:  148
batch:      90550 | loss: 5.08479 | failed:  148
batch:      90560 | loss: 5.14579 | failed:  148
batch:      90570 | loss: 5.15689 | failed:  148
batch:      90580 | loss: 5.08662 | failed:  148
batch:      90590 | loss: 5.18704 | failed:  148
batch:      90600 | loss: 5.22045 | failed:  148
batch:      90610 | loss: 5.25767 | failed:  148
batch:      90620 | loss: 5.19574 | failed:  148
batch:      90630 | loss: 5.21167 | failed:  148
batch:      90640 | loss: 5.20590 | failed:  148
batch:      90650 | loss: 5.13287 | failed:  148
batch:      90660 | loss: 5.18403 | failed:  148
batch:      90670 | loss: 5.22316 | failed:  148
batch:      90680 | loss: 5.09319 | failed:  148
batch:      90690 | loss: 5.19130 | failed:  148
batch:      90700 | loss: 5.08515 | failed:  148
batch:      90710 | loss: 5.23117 | failed:  148
batch:      90720 | loss: 5.12041 | failed:  148
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      90740 | loss: 5.11022 | failed:  154
batch:      90750 | loss: 4.97551 | failed:  154
batch:      90760 | loss: 4.88695 | failed:  154
batch:      90770 | loss: 5.04452 | failed:  154
batch:      90780 | loss: 5.05082 | failed:  154
batch:      90790 | loss: 4.99353 | failed:  154
batch:      90800 | loss: 5.02554 | failed:  154
batch:      90810 | loss: 4.86039 | failed:  154
batch:      90820 | loss: 4.92244 | failed:  154
batch:      90830 | loss: 5.21177 | failed:  154
batch:      90840 | loss: 5.17737 | failed:  154
batch:      90850 | loss: 5.06032 | failed:  154
batch:      90860 | loss: 4.98540 | failed:  154
batch:      90870 | loss: 5.07706 | failed:  154
batch:      90880 | loss: 5.15345 | failed:  154
batch:      90890 | loss: 5.05219 | failed:  154
batch:      90900 | loss: 5.04916 | failed:  154
batch:      90910 | loss: 5.15225 | failed:  154
batch:      90920 | loss: 5.02961 | failed:  154
batch:      90930 | loss: 5.12043 | failed:  154
batch:      90940 | loss: 5.16304 | failed:  154
batch:      90950 | loss: 5.23842 | failed:  154
batch:      90960 | loss: 5.03734 | failed:  154
batch:      90970 | loss: 5.07448 | failed:  154
batch:      90980 | loss: 5.06678 | failed:  154
batch:      90990 | loss: 5.02598 | failed:  154
batch:      91000 | loss: 5.00275 | failed:  154
batch:      91010 | loss: 5.08913 | failed:  154
batch:      91020 | loss: 4.94198 | failed:  154
batch:      91030 | loss: 5.00260 | failed:  154
batch:      91040 | loss: 4.96008 | failed:  154
batch:      91050 | loss: 5.12118 | failed:  154
batch:      91060 | loss: 5.18300 | failed:  154
batch:      91070 | loss: 5.20911 | failed:  154
batch:      91080 | loss: 5.11654 | failed:  154
batch:      91090 | loss: 4.94419 | failed:  154
batch:      91100 | loss: 5.12114 | failed:  154
batch:      91110 | loss: 5.15347 | failed:  154
batch:      91120 | loss: 4.97184 | failed:  154
batch:      91130 | loss: 5.03426 | failed:  154
batch:      91140 | loss: 4.88163 | failed:  154
batch:      91150 | loss: 5.06305 | failed:  154
batch:      91160 | loss: 5.12199 | failed:  154
batch:      91170 | loss: 5.04614 | failed:  154
batch:      91180 | loss: 5.25079 | failed:  154
batch:      91190 | loss: 4.86444 | failed:  154
batch:      91200 | loss: 5.11535 | failed:  154
batch:      91210 | loss: 5.08554 | failed:  154
batch:      91220 | loss: 5.14183 | failed:  154
batch:      91230 | loss: 5.12035 | failed:  154
batch:      91240 | loss: 5.14579 | failed:  154
batch:      91250 | loss: 5.07011 | failed:  154
batch:      91260 | loss: 5.06672 | failed:  154
batch:      91270 | loss: 5.19685 | failed:  154
batch:      91280 | loss: 5.10518 | failed:  154
batch:      91290 | loss: 5.09794 | failed:  154
batch:      91300 | loss: 5.26323 | failed:  154
batch:      91310 | loss: 4.92285 | failed:  154
batch:      91320 | loss: 5.14469 | failed:  154
batch:      91330 | loss: 5.12081 | failed:  154
batch:      91340 | loss: 5.12665 | failed:  154
batch:      91350 | loss: 5.02182 | failed:  154
batch:      91360 | loss: 4.83467 | failed:  154
batch:      91370 | loss: 5.17286 | failed:  154
batch:      91380 | loss: 5.20469 | failed:  154
batch:      91390 | loss: 5.10934 | failed:  154
batch:      91400 | loss: 5.12249 | failed:  154
batch:      91410 | loss: 5.11611 | failed:  154
batch:      91420 | loss: 4.95966 | failed:  154
batch:      91430 | loss: 5.07486 | failed:  154
batch:      91440 | loss: 5.10081 | failed:  154
batch:      91450 | loss: 5.23365 | failed:  154
batch:      91460 | loss: 5.08690 | failed:  154
batch:      91470 | loss: 4.99161 | failed:  154
batch:      91480 | loss: 5.25323 | failed:  154
batch:      91490 | loss: 5.21520 | failed:  154
batch:      91500 | loss: 5.00966 | failed:  154
batch:      91510 | loss: 5.20077 | failed:  154
batch:      91520 | loss: 5.21464 | failed:  154
batch:      91530 | loss: 5.21261 | failed:  154
batch:      91540 | loss: 5.23174 | failed:  154
batch:      91550 | loss: 5.18449 | failed:  154
batch:      91560 | loss: 5.17056 | failed:  154
batch:      91570 | loss: 5.19736 | failed:  154
batch:      91580 | loss: 5.10488 | failed:  154
batch:      91590 | loss: 5.13499 | failed:  154
batch:      91600 | loss: 5.23657 | failed:  154
batch:      91610 | loss: 5.22160 | failed:  154
batch:      91620 | loss: 5.17753 | failed:  154
batch:      91630 | loss: 5.29641 | failed:  154
batch:      91640 | loss: 5.15171 | failed:  154
batch:      91650 | loss: 5.18529 | failed:  154
batch:      91660 | loss: 5.16386 | failed:  154
batch:      91670 | loss: 5.09253 | failed:  154
batch:      91680 | loss: 5.19054 | failed:  154
batch:      91690 | loss: 5.14269 | failed:  154
batch:      91700 | loss: 5.14874 | failed:  154
batch:      91710 | loss: 5.15557 | failed:  154
batch:      91720 | loss: 5.26131 | failed:  154
batch:      91730 | loss: 5.06430 | failed:  154
batch:      91740 | loss: 5.19042 | failed:  154
batch:      91750 | loss: 4.93812 | failed:  154
batch:      91760 | loss: 5.08414 | failed:  154
batch:      91770 | loss: 5.07267 | failed:  154
batch:      91780 | loss: 5.05366 | failed:  154
batch:      91790 | loss: 5.03220 | failed:  154
batch:      91800 | loss: 5.15533 | failed:  154
batch:      91810 | loss: 5.05538 | failed:  154
batch:      91820 | loss: 5.10960 | failed:  154
batch:      91830 | loss: 5.22161 | failed:  154
batch:      91840 | loss: 5.18449 | failed:  154
batch:      91850 | loss: 5.15793 | failed:  154
batch:      91860 | loss: 4.87064 | failed:  154
batch:      91870 | loss: 4.95434 | failed:  154
batch:      91880 | loss: 5.20117 | failed:  154
batch:      91890 | loss: 5.27162 | failed:  154
batch:      91900 | loss: 5.15744 | failed:  154
batch:      91910 | loss: 5.12282 | failed:  154
batch:      91920 | loss: 4.96681 | failed:  154
batch:      91930 | loss: 5.08140 | failed:  154
batch:      91940 | loss: 5.23410 | failed:  154
batch:      91950 | loss: 5.03389 | failed:  154
batch:      91960 | loss: 5.22498 | failed:  154
batch:      91970 | loss: 5.12591 | failed:  154
batch:      91980 | loss: 5.08654 | failed:  154
batch:      91990 | loss: 5.07384 | failed:  154
batch:      92000 | loss: 5.16664 | failed:  154
batch:      92010 | loss: 5.08710 | failed:  154
batch:      92020 | loss: 5.14177 | failed:  154
batch:      92030 | loss: 5.03444 | failed:  154
batch:      92040 | loss: 5.06009 | failed:  154
batch:      92050 | loss: 4.99029 | failed:  154
batch:      92060 | loss: 5.09227 | failed:  154
batch:      92070 | loss: 5.24227 | failed:  154
batch:      92080 | loss: 5.09492 | failed:  154
batch:      92090 | loss: 5.15426 | failed:  154
batch:      92100 | loss: 5.17528 | failed:  154
batch:      92110 | loss: 5.21936 | failed:  154
batch:      92120 | loss: 4.99197 | failed:  154
batch:      92130 | loss: 5.00843 | failed:  154
batch:      92140 | loss: 4.90386 | failed:  154
batch:      92150 | loss: 5.09677 | failed:  154
batch:      92160 | loss: 5.22712 | failed:  154
batch:      92170 | loss: 5.17471 | failed:  154
batch:      92180 | loss: 5.14899 | failed:  154
batch:      92190 | loss: 5.13969 | failed:  154
batch:      92200 | loss: 5.17252 | failed:  154
batch:      92210 | loss: 5.04177 | failed:  154
batch:      92220 | loss: 4.97168 | failed:  154
batch:      92230 | loss: 5.02912 | failed:  154
batch:      92240 | loss: 5.17544 | failed:  154
batch:      92250 | loss: 5.04563 | failed:  154
batch:      92260 | loss: 5.18143 | failed:  154
batch:      92270 | loss: 5.13881 | failed:  154
batch:      92280 | loss: 5.09139 | failed:  154
batch:      92290 | loss: 5.17870 | failed:  154
batch:      92300 | loss: 4.97350 | failed:  154
batch:      92310 | loss: 5.24449 | failed:  154
batch:      92320 | loss: 4.95062 | failed:  154
batch:      92330 | loss: 5.18829 | failed:  154
batch:      92340 | loss: 5.25408 | failed:  154
batch:      92350 | loss: 5.23852 | failed:  154
batch:      92360 | loss: 5.12335 | failed:  154
batch:      92370 | loss: 5.08820 | failed:  154
batch:      92380 | loss: 5.15098 | failed:  154
batch:      92390 | loss: 5.07536 | failed:  154
batch:      92400 | loss: 4.11896 | failed:  154
batch:      92410 | loss: 5.17044 | failed:  154
batch:      92420 | loss: 5.21705 | failed:  154
batch:      92430 | loss: 5.08125 | failed:  154
batch:      92440 | loss: 5.17604 | failed:  154
batch:      92450 | loss: 5.13123 | failed:  154
batch:      92460 | loss: 5.18173 | failed:  154
batch:      92470 | loss: 5.19129 | failed:  154
batch:      92480 | loss: 5.15510 | failed:  154
batch:      92490 | loss: 5.01784 | failed:  154
batch:      92500 | loss: 5.02764 | failed:  154
batch:      92510 | loss: 5.19966 | failed:  154
batch:      92520 | loss: 5.16976 | failed:  154
batch:      92530 | loss: 5.23717 | failed:  154
batch:      92540 | loss: 5.21385 | failed:  154
batch:      92550 | loss: 5.25986 | failed:  154
batch:      92560 | loss: 5.16450 | failed:  154
batch:      92570 | loss: 5.25551 | failed:  154
batch:      92580 | loss: 5.13597 | failed:  154
batch:      92590 | loss: 5.15564 | failed:  154
batch:      92600 | loss: 5.13271 | failed:  154
batch:      92610 | loss: 5.24509 | failed:  154
batch:      92620 | loss: 5.08680 | failed:  154
batch:      92630 | loss: 5.09361 | failed:  154
batch:      92640 | loss: 5.13306 | failed:  154
batch:      92650 | loss: 5.17999 | failed:  154
batch:      92660 | loss: 5.17632 | failed:  154
batch:      92670 | loss: 5.14460 | failed:  154
batch:      92680 | loss: 5.16850 | failed:  154
batch:      92690 | loss: 5.12521 | failed:  154
batch:      92700 | loss: 5.17609 | failed:  154
batch:      92710 | loss: 5.16464 | failed:  154
batch:      92720 | loss: 5.20671 | failed:  154
batch:      92730 | loss: 5.22080 | failed:  154
batch:      92740 | loss: 5.07979 | failed:  154
batch:      92750 | loss: 5.09521 | failed:  154
batch:      92760 | loss: 5.36509 | failed:  154
batch:      92770 | loss: 5.12337 | failed:  154
batch:      92780 | loss: 5.13379 | failed:  154
batch:      92790 | loss: 5.15718 | failed:  154
batch:      92800 | loss: 5.18086 | failed:  154
batch:      92810 | loss: 5.20747 | failed:  154
batch:      92820 | loss: 5.00384 | failed:  154
batch:      92830 | loss: 4.95385 | failed:  154
batch:      92840 | loss: 5.30514 | failed:  154
batch:      92850 | loss: 5.19120 | failed:  154
batch:      92860 | loss: 5.22673 | failed:  154
batch:      92870 | loss: 5.19247 | failed:  154
batch:      92880 | loss: 5.22488 | failed:  154
batch:      92890 | loss: 5.22895 | failed:  154
batch:      92900 | loss: 5.09573 | failed:  154
batch:      92910 | loss: 5.16498 | failed:  154
batch:      92920 | loss: 5.16631 | failed:  154
batch:      92930 | loss: 5.27327 | failed:  154
batch:      92940 | loss: 5.10446 | failed:  154
batch:      92950 | loss: 5.16180 | failed:  154
batch:      92960 | loss: 5.15679 | failed:  154
batch:      92970 | loss: 5.06812 | failed:  154
batch:      92980 | loss: 5.14087 | failed:  154
batch:      92990 | loss: 5.12741 | failed:  154
batch:      93000 | loss: 5.16546 | failed:  154
batch:      93010 | loss: 5.17742 | failed:  154
batch:      93020 | loss: 4.97329 | failed:  154
batch:      93030 | loss: 5.14988 | failed:  154
batch:      93040 | loss: 4.88309 | failed:  154
batch:      93050 | loss: 5.03100 | failed:  154
batch:      93060 | loss: 5.11289 | failed:  154
batch:      93070 | loss: 4.97081 | failed:  154
batch:      93080 | loss: 5.12641 | failed:  154
batch:      93090 | loss: 5.11287 | failed:  154
batch:      93100 | loss: 5.16108 | failed:  154
batch:      93110 | loss: 5.07172 | failed:  154
batch:      93120 | loss: 5.15076 | failed:  154
batch:      93130 | loss: 5.23266 | failed:  154
batch:      93140 | loss: 5.23555 | failed:  154
batch:      93150 | loss: 5.14171 | failed:  154
batch:      93160 | loss: 4.99117 | failed:  154
batch:      93170 | loss: 5.11654 | failed:  154
batch:      93180 | loss: 4.99355 | failed:  154
batch:      93190 | loss: 5.09928 | failed:  154
batch:      93200 | loss: 5.08331 | failed:  154
batch:      93210 | loss: 4.93891 | failed:  154
batch:      93220 | loss: 5.00380 | failed:  154
batch:      93230 | loss: 4.94215 | failed:  154
batch:      93240 | loss: 5.29770 | failed:  154
batch:      93250 | loss: 5.29946 | failed:  154
batch:      93260 | loss: 5.25062 | failed:  154
batch:      93270 | loss: 5.23689 | failed:  154
batch:      93280 | loss: 5.15640 | failed:  154
batch:      93290 | loss: 5.09684 | failed:  154
batch:      93300 | loss: 5.19086 | failed:  154
batch:      93310 | loss: 5.13024 | failed:  154
batch:      93320 | loss: 5.05203 | failed:  154
batch:      93330 | loss: 5.14649 | failed:  154
batch:      93340 | loss: 5.01322 | failed:  154
batch:      93350 | loss: 5.05163 | failed:  154
batch:      93360 | loss: 5.18308 | failed:  154
batch:      93370 | loss: 5.17093 | failed:  154
batch:      93380 | loss: 5.07570 | failed:  154
batch:      93390 | loss: 5.08068 | failed:  154
batch:      93400 | loss: 5.08748 | failed:  154
batch:      93410 | loss: 5.05210 | failed:  154
batch:      93420 | loss: 5.20000 | failed:  154
batch:      93430 | loss: 5.21960 | failed:  154
batch:      93440 | loss: 5.08711 | failed:  154
batch:      93450 | loss: 5.10339 | failed:  154
batch:      93460 | loss: 5.17624 | failed:  154
batch:      93470 | loss: 5.18124 | failed:  154
batch:      93480 | loss: 5.17005 | failed:  154
batch:      93490 | loss: 5.10327 | failed:  154
batch:      93500 | loss: 5.05849 | failed:  154
batch:      93510 | loss: 5.07897 | failed:  154
batch:      93520 | loss: 5.13525 | failed:  154
batch:      93530 | loss: 5.11621 | failed:  154
batch:      93540 | loss: 5.19057 | failed:  154
batch:      93550 | loss: 5.09012 | failed:  154
batch:      93560 | loss: 5.19312 | failed:  154
batch:      93570 | loss: 5.03198 | failed:  154
batch:      93580 | loss: 5.16007 | failed:  154
batch:      93590 | loss: 5.05836 | failed:  154
batch:      93600 | loss: 5.12833 | failed:  154
batch:      93610 | loss: 5.04800 | failed:  154
batch:      93620 | loss: 5.17956 | failed:  154
batch:      93630 | loss: 5.18449 | failed:  154
batch:      93640 | loss: 5.27631 | failed:  154
batch:      93650 | loss: 5.14591 | failed:  154
batch:      93660 | loss: 5.16246 | failed:  154
batch:      93670 | loss: 5.15153 | failed:  154
batch:      93680 | loss: 5.23393 | failed:  154
batch:      93690 | loss: 5.16775 | failed:  154
batch:      93700 | loss: 5.13065 | failed:  154
batch:      93710 | loss: 5.08808 | failed:  154
batch:      93720 | loss: 4.75249 | failed:  154
batch:      93730 | loss: 5.07837 | failed:  154
batch:      93740 | loss: 4.96802 | failed:  154
batch:      93750 | loss: 5.20155 | failed:  154
batch:      93760 | loss: 5.01142 | failed:  154
batch:      93770 | loss: 5.04907 | failed:  154
batch:      93780 | loss: 5.13575 | failed:  154
batch:      93790 | loss: 5.09329 | failed:  154
batch:      93800 | loss: 5.11143 | failed:  154
batch:      93810 | loss: 5.20667 | failed:  154
batch:      93820 | loss: 5.23128 | failed:  154
batch:      93830 | loss: 5.23726 | failed:  154
batch:      93840 | loss: 5.23878 | failed:  154
batch:      93850 | loss: 5.07871 | failed:  154
batch:      93860 | loss: 5.17145 | failed:  154
batch:      93870 | loss: 5.09910 | failed:  154
batch:      93880 | loss: 5.13249 | failed:  154
batch:      93890 | loss: 5.18485 | failed:  154
batch:      93900 | loss: 5.17761 | failed:  154
batch:      93910 | loss: 5.11023 | failed:  154
batch:      93920 | loss: 5.00919 | failed:  154
batch:      93930 | loss: 5.30815 | failed:  154
batch:      93940 | loss: 5.29788 | failed:  154
batch:      93950 | loss: 5.04054 | failed:  154
batch:      93960 | loss: 5.20913 | failed:  154
batch:      93970 | loss: 5.04370 | failed:  154
batch:      93980 | loss: 5.14857 | failed:  154
batch:      93990 | loss: 5.10843 | failed:  154
batch:      94000 | loss: 5.25279 | failed:  154
batch:      94010 | loss: 5.18264 | failed:  154
batch:      94020 | loss: 5.09874 | failed:  154
batch:      94030 | loss: 5.18971 | failed:  154
batch:      94040 | loss: 5.14423 | failed:  154
batch:      94050 | loss: 5.18517 | failed:  154
batch:      94060 | loss: 5.11689 | failed:  154
batch:      94070 | loss: 5.18842 | failed:  154
batch:      94080 | loss: 5.12881 | failed:  154
batch:      94090 | loss: 4.63715 | failed:  154
batch:      94100 | loss: 5.11989 | failed:  154
batch:      94110 | loss: 5.21873 | failed:  154
batch:      94120 | loss: 4.97784 | failed:  154
batch:      94130 | loss: 4.98430 | failed:  154
batch:      94140 | loss: 5.06564 | failed:  154
batch:      94150 | loss: 5.18055 | failed:  154
batch:      94160 | loss: 5.18277 | failed:  154
batch:      94170 | loss: 5.12640 | failed:  154
batch:      94180 | loss: 5.17893 | failed:  154
batch:      94190 | loss: 4.98556 | failed:  154
batch:      94200 | loss: 4.57015 | failed:  154
batch:      94210 | loss: 5.32268 | failed:  154
batch:      94220 | loss: 5.42967 | failed:  154
batch:      94230 | loss: 5.12687 | failed:  154
batch:      94240 | loss: 5.25713 | failed:  154
batch:      94250 | loss: 5.22547 | failed:  154
batch:      94260 | loss: 5.24756 | failed:  154
batch:      94270 | loss: 5.22273 | failed:  154
batch:      94280 | loss: 5.02406 | failed:  154
batch:      94290 | loss: 4.89272 | failed:  154
batch:      94300 | loss: 4.97350 | failed:  154
batch:      94310 | loss: 4.69003 | failed:  154
batch:      94320 | loss: 5.16279 | failed:  154
batch:      94330 | loss: 5.07432 | failed:  154
batch:      94340 | loss: 5.18324 | failed:  154
batch:      94350 | loss: 5.12583 | failed:  154
batch:      94360 | loss: 5.18308 | failed:  154
batch:      94370 | loss: 5.15436 | failed:  154
batch:      94380 | loss: 5.14824 | failed:  154
batch:      94390 | loss: 5.15858 | failed:  154
batch:      94400 | loss: 5.11636 | failed:  154
batch:      94410 | loss: 5.14182 | failed:  154
batch:      94420 | loss: 5.13077 | failed:  154
batch:      94430 | loss: 5.08399 | failed:  154
batch:      94440 | loss: 4.86928 | failed:  154
batch:      94450 | loss: 4.80267 | failed:  154
batch:      94460 | loss: 4.99824 | failed:  154
batch:      94470 | loss: 4.99584 | failed:  154
batch:      94480 | loss: 5.14106 | failed:  154
batch:      94490 | loss: 5.03910 | failed:  154
batch:      94500 | loss: 4.92803 | failed:  154
batch:      94510 | loss: 4.94748 | failed:  154
batch:      94520 | loss: 5.18480 | failed:  154
batch:      94530 | loss: 5.21015 | failed:  154
batch:      94540 | loss: 5.19213 | failed:  154
batch:      94550 | loss: 5.16024 | failed:  154
batch:      94560 | loss: 5.15430 | failed:  154
batch:      94570 | loss: 5.16161 | failed:  154
batch:      94580 | loss: 5.19625 | failed:  154
batch:      94590 | loss: 5.09729 | failed:  154
batch:      94600 | loss: 5.23709 | failed:  154
batch:      94610 | loss: 5.14024 | failed:  154
batch:      94620 | loss: 5.16276 | failed:  154
batch:      94630 | loss: 5.12121 | failed:  154
batch:      94640 | loss: 5.03835 | failed:  154
batch:      94650 | loss: 5.21113 | failed:  154
batch:      94660 | loss: 5.16136 | failed:  154
batch:      94670 | loss: 5.21183 | failed:  154
batch:      94680 | loss: 5.17444 | failed:  154
batch:      94690 | loss: 5.16223 | failed:  154
batch:      94700 | loss: 5.16349 | failed:  154
batch:      94710 | loss: 5.15219 | failed:  154
batch:      94720 | loss: 5.21222 | failed:  154
batch:      94730 | loss: 5.08387 | failed:  154
batch:      94740 | loss: 5.23516 | failed:  154
batch:      94750 | loss: 5.23535 | failed:  154
batch:      94760 | loss: 5.28390 | failed:  154
batch:      94770 | loss: 5.23159 | failed:  154
batch:      94780 | loss: 5.27944 | failed:  154
batch:      94790 | loss: 5.27305 | failed:  154
batch:      94800 | loss: 5.14206 | failed:  154
batch:      94810 | loss: 5.17114 | failed:  154
batch:      94820 | loss: 5.05933 | failed:  154
batch:      94830 | loss: 4.92129 | failed:  154
batch:      94840 | loss: 4.96255 | failed:  154
batch:      94850 | loss: 5.06902 | failed:  154
batch:      94860 | loss: 4.97490 | failed:  154
batch:      94870 | loss: 5.16597 | failed:  154
batch:      94880 | loss: 5.19703 | failed:  154
batch:      94890 | loss: 5.09521 | failed:  154
batch:      94900 | loss: 5.02068 | failed:  154
batch:      94910 | loss: 5.06647 | failed:  154
batch:      94920 | loss: 5.17649 | failed:  154
batch:      94930 | loss: 5.27619 | failed:  154
batch:      94940 | loss: 5.22363 | failed:  154
batch:      94950 | loss: 5.25661 | failed:  154
batch:      94960 | loss: 5.22624 | failed:  154
batch:      94970 | loss: 5.10699 | failed:  154
batch:      94980 | loss: 5.15628 | failed:  154
batch:      94990 | loss: 5.24536 | failed:  154
batch:      95000 | loss: 5.20148 | failed:  154
batch:      95010 | loss: 5.07122 | failed:  154
batch:      95020 | loss: 5.11128 | failed:  154
batch:      95030 | loss: 5.13391 | failed:  154
batch:      95040 | loss: 5.12167 | failed:  154
batch:      95050 | loss: 5.08040 | failed:  154
batch:      95060 | loss: 5.24723 | failed:  154
batch:      95070 | loss: 5.02993 | failed:  154
batch:      95080 | loss: 5.14864 | failed:  154
batch:      95090 | loss: 5.07074 | failed:  154
batch:      95100 | loss: 5.14524 | failed:  154
batch:      95110 | loss: 5.15575 | failed:  154
batch:      95120 | loss: 5.16552 | failed:  154
batch:      95130 | loss: 4.97790 | failed:  154
batch:      95140 | loss: 5.10294 | failed:  154
batch:      95150 | loss: 5.09964 | failed:  154
batch:      95160 | loss: 4.97415 | failed:  154
batch:      95170 | loss: 5.14135 | failed:  154
batch:      95180 | loss: 4.84329 | failed:  154
batch:      95190 | loss: 5.14083 | failed:  154
batch:      95200 | loss: 5.03104 | failed:  154
batch:      95210 | loss: 5.08656 | failed:  154
batch:      95220 | loss: 5.10565 | failed:  154
batch:      95230 | loss: 5.16004 | failed:  154
batch:      95240 | loss: 5.05330 | failed:  154
batch:      95250 | loss: 5.12714 | failed:  154
batch:      95260 | loss: 5.15238 | failed:  154
batch:      95270 | loss: 4.99998 | failed:  154
batch:      95280 | loss: 5.00163 | failed:  154
batch:      95290 | loss: 5.21823 | failed:  154
batch:      95300 | loss: 5.16443 | failed:  154
batch:      95310 | loss: 5.07326 | failed:  154
batch:      95320 | loss: 5.33491 | failed:  154
batch:      95330 | loss: 5.23712 | failed:  154
batch:      95340 | loss: 5.24193 | failed:  154
batch:      95350 | loss: 5.24278 | failed:  154
batch:      95360 | loss: 5.17025 | failed:  154
batch:      95370 | loss: 5.12211 | failed:  154
batch:      95380 | loss: 5.06987 | failed:  154
batch:      95390 | loss: 5.09795 | failed:  154
batch:      95400 | loss: 5.11798 | failed:  154
batch:      95410 | loss: 4.88506 | failed:  154
batch:      95420 | loss: 5.09857 | failed:  154
batch:      95430 | loss: 4.35452 | failed:  154
batch:      95440 | loss: 5.05206 | failed:  154
batch:      95450 | loss: 5.09027 | failed:  154
batch:      95460 | loss: 4.94803 | failed:  154
batch:      95470 | loss: 5.05465 | failed:  154
batch:      95480 | loss: 5.29510 | failed:  154
batch:      95490 | loss: 5.18568 | failed:  154
batch:      95500 | loss: 5.14648 | failed:  154
batch:      95510 | loss: 5.09964 | failed:  154
batch:      95520 | loss: 5.11860 | failed:  154
batch:      95530 | loss: 5.15866 | failed:  154
batch:      95540 | loss: 5.04187 | failed:  154
batch:      95550 | loss: 5.02659 | failed:  154
batch:      95560 | loss: 5.24151 | failed:  154
batch:      95570 | loss: 5.06330 | failed:  154
batch:      95580 | loss: 5.02709 | failed:  154
batch:      95590 | loss: 5.20482 | failed:  154
batch:      95600 | loss: 5.14013 | failed:  154
batch:      95610 | loss: 5.03777 | failed:  154
batch:      95620 | loss: 5.08837 | failed:  154
batch:      95630 | loss: 4.96292 | failed:  154
batch:      95640 | loss: 5.13953 | failed:  154
batch:      95650 | loss: 5.20471 | failed:  154
batch:      95660 | loss: 5.20517 | failed:  154
batch:      95670 | loss: 5.12532 | failed:  154
batch:      95680 | loss: 4.69765 | failed:  154
batch:      95690 | loss: 4.91050 | failed:  154
batch:      95700 | loss: 5.22435 | failed:  154
batch:      95710 | loss: 5.18661 | failed:  154
batch:      95720 | loss: 5.23260 | failed:  154
batch:      95730 | loss: 5.30774 | failed:  154
batch:      95740 | loss: 5.03525 | failed:  154
batch:      95750 | loss: 5.24869 | failed:  154
batch:      95760 | loss: 4.98074 | failed:  154
batch:      95770 | loss: 5.16840 | failed:  154
batch:      95780 | loss: 5.22544 | failed:  154
batch:      95790 | loss: 5.18677 | failed:  154
batch:      95800 | loss: 5.14231 | failed:  154
batch:      95810 | loss: 5.12802 | failed:  154
batch:      95820 | loss: 4.94355 | failed:  154
batch:      95830 | loss: 5.22527 | failed:  154
batch:      95840 | loss: 5.24992 | failed:  154
batch:      95850 | loss: 5.17135 | failed:  154
batch:      95860 | loss: 5.14311 | failed:  154
batch:      95870 | loss: 5.19097 | failed:  154
batch:      95880 | loss: 5.19562 | failed:  154
batch:      95890 | loss: 5.11214 | failed:  154
batch:      95900 | loss: 5.20662 | failed:  154
batch:      95910 | loss: 5.25090 | failed:  154
batch:      95920 | loss: 5.17904 | failed:  154
batch:      95930 | loss: 5.14237 | failed:  154
batch:      95940 | loss: 5.20544 | failed:  154
batch:      95950 | loss: 5.20816 | failed:  154
batch:      95960 | loss: 5.17066 | failed:  154
batch:      95970 | loss: 5.13983 | failed:  154
batch:      95980 | loss: 5.02189 | failed:  154
batch:      95990 | loss: 5.00736 | failed:  154
batch:      96000 | loss: 5.14735 | failed:  154
batch:      96010 | loss: 5.20619 | failed:  154
batch:      96020 | loss: 5.18794 | failed:  154
batch:      96030 | loss: 5.20708 | failed:  154
batch:      96040 | loss: 5.04695 | failed:  154
batch:      96050 | loss: 5.16922 | failed:  154
batch:      96060 | loss: 4.88552 | failed:  154
batch:      96070 | loss: 5.22279 | failed:  154
batch:      96080 | loss: 5.13264 | failed:  154
batch:      96090 | loss: 5.10512 | failed:  154
batch:      96100 | loss: 5.09858 | failed:  154
batch:      96110 | loss: 5.49973 | failed:  154
batch:      96120 | loss: 5.08860 | failed:  154
batch:      96130 | loss: 5.10648 | failed:  154
batch:      96140 | loss: 5.16069 | failed:  154
batch:      96150 | loss: 5.14611 | failed:  154
batch:      96160 | loss: 5.20724 | failed:  154
batch:      96170 | loss: 5.12789 | failed:  154
batch:      96180 | loss: 5.13295 | failed:  154
batch:      96190 | loss: 5.01232 | failed:  154
batch:      96200 | loss: 5.12216 | failed:  154
batch:      96210 | loss: 5.12818 | failed:  154
batch:      96220 | loss: 5.13501 | failed:  154
batch:      96230 | loss: 5.16129 | failed:  154
batch:      96240 | loss: 5.11290 | failed:  154
batch:      96250 | loss: 5.17683 | failed:  154
batch:      96260 | loss: 5.13034 | failed:  154
batch:      96270 | loss: 5.17258 | failed:  154
batch:      96280 | loss: 5.11763 | failed:  154
batch:      96290 | loss: 5.14287 | failed:  154
batch:      96300 | loss: 5.01512 | failed:  154
batch:      96310 | loss: 5.07970 | failed:  154
batch:      96320 | loss: 5.23920 | failed:  154
batch:      96330 | loss: 5.29558 | failed:  154
batch:      96340 | loss: 4.96538 | failed:  154
batch:      96350 | loss: 5.09460 | failed:  154
batch:      96360 | loss: 4.94635 | failed:  154
batch:      96370 | loss: 5.09290 | failed:  154
batch:      96380 | loss: 4.95914 | failed:  154
batch:      96390 | loss: 4.92765 | failed:  154
batch:      96400 | loss: 5.17807 | failed:  154
batch:      96410 | loss: 5.13757 | failed:  154
batch:      96420 | loss: 5.17237 | failed:  154
batch:      96430 | loss: 5.21835 | failed:  154
batch:      96440 | loss: 5.24309 | failed:  154
batch:      96450 | loss: 5.04763 | failed:  154
batch:      96460 | loss: 5.07276 | failed:  154
batch:      96470 | loss: 5.04543 | failed:  154
batch:      96480 | loss: 5.12822 | failed:  154
batch:      96490 | loss: 5.19803 | failed:  154
batch:      96500 | loss: 5.15695 | failed:  154
batch:      96510 | loss: 5.05667 | failed:  154
batch:      96520 | loss: 5.15441 | failed:  154
batch:      96530 | loss: 5.14384 | failed:  154
batch:      96540 | loss: 5.24194 | failed:  154
batch:      96550 | loss: 5.22367 | failed:  154
batch:      96560 | loss: 5.10869 | failed:  154
batch:      96570 | loss: 5.20142 | failed:  154
batch:      96580 | loss: 5.20649 | failed:  154
batch:      96590 | loss: 5.13774 | failed:  154
batch:      96600 | loss: 5.15890 | failed:  154
batch:      96610 | loss: 5.20580 | failed:  154
batch:      96620 | loss: 5.12003 | failed:  154
batch:      96630 | loss: 4.89330 | failed:  154
batch:      96640 | loss: 5.14734 | failed:  154
batch:      96650 | loss: 5.14149 | failed:  154
batch:      96660 | loss: 4.82688 | failed:  154
batch:      96670 | loss: 5.17789 | failed:  154
batch:      96680 | loss: 5.14195 | failed:  154
batch:      96690 | loss: 5.12805 | failed:  154
batch:      96700 | loss: 5.09067 | failed:  154
batch:      96710 | loss: 5.20466 | failed:  154
batch:      96720 | loss: 5.15341 | failed:  154
batch:      96730 | loss: 4.97276 | failed:  154
batch:      96740 | loss: 5.15051 | failed:  154
batch:      96750 | loss: 5.03855 | failed:  154
batch:      96760 | loss: 5.04601 | failed:  154
batch:      96770 | loss: 5.12492 | failed:  154
batch:      96780 | loss: 5.11830 | failed:  154
batch:      96790 | loss: 5.23422 | failed:  154
batch:      96800 | loss: 5.18287 | failed:  154
batch:      96810 | loss: 5.10568 | failed:  154
batch:      96820 | loss: 5.16770 | failed:  154
batch:      96830 | loss: 5.17405 | failed:  154
batch:      96840 | loss: 4.99378 | failed:  154
batch:      96850 | loss: 5.09139 | failed:  154
batch:      96860 | loss: 5.24202 | failed:  154
batch:      96870 | loss: 5.04713 | failed:  154
batch:      96880 | loss: 5.14335 | failed:  154
batch:      96890 | loss: 5.09236 | failed:  154
batch:      96900 | loss: 5.12762 | failed:  154
batch:      96910 | loss: 5.15221 | failed:  154
batch:      96920 | loss: 5.13259 | failed:  154
batch:      96930 | loss: 5.13200 | failed:  154
batch:      96940 | loss: 5.12986 | failed:  154
batch:      96950 | loss: 5.20204 | failed:  154
batch:      96960 | loss: 5.26484 | failed:  154
batch:      96970 | loss: 5.25506 | failed:  154
batch:      96980 | loss: 5.13945 | failed:  154
batch:      96990 | loss: 5.10075 | failed:  154
batch:      97000 | loss: 5.07977 | failed:  154
batch:      97010 | loss: 5.03931 | failed:  154
batch:      97020 | loss: 5.11309 | failed:  154
batch:      97030 | loss: 5.10937 | failed:  154
batch:      97040 | loss: 5.12148 | failed:  154
batch:      97050 | loss: 5.14898 | failed:  154
batch:      97060 | loss: 5.10078 | failed:  154
batch:      97070 | loss: 5.19987 | failed:  154
batch:      97080 | loss: 4.97683 | failed:  154
batch:      97090 | loss: 5.19713 | failed:  154
batch:      97100 | loss: 5.19434 | failed:  154
batch:      97110 | loss: 5.17029 | failed:  154
batch:      97120 | loss: 5.13440 | failed:  154
batch:      97130 | loss: 5.13764 | failed:  154
batch:      97140 | loss: 5.23821 | failed:  154
batch:      97150 | loss: 5.04325 | failed:  154
batch:      97160 | loss: 5.12909 | failed:  154
batch:      97170 | loss: 4.84521 | failed:  154
batch:      97180 | loss: 4.96342 | failed:  154
batch:      97190 | loss: 5.08309 | failed:  154
batch:      97200 | loss: 5.00877 | failed:  154
batch:      97210 | loss: 4.99708 | failed:  154
batch:      97220 | loss: 4.74356 | failed:  154
batch:      97230 | loss: 5.19804 | failed:  154
batch:      97240 | loss: 5.08094 | failed:  154
batch:      97250 | loss: 5.18714 | failed:  154
batch:      97260 | loss: 5.13570 | failed:  154
batch:      97270 | loss: 5.18393 | failed:  154
batch:      97280 | loss: 5.20189 | failed:  154
batch:      97290 | loss: 5.16970 | failed:  154
batch:      97300 | loss: 5.17807 | failed:  154
batch:      97310 | loss: 5.25337 | failed:  154
batch:      97320 | loss: 4.86041 | failed:  154
batch:      97330 | loss: 5.12702 | failed:  154
batch:      97340 | loss: 4.00049 | failed:  154
batch:      97350 | loss: 5.12578 | failed:  154
batch:      97360 | loss: 5.01471 | failed:  154
batch:      97370 | loss: 5.07519 | failed:  154
batch:      97380 | loss: 5.17557 | failed:  154
batch:      97390 | loss: 5.22628 | failed:  154
batch:      97400 | loss: 5.15740 | failed:  154
batch:      97410 | loss: 5.04966 | failed:  154
batch:      97420 | loss: 5.07577 | failed:  154
batch:      97430 | loss: 5.07186 | failed:  154
batch:      97440 | loss: 5.13079 | failed:  154
batch:      97450 | loss: 5.17784 | failed:  154
batch:      97460 | loss: 5.06604 | failed:  154
batch:      97470 | loss: 5.05490 | failed:  154
batch:      97480 | loss: 4.99684 | failed:  154
batch:      97490 | loss: 5.11252 | failed:  154
batch:      97500 | loss: 5.15325 | failed:  154
batch:      97510 | loss: 5.22426 | failed:  154
batch:      97520 | loss: 5.11371 | failed:  154
batch:      97530 | loss: 5.08250 | failed:  154
batch:      97540 | loss: 5.12933 | failed:  154
batch:      97550 | loss: 4.95493 | failed:  154
batch:      97560 | loss: 5.07743 | failed:  154
batch:      97570 | loss: 5.00647 | failed:  154
batch:      97580 | loss: 5.13240 | failed:  154
batch:      97590 | loss: 5.11955 | failed:  154
batch:      97600 | loss: 5.24509 | failed:  154
batch:      97610 | loss: 4.97047 | failed:  154
batch:      97620 | loss: 4.94882 | failed:  154
batch:      97630 | loss: 5.08743 | failed:  154
batch:      97640 | loss: 5.05972 | failed:  154
batch:      97650 | loss: 5.24185 | failed:  154
batch:      97660 | loss: 5.21229 | failed:  154
batch:      97670 | loss: 5.17575 | failed:  154
batch:      97680 | loss: 5.13884 | failed:  154
batch:      97690 | loss: 5.21364 | failed:  154
batch:      97700 | loss: 5.12393 | failed:  154
batch:      97710 | loss: 5.07929 | failed:  154
batch:      97720 | loss: 5.16616 | failed:  154
batch:      97730 | loss: 5.15125 | failed:  154
batch:      97740 | loss: 5.07132 | failed:  154
batch:      97750 | loss: 5.22791 | failed:  154
batch:      97760 | loss: 5.13363 | failed:  154
batch:      97770 | loss: 4.95287 | failed:  154
batch:      97780 | loss: 5.09104 | failed:  154
batch:      97790 | loss: 4.97549 | failed:  154
batch:      97800 | loss: 5.12154 | failed:  154
batch:      97810 | loss: 5.26051 | failed:  154
batch:      97820 | loss: 5.01602 | failed:  154
batch:      97830 | loss: 4.98438 | failed:  154
batch:      97840 | loss: 5.10059 | failed:  154
batch:      97850 | loss: 4.92670 | failed:  154
batch:      97860 | loss: 5.20352 | failed:  154
batch:      97870 | loss: 5.17607 | failed:  154
batch:      97880 | loss: 5.16930 | failed:  154
batch:      97890 | loss: 5.13587 | failed:  154
batch:      97900 | loss: 5.23234 | failed:  154
batch:      97910 | loss: 5.16794 | failed:  154
batch:      97920 | loss: 5.20969 | failed:  154
batch:      97930 | loss: 5.18453 | failed:  154
batch:      97940 | loss: 5.16150 | failed:  154
batch:      97950 | loss: 5.21226 | failed:  154
batch:      97960 | loss: 5.10909 | failed:  154
batch:      97970 | loss: 5.06893 | failed:  154
batch:      97980 | loss: 5.09885 | failed:  154
batch:      97990 | loss: 5.15519 | failed:  154
batch:      98000 | loss: 5.13676 | failed:  154
batch:      98010 | loss: 5.20845 | failed:  154
batch:      98020 | loss: 5.14172 | failed:  154
batch:      98030 | loss: 5.05496 | failed:  154
batch:      98040 | loss: 5.07349 | failed:  154
batch:      98050 | loss: 4.60108 | failed:  154
batch:      98060 | loss: 5.16602 | failed:  154
batch:      98070 | loss: 5.04438 | failed:  154
batch:      98080 | loss: 5.20122 | failed:  154
batch:      98090 | loss: 5.20131 | failed:  154
batch:      98100 | loss: 5.14281 | failed:  154
batch:      98110 | loss: 5.07728 | failed:  154
batch:      98120 | loss: 5.24957 | failed:  154
batch:      98130 | loss: 5.19229 | failed:  154
batch:      98140 | loss: 4.88012 | failed:  154
batch:      98150 | loss: 5.24600 | failed:  154
batch:      98160 | loss: 5.12339 | failed:  154
batch:      98170 | loss: 5.14181 | failed:  154
batch:      98180 | loss: 5.09503 | failed:  154
batch:      98190 | loss: 5.05638 | failed:  154
batch:      98200 | loss: 4.93422 | failed:  154
batch:      98210 | loss: 5.08658 | failed:  154
batch:      98220 | loss: 5.07921 | failed:  154
batch:      98230 | loss: 5.13624 | failed:  154
batch:      98240 | loss: 5.07038 | failed:  154
batch:      98250 | loss: 5.06478 | failed:  154
batch:      98260 | loss: 5.11864 | failed:  154
batch:      98270 | loss: 5.03550 | failed:  154
batch:      98280 | loss: 5.02282 | failed:  154
batch:      98290 | loss: 4.97619 | failed:  154
batch:      98300 | loss: 5.07189 | failed:  154
batch:      98310 | loss: 5.06060 | failed:  154
batch:      98320 | loss: 5.01965 | failed:  154
batch:      98330 | loss: 5.21910 | failed:  154
batch:      98340 | loss: 5.24724 | failed:  154
batch:      98350 | loss: 5.39151 | failed:  154
batch:      98360 | loss: 5.22594 | failed:  154
batch:      98370 | loss: 5.18132 | failed:  154
batch:      98380 | loss: 5.10312 | failed:  154
batch:      98390 | loss: 5.04704 | failed:  154
batch:      98400 | loss: 5.10572 | failed:  154
batch:      98410 | loss: 5.14095 | failed:  154
batch:      98420 | loss: 5.17665 | failed:  154
batch:      98430 | loss: 5.10826 | failed:  154
batch:      98440 | loss: 4.97366 | failed:  154
batch:      98450 | loss: 5.09540 | failed:  154
batch:      98460 | loss: 5.23318 | failed:  154
batch:      98470 | loss: 5.05842 | failed:  154
batch:      98480 | loss: 4.95465 | failed:  154
batch:      98490 | loss: 5.15900 | failed:  154
batch:      98500 | loss: 5.02034 | failed:  154
batch:      98510 | loss: 5.13629 | failed:  154
batch:      98520 | loss: 5.13264 | failed:  154
batch:      98530 | loss: 5.03492 | failed:  154
batch:      98540 | loss: 5.07962 | failed:  154
batch:      98550 | loss: 5.23856 | failed:  154
batch:      98560 | loss: 5.26379 | failed:  154
batch:      98570 | loss: 5.24000 | failed:  154
batch:      98580 | loss: 5.13788 | failed:  154
batch:      98590 | loss: 5.14269 | failed:  154
batch:      98600 | loss: 5.13179 | failed:  154
batch:      98610 | loss: 5.10871 | failed:  154
batch:      98620 | loss: 5.17572 | failed:  154
batch:      98630 | loss: 5.17283 | failed:  154
batch:      98640 | loss: 5.07312 | failed:  154
batch:      98650 | loss: 5.14472 | failed:  154
batch:      98660 | loss: 5.14653 | failed:  154
batch:      98670 | loss: 5.10217 | failed:  154
batch:      98680 | loss: 5.02022 | failed:  154
batch:      98690 | loss: 5.05450 | failed:  154
batch:      98700 | loss: 5.13701 | failed:  154
batch:      98710 | loss: 5.13469 | failed:  154
batch:      98720 | loss: 5.15899 | failed:  154
batch:      98730 | loss: 5.10937 | failed:  154
batch:      98740 | loss: 5.22887 | failed:  154
batch:      98750 | loss: 5.11399 | failed:  154
batch:      98760 | loss: 5.09746 | failed:  154
batch:      98770 | loss: 5.02316 | failed:  154
batch:      98780 | loss: 5.12365 | failed:  154
batch:      98790 | loss: 5.17582 | failed:  154
batch:      98800 | loss: 5.13187 | failed:  154
batch:      98810 | loss: 5.13861 | failed:  154
batch:      98820 | loss: 5.05789 | failed:  154
batch:      98830 | loss: 5.19064 | failed:  154
batch:      98840 | loss: 5.03751 | failed:  154
batch:      98850 | loss: 5.11290 | failed:  154
batch:      98860 | loss: 5.23631 | failed:  154
batch:      98870 | loss: 5.22378 | failed:  154
batch:      98880 | loss: 5.13417 | failed:  154
batch:      98890 | loss: 5.02822 | failed:  154
batch:      98900 | loss: 5.18429 | failed:  154
batch:      98910 | loss: 5.21400 | failed:  154
batch:      98920 | loss: 5.16798 | failed:  154
batch:      98930 | loss: 5.12400 | failed:  154
batch:      98940 | loss: 5.12880 | failed:  154
batch:      98950 | loss: 5.04512 | failed:  154
batch:      98960 | loss: 5.17715 | failed:  154
batch:      98970 | loss: 5.19742 | failed:  154
batch:      98980 | loss: 5.21406 | failed:  154
batch:      98990 | loss: 5.14595 | failed:  154
batch:      99000 | loss: 4.86552 | failed:  154
batch:      99010 | loss: 5.13991 | failed:  154
batch:      99020 | loss: 5.03311 | failed:  154
batch:      99030 | loss: 5.11341 | failed:  154
batch:      99040 | loss: 5.14817 | failed:  154
batch:      99050 | loss: 5.20805 | failed:  154
batch:      99060 | loss: 5.12333 | failed:  154
batch:      99070 | loss: 5.10930 | failed:  154
batch:      99080 | loss: 5.14164 | failed:  154
batch:      99090 | loss: 4.96919 | failed:  154
batch:      99100 | loss: 5.10523 | failed:  154
batch:      99110 | loss: 5.13458 | failed:  154
batch:      99120 | loss: 5.07491 | failed:  154
batch:      99130 | loss: 5.12583 | failed:  154
batch:      99140 | loss: 5.18323 | failed:  154
batch:      99150 | loss: 5.13023 | failed:  154
batch:      99160 | loss: 5.21168 | failed:  154
batch:      99170 | loss: 5.14136 | failed:  154
batch:      99180 | loss: 5.13080 | failed:  154
batch:      99190 | loss: 5.22495 | failed:  154
batch:      99200 | loss: 5.06068 | failed:  154
batch:      99210 | loss: 5.18456 | failed:  154
batch:      99220 | loss: 5.12304 | failed:  154
batch:      99230 | loss: 5.06928 | failed:  154
batch:      99240 | loss: 5.18455 | failed:  154
batch:      99250 | loss: 5.06125 | failed:  154
batch:      99260 | loss: 5.11303 | failed:  154
batch:      99270 | loss: 5.09781 | failed:  154
batch:      99280 | loss: 5.15976 | failed:  154
batch:      99290 | loss: 4.98877 | failed:  154
batch:      99300 | loss: 5.01816 | failed:  154
batch:      99310 | loss: 5.34453 | failed:  154
batch:      99320 | loss: 5.17832 | failed:  154
batch:      99330 | loss: 5.12117 | failed:  154
batch:      99340 | loss: 5.09669 | failed:  154
batch:      99350 | loss: 5.15790 | failed:  154
batch:      99360 | loss: 5.19093 | failed:  154
batch:      99370 | loss: 5.15983 | failed:  154
batch:      99380 | loss: 5.15984 | failed:  154
batch:      99390 | loss: 5.16569 | failed:  154
batch:      99400 | loss: 4.99278 | failed:  154
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      99410 | loss: 5.17091 | failed:  159
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:      99430 | loss: 4.93744 | failed:  169
batch:      99440 | loss: 5.06689 | failed:  169
batch:      99450 | loss: 5.14999 | failed:  169
batch:      99460 | loss: 5.18652 | failed:  169
batch:      99470 | loss: 5.27797 | failed:  169
batch:      99480 | loss: 4.97121 | failed:  169
batch:      99490 | loss: 5.17723 | failed:  169
batch:      99500 | loss: 5.15839 | failed:  169
batch:      99510 | loss: 5.14627 | failed:  169
batch:      99520 | loss: 5.19089 | failed:  169
batch:      99530 | loss: 5.06940 | failed:  169
batch:      99540 | loss: 4.76014 | failed:  169
batch:      99550 | loss: 5.14878 | failed:  169
batch:      99560 | loss: 5.23771 | failed:  169
batch:      99570 | loss: 5.08917 | failed:  169
batch:      99580 | loss: 4.96186 | failed:  169
batch:      99590 | loss: 5.11640 | failed:  169
batch:      99600 | loss: 5.14014 | failed:  169
batch:      99610 | loss: 4.94740 | failed:  169
batch:      99620 | loss: 4.82489 | failed:  169
batch:      99630 | loss: 5.03935 | failed:  169
batch:      99640 | loss: 4.59835 | failed:  169
batch:      99650 | loss: 5.15988 | failed:  169
batch:      99660 | loss: 5.15205 | failed:  169
batch:      99670 | loss: 5.10551 | failed:  169
batch:      99680 | loss: 5.19489 | failed:  169
batch:      99690 | loss: 4.95332 | failed:  169
batch:      99700 | loss: 5.16198 | failed:  169
batch:      99710 | loss: 5.10444 | failed:  169
batch:      99720 | loss: 5.13277 | failed:  169
batch:      99730 | loss: 5.10931 | failed:  169
batch:      99740 | loss: 5.13680 | failed:  169
batch:      99750 | loss: 4.83895 | failed:  169
batch:      99760 | loss: 5.09473 | failed:  169
batch:      99770 | loss: 5.19339 | failed:  169
batch:      99780 | loss: 5.11932 | failed:  169
batch:      99790 | loss: 5.16394 | failed:  169
batch:      99800 | loss: 5.05137 | failed:  169
batch:      99810 | loss: 5.17339 | failed:  169
batch:      99820 | loss: 5.03836 | failed:  169
batch:      99830 | loss: 4.77131 | failed:  169
batch:      99840 | loss: 4.60401 | failed:  169
batch:      99850 | loss: 4.72464 | failed:  169
batch:      99860 | loss: 5.12682 | failed:  169
batch:      99870 | loss: 5.00937 | failed:  169
batch:      99880 | loss: 5.03001 | failed:  169
batch:      99890 | loss: 5.09771 | failed:  169
batch:      99900 | loss: 5.10361 | failed:  169
batch:      99910 | loss: 5.10944 | failed:  169
batch:      99920 | loss: 5.09913 | failed:  169
batch:      99930 | loss: 4.51875 | failed:  169
batch:      99940 | loss: 5.02081 | failed:  169
batch:      99950 | loss: 4.59116 | failed:  169
batch:      99960 | loss: 5.10623 | failed:  169
batch:      99970 | loss: 4.60589 | failed:  169
batch:      99980 | loss: 4.99052 | failed:  169
batch:      99990 | loss: 5.04190 | failed:  169
batch:     100000 | loss: 5.00581 | failed:  169
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     100010 | loss: 5.01576 | failed:  169
batch:     100020 | loss: 5.05587 | failed:  169
batch:     100030 | loss: 5.04030 | failed:  169
batch:     100040 | loss: 4.45179 | failed:  169
batch:     100050 | loss: 5.04712 | failed:  169
batch:     100060 | loss: 5.00391 | failed:  169
batch:     100070 | loss: 5.00244 | failed:  169
batch:     100080 | loss: 5.06581 | failed:  169
batch:     100090 | loss: 4.99353 | failed:  169
batch:     100100 | loss: 5.01965 | failed:  169
batch:     100110 | loss: 5.05862 | failed:  169
batch:     100120 | loss: 5.02465 | failed:  169
batch:     100130 | loss: 5.00919 | failed:  169
batch:     100140 | loss: 4.99162 | failed:  169
batch:     100150 | loss: 5.12971 | failed:  169
batch:     100160 | loss: 4.98460 | failed:  169
batch:     100170 | loss: 4.93969 | failed:  169
batch:     100180 | loss: 5.21124 | failed:  169
batch:     100190 | loss: 5.10707 | failed:  169
batch:     100200 | loss: 5.09079 | failed:  169
batch:     100210 | loss: 5.06802 | failed:  169
batch:     100220 | loss: 5.16429 | failed:  169
batch:     100230 | loss: 5.22933 | failed:  169
batch:     100240 | loss: 4.87499 | failed:  169
batch:     100250 | loss: 5.20254 | failed:  169
batch:     100260 | loss: 5.14233 | failed:  169
batch:     100270 | loss: 5.23242 | failed:  169
batch:     100280 | loss: 5.19744 | failed:  169
batch:     100290 | loss: 5.19126 | failed:  169
batch:     100300 | loss: 5.22847 | failed:  169
batch:     100310 | loss: 5.14955 | failed:  169
batch:     100320 | loss: 5.22898 | failed:  169
batch:     100330 | loss: 5.15925 | failed:  169
batch:     100340 | loss: 5.12656 | failed:  169
batch:     100350 | loss: 5.06144 | failed:  169
batch:     100360 | loss: 5.17428 | failed:  169
batch:     100370 | loss: 5.13454 | failed:  169
batch:     100380 | loss: 5.12889 | failed:  169
batch:     100390 | loss: 4.99650 | failed:  169
batch:     100400 | loss: 5.06094 | failed:  169
batch:     100410 | loss: 5.08753 | failed:  169
batch:     100420 | loss: 5.01517 | failed:  169
batch:     100430 | loss: 5.18355 | failed:  169
batch:     100440 | loss: 5.22251 | failed:  169
batch:     100450 | loss: 5.25367 | failed:  169
batch:     100460 | loss: 5.11386 | failed:  169
batch:     100470 | loss: 5.25356 | failed:  169
batch:     100480 | loss: 5.00350 | failed:  169
batch:     100490 | loss: 5.12157 | failed:  169
batch:     100500 | loss: 5.07684 | failed:  169
batch:     100510 | loss: 5.23226 | failed:  169
batch:     100520 | loss: 5.12499 | failed:  169
batch:     100530 | loss: 5.14618 | failed:  169
batch:     100540 | loss: 5.22848 | failed:  169
batch:     100550 | loss: 5.02921 | failed:  169
batch:     100560 | loss: 5.20073 | failed:  169
batch:     100570 | loss: 5.14455 | failed:  169
batch:     100580 | loss: 5.10291 | failed:  169
batch:     100590 | loss: 5.10596 | failed:  169
batch:     100600 | loss: 5.19572 | failed:  169
batch:     100610 | loss: 5.13170 | failed:  169
batch:     100620 | loss: 5.20094 | failed:  169
batch:     100630 | loss: 5.10320 | failed:  169
batch:     100640 | loss: 5.10482 | failed:  169
batch:     100650 | loss: 5.08641 | failed:  169
batch:     100660 | loss: 5.06663 | failed:  169
batch:     100670 | loss: 5.07193 | failed:  169
batch:     100680 | loss: 4.63410 | failed:  169
batch:     100690 | loss: 5.12762 | failed:  169
batch:     100700 | loss: 5.13249 | failed:  169
batch:     100710 | loss: 5.05670 | failed:  169
batch:     100720 | loss: 5.18321 | failed:  169
batch:     100730 | loss: 5.20308 | failed:  169
batch:     100740 | loss: 5.18890 | failed:  169
batch:     100750 | loss: 5.12820 | failed:  169
batch:     100760 | loss: 5.02879 | failed:  169
batch:     100770 | loss: 5.12400 | failed:  169
batch:     100780 | loss: 5.11701 | failed:  169
batch:     100790 | loss: 5.16762 | failed:  169
batch:     100800 | loss: 5.17714 | failed:  169
batch:     100810 | loss: 5.23584 | failed:  169
batch:     100820 | loss: 5.19369 | failed:  169
batch:     100830 | loss: 5.05502 | failed:  169
batch:     100840 | loss: 5.27972 | failed:  169
batch:     100850 | loss: 5.06648 | failed:  169
batch:     100860 | loss: 5.11857 | failed:  169
batch:     100870 | loss: 5.06470 | failed:  169
batch:     100880 | loss: 5.04057 | failed:  169
batch:     100890 | loss: 5.10143 | failed:  169
batch:     100900 | loss: 5.15979 | failed:  169
batch:     100910 | loss: 5.12475 | failed:  169
batch:     100920 | loss: 5.08706 | failed:  169
batch:     100930 | loss: 5.07119 | failed:  169
batch:     100940 | loss: 5.06425 | failed:  169
batch:     100950 | loss: 5.16280 | failed:  169
batch:     100960 | loss: 5.12610 | failed:  169
batch:     100970 | loss: 5.22661 | failed:  169
batch:     100980 | loss: 5.23340 | failed:  169
batch:     100990 | loss: 5.09582 | failed:  169
batch:     101000 | loss: 5.21850 | failed:  169
batch:     101010 | loss: 5.12502 | failed:  169
batch:     101020 | loss: 5.15375 | failed:  169
batch:     101030 | loss: 5.23257 | failed:  169
batch:     101040 | loss: 5.24480 | failed:  169
batch:     101050 | loss: 4.80065 | failed:  169
batch:     101060 | loss: 5.14280 | failed:  169
batch:     101070 | loss: 5.22355 | failed:  169
batch:     101080 | loss: 5.33487 | failed:  169
batch:     101090 | loss: 5.21297 | failed:  169
batch:     101100 | loss: 5.16179 | failed:  169
batch:     101110 | loss: 5.01169 | failed:  169
batch:     101120 | loss: 5.25687 | failed:  169
batch:     101130 | loss: 5.13232 | failed:  169
batch:     101140 | loss: 5.22145 | failed:  169
batch:     101150 | loss: 5.11776 | failed:  169
batch:     101160 | loss: 5.15334 | failed:  169
batch:     101170 | loss: 5.05728 | failed:  169
batch:     101180 | loss: 5.15944 | failed:  169
batch:     101190 | loss: 5.04914 | failed:  169
batch:     101200 | loss: 5.10156 | failed:  169
batch:     101210 | loss: 4.98179 | failed:  169
batch:     101220 | loss: 4.91737 | failed:  169
batch:     101230 | loss: 5.05428 | failed:  169
batch:     101240 | loss: 5.25563 | failed:  169
batch:     101250 | loss: 5.12416 | failed:  169
batch:     101260 | loss: 5.04260 | failed:  169
batch:     101270 | loss: 5.13242 | failed:  169
batch:     101280 | loss: 5.04554 | failed:  169
batch:     101290 | loss: 5.22634 | failed:  169
batch:     101300 | loss: 5.18071 | failed:  169
batch:     101310 | loss: 5.09334 | failed:  169
batch:     101320 | loss: 5.18176 | failed:  169
batch:     101330 | loss: 5.08733 | failed:  169
batch:     101340 | loss: 5.19632 | failed:  169
batch:     101350 | loss: 5.17936 | failed:  169
batch:     101360 | loss: 5.18555 | failed:  169
batch:     101370 | loss: 5.14952 | failed:  169
batch:     101380 | loss: 5.17499 | failed:  169
batch:     101390 | loss: 5.13394 | failed:  169
batch:     101400 | loss: 4.97893 | failed:  169
batch:     101410 | loss: 4.84590 | failed:  169
batch:     101420 | loss: 5.08469 | failed:  169
batch:     101430 | loss: 5.17600 | failed:  169
batch:     101440 | loss: 5.03660 | failed:  169
batch:     101450 | loss: 5.19013 | failed:  169
batch:     101460 | loss: 5.20213 | failed:  169
batch:     101470 | loss: 5.16852 | failed:  169
batch:     101480 | loss: 5.13623 | failed:  169
batch:     101490 | loss: 5.09578 | failed:  169
batch:     101500 | loss: 5.08533 | failed:  169
batch:     101510 | loss: 5.19965 | failed:  169
batch:     101520 | loss: 5.17087 | failed:  169
batch:     101530 | loss: 5.18943 | failed:  169
batch:     101540 | loss: 5.03464 | failed:  169
batch:     101550 | loss: 4.96585 | failed:  169
batch:     101560 | loss: 5.14358 | failed:  169
batch:     101570 | loss: 5.08578 | failed:  169
batch:     101580 | loss: 5.20029 | failed:  169
batch:     101590 | loss: 5.13845 | failed:  169
batch:     101600 | loss: 5.19518 | failed:  169
batch:     101610 | loss: 5.20737 | failed:  169
batch:     101620 | loss: 5.19073 | failed:  169
batch:     101630 | loss: 5.26574 | failed:  169
batch:     101640 | loss: 5.22218 | failed:  169
batch:     101650 | loss: 5.08662 | failed:  169
batch:     101660 | loss: 5.06402 | failed:  169
batch:     101670 | loss: 5.10921 | failed:  169
batch:     101680 | loss: 5.09697 | failed:  169
batch:     101690 | loss: 5.20120 | failed:  169
batch:     101700 | loss: 5.21095 | failed:  169
batch:     101710 | loss: 5.08057 | failed:  169
batch:     101720 | loss: 5.21192 | failed:  169
batch:     101730 | loss: 5.12672 | failed:  169
batch:     101740 | loss: 5.04681 | failed:  169
batch:     101750 | loss: 5.18701 | failed:  169
batch:     101760 | loss: 5.10511 | failed:  169
batch:     101770 | loss: 5.10103 | failed:  169
batch:     101780 | loss: 4.99336 | failed:  169
batch:     101790 | loss: 5.08815 | failed:  169
batch:     101800 | loss: 5.15745 | failed:  169
batch:     101810 | loss: 5.14422 | failed:  169
batch:     101820 | loss: 5.15182 | failed:  169
batch:     101830 | loss: 5.10253 | failed:  169
batch:     101840 | loss: 5.14527 | failed:  169
batch:     101850 | loss: 4.99499 | failed:  169
batch:     101860 | loss: 5.07192 | failed:  169
batch:     101870 | loss: 5.09721 | failed:  169
batch:     101880 | loss: 4.74227 | failed:  169
batch:     101890 | loss: 5.05777 | failed:  169
batch:     101900 | loss: 5.12124 | failed:  169
batch:     101910 | loss: 5.30623 | failed:  169
batch:     101920 | loss: 5.27713 | failed:  169
batch:     101930 | loss: 4.92073 | failed:  169
batch:     101940 | loss: 4.80118 | failed:  169
batch:     101950 | loss: 5.21541 | failed:  169
batch:     101960 | loss: 5.14508 | failed:  169
batch:     101970 | loss: 5.09598 | failed:  169
batch:     101980 | loss: 5.21879 | failed:  169
batch:     101990 | loss: 5.13224 | failed:  169
batch:     102000 | loss: 4.86025 | failed:  169
batch:     102010 | loss: 4.97557 | failed:  169
batch:     102020 | loss: 5.22836 | failed:  169
batch:     102030 | loss: 5.17875 | failed:  169
batch:     102040 | loss: 5.05065 | failed:  169
batch:     102050 | loss: 5.10714 | failed:  169
batch:     102060 | loss: 5.22239 | failed:  169
batch:     102070 | loss: 5.18027 | failed:  169
batch:     102080 | loss: 5.08277 | failed:  169
batch:     102090 | loss: 5.13036 | failed:  169
batch:     102100 | loss: 5.03616 | failed:  169
batch:     102110 | loss: 5.12904 | failed:  169
batch:     102120 | loss: 5.24112 | failed:  169
batch:     102130 | loss: 5.20573 | failed:  169
batch:     102140 | loss: 5.23277 | failed:  169
batch:     102150 | loss: 5.06815 | failed:  169
batch:     102160 | loss: 5.20269 | failed:  169
batch:     102170 | loss: 4.90110 | failed:  169
batch:     102180 | loss: 5.04762 | failed:  169
batch:     102190 | loss: 5.10797 | failed:  169
batch:     102200 | loss: 4.91817 | failed:  169
batch:     102210 | loss: 5.04689 | failed:  169
batch:     102220 | loss: 5.01919 | failed:  169
batch:     102230 | loss: 5.06309 | failed:  169
batch:     102240 | loss: 5.15268 | failed:  169
batch:     102250 | loss: 5.16296 | failed:  169
batch:     102260 | loss: 5.25804 | failed:  169
batch:     102270 | loss: 5.01240 | failed:  169
batch:     102280 | loss: 5.17102 | failed:  169
batch:     102290 | loss: 5.28430 | failed:  169
batch:     102300 | loss: 5.24852 | failed:  169
batch:     102310 | loss: 5.25406 | failed:  169
batch:     102320 | loss: 5.22822 | failed:  169
batch:     102330 | loss: 5.04407 | failed:  169
batch:     102340 | loss: 5.23024 | failed:  169
batch:     102350 | loss: 5.24166 | failed:  169
batch:     102360 | loss: 5.12398 | failed:  169
batch:     102370 | loss: 5.27220 | failed:  169
batch:     102380 | loss: 5.16655 | failed:  169
batch:     102390 | loss: 5.17251 | failed:  169
batch:     102400 | loss: 5.19825 | failed:  169
batch:     102410 | loss: 5.19105 | failed:  169
batch:     102420 | loss: 5.21910 | failed:  169
batch:     102430 | loss: 5.15791 | failed:  169
batch:     102440 | loss: 5.07516 | failed:  169
batch:     102450 | loss: 5.24577 | failed:  169
batch:     102460 | loss: 5.13859 | failed:  169
batch:     102470 | loss: 5.15390 | failed:  169
batch:     102480 | loss: 5.06661 | failed:  169
batch:     102490 | loss: 4.92539 | failed:  169
batch:     102500 | loss: 5.00989 | failed:  169
batch:     102510 | loss: 4.84888 | failed:  169
batch:     102520 | loss: 5.21089 | failed:  169
batch:     102530 | loss: 5.17215 | failed:  169
batch:     102540 | loss: 5.10733 | failed:  169
batch:     102550 | loss: 5.29235 | failed:  169
batch:     102560 | loss: 5.14771 | failed:  169
batch:     102570 | loss: 5.11863 | failed:  169
batch:     102580 | loss: 5.13976 | failed:  169
batch:     102590 | loss: 5.12965 | failed:  169
batch:     102600 | loss: 5.03785 | failed:  169
batch:     102610 | loss: 5.10019 | failed:  169
batch:     102620 | loss: 5.08951 | failed:  169
batch:     102630 | loss: 5.13311 | failed:  169
batch:     102640 | loss: 5.08051 | failed:  169
batch:     102650 | loss: 5.09638 | failed:  169
batch:     102660 | loss: 5.10731 | failed:  169
batch:     102670 | loss: 5.06127 | failed:  169
batch:     102680 | loss: 5.17821 | failed:  169
batch:     102690 | loss: 5.19457 | failed:  169
batch:     102700 | loss: 5.20182 | failed:  169
batch:     102710 | loss: 5.23106 | failed:  169
batch:     102720 | loss: 5.14216 | failed:  169
batch:     102730 | loss: 5.01056 | failed:  169
batch:     102740 | loss: 5.25304 | failed:  169
batch:     102750 | loss: 5.18489 | failed:  169
batch:     102760 | loss: 5.17702 | failed:  169
batch:     102770 | loss: 5.08330 | failed:  169
batch:     102780 | loss: 5.13406 | failed:  169
batch:     102790 | loss: 5.12692 | failed:  169
batch:     102800 | loss: 5.17186 | failed:  169
batch:     102810 | loss: 5.22475 | failed:  169
batch:     102820 | loss: 5.16745 | failed:  169
batch:     102830 | loss: 4.88516 | failed:  169
batch:     102840 | loss: 4.89092 | failed:  169
batch:     102850 | loss: 5.20138 | failed:  169
batch:     102860 | loss: 5.18736 | failed:  169
batch:     102870 | loss: 5.01272 | failed:  169
batch:     102880 | loss: 5.02999 | failed:  169
batch:     102890 | loss: 5.09505 | failed:  169
batch:     102900 | loss: 5.12417 | failed:  169
batch:     102910 | loss: 5.18961 | failed:  169
batch:     102920 | loss: 5.20489 | failed:  169
batch:     102930 | loss: 4.96102 | failed:  169
batch:     102940 | loss: 5.13083 | failed:  169
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     102950 | loss: 5.24254 | failed:  170
batch:     102960 | loss: 5.15139 | failed:  170
batch:     102970 | loss: 5.11736 | failed:  170
batch:     102980 | loss: 5.17040 | failed:  170
batch:     102990 | loss: 4.69901 | failed:  170
batch:     103000 | loss: 5.04933 | failed:  170
batch:     103010 | loss: 5.11569 | failed:  170
batch:     103020 | loss: 5.09567 | failed:  170
batch:     103030 | loss: 5.14615 | failed:  170
batch:     103040 | loss: 5.21336 | failed:  170
batch:     103050 | loss: 5.11845 | failed:  170
batch:     103060 | loss: 5.18507 | failed:  170
batch:     103070 | loss: 5.04173 | failed:  170
batch:     103080 | loss: 5.18815 | failed:  170
batch:     103090 | loss: 5.01303 | failed:  170
batch:     103100 | loss: 5.37912 | failed:  170
batch:     103110 | loss: 5.19431 | failed:  170
batch:     103120 | loss: 5.15596 | failed:  170
batch:     103130 | loss: 5.12573 | failed:  170
batch:     103140 | loss: 5.18180 | failed:  170
batch:     103150 | loss: 5.15343 | failed:  170
batch:     103160 | loss: 5.23149 | failed:  170
batch:     103170 | loss: 5.14085 | failed:  170
batch:     103180 | loss: 5.15119 | failed:  170
batch:     103190 | loss: 5.19245 | failed:  170
batch:     103200 | loss: 5.22116 | failed:  170
batch:     103210 | loss: 5.12331 | failed:  170
batch:     103220 | loss: 5.18927 | failed:  170
batch:     103230 | loss: 4.86828 | failed:  170
batch:     103240 | loss: 5.13912 | failed:  170
batch:     103250 | loss: 5.10702 | failed:  170
batch:     103260 | loss: 5.11781 | failed:  170
batch:     103270 | loss: 5.18250 | failed:  170
batch:     103280 | loss: 5.19052 | failed:  170
batch:     103290 | loss: 5.15234 | failed:  170
batch:     103300 | loss: 5.13624 | failed:  170
batch:     103310 | loss: 5.16125 | failed:  170
batch:     103320 | loss: 5.13497 | failed:  170
batch:     103330 | loss: 5.24009 | failed:  170
batch:     103340 | loss: 5.14694 | failed:  170
batch:     103350 | loss: 5.22060 | failed:  170
batch:     103360 | loss: 5.10708 | failed:  170
batch:     103370 | loss: 5.17723 | failed:  170
batch:     103380 | loss: 4.97109 | failed:  170
batch:     103390 | loss: 4.96683 | failed:  170
batch:     103400 | loss: 5.20463 | failed:  170
batch:     103410 | loss: 5.15916 | failed:  170
batch:     103420 | loss: 4.75422 | failed:  170
batch:     103430 | loss: 5.05177 | failed:  170
batch:     103440 | loss: 5.13157 | failed:  170
batch:     103450 | loss: 5.02198 | failed:  170
batch:     103460 | loss: 5.12366 | failed:  170
batch:     103470 | loss: 5.10820 | failed:  170
batch:     103480 | loss: 5.03922 | failed:  170
batch:     103490 | loss: 5.22509 | failed:  170
batch:     103500 | loss: 5.15148 | failed:  170
batch:     103510 | loss: 5.17974 | failed:  170
batch:     103520 | loss: 5.12820 | failed:  170
batch:     103530 | loss: 5.18780 | failed:  170
batch:     103540 | loss: 5.24270 | failed:  170
batch:     103550 | loss: 5.40514 | failed:  170
batch:     103560 | loss: 5.27706 | failed:  170
batch:     103570 | loss: 5.00593 | failed:  170
batch:     103580 | loss: 5.05831 | failed:  170
batch:     103590 | loss: 5.10743 | failed:  170
batch:     103600 | loss: 4.98319 | failed:  170
batch:     103610 | loss: 4.99326 | failed:  170
batch:     103620 | loss: 5.06585 | failed:  170
batch:     103630 | loss: 5.16996 | failed:  170
batch:     103640 | loss: 5.21872 | failed:  170
batch:     103650 | loss: 5.12734 | failed:  170
batch:     103660 | loss: 5.20052 | failed:  170
batch:     103670 | loss: 5.09708 | failed:  170
batch:     103680 | loss: 5.15350 | failed:  170
batch:     103690 | loss: 4.93061 | failed:  170
batch:     103700 | loss: 5.13871 | failed:  170
batch:     103710 | loss: 5.21241 | failed:  170
batch:     103720 | loss: 5.01478 | failed:  170
batch:     103730 | loss: 5.12273 | failed:  170
batch:     103740 | loss: 5.08505 | failed:  170
batch:     103750 | loss: 5.08991 | failed:  170
batch:     103760 | loss: 4.64566 | failed:  170
batch:     103770 | loss: 5.04251 | failed:  170
batch:     103780 | loss: 5.13964 | failed:  170
batch:     103790 | loss: 5.20283 | failed:  170
batch:     103800 | loss: 5.18643 | failed:  170
batch:     103810 | loss: 5.20041 | failed:  170
batch:     103820 | loss: 5.26907 | failed:  170
batch:     103830 | loss: 5.08420 | failed:  170
batch:     103840 | loss: 5.12485 | failed:  170
batch:     103850 | loss: 4.82389 | failed:  170
batch:     103860 | loss: 5.19221 | failed:  170
batch:     103870 | loss: 5.15866 | failed:  170
batch:     103880 | loss: 5.06171 | failed:  170
batch:     103890 | loss: 5.13858 | failed:  170
batch:     103900 | loss: 5.12784 | failed:  170
batch:     103910 | loss: 5.13304 | failed:  170
batch:     103920 | loss: 5.09032 | failed:  170
batch:     103930 | loss: 5.11747 | failed:  170
batch:     103940 | loss: 5.20511 | failed:  170
batch:     103950 | loss: 5.04753 | failed:  170
batch:     103960 | loss: 5.13121 | failed:  170
batch:     103970 | loss: 5.14163 | failed:  170
batch:     103980 | loss: 4.98783 | failed:  170
batch:     103990 | loss: 5.10692 | failed:  170
batch:     104000 | loss: 5.03440 | failed:  170
batch:     104010 | loss: 5.10105 | failed:  170
batch:     104020 | loss: 5.01783 | failed:  170
batch:     104030 | loss: 4.67970 | failed:  170
batch:     104040 | loss: 5.18619 | failed:  170
batch:     104050 | loss: 5.16295 | failed:  170
batch:     104060 | loss: 5.11011 | failed:  170
batch:     104070 | loss: 4.80104 | failed:  170
batch:     104080 | loss: 5.11275 | failed:  170
batch:     104090 | loss: 4.66065 | failed:  170
batch:     104100 | loss: 5.14886 | failed:  170
batch:     104110 | loss: 5.06158 | failed:  170
batch:     104120 | loss: 5.15869 | failed:  170
batch:     104130 | loss: 5.12052 | failed:  170
batch:     104140 | loss: 4.77021 | failed:  170
batch:     104150 | loss: 5.12956 | failed:  170
batch:     104160 | loss: 5.18801 | failed:  170
batch:     104170 | loss: 5.13321 | failed:  170
batch:     104180 | loss: 5.13899 | failed:  170
batch:     104190 | loss: 4.99762 | failed:  170
batch:     104200 | loss: 5.17373 | failed:  170
batch:     104210 | loss: 5.09197 | failed:  170
batch:     104220 | loss: 5.05974 | failed:  170
batch:     104230 | loss: 5.14743 | failed:  170
batch:     104240 | loss: 5.01634 | failed:  170
batch:     104250 | loss: 5.19858 | failed:  170
batch:     104260 | loss: 5.33055 | failed:  170
batch:     104270 | loss: 5.29274 | failed:  170
batch:     104280 | loss: 5.17533 | failed:  170
batch:     104290 | loss: 5.16069 | failed:  170
batch:     104300 | loss: 5.18486 | failed:  170
batch:     104310 | loss: 5.04391 | failed:  170
batch:     104320 | loss: 5.21018 | failed:  170
batch:     104330 | loss: 5.10511 | failed:  170
batch:     104340 | loss: 5.14325 | failed:  170
batch:     104350 | loss: 5.20101 | failed:  170
batch:     104360 | loss: 4.85640 | failed:  170
batch:     104370 | loss: 5.20786 | failed:  170
batch:     104380 | loss: 5.08106 | failed:  170
batch:     104390 | loss: 5.15688 | failed:  170
batch:     104400 | loss: 5.19134 | failed:  170
batch:     104410 | loss: 5.12319 | failed:  170
batch:     104420 | loss: 4.95813 | failed:  170
batch:     104430 | loss: 5.18713 | failed:  170
batch:     104440 | loss: 5.25491 | failed:  170
batch:     104450 | loss: 5.08187 | failed:  170
batch:     104460 | loss: 5.13716 | failed:  170
batch:     104470 | loss: 5.13836 | failed:  170
batch:     104480 | loss: 4.96899 | failed:  170
batch:     104490 | loss: 5.15305 | failed:  170
batch:     104500 | loss: 5.13288 | failed:  170
batch:     104510 | loss: 5.17293 | failed:  170
batch:     104520 | loss: 4.96836 | failed:  170
batch:     104530 | loss: 5.16384 | failed:  170
batch:     104540 | loss: 5.02956 | failed:  170
batch:     104550 | loss: 4.95711 | failed:  170
batch:     104560 | loss: 5.23518 | failed:  170
batch:     104570 | loss: 5.10429 | failed:  170
batch:     104580 | loss: 5.06218 | failed:  170
batch:     104590 | loss: 5.24217 | failed:  170
batch:     104600 | loss: 5.01996 | failed:  170
batch:     104610 | loss: 5.12025 | failed:  170
batch:     104620 | loss: 5.23974 | failed:  170
batch:     104630 | loss: 5.12120 | failed:  170
batch:     104640 | loss: 5.08620 | failed:  170
batch:     104650 | loss: 5.20608 | failed:  170
batch:     104660 | loss: 5.12823 | failed:  170
batch:     104670 | loss: 5.18473 | failed:  170
batch:     104680 | loss: 4.84152 | failed:  170
batch:     104690 | loss: 4.74584 | failed:  170
batch:     104700 | loss: 4.98070 | failed:  170
batch:     104710 | loss: 5.10553 | failed:  170
batch:     104720 | loss: 5.21565 | failed:  170
batch:     104730 | loss: 5.24243 | failed:  170
batch:     104740 | loss: 5.17127 | failed:  170
batch:     104750 | loss: 4.97445 | failed:  170
batch:     104760 | loss: 5.04488 | failed:  170
batch:     104770 | loss: 5.15033 | failed:  170
batch:     104780 | loss: 4.89941 | failed:  170
batch:     104790 | loss: 5.12963 | failed:  170
batch:     104800 | loss: 5.19414 | failed:  170
batch:     104810 | loss: 5.13415 | failed:  170
batch:     104820 | loss: 5.06388 | failed:  170
batch:     104830 | loss: 5.07791 | failed:  170
batch:     104840 | loss: 4.90803 | failed:  170
batch:     104850 | loss: 5.12904 | failed:  170
batch:     104860 | loss: 5.10416 | failed:  170
batch:     104870 | loss: 5.16544 | failed:  170
batch:     104880 | loss: 5.19221 | failed:  170
batch:     104890 | loss: 5.18915 | failed:  170
batch:     104900 | loss: 5.08971 | failed:  170
batch:     104910 | loss: 5.15494 | failed:  170
batch:     104920 | loss: 5.18268 | failed:  170
batch:     104930 | loss: 5.22064 | failed:  170
batch:     104940 | loss: 5.07338 | failed:  170
batch:     104950 | loss: 5.09176 | failed:  170
batch:     104960 | loss: 5.19700 | failed:  170
batch:     104970 | loss: 5.25951 | failed:  170
batch:     104980 | loss: 5.08386 | failed:  170
batch:     104990 | loss: 4.96080 | failed:  170
batch:     105000 | loss: 5.20988 | failed:  170
batch:     105010 | loss: 5.22080 | failed:  170
batch:     105020 | loss: 5.13803 | failed:  170
batch:     105030 | loss: 5.22638 | failed:  170
batch:     105040 | loss: 5.15906 | failed:  170
batch:     105050 | loss: 5.24829 | failed:  170
batch:     105060 | loss: 5.27992 | failed:  170
batch:     105070 | loss: 5.17490 | failed:  170
batch:     105080 | loss: 5.20893 | failed:  170
batch:     105090 | loss: 5.15384 | failed:  170
batch:     105100 | loss: 5.29096 | failed:  170
batch:     105110 | loss: 5.25305 | failed:  170
batch:     105120 | loss: 5.24106 | failed:  170
batch:     105130 | loss: 5.17988 | failed:  170
batch:     105140 | loss: 5.34571 | failed:  170
batch:     105150 | loss: 4.78925 | failed:  170
batch:     105160 | loss: 5.10107 | failed:  170
batch:     105170 | loss: 5.11172 | failed:  170
batch:     105180 | loss: 5.16454 | failed:  170
batch:     105190 | loss: 5.14269 | failed:  170
batch:     105200 | loss: 5.28533 | failed:  170
batch:     105210 | loss: 5.04399 | failed:  170
batch:     105220 | loss: 5.08548 | failed:  170
batch:     105230 | loss: 5.23741 | failed:  170
batch:     105240 | loss: 5.04223 | failed:  170
batch:     105250 | loss: 5.26569 | failed:  170
batch:     105260 | loss: 5.17269 | failed:  170
batch:     105270 | loss: 5.22917 | failed:  170
batch:     105280 | loss: 5.16210 | failed:  170
batch:     105290 | loss: 5.15452 | failed:  170
batch:     105300 | loss: 5.12403 | failed:  170
batch:     105310 | loss: 5.16717 | failed:  170
batch:     105320 | loss: 4.75216 | failed:  170
batch:     105330 | loss: 5.18569 | failed:  170
batch:     105340 | loss: 5.11667 | failed:  170
batch:     105350 | loss: 5.04567 | failed:  170
batch:     105360 | loss: 5.14018 | failed:  170
batch:     105370 | loss: 5.12087 | failed:  170
batch:     105380 | loss: 5.19962 | failed:  170
batch:     105390 | loss: 5.16221 | failed:  170
batch:     105400 | loss: 5.33082 | failed:  170
batch:     105410 | loss: 5.00650 | failed:  170
batch:     105420 | loss: 4.87280 | failed:  170
batch:     105430 | loss: 4.03639 | failed:  170
batch:     105440 | loss: 5.20061 | failed:  170
batch:     105450 | loss: 5.18967 | failed:  170
batch:     105460 | loss: 5.13656 | failed:  170
batch:     105470 | loss: 5.07894 | failed:  170
batch:     105480 | loss: 5.16468 | failed:  170
batch:     105490 | loss: 4.97712 | failed:  170
batch:     105500 | loss: 5.10382 | failed:  170
batch:     105510 | loss: 5.20007 | failed:  170
batch:     105520 | loss: 5.08640 | failed:  170
batch:     105530 | loss: 5.07725 | failed:  170
batch:     105540 | loss: 5.03895 | failed:  170
batch:     105550 | loss: 5.00676 | failed:  170
batch:     105560 | loss: 5.08704 | failed:  170
batch:     105570 | loss: 5.11232 | failed:  170
batch:     105580 | loss: 5.14086 | failed:  170
batch:     105590 | loss: 5.10858 | failed:  170
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105640 | loss: 4.89937 | failed:  201
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105650 | loss: 4.90667 | failed:  206
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105690 | loss: 4.83566 | failed:  230
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105700 | loss: 4.86895 | failed:  232
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105740 | loss: 4.68825 | failed:  262
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105760 | loss: 4.70151 | failed:  271
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105770 | loss: 4.77825 | failed:  275
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     105780 | loss: 5.12363 | failed:  276
batch:     105790 | loss: 5.11587 | failed:  276
batch:     105800 | loss: 5.09931 | failed:  276
batch:     105810 | loss: 5.18720 | failed:  276
batch:     105820 | loss: 5.20047 | failed:  276
batch:     105830 | loss: 5.14897 | failed:  276
batch:     105840 | loss: 5.09292 | failed:  276
batch:     105850 | loss: 5.02549 | failed:  276
batch:     105860 | loss: 5.02816 | failed:  276
batch:     105870 | loss: 4.97122 | failed:  276
batch:     105880 | loss: 5.22044 | failed:  276
batch:     105890 | loss: 5.27415 | failed:  276
batch:     105900 | loss: 5.14805 | failed:  276
batch:     105910 | loss: 5.16246 | failed:  276
batch:     105920 | loss: 5.19938 | failed:  276
batch:     105930 | loss: 5.09854 | failed:  276
batch:     105940 | loss: 5.11313 | failed:  276
batch:     105950 | loss: 5.01729 | failed:  276
batch:     105960 | loss: 5.32203 | failed:  276
batch:     105970 | loss: 5.27211 | failed:  276
batch:     105980 | loss: 5.22506 | failed:  276
batch:     105990 | loss: 5.17962 | failed:  276
batch:     106000 | loss: 5.08684 | failed:  276
batch:     106010 | loss: 5.20958 | failed:  276
batch:     106020 | loss: 5.17998 | failed:  276
batch:     106030 | loss: 5.13272 | failed:  276
batch:     106040 | loss: 5.15014 | failed:  276
batch:     106050 | loss: 5.17487 | failed:  276
batch:     106060 | loss: 5.15012 | failed:  276
batch:     106070 | loss: 5.11199 | failed:  276
batch:     106080 | loss: 5.23245 | failed:  276
batch:     106090 | loss: 5.18358 | failed:  276
batch:     106100 | loss: 5.19938 | failed:  276
batch:     106110 | loss: 4.92550 | failed:  276
batch:     106120 | loss: 5.14096 | failed:  276
batch:     106130 | loss: 5.13644 | failed:  276
batch:     106140 | loss: 5.10440 | failed:  276
batch:     106150 | loss: 5.11008 | failed:  276
batch:     106160 | loss: 5.03569 | failed:  276
batch:     106170 | loss: 4.59285 | failed:  276
batch:     106180 | loss: 5.16730 | failed:  276
batch:     106190 | loss: 5.22251 | failed:  276
batch:     106200 | loss: 5.23136 | failed:  276
batch:     106210 | loss: 5.17678 | failed:  276
batch:     106220 | loss: 5.06408 | failed:  276
batch:     106230 | loss: 5.27361 | failed:  276
batch:     106240 | loss: 5.13715 | failed:  276
batch:     106250 | loss: 5.21012 | failed:  276
batch:     106260 | loss: 5.17470 | failed:  276
batch:     106270 | loss: 5.13383 | failed:  276
batch:     106280 | loss: 5.13223 | failed:  276
batch:     106290 | loss: 5.23895 | failed:  276
batch:     106300 | loss: 5.18140 | failed:  276
batch:     106310 | loss: 5.25402 | failed:  276
batch:     106320 | loss: 5.11546 | failed:  276
batch:     106330 | loss: 4.99220 | failed:  276
batch:     106340 | loss: 5.15465 | failed:  276
batch:     106350 | loss: 5.03409 | failed:  276
batch:     106360 | loss: 5.09934 | failed:  276
batch:     106370 | loss: 5.17048 | failed:  276
batch:     106380 | loss: 4.96670 | failed:  276
batch:     106390 | loss: 5.11593 | failed:  276
batch:     106400 | loss: 5.17724 | failed:  276
batch:     106410 | loss: 5.13324 | failed:  276
batch:     106420 | loss: 5.18294 | failed:  276
batch:     106430 | loss: 5.12814 | failed:  276
batch:     106440 | loss: 5.14329 | failed:  276
batch:     106450 | loss: 5.46780 | failed:  276
batch:     106460 | loss: 5.14921 | failed:  276
batch:     106470 | loss: 5.23161 | failed:  276
batch:     106480 | loss: 5.24782 | failed:  276
batch:     106490 | loss: 5.14483 | failed:  276
batch:     106500 | loss: 5.19468 | failed:  276
batch:     106510 | loss: 5.19685 | failed:  276
batch:     106520 | loss: 5.09061 | failed:  276
batch:     106530 | loss: 5.06486 | failed:  276
batch:     106540 | loss: 5.28152 | failed:  276
batch:     106550 | loss: 5.13278 | failed:  276
batch:     106560 | loss: 5.08401 | failed:  276
batch:     106570 | loss: 4.83056 | failed:  276
batch:     106580 | loss: 5.18010 | failed:  276
batch:     106590 | loss: 5.02692 | failed:  276
batch:     106600 | loss: 5.09773 | failed:  276
batch:     106610 | loss: 4.91285 | failed:  276
batch:     106620 | loss: 5.11527 | failed:  276
batch:     106630 | loss: 5.18714 | failed:  276
batch:     106640 | loss: 5.21283 | failed:  276
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     106660 | loss: 4.80471 | failed:  288
batch:     106670 | loss: 5.25093 | failed:  288
batch:     106680 | loss: 5.11896 | failed:  288
batch:     106690 | loss: 5.19201 | failed:  288
batch:     106700 | loss: 5.11770 | failed:  288
batch:     106710 | loss: 5.24995 | failed:  288
batch:     106720 | loss: 5.04664 | failed:  288
batch:     106730 | loss: 4.99437 | failed:  288
batch:     106740 | loss: 5.15104 | failed:  288
batch:     106750 | loss: 5.22288 | failed:  288
batch:     106760 | loss: 5.06687 | failed:  288
batch:     106770 | loss: 5.07566 | failed:  288
batch:     106780 | loss: 4.96556 | failed:  288
batch:     106790 | loss: 5.03403 | failed:  288
batch:     106800 | loss: 5.14700 | failed:  288
batch:     106810 | loss: 5.13741 | failed:  288
batch:     106820 | loss: 5.06031 | failed:  288
batch:     106830 | loss: 5.14622 | failed:  288
batch:     106840 | loss: 4.96142 | failed:  288
batch:     106850 | loss: 5.20733 | failed:  288
batch:     106860 | loss: 5.28235 | failed:  288
batch:     106870 | loss: 5.09396 | failed:  288
batch:     106880 | loss: 5.15783 | failed:  288
batch:     106890 | loss: 5.06726 | failed:  288
batch:     106900 | loss: 5.09418 | failed:  288
batch:     106910 | loss: 5.09950 | failed:  288
batch:     106920 | loss: 5.25455 | failed:  288
batch:     106930 | loss: 5.12625 | failed:  288
batch:     106940 | loss: 5.13622 | failed:  288
batch:     106950 | loss: 5.22346 | failed:  288
batch:     106960 | loss: 5.09738 | failed:  288
batch:     106970 | loss: 5.19242 | failed:  288
batch:     106980 | loss: 5.18017 | failed:  288
batch:     106990 | loss: 4.83341 | failed:  288
batch:     107000 | loss: 4.04805 | failed:  288
batch:     107010 | loss: 5.24649 | failed:  288
batch:     107020 | loss: 5.17605 | failed:  288
batch:     107030 | loss: 5.19402 | failed:  288
batch:     107040 | loss: 5.14995 | failed:  288
batch:     107050 | loss: 5.09974 | failed:  288
batch:     107060 | loss: 5.12115 | failed:  288
batch:     107070 | loss: 5.12915 | failed:  288
batch:     107080 | loss: 5.06806 | failed:  288
batch:     107090 | loss: 5.15620 | failed:  288
batch:     107100 | loss: 5.20323 | failed:  288
batch:     107110 | loss: 5.14210 | failed:  288
batch:     107120 | loss: 5.14211 | failed:  288
batch:     107130 | loss: 5.11019 | failed:  288
batch:     107140 | loss: 5.13942 | failed:  288
batch:     107150 | loss: 5.09331 | failed:  288
batch:     107160 | loss: 5.12857 | failed:  288
batch:     107170 | loss: 5.18771 | failed:  288
batch:     107180 | loss: 5.18149 | failed:  288
batch:     107190 | loss: 5.11247 | failed:  288
batch:     107200 | loss: 5.06485 | failed:  288
batch:     107210 | loss: 5.14172 | failed:  288
batch:     107220 | loss: 5.15525 | failed:  288
batch:     107230 | loss: 5.20036 | failed:  288
batch:     107240 | loss: 5.23986 | failed:  288
batch:     107250 | loss: 5.15299 | failed:  288
batch:     107260 | loss: 5.16403 | failed:  288
batch:     107270 | loss: 5.13244 | failed:  288
batch:     107280 | loss: 5.02209 | failed:  288
batch:     107290 | loss: 5.14302 | failed:  288
batch:     107300 | loss: 5.09208 | failed:  288
batch:     107310 | loss: 3.92236 | failed:  288
batch:     107320 | loss: 5.14602 | failed:  288
batch:     107330 | loss: 5.13550 | failed:  288
batch:     107340 | loss: 4.98849 | failed:  288
batch:     107350 | loss: 5.17755 | failed:  288
batch:     107360 | loss: 5.04526 | failed:  288
batch:     107370 | loss: 4.98874 | failed:  288
batch:     107380 | loss: 5.02138 | failed:  288
batch:     107390 | loss: 5.06261 | failed:  288
batch:     107400 | loss: 4.17850 | failed:  288
batch:     107410 | loss: 5.10780 | failed:  288
batch:     107420 | loss: 5.23412 | failed:  288
batch:     107430 | loss: 5.20559 | failed:  288
batch:     107440 | loss: 5.06914 | failed:  288
batch:     107450 | loss: 5.14885 | failed:  288
batch:     107460 | loss: 5.20791 | failed:  288
batch:     107470 | loss: 5.17934 | failed:  288
batch:     107480 | loss: 4.91420 | failed:  288
batch:     107490 | loss: 5.11132 | failed:  288
batch:     107500 | loss: 5.19963 | failed:  288
batch:     107510 | loss: 5.26856 | failed:  288
batch:     107520 | loss: 5.18858 | failed:  288
batch:     107530 | loss: 5.08593 | failed:  288
batch:     107540 | loss: 5.17582 | failed:  288
batch:     107550 | loss: 5.22894 | failed:  288
batch:     107560 | loss: 5.23417 | failed:  288
batch:     107570 | loss: 5.23286 | failed:  288
batch:     107580 | loss: 5.15259 | failed:  288
batch:     107590 | loss: 5.19505 | failed:  288
batch:     107600 | loss: 5.15815 | failed:  288
batch:     107610 | loss: 5.12205 | failed:  288
batch:     107620 | loss: 5.23625 | failed:  288
batch:     107630 | loss: 5.21004 | failed:  288
batch:     107640 | loss: 4.87563 | failed:  288
batch:     107650 | loss: 5.09632 | failed:  288
batch:     107660 | loss: 5.14319 | failed:  288
batch:     107670 | loss: 5.05990 | failed:  288
batch:     107680 | loss: 5.11894 | failed:  288
batch:     107690 | loss: 4.91282 | failed:  288
batch:     107700 | loss: 4.85251 | failed:  288
batch:     107710 | loss: 4.58845 | failed:  288
batch:     107720 | loss: 5.16695 | failed:  288
batch:     107730 | loss: 5.16193 | failed:  288
batch:     107740 | loss: 5.16508 | failed:  288
batch:     107750 | loss: 5.19779 | failed:  288
batch:     107760 | loss: 4.83916 | failed:  288
batch:     107770 | loss: 5.28464 | failed:  288
batch:     107780 | loss: 5.09773 | failed:  288
batch:     107790 | loss: 5.15170 | failed:  288
batch:     107800 | loss: 5.03239 | failed:  288
batch:     107810 | loss: 5.00540 | failed:  288
batch:     107820 | loss: 5.13557 | failed:  288
batch:     107830 | loss: 5.15604 | failed:  288
batch:     107840 | loss: 5.07524 | failed:  288
batch:     107850 | loss: 5.09899 | failed:  288
batch:     107860 | loss: 5.27901 | failed:  288
batch:     107870 | loss: 5.07805 | failed:  288
batch:     107880 | loss: 5.13924 | failed:  288
batch:     107890 | loss: 5.11674 | failed:  288
batch:     107900 | loss: 5.14869 | failed:  288
batch:     107910 | loss: 5.24496 | failed:  288
batch:     107920 | loss: 5.15896 | failed:  288
batch:     107930 | loss: 5.19746 | failed:  288
batch:     107940 | loss: 5.14436 | failed:  288
batch:     107950 | loss: 5.12818 | failed:  288
batch:     107960 | loss: 5.18494 | failed:  288
batch:     107970 | loss: 5.13218 | failed:  288
batch:     107980 | loss: 5.03528 | failed:  288
batch:     107990 | loss: 5.04010 | failed:  288
batch:     108000 | loss: 5.01055 | failed:  288
batch:     108010 | loss: 5.25448 | failed:  288
batch:     108020 | loss: 5.18660 | failed:  288
batch:     108030 | loss: 4.99230 | failed:  288
batch:     108040 | loss: 5.14554 | failed:  288
batch:     108050 | loss: 5.14911 | failed:  288
batch:     108060 | loss: 5.06734 | failed:  288
batch:     108070 | loss: 4.87766 | failed:  288
batch:     108080 | loss: 5.22626 | failed:  288
batch:     108090 | loss: 5.16526 | failed:  288
batch:     108100 | loss: 5.07177 | failed:  288
batch:     108110 | loss: 5.07533 | failed:  288
batch:     108120 | loss: 4.78746 | failed:  288
batch:     108130 | loss: 4.85983 | failed:  288
batch:     108140 | loss: 4.90026 | failed:  288
batch:     108150 | loss: 5.09237 | failed:  288
batch:     108160 | loss: 5.18150 | failed:  288
batch:     108170 | loss: 5.07972 | failed:  288
batch:     108180 | loss: 5.08553 | failed:  288
batch:     108190 | loss: 5.07011 | failed:  288
batch:     108200 | loss: 5.16249 | failed:  288
batch:     108210 | loss: 5.13921 | failed:  288
batch:     108220 | loss: 5.21702 | failed:  288
batch:     108230 | loss: 5.26708 | failed:  288
batch:     108240 | loss: 5.20987 | failed:  288
batch:     108250 | loss: 5.14275 | failed:  288
batch:     108260 | loss: 4.71189 | failed:  288
batch:     108270 | loss: 5.22339 | failed:  288
batch:     108280 | loss: 5.20263 | failed:  288
batch:     108290 | loss: 5.25113 | failed:  288
batch:     108300 | loss: 5.10799 | failed:  288
batch:     108310 | loss: 4.97029 | failed:  288
batch:     108320 | loss: 5.02852 | failed:  288
batch:     108330 | loss: 5.03363 | failed:  288
batch:     108340 | loss: 5.15367 | failed:  288
batch:     108350 | loss: 5.14493 | failed:  288
batch:     108360 | loss: 4.65397 | failed:  288
batch:     108370 | loss: 5.18133 | failed:  288
batch:     108380 | loss: 5.04648 | failed:  288
batch:     108390 | loss: 5.22990 | failed:  288
batch:     108400 | loss: 5.22141 | failed:  288
batch:     108410 | loss: 5.10862 | failed:  288
batch:     108420 | loss: 5.15947 | failed:  288
batch:     108430 | loss: 5.08392 | failed:  288
batch:     108440 | loss: 5.02387 | failed:  288
batch:     108450 | loss: 5.09240 | failed:  288
batch:     108460 | loss: 5.24905 | failed:  288
batch:     108470 | loss: 5.12473 | failed:  288
batch:     108480 | loss: 5.17627 | failed:  288
batch:     108490 | loss: 5.14925 | failed:  288
batch:     108500 | loss: 5.12562 | failed:  288
batch:     108510 | loss: 5.10028 | failed:  288
batch:     108520 | loss: 5.22729 | failed:  288
batch:     108530 | loss: 5.29884 | failed:  288
batch:     108540 | loss: 5.10303 | failed:  288
batch:     108550 | loss: 5.22435 | failed:  288
batch:     108560 | loss: 5.20704 | failed:  288
batch:     108570 | loss: 5.03601 | failed:  288
batch:     108580 | loss: 5.04253 | failed:  288
batch:     108590 | loss: 5.16385 | failed:  288
batch:     108600 | loss: 4.85146 | failed:  288
batch:     108610 | loss: 5.16350 | failed:  288
batch:     108620 | loss: 5.00696 | failed:  288
batch:     108630 | loss: 5.20115 | failed:  288
batch:     108640 | loss: 4.75802 | failed:  288
batch:     108650 | loss: 5.12293 | failed:  288
batch:     108660 | loss: 5.14344 | failed:  288
batch:     108670 | loss: 5.15861 | failed:  288
batch:     108680 | loss: 5.19890 | failed:  288
batch:     108690 | loss: 5.09032 | failed:  288
batch:     108700 | loss: 5.12364 | failed:  288
batch:     108710 | loss: 5.03774 | failed:  288
batch:     108720 | loss: 5.08519 | failed:  288
batch:     108730 | loss: 5.24207 | failed:  288
batch:     108740 | loss: 5.23371 | failed:  288
batch:     108750 | loss: 5.15122 | failed:  288
batch:     108760 | loss: 5.22880 | failed:  288
batch:     108770 | loss: 5.17615 | failed:  288
batch:     108780 | loss: 5.15656 | failed:  288
batch:     108790 | loss: 5.10822 | failed:  288
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     108810 | loss: 5.10107 | failed:  292
batch:     108820 | loss: 5.08306 | failed:  292
batch:     108830 | loss: 5.10943 | failed:  292
batch:     108840 | loss: 5.15358 | failed:  292
batch:     108850 | loss: 5.14827 | failed:  292
batch:     108860 | loss: 5.13810 | failed:  292
batch:     108870 | loss: 5.10651 | failed:  292
batch:     108880 | loss: 5.21889 | failed:  292
batch:     108890 | loss: 5.15157 | failed:  292
batch:     108900 | loss: 5.11702 | failed:  292
batch:     108910 | loss: 5.10450 | failed:  292
batch:     108920 | loss: 5.10290 | failed:  292
batch:     108930 | loss: 5.06724 | failed:  292
batch:     108940 | loss: 5.11621 | failed:  292
batch:     108950 | loss: 3.93609 | failed:  292
batch:     108960 | loss: 5.12700 | failed:  292
batch:     108970 | loss: 5.11210 | failed:  292
batch:     108980 | loss: 4.99838 | failed:  292
batch:     108990 | loss: 4.94221 | failed:  292
batch:     109000 | loss: 5.10156 | failed:  292
batch:     109010 | loss: 5.19075 | failed:  292
batch:     109020 | loss: 5.11604 | failed:  292
batch:     109030 | loss: 5.19112 | failed:  292
batch:     109040 | loss: 5.02178 | failed:  292
batch:     109050 | loss: 5.12894 | failed:  292
batch:     109060 | loss: 5.08434 | failed:  292
batch:     109070 | loss: 5.16659 | failed:  292
batch:     109080 | loss: 5.10723 | failed:  292
batch:     109090 | loss: 5.10855 | failed:  292
batch:     109100 | loss: 5.13070 | failed:  292
batch:     109110 | loss: 5.05990 | failed:  292
batch:     109120 | loss: 5.02178 | failed:  292
batch:     109130 | loss: 5.20355 | failed:  292
batch:     109140 | loss: 5.31471 | failed:  292
batch:     109150 | loss: 5.34283 | failed:  292
batch:     109160 | loss: 5.13149 | failed:  292
batch:     109170 | loss: 5.19227 | failed:  292
batch:     109180 | loss: 5.15733 | failed:  292
batch:     109190 | loss: 5.11786 | failed:  292
batch:     109200 | loss: 5.18801 | failed:  292
batch:     109210 | loss: 5.12524 | failed:  292
batch:     109220 | loss: 5.15508 | failed:  292
batch:     109230 | loss: 4.88579 | failed:  292
batch:     109240 | loss: 5.19911 | failed:  292
batch:     109250 | loss: 5.17371 | failed:  292
batch:     109260 | loss: 5.17735 | failed:  292
batch:     109270 | loss: 5.02244 | failed:  292
batch:     109280 | loss: 5.22155 | failed:  292
batch:     109290 | loss: 5.27662 | failed:  292
batch:     109300 | loss: 5.10475 | failed:  292
batch:     109310 | loss: 5.31397 | failed:  292
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     109320 | loss: 5.20865 | failed:  294
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     109330 | loss: 5.22460 | failed:  296
batch:     109340 | loss: 5.06223 | failed:  296
batch:     109350 | loss: 5.13744 | failed:  296
batch:     109360 | loss: 5.04655 | failed:  296
batch:     109370 | loss: 4.97408 | failed:  296
batch:     109380 | loss: 5.00955 | failed:  296
batch:     109390 | loss: 4.89528 | failed:  296
batch:     109400 | loss: 5.21944 | failed:  296
batch:     109410 | loss: 5.01635 | failed:  296
batch:     109420 | loss: 5.05516 | failed:  296
batch:     109430 | loss: 5.06112 | failed:  296
batch:     109440 | loss: 4.95928 | failed:  296
batch:     109450 | loss: 5.28773 | failed:  296
batch:     109460 | loss: 5.05085 | failed:  296
batch:     109470 | loss: 4.91167 | failed:  296
batch:     109480 | loss: 5.23313 | failed:  296
batch:     109490 | loss: 5.09053 | failed:  296
batch:     109500 | loss: 5.04164 | failed:  296
batch:     109510 | loss: 5.04141 | failed:  296
batch:     109520 | loss: 5.15718 | failed:  296
batch:     109530 | loss: 5.11415 | failed:  296
batch:     109540 | loss: 5.08212 | failed:  296
batch:     109550 | loss: 4.99038 | failed:  296
batch:     109560 | loss: 5.04881 | failed:  296
batch:     109570 | loss: 5.17074 | failed:  296
batch:     109580 | loss: 5.18208 | failed:  296
batch:     109590 | loss: 5.18873 | failed:  296
batch:     109600 | loss: 5.13922 | failed:  296
batch:     109610 | loss: 5.05646 | failed:  296
batch:     109620 | loss: 4.95329 | failed:  296
batch:     109630 | loss: 5.17075 | failed:  296
batch:     109640 | loss: 5.09046 | failed:  296
batch:     109650 | loss: 5.13095 | failed:  296
batch:     109660 | loss: 5.12247 | failed:  296
batch:     109670 | loss: 5.17643 | failed:  296
batch:     109680 | loss: 5.09845 | failed:  296
batch:     109690 | loss: 5.02993 | failed:  296
batch:     109700 | loss: 5.16027 | failed:  296
batch:     109710 | loss: 5.18570 | failed:  296
batch:     109720 | loss: 4.87063 | failed:  296
batch:     109730 | loss: 5.20596 | failed:  296
batch:     109740 | loss: 5.08662 | failed:  296
batch:     109750 | loss: 5.00882 | failed:  296
batch:     109760 | loss: 5.09937 | failed:  296
batch:     109770 | loss: 5.15264 | failed:  296
batch:     109780 | loss: 5.09272 | failed:  296
batch:     109790 | loss: 5.07476 | failed:  296
batch:     109800 | loss: 5.11676 | failed:  296
batch:     109810 | loss: 5.22566 | failed:  296
batch:     109820 | loss: 5.16007 | failed:  296
batch:     109830 | loss: 5.12766 | failed:  296
batch:     109840 | loss: 5.15881 | failed:  296
batch:     109850 | loss: 5.17747 | failed:  296
batch:     109860 | loss: 5.20203 | failed:  296
batch:     109870 | loss: 5.22958 | failed:  296
batch:     109880 | loss: 5.15680 | failed:  296
batch:     109890 | loss: 5.12963 | failed:  296
batch:     109900 | loss: 5.08183 | failed:  296
batch:     109910 | loss: 5.13984 | failed:  296
batch:     109920 | loss: 5.11351 | failed:  296
batch:     109930 | loss: 5.04533 | failed:  296
batch:     109940 | loss: 5.16598 | failed:  296
batch:     109950 | loss: 5.20311 | failed:  296
batch:     109960 | loss: 5.20696 | failed:  296
batch:     109970 | loss: 5.20419 | failed:  296
batch:     109980 | loss: 5.18625 | failed:  296
batch:     109990 | loss: 5.21471 | failed:  296
batch:     110000 | loss: 4.94890 | failed:  296
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     110010 | loss: 5.05626 | failed:  296
batch:     110020 | loss: 5.22148 | failed:  296
batch:     110030 | loss: 5.11816 | failed:  296
batch:     110040 | loss: 5.06244 | failed:  296
batch:     110050 | loss: 5.20707 | failed:  296
batch:     110060 | loss: 5.17459 | failed:  296
batch:     110070 | loss: 4.95795 | failed:  296
batch:     110080 | loss: 5.07366 | failed:  296
batch:     110090 | loss: 5.23186 | failed:  296
batch:     110100 | loss: 5.17078 | failed:  296
batch:     110110 | loss: 5.13594 | failed:  296
batch:     110120 | loss: 5.22337 | failed:  296
batch:     110130 | loss: 5.30417 | failed:  296
batch:     110140 | loss: 5.25316 | failed:  296
batch:     110150 | loss: 4.98081 | failed:  296
batch:     110160 | loss: 5.11497 | failed:  296
batch:     110170 | loss: 5.05815 | failed:  296
batch:     110180 | loss: 4.99917 | failed:  296
batch:     110190 | loss: 5.07714 | failed:  296
batch:     110200 | loss: 5.22356 | failed:  296
batch:     110210 | loss: 5.14698 | failed:  296
batch:     110220 | loss: 5.10821 | failed:  296
batch:     110230 | loss: 5.02856 | failed:  296
batch:     110240 | loss: 5.03054 | failed:  296
batch:     110250 | loss: 5.16356 | failed:  296
batch:     110260 | loss: 5.06365 | failed:  296
batch:     110270 | loss: 5.06020 | failed:  296
batch:     110280 | loss: 5.01956 | failed:  296
batch:     110290 | loss: 5.15274 | failed:  296
batch:     110300 | loss: 5.03776 | failed:  296
batch:     110310 | loss: 5.13109 | failed:  296
batch:     110320 | loss: 5.20532 | failed:  296
batch:     110330 | loss: 5.05999 | failed:  296
batch:     110340 | loss: 5.14139 | failed:  296
batch:     110350 | loss: 5.13752 | failed:  296
batch:     110360 | loss: 5.10109 | failed:  296
batch:     110370 | loss: 4.97954 | failed:  296
batch:     110380 | loss: 5.12926 | failed:  296
batch:     110390 | loss: 5.09456 | failed:  296
batch:     110400 | loss: 5.02086 | failed:  296
batch:     110410 | loss: 5.00765 | failed:  296
batch:     110420 | loss: 5.08542 | failed:  296
batch:     110430 | loss: 5.05695 | failed:  296
batch:     110440 | loss: 5.00212 | failed:  296
batch:     110450 | loss: 5.21790 | failed:  296
batch:     110460 | loss: 5.13004 | failed:  296
batch:     110470 | loss: 5.12022 | failed:  296
batch:     110480 | loss: 5.02246 | failed:  296
batch:     110490 | loss: 4.98029 | failed:  296
batch:     110500 | loss: 5.03208 | failed:  296
batch:     110510 | loss: 5.15852 | failed:  296
batch:     110520 | loss: 5.30577 | failed:  296
batch:     110530 | loss: 5.20973 | failed:  296
batch:     110540 | loss: 5.11659 | failed:  296
batch:     110550 | loss: 5.13500 | failed:  296
batch:     110560 | loss: 5.26176 | failed:  296
batch:     110570 | loss: 5.15033 | failed:  296
batch:     110580 | loss: 5.00987 | failed:  296
batch:     110590 | loss: 4.38914 | failed:  296
batch:     110600 | loss: 5.25567 | failed:  296
batch:     110610 | loss: 5.17659 | failed:  296
batch:     110620 | loss: 4.86321 | failed:  296
batch:     110630 | loss: 4.78760 | failed:  296
batch:     110640 | loss: 5.24816 | failed:  296
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     110650 | loss: 5.04473 | failed:  301
batch:     110660 | loss: 4.83883 | failed:  301
batch:     110670 | loss: 5.17135 | failed:  301
batch:     110680 | loss: 4.77298 | failed:  301
batch:     110690 | loss: 5.12813 | failed:  301
batch:     110700 | loss: 5.02022 | failed:  301
batch:     110710 | loss: 5.07056 | failed:  301
batch:     110720 | loss: 5.13636 | failed:  301
batch:     110730 | loss: 5.01947 | failed:  301
batch:     110740 | loss: 5.22094 | failed:  301
batch:     110750 | loss: 5.17417 | failed:  301
batch:     110760 | loss: 5.10626 | failed:  301
batch:     110770 | loss: 5.03476 | failed:  301
batch:     110780 | loss: 5.07998 | failed:  301
batch:     110790 | loss: 4.85197 | failed:  301
batch:     110800 | loss: 5.14733 | failed:  301
batch:     110810 | loss: 5.15238 | failed:  301
batch:     110820 | loss: 5.25158 | failed:  301
batch:     110830 | loss: 4.84720 | failed:  301
batch:     110840 | loss: 5.13630 | failed:  301
batch:     110850 | loss: 5.19178 | failed:  301
batch:     110860 | loss: 5.04613 | failed:  301
batch:     110870 | loss: 5.24769 | failed:  301
batch:     110880 | loss: 5.19690 | failed:  301
batch:     110890 | loss: 5.01289 | failed:  301
batch:     110900 | loss: 5.26978 | failed:  301
batch:     110910 | loss: 5.16381 | failed:  301
batch:     110920 | loss: 4.89249 | failed:  301
batch:     110930 | loss: 5.17973 | failed:  301
batch:     110940 | loss: 5.19090 | failed:  301
batch:     110950 | loss: 5.09961 | failed:  301
batch:     110960 | loss: 5.07561 | failed:  301
batch:     110970 | loss: 5.19525 | failed:  301
batch:     110980 | loss: 5.19609 | failed:  301
batch:     110990 | loss: 5.11352 | failed:  301
batch:     111000 | loss: 4.94296 | failed:  301
batch:     111010 | loss: 5.13910 | failed:  301
batch:     111020 | loss: 5.24924 | failed:  301
batch:     111030 | loss: 5.11584 | failed:  301
batch:     111040 | loss: 5.31147 | failed:  301
batch:     111050 | loss: 5.13000 | failed:  301
batch:     111060 | loss: 5.03197 | failed:  301
batch:     111070 | loss: 5.12045 | failed:  301
batch:     111080 | loss: 5.08164 | failed:  301
batch:     111090 | loss: 5.34015 | failed:  301
batch:     111100 | loss: 4.97583 | failed:  301
batch:     111110 | loss: 5.19913 | failed:  301
batch:     111120 | loss: 5.25593 | failed:  301
batch:     111130 | loss: 5.14517 | failed:  301
batch:     111140 | loss: 5.17989 | failed:  301
batch:     111150 | loss: 5.17706 | failed:  301
batch:     111160 | loss: 5.19099 | failed:  301
batch:     111170 | loss: 5.14290 | failed:  301
batch:     111180 | loss: 5.17751 | failed:  301
batch:     111190 | loss: 5.14766 | failed:  301
batch:     111200 | loss: 5.16071 | failed:  301
batch:     111210 | loss: 5.12155 | failed:  301
batch:     111220 | loss: 5.23440 | failed:  301
batch:     111230 | loss: 5.20800 | failed:  301
batch:     111240 | loss: 5.16036 | failed:  301
batch:     111250 | loss: 5.09399 | failed:  301
batch:     111260 | loss: 5.05243 | failed:  301
batch:     111270 | loss: 4.83367 | failed:  301
batch:     111280 | loss: 5.21540 | failed:  301
batch:     111290 | loss: 5.22406 | failed:  301
batch:     111300 | loss: 5.15398 | failed:  301
batch:     111310 | loss: 5.21104 | failed:  301
batch:     111320 | loss: 5.05215 | failed:  301
batch:     111330 | loss: 5.18674 | failed:  301
batch:     111340 | loss: 5.19143 | failed:  301
batch:     111350 | loss: 5.23829 | failed:  301
batch:     111360 | loss: 5.13121 | failed:  301
batch:     111370 | loss: 5.15760 | failed:  301
batch:     111380 | loss: 5.11944 | failed:  301
batch:     111390 | loss: 5.16275 | failed:  301
batch:     111400 | loss: 5.08804 | failed:  301
batch:     111410 | loss: 4.95805 | failed:  301
batch:     111420 | loss: 5.11671 | failed:  301
batch:     111430 | loss: 5.22793 | failed:  301
batch:     111440 | loss: 5.01373 | failed:  301
batch:     111450 | loss: 4.62142 | failed:  301
batch:     111460 | loss: 3.96745 | failed:  301
batch:     111470 | loss: 5.34279 | failed:  301
batch:     111480 | loss: 5.15941 | failed:  301
batch:     111490 | loss: 5.12946 | failed:  301
batch:     111500 | loss: 5.22439 | failed:  301
batch:     111510 | loss: 5.06839 | failed:  301
batch:     111520 | loss: 5.07653 | failed:  301
batch:     111530 | loss: 5.06663 | failed:  301
batch:     111540 | loss: 5.12656 | failed:  301
batch:     111550 | loss: 4.97731 | failed:  301
batch:     111560 | loss: 5.20443 | failed:  301
batch:     111570 | loss: 5.13929 | failed:  301
batch:     111580 | loss: 5.10254 | failed:  301
batch:     111590 | loss: 5.06527 | failed:  301
batch:     111600 | loss: 5.00539 | failed:  301
batch:     111610 | loss: 5.06442 | failed:  301
batch:     111620 | loss: 5.00505 | failed:  301
batch:     111630 | loss: 5.09276 | failed:  301
batch:     111640 | loss: 5.18907 | failed:  301
batch:     111650 | loss: 5.20808 | failed:  301
batch:     111660 | loss: 5.03493 | failed:  301
batch:     111670 | loss: 5.17097 | failed:  301
batch:     111680 | loss: 5.16846 | failed:  301
batch:     111690 | loss: 5.02860 | failed:  301
batch:     111700 | loss: 5.11793 | failed:  301
batch:     111710 | loss: 5.06342 | failed:  301
batch:     111720 | loss: 4.89562 | failed:  301
batch:     111730 | loss: 5.22986 | failed:  301
batch:     111740 | loss: 5.14953 | failed:  301
batch:     111750 | loss: 5.19103 | failed:  301
batch:     111760 | loss: 5.28630 | failed:  301
batch:     111770 | loss: 5.23948 | failed:  301
batch:     111780 | loss: 5.18658 | failed:  301
batch:     111790 | loss: 5.17865 | failed:  301
batch:     111800 | loss: 5.04484 | failed:  301
batch:     111810 | loss: 5.03386 | failed:  301
batch:     111820 | loss: 4.98454 | failed:  301
batch:     111830 | loss: 5.07068 | failed:  301
batch:     111840 | loss: 5.03010 | failed:  301
batch:     111850 | loss: 4.95859 | failed:  301
batch:     111860 | loss: 5.16795 | failed:  301
batch:     111870 | loss: 5.00438 | failed:  301
batch:     111880 | loss: 4.95602 | failed:  301
batch:     111890 | loss: 5.10325 | failed:  301
batch:     111900 | loss: 5.12821 | failed:  301
batch:     111910 | loss: 5.28616 | failed:  301
batch:     111920 | loss: 5.11017 | failed:  301
batch:     111930 | loss: 5.08805 | failed:  301
batch:     111940 | loss: 4.94043 | failed:  301
batch:     111950 | loss: 5.22788 | failed:  301
batch:     111960 | loss: 5.20864 | failed:  301
batch:     111970 | loss: 5.43884 | failed:  301
batch:     111980 | loss: 5.19994 | failed:  301
batch:     111990 | loss: 5.24415 | failed:  301
batch:     112000 | loss: 5.20088 | failed:  301
batch:     112010 | loss: 5.08730 | failed:  301
batch:     112020 | loss: 5.12195 | failed:  301
batch:     112030 | loss: 5.14281 | failed:  301
batch:     112040 | loss: 5.14577 | failed:  301
batch:     112050 | loss: 4.91679 | failed:  301
batch:     112060 | loss: 5.04183 | failed:  301
batch:     112070 | loss: 5.22305 | failed:  301
batch:     112080 | loss: 5.11154 | failed:  301
batch:     112090 | loss: 5.25347 | failed:  301
batch:     112100 | loss: 4.11762 | failed:  301
batch:     112110 | loss: 5.17641 | failed:  301
batch:     112120 | loss: 5.10321 | failed:  301
batch:     112130 | loss: 5.05505 | failed:  301
batch:     112140 | loss: 5.10802 | failed:  301
batch:     112150 | loss: 5.26242 | failed:  301
batch:     112160 | loss: 5.16959 | failed:  301
batch:     112170 | loss: 4.94572 | failed:  301
batch:     112180 | loss: 5.14963 | failed:  301
batch:     112190 | loss: 5.17250 | failed:  301
batch:     112200 | loss: 5.11023 | failed:  301
batch:     112210 | loss: 5.10862 | failed:  301
batch:     112220 | loss: 5.22166 | failed:  301
batch:     112230 | loss: 4.39089 | failed:  301
batch:     112240 | loss: 5.00762 | failed:  301
batch:     112250 | loss: 4.85757 | failed:  301
batch:     112260 | loss: 4.75886 | failed:  301
batch:     112270 | loss: 4.98931 | failed:  301
batch:     112280 | loss: 4.49533 | failed:  301
batch:     112290 | loss: 4.62739 | failed:  301
batch:     112300 | loss: 4.42729 | failed:  301
batch:     112310 | loss: 4.31545 | failed:  301
batch:     112320 | loss: 4.34733 | failed:  301
batch:     112330 | loss: 4.25162 | failed:  301
batch:     112340 | loss: 4.82115 | failed:  301
batch:     112350 | loss: 4.93744 | failed:  301
batch:     112360 | loss: 5.11515 | failed:  301
batch:     112370 | loss: 5.15060 | failed:  301
batch:     112380 | loss: 5.14974 | failed:  301
batch:     112390 | loss: 5.29005 | failed:  301
batch:     112400 | loss: 5.24892 | failed:  301
batch:     112410 | loss: 5.15780 | failed:  301
batch:     112420 | loss: 5.19020 | failed:  301
batch:     112430 | loss: 5.21646 | failed:  301
batch:     112440 | loss: 5.03299 | failed:  301
batch:     112450 | loss: 5.21678 | failed:  301
batch:     112460 | loss: 5.24673 | failed:  301
batch:     112470 | loss: 5.19634 | failed:  301
batch:     112480 | loss: 5.19417 | failed:  301
batch:     112490 | loss: 5.09141 | failed:  301
batch:     112500 | loss: 5.09990 | failed:  301
batch:     112510 | loss: 5.12942 | failed:  301
batch:     112520 | loss: 5.00671 | failed:  301
batch:     112530 | loss: 5.18490 | failed:  301
batch:     112540 | loss: 5.14715 | failed:  301
batch:     112550 | loss: 5.11773 | failed:  301
batch:     112560 | loss: 4.96618 | failed:  301
batch:     112570 | loss: 5.17461 | failed:  301
batch:     112580 | loss: 5.28347 | failed:  301
batch:     112590 | loss: 5.26823 | failed:  301
batch:     112600 | loss: 5.18739 | failed:  301
batch:     112610 | loss: 5.17345 | failed:  301
batch:     112620 | loss: 5.10893 | failed:  301
batch:     112630 | loss: 5.09655 | failed:  301
batch:     112640 | loss: 5.23240 | failed:  301
batch:     112650 | loss: 5.17093 | failed:  301
batch:     112660 | loss: 5.17300 | failed:  301
batch:     112670 | loss: 5.08916 | failed:  301
batch:     112680 | loss: 5.22946 | failed:  301
batch:     112690 | loss: 5.07458 | failed:  301
batch:     112700 | loss: 5.14372 | failed:  301
batch:     112710 | loss: 5.02409 | failed:  301
batch:     112720 | loss: 5.16975 | failed:  301
batch:     112730 | loss: 5.03013 | failed:  301
batch:     112740 | loss: 5.14800 | failed:  301
batch:     112750 | loss: 5.08037 | failed:  301
batch:     112760 | loss: 5.03064 | failed:  301
batch:     112770 | loss: 5.26675 | failed:  301
batch:     112780 | loss: 5.04346 | failed:  301
batch:     112790 | loss: 5.21508 | failed:  301
batch:     112800 | loss: 5.19515 | failed:  301
batch:     112810 | loss: 5.20076 | failed:  301
batch:     112820 | loss: 5.14294 | failed:  301
batch:     112830 | loss: 5.19089 | failed:  301
batch:     112840 | loss: 5.11361 | failed:  301
batch:     112850 | loss: 5.12151 | failed:  301
batch:     112860 | loss: 4.77064 | failed:  301
batch:     112870 | loss: 5.10903 | failed:  301
batch:     112880 | loss: 5.18104 | failed:  301
batch:     112890 | loss: 5.13323 | failed:  301
batch:     112900 | loss: 4.93425 | failed:  301
batch:     112910 | loss: 5.28055 | failed:  301
batch:     112920 | loss: 5.25768 | failed:  301
batch:     112930 | loss: 5.13483 | failed:  301
batch:     112940 | loss: 5.15930 | failed:  301
batch:     112950 | loss: 5.07720 | failed:  301
batch:     112960 | loss: 4.98117 | failed:  301
batch:     112970 | loss: 5.20308 | failed:  301
batch:     112980 | loss: 5.14483 | failed:  301
batch:     112990 | loss: 5.10561 | failed:  301
batch:     113000 | loss: 5.18886 | failed:  301
batch:     113010 | loss: 4.92111 | failed:  301
batch:     113020 | loss: 4.71920 | failed:  301
batch:     113030 | loss: 5.22838 | failed:  301
batch:     113040 | loss: 4.76772 | failed:  301
batch:     113050 | loss: 5.30478 | failed:  301
batch:     113060 | loss: 5.08920 | failed:  301
batch:     113070 | loss: 5.17974 | failed:  301
batch:     113080 | loss: 5.18327 | failed:  301
batch:     113090 | loss: 5.10258 | failed:  301
batch:     113100 | loss: 5.10067 | failed:  301
batch:     113110 | loss: 5.16612 | failed:  301
batch:     113120 | loss: 4.97959 | failed:  301
batch:     113130 | loss: 5.18820 | failed:  301
batch:     113140 | loss: 5.14007 | failed:  301
batch:     113150 | loss: 5.12076 | failed:  301
batch:     113160 | loss: 5.15115 | failed:  301
batch:     113170 | loss: 5.10797 | failed:  301
batch:     113180 | loss: 5.05521 | failed:  301
batch:     113190 | loss: 4.91702 | failed:  301
batch:     113200 | loss: 5.11785 | failed:  301
batch:     113210 | loss: 5.17241 | failed:  301
batch:     113220 | loss: 5.07247 | failed:  301
batch:     113230 | loss: 5.05580 | failed:  301
batch:     113240 | loss: 5.11810 | failed:  301
batch:     113250 | loss: 5.04819 | failed:  301
batch:     113260 | loss: 5.12778 | failed:  301
batch:     113270 | loss: 5.07212 | failed:  301
batch:     113280 | loss: 5.14184 | failed:  301
batch:     113290 | loss: 5.12730 | failed:  301
batch:     113300 | loss: 5.08617 | failed:  301
batch:     113310 | loss: 5.11696 | failed:  301
batch:     113320 | loss: 5.01481 | failed:  301
batch:     113330 | loss: 4.98227 | failed:  301
batch:     113340 | loss: 4.79538 | failed:  301
batch:     113350 | loss: 5.02092 | failed:  301
batch:     113360 | loss: 5.03993 | failed:  301
batch:     113370 | loss: 5.13158 | failed:  301
batch:     113380 | loss: 4.53905 | failed:  301
batch:     113390 | loss: 4.74063 | failed:  301
batch:     113400 | loss: 5.05815 | failed:  301
batch:     113410 | loss: 5.11112 | failed:  301
batch:     113420 | loss: 5.17673 | failed:  301
batch:     113430 | loss: 5.13508 | failed:  301
batch:     113440 | loss: 5.24863 | failed:  301
batch:     113450 | loss: 5.12636 | failed:  301
batch:     113460 | loss: 4.95967 | failed:  301
batch:     113470 | loss: 5.06101 | failed:  301
batch:     113480 | loss: 5.15537 | failed:  301
batch:     113490 | loss: 5.16574 | failed:  301
batch:     113500 | loss: 5.10731 | failed:  301
batch:     113510 | loss: 5.05334 | failed:  301
batch:     113520 | loss: 5.27694 | failed:  301
batch:     113530 | loss: 5.23226 | failed:  301
batch:     113540 | loss: 5.23458 | failed:  301
batch:     113550 | loss: 5.14182 | failed:  301
batch:     113560 | loss: 5.15404 | failed:  301
batch:     113570 | loss: 5.14952 | failed:  301
batch:     113580 | loss: 5.08023 | failed:  301
batch:     113590 | loss: 5.15995 | failed:  301
batch:     113600 | loss: 5.14444 | failed:  301
batch:     113610 | loss: 5.08624 | failed:  301
batch:     113620 | loss: 5.40392 | failed:  301
batch:     113630 | loss: 5.18771 | failed:  301
batch:     113640 | loss: 5.06698 | failed:  301
batch:     113650 | loss: 5.03615 | failed:  301
batch:     113660 | loss: 5.18502 | failed:  301
batch:     113670 | loss: 5.16379 | failed:  301
batch:     113680 | loss: 5.10785 | failed:  301
batch:     113690 | loss: 5.05658 | failed:  301
batch:     113700 | loss: 5.08879 | failed:  301
batch:     113710 | loss: 5.13309 | failed:  301
batch:     113720 | loss: 4.96487 | failed:  301
batch:     113730 | loss: 5.16672 | failed:  301
batch:     113740 | loss: 5.04739 | failed:  301
batch:     113750 | loss: 5.07125 | failed:  301
batch:     113760 | loss: 5.19267 | failed:  301
batch:     113770 | loss: 5.14458 | failed:  301
batch:     113780 | loss: 5.10556 | failed:  301
batch:     113790 | loss: 5.16393 | failed:  301
batch:     113800 | loss: 5.13891 | failed:  301
batch:     113810 | loss: 5.27031 | failed:  301
batch:     113820 | loss: 5.17418 | failed:  301
batch:     113830 | loss: 5.01995 | failed:  301
batch:     113840 | loss: 5.08342 | failed:  301
batch:     113850 | loss: 5.18331 | failed:  301
batch:     113860 | loss: 5.11089 | failed:  301
batch:     113870 | loss: 5.14239 | failed:  301
batch:     113880 | loss: 5.31703 | failed:  301
batch:     113890 | loss: 5.20040 | failed:  301
batch:     113900 | loss: 4.89319 | failed:  301
batch:     113910 | loss: 5.24568 | failed:  301
batch:     113920 | loss: 5.06381 | failed:  301
batch:     113930 | loss: 5.03930 | failed:  301
batch:     113940 | loss: 5.10469 | failed:  301
batch:     113950 | loss: 5.09315 | failed:  301
batch:     113960 | loss: 5.01284 | failed:  301
batch:     113970 | loss: 4.66373 | failed:  301
batch:     113980 | loss: 5.16830 | failed:  301
batch:     113990 | loss: 5.33242 | failed:  301
batch:     114000 | loss: 5.15032 | failed:  301
batch:     114010 | loss: 5.11575 | failed:  301
batch:     114020 | loss: 5.10670 | failed:  301
batch:     114030 | loss: 5.06352 | failed:  301
batch:     114040 | loss: 5.07612 | failed:  301
batch:     114050 | loss: 5.10666 | failed:  301
batch:     114060 | loss: 5.24589 | failed:  301
batch:     114070 | loss: 5.13063 | failed:  301
batch:     114080 | loss: 5.17630 | failed:  301
batch:     114090 | loss: 5.24013 | failed:  301
batch:     114100 | loss: 4.98623 | failed:  301
batch:     114110 | loss: 4.93460 | failed:  301
batch:     114120 | loss: 5.01597 | failed:  301
batch:     114130 | loss: 4.83359 | failed:  301
batch:     114140 | loss: 4.87423 | failed:  301
batch:     114150 | loss: 5.07157 | failed:  301
batch:     114160 | loss: 5.30036 | failed:  301
batch:     114170 | loss: 5.26998 | failed:  301
batch:     114180 | loss: 5.08195 | failed:  301
batch:     114190 | loss: 5.13827 | failed:  301
batch:     114200 | loss: 5.08274 | failed:  301
batch:     114210 | loss: 5.00457 | failed:  301
batch:     114220 | loss: 5.08541 | failed:  301
batch:     114230 | loss: 5.08639 | failed:  301
batch:     114240 | loss: 4.97079 | failed:  301
batch:     114250 | loss: 5.08609 | failed:  301
batch:     114260 | loss: 5.29113 | failed:  301
batch:     114270 | loss: 5.25892 | failed:  301
batch:     114280 | loss: 5.05354 | failed:  301
batch:     114290 | loss: 5.11721 | failed:  301
batch:     114300 | loss: 5.20121 | failed:  301
batch:     114310 | loss: 5.24189 | failed:  301
batch:     114320 | loss: 5.12327 | failed:  301
batch:     114330 | loss: 5.16150 | failed:  301
batch:     114340 | loss: 5.00089 | failed:  301
batch:     114350 | loss: 4.76768 | failed:  301
batch:     114360 | loss: 5.19636 | failed:  301
batch:     114370 | loss: 5.16811 | failed:  301
batch:     114380 | loss: 5.19789 | failed:  301
batch:     114390 | loss: 5.05185 | failed:  301
batch:     114400 | loss: 5.05932 | failed:  301
batch:     114410 | loss: 5.23310 | failed:  301
batch:     114420 | loss: 5.11523 | failed:  301
batch:     114430 | loss: 5.19609 | failed:  301
batch:     114440 | loss: 5.09157 | failed:  301
batch:     114450 | loss: 5.16669 | failed:  301
batch:     114460 | loss: 5.03597 | failed:  301
batch:     114470 | loss: 5.11455 | failed:  301
batch:     114480 | loss: 5.06034 | failed:  301
batch:     114490 | loss: 5.25816 | failed:  301
batch:     114500 | loss: 5.15729 | failed:  301
batch:     114510 | loss: 5.11974 | failed:  301
batch:     114520 | loss: 5.14248 | failed:  301
batch:     114530 | loss: 4.85151 | failed:  301
batch:     114540 | loss: 5.01305 | failed:  301
batch:     114550 | loss: 5.07986 | failed:  301
batch:     114560 | loss: 4.85890 | failed:  301
batch:     114570 | loss: 5.17989 | failed:  301
batch:     114580 | loss: 5.11705 | failed:  301
batch:     114590 | loss: 5.00499 | failed:  301
batch:     114600 | loss: 5.02138 | failed:  301
batch:     114610 | loss: 5.21852 | failed:  301
batch:     114620 | loss: 5.02638 | failed:  301
batch:     114630 | loss: 5.25304 | failed:  301
batch:     114640 | loss: 5.11860 | failed:  301
batch:     114650 | loss: 5.27131 | failed:  301
batch:     114660 | loss: 5.13509 | failed:  301
batch:     114670 | loss: 5.08418 | failed:  301
batch:     114680 | loss: 5.15784 | failed:  301
batch:     114690 | loss: 5.16066 | failed:  301
batch:     114700 | loss: 5.08908 | failed:  301
batch:     114710 | loss: 5.04340 | failed:  301
batch:     114720 | loss: 5.24721 | failed:  301
batch:     114730 | loss: 5.19244 | failed:  301
batch:     114740 | loss: 5.09435 | failed:  301
batch:     114750 | loss: 5.14588 | failed:  301
batch:     114760 | loss: 5.06801 | failed:  301
batch:     114770 | loss: 5.21062 | failed:  301
batch:     114780 | loss: 5.18125 | failed:  301
batch:     114790 | loss: 5.17034 | failed:  301
batch:     114800 | loss: 5.10460 | failed:  301
batch:     114810 | loss: 5.16189 | failed:  301
batch:     114820 | loss: 5.14643 | failed:  301
batch:     114830 | loss: 5.03709 | failed:  301
batch:     114840 | loss: 5.13900 | failed:  301
batch:     114850 | loss: 5.00889 | failed:  301
batch:     114860 | loss: 5.30523 | failed:  301
batch:     114870 | loss: 5.14737 | failed:  301
batch:     114880 | loss: 5.00809 | failed:  301
batch:     114890 | loss: 5.04420 | failed:  301
batch:     114900 | loss: 5.20754 | failed:  301
batch:     114910 | loss: 5.01526 | failed:  301
batch:     114920 | loss: 5.04193 | failed:  301
batch:     114930 | loss: 5.16216 | failed:  301
batch:     114940 | loss: 5.07092 | failed:  301
batch:     114950 | loss: 5.10878 | failed:  301
batch:     114960 | loss: 5.11411 | failed:  301
batch:     114970 | loss: 5.22383 | failed:  301
batch:     114980 | loss: 5.16937 | failed:  301
batch:     114990 | loss: 5.13967 | failed:  301
batch:     115000 | loss: 5.15539 | failed:  301
batch:     115010 | loss: 5.19286 | failed:  301
batch:     115020 | loss: 5.15858 | failed:  301
batch:     115030 | loss: 5.24132 | failed:  301
batch:     115040 | loss: 5.15922 | failed:  301
batch:     115050 | loss: 5.04575 | failed:  301
batch:     115060 | loss: 5.00888 | failed:  301
batch:     115070 | loss: 4.94874 | failed:  301
batch:     115080 | loss: 5.19001 | failed:  301
batch:     115090 | loss: 5.25627 | failed:  301
batch:     115100 | loss: 5.21581 | failed:  301
batch:     115110 | loss: 5.03446 | failed:  301
batch:     115120 | loss: 5.29334 | failed:  301
batch:     115130 | loss: 5.18757 | failed:  301
batch:     115140 | loss: 5.13653 | failed:  301
batch:     115150 | loss: 5.03492 | failed:  301
batch:     115160 | loss: 5.13457 | failed:  301
batch:     115170 | loss: 5.07213 | failed:  301
batch:     115180 | loss: 5.19712 | failed:  301
batch:     115190 | loss: 5.23524 | failed:  301
batch:     115200 | loss: 5.00068 | failed:  301
batch:     115210 | loss: 5.07962 | failed:  301
batch:     115220 | loss: 5.19846 | failed:  301
batch:     115230 | loss: 5.18325 | failed:  301
batch:     115240 | loss: 5.03070 | failed:  301
batch:     115250 | loss: 5.13190 | failed:  301
batch:     115260 | loss: 5.07985 | failed:  301
batch:     115270 | loss: 5.07541 | failed:  301
batch:     115280 | loss: 5.00692 | failed:  301
batch:     115290 | loss: 5.06615 | failed:  301
batch:     115300 | loss: 5.08025 | failed:  301
batch:     115310 | loss: 4.93756 | failed:  301
batch:     115320 | loss: 4.80395 | failed:  301
batch:     115330 | loss: 4.69035 | failed:  301
batch:     115340 | loss: 4.80389 | failed:  301
batch:     115350 | loss: 4.54579 | failed:  301
batch:     115360 | loss: 4.51310 | failed:  301
batch:     115370 | loss: 4.66703 | failed:  301
batch:     115380 | loss: 4.41907 | failed:  301
batch:     115390 | loss: 4.57216 | failed:  301
batch:     115400 | loss: 4.37353 | failed:  301
batch:     115410 | loss: 4.60924 | failed:  301
batch:     115420 | loss: 4.43384 | failed:  301
batch:     115430 | loss: 5.22276 | failed:  301
batch:     115440 | loss: 5.19455 | failed:  301
batch:     115450 | loss: 5.22678 | failed:  301
batch:     115460 | loss: 5.15077 | failed:  301
batch:     115470 | loss: 5.13147 | failed:  301
batch:     115480 | loss: 5.09822 | failed:  301
batch:     115490 | loss: 4.98537 | failed:  301
batch:     115500 | loss: 4.99330 | failed:  301
batch:     115510 | loss: 5.08900 | failed:  301
batch:     115520 | loss: 4.98056 | failed:  301
batch:     115530 | loss: 4.60636 | failed:  301
batch:     115540 | loss: 4.36040 | failed:  301
batch:     115550 | loss: 5.22179 | failed:  301
batch:     115560 | loss: 5.12354 | failed:  301
batch:     115570 | loss: 5.20367 | failed:  301
batch:     115580 | loss: 5.14266 | failed:  301
batch:     115590 | loss: 4.82343 | failed:  301
batch:     115600 | loss: 5.20512 | failed:  301
batch:     115610 | loss: 5.10385 | failed:  301
batch:     115620 | loss: 5.07048 | failed:  301
batch:     115630 | loss: 5.11090 | failed:  301
batch:     115640 | loss: 5.15965 | failed:  301
batch:     115650 | loss: 5.22713 | failed:  301
batch:     115660 | loss: 5.18162 | failed:  301
batch:     115670 | loss: 5.13311 | failed:  301
batch:     115680 | loss: 4.98495 | failed:  301
batch:     115690 | loss: 5.22544 | failed:  301
batch:     115700 | loss: 5.18849 | failed:  301
batch:     115710 | loss: 5.21292 | failed:  301
batch:     115720 | loss: 5.15831 | failed:  301
batch:     115730 | loss: 5.05117 | failed:  301
batch:     115740 | loss: 5.05480 | failed:  301
batch:     115750 | loss: 5.19503 | failed:  301
batch:     115760 | loss: 5.19397 | failed:  301
batch:     115770 | loss: 5.17252 | failed:  301
batch:     115780 | loss: 5.13403 | failed:  301
batch:     115790 | loss: 5.13343 | failed:  301
batch:     115800 | loss: 5.09974 | failed:  301
batch:     115810 | loss: 5.07931 | failed:  301
batch:     115820 | loss: 5.14143 | failed:  301
batch:     115830 | loss: 5.14016 | failed:  301
batch:     115840 | loss: 5.17963 | failed:  301
batch:     115850 | loss: 5.19605 | failed:  301
batch:     115860 | loss: 5.19564 | failed:  301
batch:     115870 | loss: 5.15886 | failed:  301
batch:     115880 | loss: 5.06659 | failed:  301
batch:     115890 | loss: 5.20938 | failed:  301
batch:     115900 | loss: 5.15424 | failed:  301
batch:     115910 | loss: 5.02855 | failed:  301
batch:     115920 | loss: 5.12783 | failed:  301
batch:     115930 | loss: 5.08228 | failed:  301
batch:     115940 | loss: 4.98775 | failed:  301
batch:     115950 | loss: 5.21241 | failed:  301
batch:     115960 | loss: 5.18045 | failed:  301
batch:     115970 | loss: 5.17525 | failed:  301
batch:     115980 | loss: 5.21484 | failed:  301
batch:     115990 | loss: 5.11845 | failed:  301
batch:     116000 | loss: 5.22731 | failed:  301
batch:     116010 | loss: 5.20668 | failed:  301
batch:     116020 | loss: 5.18712 | failed:  301
batch:     116030 | loss: 5.20763 | failed:  301
batch:     116040 | loss: 5.14577 | failed:  301
batch:     116050 | loss: 5.08022 | failed:  301
batch:     116060 | loss: 5.13581 | failed:  301
batch:     116070 | loss: 5.21727 | failed:  301
batch:     116080 | loss: 5.27160 | failed:  301
batch:     116090 | loss: 5.13779 | failed:  301
batch:     116100 | loss: 5.15613 | failed:  301
batch:     116110 | loss: 5.06980 | failed:  301
batch:     116120 | loss: 4.93201 | failed:  301
batch:     116130 | loss: 5.14133 | failed:  301
batch:     116140 | loss: 5.22131 | failed:  301
batch:     116150 | loss: 5.17584 | failed:  301
batch:     116160 | loss: 5.20324 | failed:  301
batch:     116170 | loss: 5.16012 | failed:  301
batch:     116180 | loss: 5.23910 | failed:  301
batch:     116190 | loss: 5.02781 | failed:  301
batch:     116200 | loss: 5.15067 | failed:  301
batch:     116210 | loss: 5.10644 | failed:  301
batch:     116220 | loss: 5.05065 | failed:  301
batch:     116230 | loss: 5.11311 | failed:  301
batch:     116240 | loss: 5.12926 | failed:  301
batch:     116250 | loss: 5.17803 | failed:  301
batch:     116260 | loss: 5.07646 | failed:  301
batch:     116270 | loss: 5.14868 | failed:  301
batch:     116280 | loss: 5.04303 | failed:  301
batch:     116290 | loss: 5.19555 | failed:  301
batch:     116300 | loss: 5.05705 | failed:  301
batch:     116310 | loss: 5.22897 | failed:  301
batch:     116320 | loss: 5.21134 | failed:  301
batch:     116330 | loss: 5.17313 | failed:  301
batch:     116340 | loss: 5.19512 | failed:  301
batch:     116350 | loss: 5.20457 | failed:  301
batch:     116360 | loss: 5.19843 | failed:  301
batch:     116370 | loss: 5.16703 | failed:  301
batch:     116380 | loss: 5.15727 | failed:  301
batch:     116390 | loss: 5.16798 | failed:  301
batch:     116400 | loss: 5.17425 | failed:  301
batch:     116410 | loss: 5.16761 | failed:  301
batch:     116420 | loss: 5.25167 | failed:  301
batch:     116430 | loss: 5.06380 | failed:  301
batch:     116440 | loss: 5.11167 | failed:  301
batch:     116450 | loss: 5.13404 | failed:  301
batch:     116460 | loss: 5.24497 | failed:  301
batch:     116470 | loss: 5.00127 | failed:  301
batch:     116480 | loss: 4.97602 | failed:  301
batch:     116490 | loss: 5.13488 | failed:  301
batch:     116500 | loss: 5.10835 | failed:  301
batch:     116510 | loss: 5.17384 | failed:  301
batch:     116520 | loss: 5.11005 | failed:  301
batch:     116530 | loss: 5.09716 | failed:  301
batch:     116540 | loss: 4.95979 | failed:  301
batch:     116550 | loss: 5.09957 | failed:  301
batch:     116560 | loss: 5.09354 | failed:  301
batch:     116570 | loss: 5.12239 | failed:  301
batch:     116580 | loss: 5.27315 | failed:  301
batch:     116590 | loss: 5.01679 | failed:  301
batch:     116600 | loss: 5.15701 | failed:  301
batch:     116610 | loss: 5.16728 | failed:  301
batch:     116620 | loss: 5.13185 | failed:  301
batch:     116630 | loss: 5.14734 | failed:  301
batch:     116640 | loss: 5.20841 | failed:  301
batch:     116650 | loss: 5.13165 | failed:  301
batch:     116660 | loss: 5.14443 | failed:  301
batch:     116670 | loss: 5.16063 | failed:  301
batch:     116680 | loss: 5.14740 | failed:  301
batch:     116690 | loss: 4.87685 | failed:  301
batch:     116700 | loss: 5.10004 | failed:  301
batch:     116710 | loss: 5.03901 | failed:  301
batch:     116720 | loss: 5.02995 | failed:  301
batch:     116730 | loss: 4.91424 | failed:  301
batch:     116740 | loss: 4.85519 | failed:  301
batch:     116750 | loss: 4.69348 | failed:  301
batch:     116760 | loss: 4.38687 | failed:  301
batch:     116770 | loss: 4.50177 | failed:  301
batch:     116780 | loss: 4.26405 | failed:  301
batch:     116790 | loss: 4.43623 | failed:  301
batch:     116800 | loss: 4.35074 | failed:  301
batch:     116810 | loss: 4.14858 | failed:  301
batch:     116820 | loss: 4.08510 | failed:  301
batch:     116830 | loss: 3.94611 | failed:  301
batch:     116840 | loss: 4.34961 | failed:  301
batch:     116850 | loss: 4.25396 | failed:  301
batch:     116860 | loss: 4.20264 | failed:  301
batch:     116870 | loss: 4.52035 | failed:  301
batch:     116880 | loss: 4.68383 | failed:  301
batch:     116890 | loss: 4.90606 | failed:  301
batch:     116900 | loss: 4.67110 | failed:  301
batch:     116910 | loss: 4.54648 | failed:  301
batch:     116920 | loss: 4.37835 | failed:  301
batch:     116930 | loss: 4.07036 | failed:  301
batch:     116940 | loss: 4.77250 | failed:  301
batch:     116950 | loss: 4.36371 | failed:  301
batch:     116960 | loss: 4.56798 | failed:  301
batch:     116970 | loss: 4.35511 | failed:  301
batch:     116980 | loss: 4.68599 | failed:  301
batch:     116990 | loss: 4.53183 | failed:  301
batch:     117000 | loss: 4.48520 | failed:  301
batch:     117010 | loss: 4.71882 | failed:  301
batch:     117020 | loss: 4.93907 | failed:  301
batch:     117030 | loss: 4.83451 | failed:  301
batch:     117040 | loss: 4.90110 | failed:  301
batch:     117050 | loss: 5.27197 | failed:  301
batch:     117060 | loss: 5.14359 | failed:  301
batch:     117070 | loss: 5.25468 | failed:  301
batch:     117080 | loss: 5.30584 | failed:  301
batch:     117090 | loss: 5.21616 | failed:  301
batch:     117100 | loss: 5.20479 | failed:  301
batch:     117110 | loss: 5.28659 | failed:  301
batch:     117120 | loss: 4.98564 | failed:  301
batch:     117130 | loss: 5.16105 | failed:  301
batch:     117140 | loss: 5.16229 | failed:  301
batch:     117150 | loss: 5.04315 | failed:  301
batch:     117160 | loss: 4.90263 | failed:  301
batch:     117170 | loss: 5.07906 | failed:  301
batch:     117180 | loss: 4.82748 | failed:  301
batch:     117190 | loss: 5.19655 | failed:  301
batch:     117200 | loss: 5.12307 | failed:  301
batch:     117210 | loss: 5.28044 | failed:  301
batch:     117220 | loss: 5.10507 | failed:  301
batch:     117230 | loss: 5.14123 | failed:  301
batch:     117240 | loss: 5.13384 | failed:  301
batch:     117250 | loss: 5.04586 | failed:  301
batch:     117260 | loss: 5.18774 | failed:  301
batch:     117270 | loss: 5.14085 | failed:  301
batch:     117280 | loss: 5.11005 | failed:  301
batch:     117290 | loss: 5.19817 | failed:  301
batch:     117300 | loss: 4.92668 | failed:  301
batch:     117310 | loss: 5.05971 | failed:  301
batch:     117320 | loss: 4.77827 | failed:  301
batch:     117330 | loss: 5.05701 | failed:  301
batch:     117340 | loss: 5.18151 | failed:  301
batch:     117350 | loss: 5.10449 | failed:  301
batch:     117360 | loss: 5.09613 | failed:  301
batch:     117370 | loss: 5.01160 | failed:  301
batch:     117380 | loss: 5.08605 | failed:  301
batch:     117390 | loss: 4.97055 | failed:  301
batch:     117400 | loss: 5.15058 | failed:  301
batch:     117410 | loss: 5.01075 | failed:  301
batch:     117420 | loss: 5.13969 | failed:  301
batch:     117430 | loss: 4.89302 | failed:  301
batch:     117440 | loss: 5.07750 | failed:  301
batch:     117450 | loss: 5.23398 | failed:  301
batch:     117460 | loss: 5.21405 | failed:  301
batch:     117470 | loss: 4.97818 | failed:  301
batch:     117480 | loss: 5.03200 | failed:  301
batch:     117490 | loss: 4.95955 | failed:  301
batch:     117500 | loss: 5.06991 | failed:  301
batch:     117510 | loss: 5.13754 | failed:  301
batch:     117520 | loss: 5.16045 | failed:  301
batch:     117530 | loss: 5.09928 | failed:  301
batch:     117540 | loss: 5.04265 | failed:  301
batch:     117550 | loss: 5.00546 | failed:  301
batch:     117560 | loss: 5.22755 | failed:  301
batch:     117570 | loss: 5.22143 | failed:  301
batch:     117580 | loss: 5.09176 | failed:  301
batch:     117590 | loss: 5.12336 | failed:  301
batch:     117600 | loss: 5.03527 | failed:  301
batch:     117610 | loss: 4.86140 | failed:  301
batch:     117620 | loss: 5.16414 | failed:  301
batch:     117630 | loss: 5.14998 | failed:  301
batch:     117640 | loss: 5.14502 | failed:  301
batch:     117650 | loss: 5.06422 | failed:  301
batch:     117660 | loss: 5.19153 | failed:  301
batch:     117670 | loss: 5.12434 | failed:  301
batch:     117680 | loss: 5.15427 | failed:  301
batch:     117690 | loss: 5.12131 | failed:  301
batch:     117700 | loss: 4.97140 | failed:  301
batch:     117710 | loss: 5.10103 | failed:  301
batch:     117720 | loss: 4.92739 | failed:  301
batch:     117730 | loss: 5.12517 | failed:  301
batch:     117740 | loss: 5.13461 | failed:  301
batch:     117750 | loss: 5.17745 | failed:  301
batch:     117760 | loss: 5.09840 | failed:  301
batch:     117770 | loss: 5.25406 | failed:  301
batch:     117780 | loss: 5.10659 | failed:  301
batch:     117790 | loss: 4.97722 | failed:  301
batch:     117800 | loss: 5.18556 | failed:  301
batch:     117810 | loss: 5.16110 | failed:  301
batch:     117820 | loss: 5.18039 | failed:  301
batch:     117830 | loss: 5.17754 | failed:  301
batch:     117840 | loss: 5.17186 | failed:  301
batch:     117850 | loss: 5.07170 | failed:  301
batch:     117860 | loss: 5.21085 | failed:  301
batch:     117870 | loss: 5.19889 | failed:  301
batch:     117880 | loss: 5.09907 | failed:  301
batch:     117890 | loss: 5.17872 | failed:  301
batch:     117900 | loss: 5.06367 | failed:  301
batch:     117910 | loss: 5.08256 | failed:  301
batch:     117920 | loss: 5.13849 | failed:  301
batch:     117930 | loss: 5.06235 | failed:  301
batch:     117940 | loss: 5.09859 | failed:  301
batch:     117950 | loss: 5.03060 | failed:  301
batch:     117960 | loss: 5.12880 | failed:  301
batch:     117970 | loss: 5.12607 | failed:  301
batch:     117980 | loss: 5.04036 | failed:  301
batch:     117990 | loss: 5.10781 | failed:  301
batch:     118000 | loss: 5.11951 | failed:  301
batch:     118010 | loss: 5.14254 | failed:  301
batch:     118020 | loss: 5.14381 | failed:  301
batch:     118030 | loss: 5.18039 | failed:  301
batch:     118040 | loss: 5.07941 | failed:  301
batch:     118050 | loss: 5.11673 | failed:  301
batch:     118060 | loss: 5.16971 | failed:  301
batch:     118070 | loss: 5.13851 | failed:  301
batch:     118080 | loss: 5.09642 | failed:  301
batch:     118090 | loss: 3.81963 | failed:  301
batch:     118100 | loss: 5.12748 | failed:  301
batch:     118110 | loss: 5.12752 | failed:  301
batch:     118120 | loss: 4.99428 | failed:  301
batch:     118130 | loss: 5.26206 | failed:  301
batch:     118140 | loss: 5.25982 | failed:  301
batch:     118150 | loss: 5.20185 | failed:  301
batch:     118160 | loss: 5.12385 | failed:  301
batch:     118170 | loss: 5.07403 | failed:  301
batch:     118180 | loss: 5.08770 | failed:  301
batch:     118190 | loss: 5.20039 | failed:  301
batch:     118200 | loss: 5.25281 | failed:  301
batch:     118210 | loss: 5.12474 | failed:  301
batch:     118220 | loss: 5.05270 | failed:  301
batch:     118230 | loss: 5.18594 | failed:  301
batch:     118240 | loss: 5.18795 | failed:  301
batch:     118250 | loss: 5.17227 | failed:  301
batch:     118260 | loss: 5.22441 | failed:  301
batch:     118270 | loss: 4.99073 | failed:  301
batch:     118280 | loss: 4.87950 | failed:  301
batch:     118290 | loss: 4.36623 | failed:  301
batch:     118300 | loss: 5.27460 | failed:  301
batch:     118310 | loss: 5.09447 | failed:  301
batch:     118320 | loss: 5.19671 | failed:  301
batch:     118330 | loss: 5.02914 | failed:  301
batch:     118340 | loss: 4.84230 | failed:  301
batch:     118350 | loss: 5.25096 | failed:  301
batch:     118360 | loss: 5.23036 | failed:  301
batch:     118370 | loss: 5.17822 | failed:  301
batch:     118380 | loss: 5.24513 | failed:  301
batch:     118390 | loss: 5.08263 | failed:  301
batch:     118400 | loss: 5.19819 | failed:  301
batch:     118410 | loss: 5.23094 | failed:  301
batch:     118420 | loss: 5.08407 | failed:  301
batch:     118430 | loss: 5.14181 | failed:  301
batch:     118440 | loss: 5.14113 | failed:  301
batch:     118450 | loss: 5.16229 | failed:  301
batch:     118460 | loss: 5.05301 | failed:  301
batch:     118470 | loss: 5.20038 | failed:  301
batch:     118480 | loss: 5.30581 | failed:  301
batch:     118490 | loss: 5.20973 | failed:  301
batch:     118500 | loss: 5.19099 | failed:  301
batch:     118510 | loss: 5.21436 | failed:  301
batch:     118520 | loss: 5.11639 | failed:  301
batch:     118530 | loss: 5.15632 | failed:  301
batch:     118540 | loss: 4.92541 | failed:  301
batch:     118550 | loss: 5.24414 | failed:  301
batch:     118560 | loss: 5.04452 | failed:  301
batch:     118570 | loss: 5.09918 | failed:  301
batch:     118580 | loss: 5.07071 | failed:  301
batch:     118590 | loss: 5.11243 | failed:  301
batch:     118600 | loss: 5.17938 | failed:  301
batch:     118610 | loss: 5.06382 | failed:  301
batch:     118620 | loss: 5.16151 | failed:  301
batch:     118630 | loss: 5.12758 | failed:  301
batch:     118640 | loss: 5.23205 | failed:  301
batch:     118650 | loss: 5.21403 | failed:  301
batch:     118660 | loss: 5.09216 | failed:  301
batch:     118670 | loss: 5.20435 | failed:  301
batch:     118680 | loss: 5.14801 | failed:  301
batch:     118690 | loss: 5.17246 | failed:  301
batch:     118700 | loss: 5.23288 | failed:  301
batch:     118710 | loss: 5.17049 | failed:  301
batch:     118720 | loss: 5.18717 | failed:  301
batch:     118730 | loss: 4.99587 | failed:  301
batch:     118740 | loss: 4.95067 | failed:  301
batch:     118750 | loss: 4.93514 | failed:  301
batch:     118760 | loss: 4.97005 | failed:  301
batch:     118770 | loss: 4.77419 | failed:  301
batch:     118780 | loss: 5.13270 | failed:  301
batch:     118790 | loss: 5.16401 | failed:  301
batch:     118800 | loss: 4.98683 | failed:  301
batch:     118810 | loss: 5.12513 | failed:  301
batch:     118820 | loss: 4.83498 | failed:  301
batch:     118830 | loss: 5.13850 | failed:  301
batch:     118840 | loss: 5.20806 | failed:  301
batch:     118850 | loss: 5.20398 | failed:  301
batch:     118860 | loss: 5.22249 | failed:  301
batch:     118870 | loss: 5.17802 | failed:  301
batch:     118880 | loss: 5.15157 | failed:  301
batch:     118890 | loss: 4.97208 | failed:  301
batch:     118900 | loss: 5.02567 | failed:  301
batch:     118910 | loss: 5.05118 | failed:  301
batch:     118920 | loss: 5.03700 | failed:  301
batch:     118930 | loss: 5.10713 | failed:  301
batch:     118940 | loss: 5.19599 | failed:  301
batch:     118950 | loss: 5.01400 | failed:  301
batch:     118960 | loss: 5.03639 | failed:  301
batch:     118970 | loss: 5.08698 | failed:  301
batch:     118980 | loss: 5.19166 | failed:  301
batch:     118990 | loss: 5.50226 | failed:  301
batch:     119000 | loss: 5.26370 | failed:  301
batch:     119010 | loss: 5.20290 | failed:  301
batch:     119020 | loss: 5.21466 | failed:  301
batch:     119030 | loss: 5.20388 | failed:  301
batch:     119040 | loss: 5.09031 | failed:  301
batch:     119050 | loss: 5.02794 | failed:  301
batch:     119060 | loss: 5.11557 | failed:  301
batch:     119070 | loss: 5.09683 | failed:  301
batch:     119080 | loss: 5.19078 | failed:  301
batch:     119090 | loss: 5.21932 | failed:  301
batch:     119100 | loss: 4.99963 | failed:  301
batch:     119110 | loss: 5.17110 | failed:  301
batch:     119120 | loss: 5.14630 | failed:  301
batch:     119130 | loss: 5.12037 | failed:  301
batch:     119140 | loss: 5.04785 | failed:  301
batch:     119150 | loss: 5.20674 | failed:  301
batch:     119160 | loss: 5.18768 | failed:  301
batch:     119170 | loss: 5.23258 | failed:  301
batch:     119180 | loss: 5.21838 | failed:  301
batch:     119190 | loss: 5.13623 | failed:  301
batch:     119200 | loss: 5.16974 | failed:  301
batch:     119210 | loss: 5.14780 | failed:  301
batch:     119220 | loss: 5.13231 | failed:  301
batch:     119230 | loss: 5.05660 | failed:  301
batch:     119240 | loss: 5.08095 | failed:  301
batch:     119250 | loss: 5.12144 | failed:  301
batch:     119260 | loss: 5.17460 | failed:  301
batch:     119270 | loss: 5.12842 | failed:  301
batch:     119280 | loss: 5.12234 | failed:  301
batch:     119290 | loss: 5.24175 | failed:  301
batch:     119300 | loss: 5.18154 | failed:  301
batch:     119310 | loss: 5.16099 | failed:  301
batch:     119320 | loss: 4.99595 | failed:  301
batch:     119330 | loss: 4.92612 | failed:  301
batch:     119340 | loss: 5.10781 | failed:  301
batch:     119350 | loss: 4.94487 | failed:  301
batch:     119360 | loss: 5.12900 | failed:  301
batch:     119370 | loss: 5.21128 | failed:  301
batch:     119380 | loss: 5.13753 | failed:  301
batch:     119390 | loss: 5.31725 | failed:  301
batch:     119400 | loss: 5.11227 | failed:  301
batch:     119410 | loss: 5.13033 | failed:  301
batch:     119420 | loss: 5.00958 | failed:  301
batch:     119430 | loss: 5.15719 | failed:  301
batch:     119440 | loss: 5.10743 | failed:  301
batch:     119450 | loss: 5.07209 | failed:  301
batch:     119460 | loss: 5.22052 | failed:  301
batch:     119470 | loss: 5.20031 | failed:  301
batch:     119480 | loss: 5.09047 | failed:  301
batch:     119490 | loss: 5.13999 | failed:  301
batch:     119500 | loss: 5.08052 | failed:  301
batch:     119510 | loss: 5.21721 | failed:  301
batch:     119520 | loss: 5.10381 | failed:  301
batch:     119530 | loss: 5.13610 | failed:  301
batch:     119540 | loss: 5.09809 | failed:  301
batch:     119550 | loss: 5.02076 | failed:  301
batch:     119560 | loss: 5.13120 | failed:  301
batch:     119570 | loss: 5.07295 | failed:  301
batch:     119580 | loss: 5.12398 | failed:  301
batch:     119590 | loss: 5.16104 | failed:  301
batch:     119600 | loss: 5.11857 | failed:  301
batch:     119610 | loss: 5.15593 | failed:  301
batch:     119620 | loss: 4.96741 | failed:  301
batch:     119630 | loss: 5.21704 | failed:  301
batch:     119640 | loss: 5.08085 | failed:  301
batch:     119650 | loss: 5.14016 | failed:  301
batch:     119660 | loss: 5.09430 | failed:  301
batch:     119670 | loss: 5.07178 | failed:  301
batch:     119680 | loss: 5.12634 | failed:  301
batch:     119690 | loss: 5.14344 | failed:  301
batch:     119700 | loss: 5.31632 | failed:  301
batch:     119710 | loss: 5.16558 | failed:  301
batch:     119720 | loss: 4.83919 | failed:  301
batch:     119730 | loss: 5.21950 | failed:  301
batch:     119740 | loss: 5.22753 | failed:  301
batch:     119750 | loss: 5.11903 | failed:  301
batch:     119760 | loss: 5.19269 | failed:  301
batch:     119770 | loss: 5.03216 | failed:  301
batch:     119780 | loss: 5.06966 | failed:  301
batch:     119790 | loss: 5.11115 | failed:  301
batch:     119800 | loss: 5.16691 | failed:  301
batch:     119810 | loss: 5.16561 | failed:  301
batch:     119820 | loss: 5.12602 | failed:  301
batch:     119830 | loss: 5.03192 | failed:  301
batch:     119840 | loss: 5.11932 | failed:  301
batch:     119850 | loss: 5.01417 | failed:  301
batch:     119860 | loss: 5.16379 | failed:  301
batch:     119870 | loss: 5.13010 | failed:  301
batch:     119880 | loss: 5.15776 | failed:  301
batch:     119890 | loss: 5.06880 | failed:  301
batch:     119900 | loss: 4.91157 | failed:  301
batch:     119910 | loss: 5.04142 | failed:  301
batch:     119920 | loss: 5.09737 | failed:  301
batch:     119930 | loss: 5.24936 | failed:  301
batch:     119940 | loss: 5.20363 | failed:  301
batch:     119950 | loss: 5.14842 | failed:  301
batch:     119960 | loss: 5.03445 | failed:  301
batch:     119970 | loss: 5.07658 | failed:  301
batch:     119980 | loss: 5.25196 | failed:  301
batch:     119990 | loss: 5.08095 | failed:  301
batch:     120000 | loss: 5.05183 | failed:  301
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     120010 | loss: 4.94794 | failed:  301
batch:     120020 | loss: 4.93302 | failed:  301
batch:     120030 | loss: 4.65543 | failed:  301
batch:     120040 | loss: 4.73276 | failed:  301
batch:     120050 | loss: 5.14017 | failed:  301
batch:     120060 | loss: 4.86736 | failed:  301
batch:     120070 | loss: 5.06606 | failed:  301
batch:     120080 | loss: 4.96761 | failed:  301
batch:     120090 | loss: 5.07929 | failed:  301
batch:     120100 | loss: 5.18863 | failed:  301
batch:     120110 | loss: 5.12069 | failed:  301
batch:     120120 | loss: 5.11152 | failed:  301
batch:     120130 | loss: 5.10021 | failed:  301
batch:     120140 | loss: 5.07366 | failed:  301
batch:     120150 | loss: 5.05046 | failed:  301
batch:     120160 | loss: 5.25668 | failed:  301
batch:     120170 | loss: 5.23774 | failed:  301
batch:     120180 | loss: 5.12667 | failed:  301
batch:     120190 | loss: 5.21392 | failed:  301
batch:     120200 | loss: 5.23054 | failed:  301
batch:     120210 | loss: 5.10487 | failed:  301
batch:     120220 | loss: 5.03170 | failed:  301
batch:     120230 | loss: 5.02687 | failed:  301
batch:     120240 | loss: 5.12353 | failed:  301
batch:     120250 | loss: 5.08786 | failed:  301
batch:     120260 | loss: 5.18848 | failed:  301
batch:     120270 | loss: 5.14867 | failed:  301
batch:     120280 | loss: 5.18541 | failed:  301
batch:     120290 | loss: 5.08121 | failed:  301
batch:     120300 | loss: 4.92501 | failed:  301
batch:     120310 | loss: 5.08115 | failed:  301
batch:     120320 | loss: 5.05315 | failed:  301
batch:     120330 | loss: 5.20507 | failed:  301
batch:     120340 | loss: 5.09410 | failed:  301
batch:     120350 | loss: 5.14982 | failed:  301
batch:     120360 | loss: 5.13807 | failed:  301
batch:     120370 | loss: 5.19464 | failed:  301
batch:     120380 | loss: 5.14588 | failed:  301
batch:     120390 | loss: 5.23388 | failed:  301
batch:     120400 | loss: 5.02947 | failed:  301
batch:     120410 | loss: 5.20778 | failed:  301
batch:     120420 | loss: 5.15004 | failed:  301
batch:     120430 | loss: 5.14667 | failed:  301
batch:     120440 | loss: 4.99143 | failed:  301
batch:     120450 | loss: 5.11321 | failed:  301
batch:     120460 | loss: 5.11544 | failed:  301
batch:     120470 | loss: 4.99993 | failed:  301
batch:     120480 | loss: 5.07742 | failed:  301
batch:     120490 | loss: 5.10578 | failed:  301
batch:     120500 | loss: 4.76400 | failed:  301
batch:     120510 | loss: 5.13023 | failed:  301
batch:     120520 | loss: 5.14704 | failed:  301
batch:     120530 | loss: 5.19611 | failed:  301
batch:     120540 | loss: 5.30427 | failed:  301
batch:     120550 | loss: 4.93534 | failed:  301
batch:     120560 | loss: 4.96824 | failed:  301
batch:     120570 | loss: 5.13288 | failed:  301
batch:     120580 | loss: 5.04808 | failed:  301
batch:     120590 | loss: 5.23080 | failed:  301
batch:     120600 | loss: 5.09244 | failed:  301
batch:     120610 | loss: 5.03915 | failed:  301
batch:     120620 | loss: 5.05163 | failed:  301
batch:     120630 | loss: 5.24346 | failed:  301
batch:     120640 | loss: 5.25580 | failed:  301
batch:     120650 | loss: 5.07441 | failed:  301
batch:     120660 | loss: 5.08900 | failed:  301
batch:     120670 | loss: 5.11792 | failed:  301
batch:     120680 | loss: 5.29829 | failed:  301
batch:     120690 | loss: 5.20646 | failed:  301
batch:     120700 | loss: 5.15674 | failed:  301
batch:     120710 | loss: 5.19641 | failed:  301
batch:     120720 | loss: 5.14134 | failed:  301
batch:     120730 | loss: 5.14543 | failed:  301
batch:     120740 | loss: 5.18983 | failed:  301
batch:     120750 | loss: 5.26323 | failed:  301
batch:     120760 | loss: 5.19523 | failed:  301
batch:     120770 | loss: 5.17502 | failed:  301
batch:     120780 | loss: 5.13925 | failed:  301
batch:     120790 | loss: 5.16550 | failed:  301
batch:     120800 | loss: 5.24985 | failed:  301
batch:     120810 | loss: 5.24856 | failed:  301
batch:     120820 | loss: 5.18198 | failed:  301
batch:     120830 | loss: 5.07797 | failed:  301
batch:     120840 | loss: 5.14473 | failed:  301
batch:     120850 | loss: 5.20361 | failed:  301
batch:     120860 | loss: 4.98346 | failed:  301
batch:     120870 | loss: 5.15491 | failed:  301
batch:     120880 | loss: 5.08119 | failed:  301
batch:     120890 | loss: 5.15063 | failed:  301
batch:     120900 | loss: 5.10710 | failed:  301
batch:     120910 | loss: 5.10446 | failed:  301
batch:     120920 | loss: 5.05691 | failed:  301
batch:     120930 | loss: 5.15004 | failed:  301
batch:     120940 | loss: 5.12806 | failed:  301
batch:     120950 | loss: 5.17576 | failed:  301
batch:     120960 | loss: 4.98717 | failed:  301
batch:     120970 | loss: 5.12977 | failed:  301
batch:     120980 | loss: 5.16656 | failed:  301
batch:     120990 | loss: 5.16345 | failed:  301
batch:     121000 | loss: 5.13547 | failed:  301
batch:     121010 | loss: 5.15351 | failed:  301
batch:     121020 | loss: 4.82969 | failed:  301
batch:     121030 | loss: 5.09319 | failed:  301
batch:     121040 | loss: 5.09848 | failed:  301
batch:     121050 | loss: 5.22965 | failed:  301
batch:     121060 | loss: 5.03983 | failed:  301
batch:     121070 | loss: 5.13379 | failed:  301
batch:     121080 | loss: 5.13953 | failed:  301
batch:     121090 | loss: 5.06576 | failed:  301
batch:     121100 | loss: 5.13143 | failed:  301
batch:     121110 | loss: 5.11503 | failed:  301
batch:     121120 | loss: 5.17201 | failed:  301
batch:     121130 | loss: 5.23139 | failed:  301
batch:     121140 | loss: 4.97632 | failed:  301
batch:     121150 | loss: 5.20135 | failed:  301
batch:     121160 | loss: 5.19165 | failed:  301
batch:     121170 | loss: 5.15614 | failed:  301
batch:     121180 | loss: 5.19021 | failed:  301
batch:     121190 | loss: 5.09408 | failed:  301
batch:     121200 | loss: 5.11371 | failed:  301
batch:     121210 | loss: 5.09192 | failed:  301
batch:     121220 | loss: 5.02765 | failed:  301
batch:     121230 | loss: 5.23992 | failed:  301
batch:     121240 | loss: 5.25291 | failed:  301
batch:     121250 | loss: 5.17862 | failed:  301
batch:     121260 | loss: 5.01669 | failed:  301
batch:     121270 | loss: 5.14637 | failed:  301
batch:     121280 | loss: 5.10295 | failed:  301
batch:     121290 | loss: 5.14327 | failed:  301
batch:     121300 | loss: 5.12047 | failed:  301
batch:     121310 | loss: 5.19394 | failed:  301
batch:     121320 | loss: 5.26157 | failed:  301
batch:     121330 | loss: 5.12684 | failed:  301
batch:     121340 | loss: 5.07158 | failed:  301
batch:     121350 | loss: 5.08539 | failed:  301
batch:     121360 | loss: 5.15252 | failed:  301
batch:     121370 | loss: 5.09429 | failed:  301
batch:     121380 | loss: 5.06964 | failed:  301
batch:     121390 | loss: 5.10599 | failed:  301
batch:     121400 | loss: 5.11727 | failed:  301
batch:     121410 | loss: 5.27682 | failed:  301
batch:     121420 | loss: 5.17343 | failed:  301
batch:     121430 | loss: 5.11853 | failed:  301
batch:     121440 | loss: 5.10598 | failed:  301
batch:     121450 | loss: 5.18903 | failed:  301
batch:     121460 | loss: 5.10868 | failed:  301
batch:     121470 | loss: 5.02499 | failed:  301
batch:     121480 | loss: 5.32540 | failed:  301
batch:     121490 | loss: 5.14774 | failed:  301
batch:     121500 | loss: 5.08261 | failed:  301
batch:     121510 | loss: 5.17930 | failed:  301
batch:     121520 | loss: 5.11442 | failed:  301
batch:     121530 | loss: 4.97389 | failed:  301
batch:     121540 | loss: 5.12665 | failed:  301
batch:     121550 | loss: 5.10850 | failed:  301
batch:     121560 | loss: 5.10403 | failed:  301
batch:     121570 | loss: 4.95188 | failed:  301
batch:     121580 | loss: 5.03992 | failed:  301
batch:     121590 | loss: 5.23671 | failed:  301
batch:     121600 | loss: 5.12472 | failed:  301
batch:     121610 | loss: 5.12296 | failed:  301
batch:     121620 | loss: 4.88111 | failed:  301
batch:     121630 | loss: 5.13703 | failed:  301
batch:     121640 | loss: 5.09844 | failed:  301
batch:     121650 | loss: 5.22356 | failed:  301
batch:     121660 | loss: 4.82604 | failed:  301
batch:     121670 | loss: 5.20872 | failed:  301
batch:     121680 | loss: 5.11058 | failed:  301
batch:     121690 | loss: 4.97367 | failed:  301
batch:     121700 | loss: 4.94729 | failed:  301
batch:     121710 | loss: 5.22802 | failed:  301
batch:     121720 | loss: 5.12149 | failed:  301
batch:     121730 | loss: 5.17194 | failed:  301
batch:     121740 | loss: 5.06141 | failed:  301
batch:     121750 | loss: 5.05488 | failed:  301
batch:     121760 | loss: 5.04499 | failed:  301
batch:     121770 | loss: 5.03449 | failed:  301
batch:     121780 | loss: 5.00573 | failed:  301
batch:     121790 | loss: 4.98479 | failed:  301
batch:     121800 | loss: 5.18046 | failed:  301
batch:     121810 | loss: 5.02472 | failed:  301
batch:     121820 | loss: 5.11184 | failed:  301
batch:     121830 | loss: 5.16097 | failed:  301
batch:     121840 | loss: 5.07765 | failed:  301
batch:     121850 | loss: 5.13649 | failed:  301
batch:     121860 | loss: 5.03897 | failed:  301
batch:     121870 | loss: 5.19335 | failed:  301
batch:     121880 | loss: 5.04017 | failed:  301
batch:     121890 | loss: 5.12343 | failed:  301
batch:     121900 | loss: 4.84588 | failed:  301
batch:     121910 | loss: 4.93993 | failed:  301
batch:     121920 | loss: 5.14631 | failed:  301
batch:     121930 | loss: 5.10272 | failed:  301
batch:     121940 | loss: 5.21690 | failed:  301
batch:     121950 | loss: 4.93071 | failed:  301
batch:     121960 | loss: 5.10910 | failed:  301
batch:     121970 | loss: 5.22405 | failed:  301
batch:     121980 | loss: 5.20710 | failed:  301
batch:     121990 | loss: 5.09442 | failed:  301
batch:     122000 | loss: 4.92237 | failed:  301
batch:     122010 | loss: 4.89225 | failed:  301
batch:     122020 | loss: 5.08003 | failed:  301
batch:     122030 | loss: 5.05131 | failed:  301
batch:     122040 | loss: 5.12687 | failed:  301
batch:     122050 | loss: 5.11926 | failed:  301
batch:     122060 | loss: 5.17567 | failed:  301
batch:     122070 | loss: 5.10218 | failed:  301
batch:     122080 | loss: 5.16792 | failed:  301
batch:     122090 | loss: 4.99568 | failed:  301
batch:     122100 | loss: 4.98110 | failed:  301
batch:     122110 | loss: 5.21216 | failed:  301
batch:     122120 | loss: 5.07965 | failed:  301
batch:     122130 | loss: 4.97622 | failed:  301
batch:     122140 | loss: 4.77917 | failed:  301
batch:     122150 | loss: 5.06214 | failed:  301
batch:     122160 | loss: 5.04954 | failed:  301
batch:     122170 | loss: 4.96803 | failed:  301
batch:     122180 | loss: 5.24213 | failed:  301
batch:     122190 | loss: 5.27591 | failed:  301
batch:     122200 | loss: 5.02168 | failed:  301
batch:     122210 | loss: 4.97260 | failed:  301
batch:     122220 | loss: 5.08858 | failed:  301
batch:     122230 | loss: 4.92057 | failed:  301
batch:     122240 | loss: 4.97451 | failed:  301
batch:     122250 | loss: 5.07036 | failed:  301
batch:     122260 | loss: 4.97638 | failed:  301
batch:     122270 | loss: 5.16456 | failed:  301
batch:     122280 | loss: 5.23972 | failed:  301
batch:     122290 | loss: 5.25857 | failed:  301
batch:     122300 | loss: 5.16470 | failed:  301
batch:     122310 | loss: 5.00319 | failed:  301
batch:     122320 | loss: 4.81009 | failed:  301
batch:     122330 | loss: 5.14483 | failed:  301
batch:     122340 | loss: 5.10461 | failed:  301
batch:     122350 | loss: 5.10038 | failed:  301
batch:     122360 | loss: 5.32235 | failed:  301
batch:     122370 | loss: 5.19138 | failed:  301
batch:     122380 | loss: 5.03156 | failed:  301
batch:     122390 | loss: 5.12757 | failed:  301
batch:     122400 | loss: 5.20130 | failed:  301
batch:     122410 | loss: 5.18236 | failed:  301
batch:     122420 | loss: 5.14642 | failed:  301
batch:     122430 | loss: 5.10283 | failed:  301
batch:     122440 | loss: 5.12407 | failed:  301
batch:     122450 | loss: 5.12377 | failed:  301
batch:     122460 | loss: 4.95841 | failed:  301
batch:     122470 | loss: 5.12649 | failed:  301
batch:     122480 | loss: 4.96918 | failed:  301
batch:     122490 | loss: 5.12202 | failed:  301
batch:     122500 | loss: 5.19600 | failed:  301
batch:     122510 | loss: 5.20923 | failed:  301
batch:     122520 | loss: 5.22203 | failed:  301
batch:     122530 | loss: 4.02404 | failed:  301
batch:     122540 | loss: 5.18731 | failed:  301
batch:     122550 | loss: 5.11757 | failed:  301
batch:     122560 | loss: 5.45243 | failed:  301
batch:     122570 | loss: 4.98143 | failed:  301
batch:     122580 | loss: 5.15429 | failed:  301
batch:     122590 | loss: 3.65402 | failed:  301
batch:     122600 | loss: 3.53515 | failed:  301
batch:     122610 | loss: 5.50168 | failed:  301
batch:     122620 | loss: 5.31971 | failed:  301
batch:     122630 | loss: 4.97635 | failed:  301
batch:     122640 | loss: 5.31828 | failed:  301
batch:     122650 | loss: 3.53654 | failed:  301
batch:     122660 | loss: 3.63594 | failed:  301
batch:     122670 | loss: 5.14842 | failed:  301
batch:     122680 | loss: 5.31212 | failed:  301
batch:     122690 | loss: 5.35876 | failed:  301
batch:     122700 | loss: 5.21500 | failed:  301
batch:     122710 | loss: 5.01879 | failed:  301
batch:     122720 | loss: 5.14596 | failed:  301
batch:     122730 | loss: 5.19744 | failed:  301
batch:     122740 | loss: 5.09550 | failed:  301
batch:     122750 | loss: 5.11276 | failed:  301
batch:     122760 | loss: 5.17027 | failed:  301
batch:     122770 | loss: 5.04548 | failed:  301
batch:     122780 | loss: 5.08182 | failed:  301
batch:     122790 | loss: 5.17342 | failed:  301
batch:     122800 | loss: 5.07674 | failed:  301
batch:     122810 | loss: 5.19031 | failed:  301
batch:     122820 | loss: 5.18909 | failed:  301
batch:     122830 | loss: 5.15319 | failed:  301
batch:     122840 | loss: 5.18262 | failed:  301
batch:     122850 | loss: 5.21825 | failed:  301
batch:     122860 | loss: 5.09526 | failed:  301
batch:     122870 | loss: 5.12125 | failed:  301
batch:     122880 | loss: 5.17601 | failed:  301
batch:     122890 | loss: 5.17051 | failed:  301
batch:     122900 | loss: 5.15851 | failed:  301
batch:     122910 | loss: 5.07800 | failed:  301
batch:     122920 | loss: 4.87487 | failed:  301
batch:     122930 | loss: 5.16699 | failed:  301
batch:     122940 | loss: 5.18876 | failed:  301
batch:     122950 | loss: 5.15491 | failed:  301
batch:     122960 | loss: 5.22858 | failed:  301
batch:     122970 | loss: 5.09681 | failed:  301
batch:     122980 | loss: 4.88876 | failed:  301
batch:     122990 | loss: 5.30513 | failed:  301
batch:     123000 | loss: 5.26118 | failed:  301
batch:     123010 | loss: 5.21295 | failed:  301
batch:     123020 | loss: 5.23757 | failed:  301
batch:     123030 | loss: 5.21673 | failed:  301
batch:     123040 | loss: 5.22349 | failed:  301
batch:     123050 | loss: 5.12852 | failed:  301
batch:     123060 | loss: 5.18775 | failed:  301
batch:     123070 | loss: 5.21002 | failed:  301
batch:     123080 | loss: 5.09027 | failed:  301
batch:     123090 | loss: 5.16773 | failed:  301
batch:     123100 | loss: 5.11443 | failed:  301
batch:     123110 | loss: 5.18477 | failed:  301
batch:     123120 | loss: 5.13111 | failed:  301
batch:     123130 | loss: 5.12325 | failed:  301
batch:     123140 | loss: 5.10803 | failed:  301
batch:     123150 | loss: 5.07016 | failed:  301
batch:     123160 | loss: 5.22435 | failed:  301
batch:     123170 | loss: 5.16678 | failed:  301
batch:     123180 | loss: 5.13515 | failed:  301
batch:     123190 | loss: 5.12765 | failed:  301
batch:     123200 | loss: 5.21379 | failed:  301
batch:     123210 | loss: 3.93458 | failed:  301
batch:     123220 | loss: 3.52004 | failed:  301
batch:     123230 | loss: 3.65220 | failed:  301
batch:     123240 | loss: 5.32084 | failed:  301
batch:     123250 | loss: 5.21529 | failed:  301
batch:     123260 | loss: 5.13589 | failed:  301
batch:     123270 | loss: 5.15539 | failed:  301
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     123290 | loss: 3.95068 | failed:  304
batch:     123300 | loss: 5.06059 | failed:  304
batch:     123310 | loss: 5.06055 | failed:  304
batch:     123320 | loss: 5.25834 | failed:  304
batch:     123330 | loss: 5.27042 | failed:  304
batch:     123340 | loss: 5.08563 | failed:  304
batch:     123350 | loss: 5.07856 | failed:  304
batch:     123360 | loss: 5.21595 | failed:  304
batch:     123370 | loss: 3.50921 | failed:  304
batch:     123380 | loss: 5.24397 | failed:  304
batch:     123390 | loss: 5.22533 | failed:  304
batch:     123400 | loss: 5.22981 | failed:  304
batch:     123410 | loss: 5.01465 | failed:  304
batch:     123420 | loss: 5.21007 | failed:  304
batch:     123430 | loss: 5.14708 | failed:  304
batch:     123440 | loss: 5.15326 | failed:  304
batch:     123450 | loss: 5.14449 | failed:  304
batch:     123460 | loss: 5.13780 | failed:  304
batch:     123470 | loss: 5.03740 | failed:  304
batch:     123480 | loss: 5.17887 | failed:  304
batch:     123490 | loss: 5.12918 | failed:  304
batch:     123500 | loss: 5.13814 | failed:  304
batch:     123510 | loss: 5.18255 | failed:  304
batch:     123520 | loss: 5.10931 | failed:  304
batch:     123530 | loss: 5.21676 | failed:  304
batch:     123540 | loss: 5.20436 | failed:  304
batch:     123550 | loss: 5.17366 | failed:  304
batch:     123560 | loss: 5.07151 | failed:  304
batch:     123570 | loss: 5.16245 | failed:  304
batch:     123580 | loss: 5.10907 | failed:  304
batch:     123590 | loss: 5.21262 | failed:  304
batch:     123600 | loss: 5.07507 | failed:  304
batch:     123610 | loss: 5.08462 | failed:  304
batch:     123620 | loss: 5.12198 | failed:  304
batch:     123630 | loss: 4.95782 | failed:  304
batch:     123640 | loss: 5.17359 | failed:  304
batch:     123650 | loss: 5.15333 | failed:  304
batch:     123660 | loss: 5.21954 | failed:  304
batch:     123670 | loss: 4.92715 | failed:  304
batch:     123680 | loss: 5.16466 | failed:  304
batch:     123690 | loss: 5.15852 | failed:  304
batch:     123700 | loss: 5.23408 | failed:  304
batch:     123710 | loss: 5.22186 | failed:  304
batch:     123720 | loss: 5.18547 | failed:  304
batch:     123730 | loss: 5.19199 | failed:  304
batch:     123740 | loss: 4.94969 | failed:  304
batch:     123750 | loss: 5.23360 | failed:  304
batch:     123760 | loss: 5.22401 | failed:  304
batch:     123770 | loss: 5.24009 | failed:  304
batch:     123780 | loss: 5.14376 | failed:  304
batch:     123790 | loss: 5.18392 | failed:  304
batch:     123800 | loss: 5.18149 | failed:  304
batch:     123810 | loss: 5.18037 | failed:  304
batch:     123820 | loss: 5.17858 | failed:  304
batch:     123830 | loss: 5.19307 | failed:  304
batch:     123840 | loss: 5.14981 | failed:  304
batch:     123850 | loss: 5.17996 | failed:  304
batch:     123860 | loss: 5.12522 | failed:  304
batch:     123870 | loss: 4.94952 | failed:  304
batch:     123880 | loss: 5.19188 | failed:  304
batch:     123890 | loss: 5.14526 | failed:  304
batch:     123900 | loss: 5.14911 | failed:  304
batch:     123910 | loss: 5.26079 | failed:  304
batch:     123920 | loss: 5.22873 | failed:  304
batch:     123930 | loss: 5.07205 | failed:  304
batch:     123940 | loss: 5.16268 | failed:  304
batch:     123950 | loss: 5.13384 | failed:  304
batch:     123960 | loss: 5.09472 | failed:  304
batch:     123970 | loss: 5.13093 | failed:  304
batch:     123980 | loss: 5.15199 | failed:  304
batch:     123990 | loss: 5.07645 | failed:  304
batch:     124000 | loss: 5.05442 | failed:  304
batch:     124010 | loss: 5.01757 | failed:  304
batch:     124020 | loss: 4.99814 | failed:  304
batch:     124030 | loss: 5.03605 | failed:  304
batch:     124040 | loss: 5.15392 | failed:  304
batch:     124050 | loss: 4.98813 | failed:  304
batch:     124060 | loss: 5.09844 | failed:  304
batch:     124070 | loss: 5.15212 | failed:  304
batch:     124080 | loss: 5.19358 | failed:  304
batch:     124090 | loss: 5.17633 | failed:  304
batch:     124100 | loss: 5.28588 | failed:  304
batch:     124110 | loss: 4.74771 | failed:  304
batch:     124120 | loss: 5.11234 | failed:  304
batch:     124130 | loss: 5.06019 | failed:  304
batch:     124140 | loss: 5.24959 | failed:  304
batch:     124150 | loss: 5.15434 | failed:  304
batch:     124160 | loss: 5.07649 | failed:  304
batch:     124170 | loss: 5.14817 | failed:  304
batch:     124180 | loss: 5.12244 | failed:  304
batch:     124190 | loss: 5.10579 | failed:  304
batch:     124200 | loss: 5.23025 | failed:  304
batch:     124210 | loss: 5.14048 | failed:  304
batch:     124220 | loss: 5.15531 | failed:  304
batch:     124230 | loss: 4.99077 | failed:  304
batch:     124240 | loss: 5.00308 | failed:  304
batch:     124250 | loss: 4.86460 | failed:  304
batch:     124260 | loss: 5.13638 | failed:  304
batch:     124270 | loss: 5.18368 | failed:  304
batch:     124280 | loss: 5.16240 | failed:  304
batch:     124290 | loss: 5.16183 | failed:  304
batch:     124300 | loss: 5.17192 | failed:  304
batch:     124310 | loss: 4.98184 | failed:  304
batch:     124320 | loss: 5.05524 | failed:  304
batch:     124330 | loss: 5.28315 | failed:  304
batch:     124340 | loss: 5.21474 | failed:  304
batch:     124350 | loss: 5.08093 | failed:  304
batch:     124360 | loss: 4.77746 | failed:  304
batch:     124370 | loss: 5.14601 | failed:  304
batch:     124380 | loss: 5.17597 | failed:  304
batch:     124390 | loss: 5.24661 | failed:  304
batch:     124400 | loss: 5.22106 | failed:  304
batch:     124410 | loss: 5.13221 | failed:  304
batch:     124420 | loss: 5.11419 | failed:  304
batch:     124430 | loss: 5.18088 | failed:  304
batch:     124440 | loss: 5.18802 | failed:  304
batch:     124450 | loss: 5.10569 | failed:  304
batch:     124460 | loss: 5.10001 | failed:  304
batch:     124470 | loss: 5.10830 | failed:  304
batch:     124480 | loss: 5.24708 | failed:  304
batch:     124490 | loss: 4.91260 | failed:  304
batch:     124500 | loss: 5.41717 | failed:  304
batch:     124510 | loss: 5.11591 | failed:  304
batch:     124520 | loss: 5.07737 | failed:  304
batch:     124530 | loss: 5.16832 | failed:  304
batch:     124540 | loss: 5.23950 | failed:  304
batch:     124550 | loss: 5.17517 | failed:  304
batch:     124560 | loss: 5.26087 | failed:  304
batch:     124570 | loss: 5.17251 | failed:  304
batch:     124580 | loss: 5.02769 | failed:  304
batch:     124590 | loss: 5.15037 | failed:  304
batch:     124600 | loss: 5.17014 | failed:  304
batch:     124610 | loss: 4.96735 | failed:  304
batch:     124620 | loss: 4.84091 | failed:  304
batch:     124630 | loss: 4.84489 | failed:  304
batch:     124640 | loss: 5.08933 | failed:  304
batch:     124650 | loss: 5.19511 | failed:  304
batch:     124660 | loss: 5.16041 | failed:  304
batch:     124670 | loss: 5.07029 | failed:  304
batch:     124680 | loss: 5.13217 | failed:  304
batch:     124690 | loss: 5.10245 | failed:  304
batch:     124700 | loss: 5.07842 | failed:  304
batch:     124710 | loss: 5.13093 | failed:  304
batch:     124720 | loss: 5.08526 | failed:  304
batch:     124730 | loss: 5.25994 | failed:  304
batch:     124740 | loss: 5.10608 | failed:  304
batch:     124750 | loss: 5.13117 | failed:  304
batch:     124760 | loss: 5.18671 | failed:  304
batch:     124770 | loss: 5.13853 | failed:  304
batch:     124780 | loss: 5.17540 | failed:  304
batch:     124790 | loss: 5.04077 | failed:  304
batch:     124800 | loss: 5.06114 | failed:  304
batch:     124810 | loss: 5.20322 | failed:  304
batch:     124820 | loss: 5.04542 | failed:  304
batch:     124830 | loss: 5.15922 | failed:  304
batch:     124840 | loss: 5.25968 | failed:  304
batch:     124850 | loss: 5.09579 | failed:  304
batch:     124860 | loss: 5.14599 | failed:  304
batch:     124870 | loss: 5.09535 | failed:  304
batch:     124880 | loss: 5.09275 | failed:  304
batch:     124890 | loss: 5.07354 | failed:  304
batch:     124900 | loss: 5.21873 | failed:  304
batch:     124910 | loss: 4.74974 | failed:  304
batch:     124920 | loss: 4.47954 | failed:  304
batch:     124930 | loss: 5.10216 | failed:  304
batch:     124940 | loss: 5.16327 | failed:  304
batch:     124950 | loss: 5.07352 | failed:  304
batch:     124960 | loss: 5.09092 | failed:  304
batch:     124970 | loss: 5.12121 | failed:  304
batch:     124980 | loss: 5.01543 | failed:  304
batch:     124990 | loss: 5.20827 | failed:  304
batch:     125000 | loss: 5.10882 | failed:  304
batch:     125010 | loss: 5.15793 | failed:  304
batch:     125020 | loss: 5.19694 | failed:  304
batch:     125030 | loss: 5.20916 | failed:  304
batch:     125040 | loss: 5.09776 | failed:  304
batch:     125050 | loss: 5.02455 | failed:  304
batch:     125060 | loss: 5.26301 | failed:  304
batch:     125070 | loss: 5.27539 | failed:  304
batch:     125080 | loss: 5.20956 | failed:  304
batch:     125090 | loss: 5.22207 | failed:  304
batch:     125100 | loss: 5.18832 | failed:  304
batch:     125110 | loss: 5.22007 | failed:  304
batch:     125120 | loss: 5.17965 | failed:  304
batch:     125130 | loss: 5.17316 | failed:  304
batch:     125140 | loss: 5.11987 | failed:  304
batch:     125150 | loss: 5.12642 | failed:  304
batch:     125160 | loss: 5.01779 | failed:  304
batch:     125170 | loss: 5.09488 | failed:  304
batch:     125180 | loss: 5.09153 | failed:  304
batch:     125190 | loss: 5.13294 | failed:  304
batch:     125200 | loss: 5.18675 | failed:  304
batch:     125210 | loss: 5.01135 | failed:  304
batch:     125220 | loss: 5.17852 | failed:  304
batch:     125230 | loss: 5.10950 | failed:  304
batch:     125240 | loss: 4.98173 | failed:  304
batch:     125250 | loss: 5.03584 | failed:  304
batch:     125260 | loss: 5.03694 | failed:  304
batch:     125270 | loss: 4.60760 | failed:  304
batch:     125280 | loss: 5.01876 | failed:  304
batch:     125290 | loss: 5.22157 | failed:  304
batch:     125300 | loss: 5.15604 | failed:  304
batch:     125310 | loss: 5.17755 | failed:  304
batch:     125320 | loss: 5.03240 | failed:  304
batch:     125330 | loss: 4.97721 | failed:  304
batch:     125340 | loss: 5.12902 | failed:  304
batch:     125350 | loss: 5.03355 | failed:  304
batch:     125360 | loss: 5.11889 | failed:  304
batch:     125370 | loss: 5.05645 | failed:  304
batch:     125380 | loss: 4.96341 | failed:  304
batch:     125390 | loss: 5.00644 | failed:  304
batch:     125400 | loss: 5.09126 | failed:  304
batch:     125410 | loss: 5.18759 | failed:  304
batch:     125420 | loss: 4.71996 | failed:  304
batch:     125430 | loss: 3.87518 | failed:  304
batch:     125440 | loss: 4.94624 | failed:  304
batch:     125450 | loss: 5.09525 | failed:  304
batch:     125460 | loss: 5.18275 | failed:  304
batch:     125470 | loss: 5.23087 | failed:  304
batch:     125480 | loss: 5.13391 | failed:  304
batch:     125490 | loss: 5.16607 | failed:  304
batch:     125500 | loss: 5.10380 | failed:  304
batch:     125510 | loss: 5.03581 | failed:  304
batch:     125520 | loss: 5.02283 | failed:  304
batch:     125530 | loss: 5.20766 | failed:  304
batch:     125540 | loss: 5.22131 | failed:  304
batch:     125550 | loss: 5.16643 | failed:  304
batch:     125560 | loss: 5.13226 | failed:  304
batch:     125570 | loss: 5.30999 | failed:  304
batch:     125580 | loss: 5.28089 | failed:  304
batch:     125590 | loss: 5.23769 | failed:  304
batch:     125600 | loss: 5.09583 | failed:  304
batch:     125610 | loss: 5.16135 | failed:  304
batch:     125620 | loss: 5.15844 | failed:  304
batch:     125630 | loss: 5.18097 | failed:  304
batch:     125640 | loss: 5.15658 | failed:  304
batch:     125650 | loss: 5.08315 | failed:  304
batch:     125660 | loss: 5.08464 | failed:  304
batch:     125670 | loss: 5.02578 | failed:  304
batch:     125680 | loss: 4.90827 | failed:  304
batch:     125690 | loss: 5.55888 | failed:  304
batch:     125700 | loss: 4.99791 | failed:  304
batch:     125710 | loss: 5.28335 | failed:  304
batch:     125720 | loss: 5.29100 | failed:  304
batch:     125730 | loss: 4.94849 | failed:  304
batch:     125740 | loss: 4.99155 | failed:  304
batch:     125750 | loss: 4.82279 | failed:  304
batch:     125760 | loss: 5.15072 | failed:  304
batch:     125770 | loss: 5.18444 | failed:  304
batch:     125780 | loss: 5.09624 | failed:  304
batch:     125790 | loss: 5.27985 | failed:  304
batch:     125800 | loss: 5.23739 | failed:  304
batch:     125810 | loss: 5.22906 | failed:  304
batch:     125820 | loss: 5.11972 | failed:  304
batch:     125830 | loss: 5.22036 | failed:  304
batch:     125840 | loss: 5.00841 | failed:  304
batch:     125850 | loss: 5.07325 | failed:  304
batch:     125860 | loss: 5.13805 | failed:  304
batch:     125870 | loss: 5.18477 | failed:  304
batch:     125880 | loss: 5.04875 | failed:  304
batch:     125890 | loss: 5.17809 | failed:  304
batch:     125900 | loss: 5.13644 | failed:  304
batch:     125910 | loss: 5.03722 | failed:  304
batch:     125920 | loss: 4.88556 | failed:  304
batch:     125930 | loss: 5.16527 | failed:  304
batch:     125940 | loss: 5.17275 | failed:  304
batch:     125950 | loss: 5.17528 | failed:  304
batch:     125960 | loss: 5.15821 | failed:  304
batch:     125970 | loss: 5.11222 | failed:  304
batch:     125980 | loss: 5.12522 | failed:  304
batch:     125990 | loss: 5.10607 | failed:  304
batch:     126000 | loss: 5.06453 | failed:  304
batch:     126010 | loss: 5.12962 | failed:  304
batch:     126020 | loss: 5.05993 | failed:  304
batch:     126030 | loss: 5.05934 | failed:  304
batch:     126040 | loss: 5.07179 | failed:  304
batch:     126050 | loss: 5.19327 | failed:  304
batch:     126060 | loss: 5.24765 | failed:  304
batch:     126070 | loss: 5.18173 | failed:  304
batch:     126080 | loss: 4.89141 | failed:  304
batch:     126090 | loss: 5.05891 | failed:  304
batch:     126100 | loss: 5.18239 | failed:  304
batch:     126110 | loss: 5.07213 | failed:  304
batch:     126120 | loss: 5.16678 | failed:  304
batch:     126130 | loss: 4.77156 | failed:  304
batch:     126140 | loss: 4.73878 | failed:  304
batch:     126150 | loss: 5.14857 | failed:  304
batch:     126160 | loss: 5.13908 | failed:  304
batch:     126170 | loss: 5.10771 | failed:  304
batch:     126180 | loss: 5.22195 | failed:  304
batch:     126190 | loss: 5.17810 | failed:  304
batch:     126200 | loss: 5.14486 | failed:  304
batch:     126210 | loss: 5.20255 | failed:  304
batch:     126220 | loss: 5.17769 | failed:  304
batch:     126230 | loss: 5.07416 | failed:  304
batch:     126240 | loss: 5.09182 | failed:  304
batch:     126250 | loss: 5.10123 | failed:  304
batch:     126260 | loss: 5.16326 | failed:  304
batch:     126270 | loss: 5.10850 | failed:  304
batch:     126280 | loss: 5.19389 | failed:  304
batch:     126290 | loss: 5.14260 | failed:  304
batch:     126300 | loss: 5.23544 | failed:  304
batch:     126310 | loss: 5.20561 | failed:  304
batch:     126320 | loss: 5.14655 | failed:  304
batch:     126330 | loss: 5.10156 | failed:  304
batch:     126340 | loss: 4.90903 | failed:  304
batch:     126350 | loss: 5.09955 | failed:  304
batch:     126360 | loss: 5.12146 | failed:  304
batch:     126370 | loss: 5.20426 | failed:  304
batch:     126380 | loss: 5.14220 | failed:  304
batch:     126390 | loss: 5.30916 | failed:  304
batch:     126400 | loss: 5.17463 | failed:  304
batch:     126410 | loss: 5.10420 | failed:  304
batch:     126420 | loss: 5.21895 | failed:  304
batch:     126430 | loss: 5.08262 | failed:  304
batch:     126440 | loss: 5.00728 | failed:  304
batch:     126450 | loss: 5.05817 | failed:  304
batch:     126460 | loss: 4.91812 | failed:  304
batch:     126470 | loss: 5.17654 | failed:  304
batch:     126480 | loss: 5.12662 | failed:  304
batch:     126490 | loss: 5.14225 | failed:  304
batch:     126500 | loss: 5.20099 | failed:  304
batch:     126510 | loss: 4.99538 | failed:  304
batch:     126520 | loss: 5.17602 | failed:  304
batch:     126530 | loss: 5.06244 | failed:  304
batch:     126540 | loss: 5.18254 | failed:  304
batch:     126550 | loss: 5.23591 | failed:  304
batch:     126560 | loss: 5.07408 | failed:  304
batch:     126570 | loss: 5.14203 | failed:  304
batch:     126580 | loss: 4.97132 | failed:  304
batch:     126590 | loss: 5.03000 | failed:  304
batch:     126600 | loss: 5.17031 | failed:  304
batch:     126610 | loss: 5.04312 | failed:  304
batch:     126620 | loss: 4.83171 | failed:  304
batch:     126630 | loss: 5.09767 | failed:  304
batch:     126640 | loss: 5.12152 | failed:  304
batch:     126650 | loss: 5.08896 | failed:  304
batch:     126660 | loss: 5.04593 | failed:  304
batch:     126670 | loss: 5.10422 | failed:  304
batch:     126680 | loss: 5.20362 | failed:  304
batch:     126690 | loss: 5.08912 | failed:  304
batch:     126700 | loss: 5.10641 | failed:  304
batch:     126710 | loss: 5.22918 | failed:  304
batch:     126720 | loss: 5.20732 | failed:  304
batch:     126730 | loss: 5.16628 | failed:  304
batch:     126740 | loss: 5.17641 | failed:  304
batch:     126750 | loss: 5.18361 | failed:  304
batch:     126760 | loss: 5.09171 | failed:  304
batch:     126770 | loss: 5.10308 | failed:  304
batch:     126780 | loss: 5.18224 | failed:  304
batch:     126790 | loss: 5.02859 | failed:  304
batch:     126800 | loss: 5.08266 | failed:  304
batch:     126810 | loss: 4.82604 | failed:  304
batch:     126820 | loss: 5.20668 | failed:  304
batch:     126830 | loss: 5.17253 | failed:  304
batch:     126840 | loss: 5.05601 | failed:  304
batch:     126850 | loss: 5.01528 | failed:  304
batch:     126860 | loss: 5.05584 | failed:  304
batch:     126870 | loss: 5.18087 | failed:  304
batch:     126880 | loss: 5.18722 | failed:  304
batch:     126890 | loss: 5.13052 | failed:  304
batch:     126900 | loss: 5.09047 | failed:  304
batch:     126910 | loss: 5.14180 | failed:  304
batch:     126920 | loss: 5.24954 | failed:  304
batch:     126930 | loss: 5.24809 | failed:  304
batch:     126940 | loss: 5.19042 | failed:  304
batch:     126950 | loss: 5.19248 | failed:  304
batch:     126960 | loss: 5.10392 | failed:  304
batch:     126970 | loss: 5.22436 | failed:  304
batch:     126980 | loss: 5.21658 | failed:  304
batch:     126990 | loss: 5.11906 | failed:  304
batch:     127000 | loss: 5.14332 | failed:  304
batch:     127010 | loss: 5.08570 | failed:  304
batch:     127020 | loss: 5.18592 | failed:  304
batch:     127030 | loss: 5.14264 | failed:  304
batch:     127040 | loss: 5.19002 | failed:  304
batch:     127050 | loss: 5.13689 | failed:  304
batch:     127060 | loss: 5.28933 | failed:  304
batch:     127070 | loss: 5.01278 | failed:  304
batch:     127080 | loss: 5.07751 | failed:  304
batch:     127090 | loss: 5.25937 | failed:  304
batch:     127100 | loss: 5.11044 | failed:  304
batch:     127110 | loss: 5.23244 | failed:  304
batch:     127120 | loss: 5.13483 | failed:  304
batch:     127130 | loss: 5.13074 | failed:  304
batch:     127140 | loss: 5.09528 | failed:  304
batch:     127150 | loss: 5.16873 | failed:  304
batch:     127160 | loss: 5.23399 | failed:  304
batch:     127170 | loss: 5.19441 | failed:  304
batch:     127180 | loss: 5.25919 | failed:  304
batch:     127190 | loss: 5.22247 | failed:  304
batch:     127200 | loss: 4.77410 | failed:  304
batch:     127210 | loss: 5.09946 | failed:  304
batch:     127220 | loss: 5.09884 | failed:  304
batch:     127230 | loss: 5.12206 | failed:  304
batch:     127240 | loss: 4.97018 | failed:  304
batch:     127250 | loss: 5.11000 | failed:  304
batch:     127260 | loss: 5.14545 | failed:  304
batch:     127270 | loss: 5.16756 | failed:  304
batch:     127280 | loss: 5.16373 | failed:  304
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     127300 | loss: 4.75035 | failed:  320
batch:     127310 | loss: 5.07098 | failed:  320
batch:     127320 | loss: 5.01775 | failed:  320
batch:     127330 | loss: 4.96895 | failed:  320
batch:     127340 | loss: 5.23032 | failed:  320
batch:     127350 | loss: 5.13838 | failed:  320
batch:     127360 | loss: 5.23876 | failed:  320
batch:     127370 | loss: 5.15987 | failed:  320
batch:     127380 | loss: 5.09420 | failed:  320
batch:     127390 | loss: 5.06771 | failed:  320
batch:     127400 | loss: 5.19846 | failed:  320
batch:     127410 | loss: 5.16004 | failed:  320
batch:     127420 | loss: 5.14796 | failed:  320
batch:     127430 | loss: 4.90071 | failed:  320
batch:     127440 | loss: 5.19336 | failed:  320
batch:     127450 | loss: 5.22763 | failed:  320
batch:     127460 | loss: 5.20728 | failed:  320
batch:     127470 | loss: 5.17976 | failed:  320
batch:     127480 | loss: 5.13422 | failed:  320
batch:     127490 | loss: 5.18189 | failed:  320
batch:     127500 | loss: 5.14769 | failed:  320
batch:     127510 | loss: 5.13649 | failed:  320
batch:     127520 | loss: 5.05836 | failed:  320
batch:     127530 | loss: 5.00628 | failed:  320
batch:     127540 | loss: 5.25034 | failed:  320
batch:     127550 | loss: 4.99882 | failed:  320
batch:     127560 | loss: 4.85179 | failed:  320
batch:     127570 | loss: 4.85429 | failed:  320
batch:     127580 | loss: 4.77847 | failed:  320
batch:     127590 | loss: 4.86174 | failed:  320
batch:     127600 | loss: 5.32840 | failed:  320
batch:     127610 | loss: 5.19286 | failed:  320
batch:     127620 | loss: 5.15097 | failed:  320
batch:     127630 | loss: 5.03231 | failed:  320
batch:     127640 | loss: 4.59310 | failed:  320
batch:     127650 | loss: 4.66809 | failed:  320
batch:     127660 | loss: 5.22732 | failed:  320
batch:     127670 | loss: 5.08174 | failed:  320
batch:     127680 | loss: 5.21266 | failed:  320
batch:     127690 | loss: 4.97934 | failed:  320
batch:     127700 | loss: 5.17267 | failed:  320
batch:     127710 | loss: 5.12426 | failed:  320
batch:     127720 | loss: 5.33637 | failed:  320
batch:     127730 | loss: 5.20653 | failed:  320
batch:     127740 | loss: 5.16612 | failed:  320
batch:     127750 | loss: 5.22917 | failed:  320
batch:     127760 | loss: 3.89947 | failed:  320
batch:     127770 | loss: 5.12894 | failed:  320
batch:     127780 | loss: 5.11267 | failed:  320
batch:     127790 | loss: 5.12512 | failed:  320
batch:     127800 | loss: 5.18814 | failed:  320
batch:     127810 | loss: 5.14808 | failed:  320
batch:     127820 | loss: 5.07595 | failed:  320
batch:     127830 | loss: 5.10851 | failed:  320
batch:     127840 | loss: 5.12831 | failed:  320
batch:     127850 | loss: 5.13978 | failed:  320
batch:     127860 | loss: 5.08265 | failed:  320
batch:     127870 | loss: 5.13304 | failed:  320
batch:     127880 | loss: 5.20322 | failed:  320
batch:     127890 | loss: 5.10861 | failed:  320
batch:     127900 | loss: 5.03396 | failed:  320
batch:     127910 | loss: 5.04401 | failed:  320
batch:     127920 | loss: 5.15206 | failed:  320
batch:     127930 | loss: 4.99515 | failed:  320
batch:     127940 | loss: 5.09786 | failed:  320
batch:     127950 | loss: 4.74616 | failed:  320
batch:     127960 | loss: 5.10700 | failed:  320
batch:     127970 | loss: 5.16729 | failed:  320
batch:     127980 | loss: 4.91138 | failed:  320
batch:     127990 | loss: 5.07782 | failed:  320
batch:     128000 | loss: 5.14223 | failed:  320
batch:     128010 | loss: 5.16151 | failed:  320
batch:     128020 | loss: 5.10327 | failed:  320
batch:     128030 | loss: 5.04313 | failed:  320
batch:     128040 | loss: 5.03882 | failed:  320
batch:     128050 | loss: 5.08539 | failed:  320
batch:     128060 | loss: 5.14158 | failed:  320
batch:     128070 | loss: 5.06393 | failed:  320
batch:     128080 | loss: 5.12601 | failed:  320
batch:     128090 | loss: 5.09904 | failed:  320
batch:     128100 | loss: 4.98904 | failed:  320
batch:     128110 | loss: 5.27244 | failed:  320
batch:     128120 | loss: 4.92113 | failed:  320
batch:     128130 | loss: 5.27827 | failed:  320
batch:     128140 | loss: 5.12403 | failed:  320
batch:     128150 | loss: 5.17131 | failed:  320
batch:     128160 | loss: 5.09976 | failed:  320
batch:     128170 | loss: 5.02110 | failed:  320
batch:     128180 | loss: 5.19873 | failed:  320
batch:     128190 | loss: 5.23109 | failed:  320
batch:     128200 | loss: 5.23213 | failed:  320
batch:     128210 | loss: 5.15301 | failed:  320
batch:     128220 | loss: 5.12141 | failed:  320
batch:     128230 | loss: 5.19088 | failed:  320
batch:     128240 | loss: 5.14199 | failed:  320
batch:     128250 | loss: 5.14329 | failed:  320
batch:     128260 | loss: 5.14728 | failed:  320
batch:     128270 | loss: 5.06946 | failed:  320
batch:     128280 | loss: 5.16534 | failed:  320
batch:     128290 | loss: 5.12854 | failed:  320
batch:     128300 | loss: 5.15427 | failed:  320
batch:     128310 | loss: 5.07469 | failed:  320
batch:     128320 | loss: 5.18690 | failed:  320
batch:     128330 | loss: 5.12291 | failed:  320
batch:     128340 | loss: 5.09415 | failed:  320
batch:     128350 | loss: 5.18627 | failed:  320
batch:     128360 | loss: 5.12050 | failed:  320
batch:     128370 | loss: 5.08542 | failed:  320
batch:     128380 | loss: 5.19581 | failed:  320
batch:     128390 | loss: 5.07319 | failed:  320
batch:     128400 | loss: 5.09305 | failed:  320
batch:     128410 | loss: 5.01469 | failed:  320
batch:     128420 | loss: 5.06086 | failed:  320
batch:     128430 | loss: 4.98579 | failed:  320
batch:     128440 | loss: 5.11659 | failed:  320
batch:     128450 | loss: 5.02769 | failed:  320
batch:     128460 | loss: 5.18875 | failed:  320
batch:     128470 | loss: 5.10839 | failed:  320
batch:     128480 | loss: 5.10076 | failed:  320
batch:     128490 | loss: 5.04288 | failed:  320
batch:     128500 | loss: 5.25977 | failed:  320
batch:     128510 | loss: 5.26706 | failed:  320
batch:     128520 | loss: 5.23816 | failed:  320
batch:     128530 | loss: 5.19201 | failed:  320
batch:     128540 | loss: 5.22842 | failed:  320
batch:     128550 | loss: 5.17673 | failed:  320
batch:     128560 | loss: 5.21431 | failed:  320
batch:     128570 | loss: 5.13770 | failed:  320
batch:     128580 | loss: 5.07962 | failed:  320
batch:     128590 | loss: 5.16473 | failed:  320
batch:     128600 | loss: 5.12506 | failed:  320
batch:     128610 | loss: 5.03295 | failed:  320
batch:     128620 | loss: 5.06961 | failed:  320
batch:     128630 | loss: 5.04808 | failed:  320
batch:     128640 | loss: 5.10666 | failed:  320
batch:     128650 | loss: 5.11554 | failed:  320
batch:     128660 | loss: 5.17119 | failed:  320
batch:     128670 | loss: 4.94372 | failed:  320
batch:     128680 | loss: 5.20587 | failed:  320
batch:     128690 | loss: 5.18348 | failed:  320
batch:     128700 | loss: 5.08397 | failed:  320
batch:     128710 | loss: 5.06993 | failed:  320
batch:     128720 | loss: 5.01281 | failed:  320
batch:     128730 | loss: 5.02921 | failed:  320
batch:     128740 | loss: 5.24810 | failed:  320
batch:     128750 | loss: 5.12229 | failed:  320
batch:     128760 | loss: 5.13314 | failed:  320
batch:     128770 | loss: 5.06978 | failed:  320
batch:     128780 | loss: 5.12967 | failed:  320
batch:     128790 | loss: 5.14285 | failed:  320
batch:     128800 | loss: 5.14561 | failed:  320
batch:     128810 | loss: 5.15918 | failed:  320
batch:     128820 | loss: 5.03626 | failed:  320
batch:     128830 | loss: 5.14623 | failed:  320
batch:     128840 | loss: 5.25830 | failed:  320
batch:     128850 | loss: 5.14824 | failed:  320
batch:     128860 | loss: 5.06680 | failed:  320
batch:     128870 | loss: 5.07011 | failed:  320
batch:     128880 | loss: 5.05714 | failed:  320
batch:     128890 | loss: 5.17252 | failed:  320
batch:     128900 | loss: 5.16668 | failed:  320
batch:     128910 | loss: 5.07706 | failed:  320
batch:     128920 | loss: 5.07124 | failed:  320
batch:     128930 | loss: 5.15605 | failed:  320
batch:     128940 | loss: 5.10654 | failed:  320
batch:     128950 | loss: 5.15720 | failed:  320
batch:     128960 | loss: 5.13170 | failed:  320
batch:     128970 | loss: 5.06840 | failed:  320
batch:     128980 | loss: 5.03339 | failed:  320
batch:     128990 | loss: 5.10452 | failed:  320
batch:     129000 | loss: 5.24426 | failed:  320
batch:     129010 | loss: 5.15058 | failed:  320
batch:     129020 | loss: 5.10332 | failed:  320
batch:     129030 | loss: 5.17665 | failed:  320
batch:     129040 | loss: 5.08805 | failed:  320
batch:     129050 | loss: 5.15314 | failed:  320
batch:     129060 | loss: 5.14388 | failed:  320
batch:     129070 | loss: 5.04801 | failed:  320
batch:     129080 | loss: 5.12070 | failed:  320
batch:     129090 | loss: 5.08480 | failed:  320
batch:     129100 | loss: 5.02768 | failed:  320
batch:     129110 | loss: 5.04467 | failed:  320
batch:     129120 | loss: 4.85630 | failed:  320
batch:     129130 | loss: 5.09067 | failed:  320
batch:     129140 | loss: 4.98346 | failed:  320
batch:     129150 | loss: 4.95951 | failed:  320
batch:     129160 | loss: 5.22674 | failed:  320
batch:     129170 | loss: 5.18645 | failed:  320
batch:     129180 | loss: 5.10311 | failed:  320
batch:     129190 | loss: 5.12419 | failed:  320
batch:     129200 | loss: 5.11136 | failed:  320
batch:     129210 | loss: 5.10598 | failed:  320
batch:     129220 | loss: 5.11459 | failed:  320
batch:     129230 | loss: 5.17361 | failed:  320
batch:     129240 | loss: 5.07715 | failed:  320
batch:     129250 | loss: 5.13247 | failed:  320
batch:     129260 | loss: 5.22108 | failed:  320
batch:     129270 | loss: 5.17055 | failed:  320
batch:     129280 | loss: 5.01759 | failed:  320
batch:     129290 | loss: 5.17491 | failed:  320
batch:     129300 | loss: 5.12359 | failed:  320
batch:     129310 | loss: 4.82615 | failed:  320
batch:     129320 | loss: 5.16191 | failed:  320
batch:     129330 | loss: 5.09926 | failed:  320
batch:     129340 | loss: 5.10574 | failed:  320
batch:     129350 | loss: 5.11968 | failed:  320
batch:     129360 | loss: 5.06762 | failed:  320
batch:     129370 | loss: 5.12149 | failed:  320
batch:     129380 | loss: 5.07765 | failed:  320
batch:     129390 | loss: 4.96243 | failed:  320
batch:     129400 | loss: 4.91285 | failed:  320
batch:     129410 | loss: 4.88420 | failed:  320
batch:     129420 | loss: 5.00750 | failed:  320
batch:     129430 | loss: 4.82951 | failed:  320
batch:     129440 | loss: 5.13048 | failed:  320
batch:     129450 | loss: 5.20869 | failed:  320
batch:     129460 | loss: 5.15684 | failed:  320
batch:     129470 | loss: 5.05071 | failed:  320
batch:     129480 | loss: 5.11525 | failed:  320
batch:     129490 | loss: 5.01502 | failed:  320
batch:     129500 | loss: 5.22388 | failed:  320
batch:     129510 | loss: 5.32271 | failed:  320
batch:     129520 | loss: 5.19624 | failed:  320
batch:     129530 | loss: 5.11725 | failed:  320
batch:     129540 | loss: 5.25768 | failed:  320
batch:     129550 | loss: 5.16582 | failed:  320
batch:     129560 | loss: 5.14413 | failed:  320
batch:     129570 | loss: 5.03540 | failed:  320
batch:     129580 | loss: 5.10769 | failed:  320
batch:     129590 | loss: 5.07956 | failed:  320
batch:     129600 | loss: 5.10800 | failed:  320
batch:     129610 | loss: 5.17593 | failed:  320
batch:     129620 | loss: 5.26668 | failed:  320
batch:     129630 | loss: 5.19572 | failed:  320
batch:     129640 | loss: 5.00905 | failed:  320
batch:     129650 | loss: 5.05956 | failed:  320
batch:     129660 | loss: 5.21929 | failed:  320
batch:     129670 | loss: 5.23536 | failed:  320
batch:     129680 | loss: 5.13124 | failed:  320
batch:     129690 | loss: 5.15359 | failed:  320
batch:     129700 | loss: 5.11893 | failed:  320
batch:     129710 | loss: 4.98510 | failed:  320
batch:     129720 | loss: 4.36114 | failed:  320
batch:     129730 | loss: 3.14187 | failed:  320
batch:     129740 | loss: 5.45065 | failed:  320
batch:     129750 | loss: 5.27877 | failed:  320
batch:     129760 | loss: 5.05748 | failed:  320
batch:     129770 | loss: 5.14000 | failed:  320
batch:     129780 | loss: 4.85929 | failed:  320
batch:     129790 | loss: 4.72438 | failed:  320
batch:     129800 | loss: 4.55771 | failed:  320
batch:     129810 | loss: 4.59455 | failed:  320
batch:     129820 | loss: 4.47804 | failed:  320
batch:     129830 | loss: 4.46396 | failed:  320
batch:     129840 | loss: 4.61986 | failed:  320
batch:     129850 | loss: 4.60817 | failed:  320
batch:     129860 | loss: 4.36669 | failed:  320
batch:     129870 | loss: 4.49363 | failed:  320
batch:     129880 | loss: 4.50702 | failed:  320
batch:     129890 | loss: 4.26216 | failed:  320
batch:     129900 | loss: 4.34712 | failed:  320
batch:     129910 | loss: 4.35502 | failed:  320
batch:     129920 | loss: 4.25631 | failed:  320
batch:     129930 | loss: 4.20591 | failed:  320
batch:     129940 | loss: 4.18183 | failed:  320
batch:     129950 | loss: 4.08860 | failed:  320
batch:     129960 | loss: 4.14948 | failed:  320
batch:     129970 | loss: 4.35250 | failed:  320
batch:     129980 | loss: 4.22920 | failed:  320
batch:     129990 | loss: 4.32929 | failed:  320
batch:     130000 | loss: 4.30587 | failed:  320
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     130010 | loss: 4.03425 | failed:  320
batch:     130020 | loss: 4.04672 | failed:  320
batch:     130030 | loss: 4.11306 | failed:  320
batch:     130040 | loss: 4.24791 | failed:  320
batch:     130050 | loss: 3.96140 | failed:  320
batch:     130060 | loss: 4.03130 | failed:  320
batch:     130070 | loss: 5.29544 | failed:  320
batch:     130080 | loss: 5.23599 | failed:  320
batch:     130090 | loss: 5.26598 | failed:  320
batch:     130100 | loss: 5.12619 | failed:  320
batch:     130110 | loss: 5.18310 | failed:  320
batch:     130120 | loss: 5.18017 | failed:  320
batch:     130130 | loss: 5.16076 | failed:  320
batch:     130140 | loss: 5.09445 | failed:  320
batch:     130150 | loss: 5.10871 | failed:  320
batch:     130160 | loss: 5.13272 | failed:  320
batch:     130170 | loss: 5.10099 | failed:  320
batch:     130180 | loss: 5.18223 | failed:  320
batch:     130190 | loss: 5.15444 | failed:  320
batch:     130200 | loss: 5.20811 | failed:  320
batch:     130210 | loss: 5.13266 | failed:  320
batch:     130220 | loss: 5.15768 | failed:  320
batch:     130230 | loss: 5.07622 | failed:  320
batch:     130240 | loss: 5.14883 | failed:  320
batch:     130250 | loss: 5.13428 | failed:  320
batch:     130260 | loss: 5.18014 | failed:  320
batch:     130270 | loss: 5.07317 | failed:  320
batch:     130280 | loss: 5.21199 | failed:  320
batch:     130290 | loss: 4.98652 | failed:  320
batch:     130300 | loss: 5.12230 | failed:  320
batch:     130310 | loss: 5.07721 | failed:  320
batch:     130320 | loss: 5.11130 | failed:  320
batch:     130330 | loss: 5.07305 | failed:  320
batch:     130340 | loss: 4.99353 | failed:  320
batch:     130350 | loss: 5.13982 | failed:  320
batch:     130360 | loss: 5.02910 | failed:  320
batch:     130370 | loss: 4.97899 | failed:  320
batch:     130380 | loss: 5.08067 | failed:  320
batch:     130390 | loss: 5.19745 | failed:  320
batch:     130400 | loss: 5.15883 | failed:  320
batch:     130410 | loss: 5.08091 | failed:  320
batch:     130420 | loss: 4.97392 | failed:  320
batch:     130430 | loss: 5.03481 | failed:  320
batch:     130440 | loss: 4.90705 | failed:  320
batch:     130450 | loss: 4.95360 | failed:  320
batch:     130460 | loss: 5.10582 | failed:  320
batch:     130470 | loss: 5.18751 | failed:  320
batch:     130480 | loss: 5.13817 | failed:  320
batch:     130490 | loss: 5.07885 | failed:  320
batch:     130500 | loss: 5.16545 | failed:  320
batch:     130510 | loss: 5.20852 | failed:  320
batch:     130520 | loss: 5.14390 | failed:  320
batch:     130530 | loss: 5.16867 | failed:  320
batch:     130540 | loss: 5.13066 | failed:  320
batch:     130550 | loss: 5.13917 | failed:  320
batch:     130560 | loss: 5.10270 | failed:  320
batch:     130570 | loss: 4.83153 | failed:  320
batch:     130580 | loss: 5.07051 | failed:  320
batch:     130590 | loss: 4.98256 | failed:  320
batch:     130600 | loss: 4.94818 | failed:  320
batch:     130610 | loss: 5.16836 | failed:  320
batch:     130620 | loss: 5.00467 | failed:  320
batch:     130630 | loss: 5.01559 | failed:  320
batch:     130640 | loss: 5.00211 | failed:  320
batch:     130650 | loss: 5.03490 | failed:  320
batch:     130660 | loss: 5.17547 | failed:  320
batch:     130670 | loss: 5.07226 | failed:  320
batch:     130680 | loss: 4.98508 | failed:  320
batch:     130690 | loss: 5.32622 | failed:  320
batch:     130700 | loss: 5.17904 | failed:  320
batch:     130710 | loss: 5.00573 | failed:  320
batch:     130720 | loss: 5.00574 | failed:  320
batch:     130730 | loss: 5.19153 | failed:  320
batch:     130740 | loss: 4.89976 | failed:  320
batch:     130750 | loss: 4.88524 | failed:  320
batch:     130760 | loss: 5.09973 | failed:  320
batch:     130770 | loss: 5.06443 | failed:  320
batch:     130780 | loss: 5.10413 | failed:  320
batch:     130790 | loss: 5.17188 | failed:  320
batch:     130800 | loss: 5.16599 | failed:  320
batch:     130810 | loss: 5.12738 | failed:  320
batch:     130820 | loss: 5.12433 | failed:  320
batch:     130830 | loss: 5.10212 | failed:  320
batch:     130840 | loss: 4.96739 | failed:  320
batch:     130850 | loss: 4.94579 | failed:  320
batch:     130860 | loss: 4.96444 | failed:  320
batch:     130870 | loss: 4.91866 | failed:  320
batch:     130880 | loss: 5.14451 | failed:  320
batch:     130890 | loss: 5.04405 | failed:  320
batch:     130900 | loss: 4.95665 | failed:  320
batch:     130910 | loss: 5.07884 | failed:  320
batch:     130920 | loss: 5.19671 | failed:  320
batch:     130930 | loss: 5.12878 | failed:  320
batch:     130940 | loss: 5.19614 | failed:  320
batch:     130950 | loss: 5.09692 | failed:  320
batch:     130960 | loss: 5.24051 | failed:  320
batch:     130970 | loss: 5.02275 | failed:  320
batch:     130980 | loss: 5.18785 | failed:  320
batch:     130990 | loss: 5.17297 | failed:  320
batch:     131000 | loss: 5.13346 | failed:  320
batch:     131010 | loss: 5.20352 | failed:  320
batch:     131020 | loss: 5.13573 | failed:  320
batch:     131030 | loss: 5.13167 | failed:  320
batch:     131040 | loss: 5.07795 | failed:  320
batch:     131050 | loss: 4.93704 | failed:  320
batch:     131060 | loss: 4.93095 | failed:  320
batch:     131070 | loss: 5.20512 | failed:  320
batch:     131080 | loss: 5.08351 | failed:  320
batch:     131090 | loss: 5.00806 | failed:  320
batch:     131100 | loss: 5.22415 | failed:  320
batch:     131110 | loss: 5.17167 | failed:  320
batch:     131120 | loss: 5.20659 | failed:  320
batch:     131130 | loss: 5.03276 | failed:  320
batch:     131140 | loss: 5.21686 | failed:  320
batch:     131150 | loss: 5.18728 | failed:  320
batch:     131160 | loss: 5.20186 | failed:  320
batch:     131170 | loss: 5.10044 | failed:  320
batch:     131180 | loss: 5.17663 | failed:  320
batch:     131190 | loss: 5.04812 | failed:  320
batch:     131200 | loss: 5.13186 | failed:  320
batch:     131210 | loss: 5.07653 | failed:  320
batch:     131220 | loss: 5.01601 | failed:  320
batch:     131230 | loss: 5.10818 | failed:  320
batch:     131240 | loss: 5.23299 | failed:  320
batch:     131250 | loss: 5.07778 | failed:  320
batch:     131260 | loss: 5.10487 | failed:  320
batch:     131270 | loss: 5.06258 | failed:  320
batch:     131280 | loss: 5.19693 | failed:  320
batch:     131290 | loss: 5.12505 | failed:  320
batch:     131300 | loss: 5.20830 | failed:  320
batch:     131310 | loss: 5.29768 | failed:  320
batch:     131320 | loss: 4.97507 | failed:  320
batch:     131330 | loss: 5.17099 | failed:  320
batch:     131340 | loss: 5.16916 | failed:  320
batch:     131350 | loss: 5.09785 | failed:  320
batch:     131360 | loss: 5.14715 | failed:  320
batch:     131370 | loss: 5.12505 | failed:  320
batch:     131380 | loss: 5.05127 | failed:  320
batch:     131390 | loss: 5.14213 | failed:  320
batch:     131400 | loss: 5.23536 | failed:  320
batch:     131410 | loss: 4.87339 | failed:  320
batch:     131420 | loss: 4.90744 | failed:  320
batch:     131430 | loss: 5.22132 | failed:  320
batch:     131440 | loss: 5.17564 | failed:  320
batch:     131450 | loss: 5.27528 | failed:  320
batch:     131460 | loss: 5.13838 | failed:  320
batch:     131470 | loss: 4.80075 | failed:  320
batch:     131480 | loss: 5.04500 | failed:  320
batch:     131490 | loss: 5.09017 | failed:  320
batch:     131500 | loss: 5.21921 | failed:  320
batch:     131510 | loss: 5.13211 | failed:  320
batch:     131520 | loss: 5.17068 | failed:  320
batch:     131530 | loss: 5.15537 | failed:  320
batch:     131540 | loss: 5.42832 | failed:  320
batch:     131550 | loss: 5.21328 | failed:  320
batch:     131560 | loss: 5.14661 | failed:  320
batch:     131570 | loss: 5.15171 | failed:  320
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     131580 | loss: 5.10809 | failed:  322
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     131590 | loss: 4.97592 | failed:  326
batch:     131600 | loss: 5.09684 | failed:  326
batch:     131610 | loss: 5.08403 | failed:  326
batch:     131620 | loss: 5.19940 | failed:  326
batch:     131630 | loss: 5.01041 | failed:  326
batch:     131640 | loss: 5.11411 | failed:  326
batch:     131650 | loss: 5.02863 | failed:  326
batch:     131660 | loss: 5.19005 | failed:  326
batch:     131670 | loss: 5.05147 | failed:  326
batch:     131680 | loss: 5.10299 | failed:  326
batch:     131690 | loss: 5.11819 | failed:  326
batch:     131700 | loss: 4.96048 | failed:  326
batch:     131710 | loss: 5.10534 | failed:  326
batch:     131720 | loss: 4.97592 | failed:  326
batch:     131730 | loss: 5.13736 | failed:  326
batch:     131740 | loss: 5.22114 | failed:  326
batch:     131750 | loss: 5.11028 | failed:  326
batch:     131760 | loss: 5.07059 | failed:  326
batch:     131770 | loss: 5.07805 | failed:  326
batch:     131780 | loss: 5.04988 | failed:  326
batch:     131790 | loss: 5.12071 | failed:  326
batch:     131800 | loss: 5.05254 | failed:  326
batch:     131810 | loss: 5.06674 | failed:  326
batch:     131820 | loss: 5.12599 | failed:  326
batch:     131830 | loss: 4.93358 | failed:  326
batch:     131840 | loss: 5.24536 | failed:  326
batch:     131850 | loss: 5.09339 | failed:  326
batch:     131860 | loss: 5.22379 | failed:  326
batch:     131870 | loss: 5.05128 | failed:  326
batch:     131880 | loss: 5.18835 | failed:  326
batch:     131890 | loss: 5.11953 | failed:  326
batch:     131900 | loss: 5.05286 | failed:  326
batch:     131910 | loss: 5.09110 | failed:  326
batch:     131920 | loss: 5.07115 | failed:  326
batch:     131930 | loss: 5.00534 | failed:  326
batch:     131940 | loss: 5.04392 | failed:  326
batch:     131950 | loss: 5.01256 | failed:  326
batch:     131960 | loss: 5.11005 | failed:  326
batch:     131970 | loss: 5.15857 | failed:  326
batch:     131980 | loss: 5.10878 | failed:  326
batch:     131990 | loss: 5.11397 | failed:  326
batch:     132000 | loss: 4.97610 | failed:  326
batch:     132010 | loss: 5.22878 | failed:  326
batch:     132020 | loss: 5.14823 | failed:  326
batch:     132030 | loss: 5.13973 | failed:  326
batch:     132040 | loss: 5.14943 | failed:  326
batch:     132050 | loss: 5.14643 | failed:  326
batch:     132060 | loss: 5.08015 | failed:  326
batch:     132070 | loss: 4.92297 | failed:  326
batch:     132080 | loss: 5.14188 | failed:  326
batch:     132090 | loss: 4.96847 | failed:  326
batch:     132100 | loss: 4.66818 | failed:  326
batch:     132110 | loss: 5.09708 | failed:  326
batch:     132120 | loss: 5.14042 | failed:  326
batch:     132130 | loss: 5.09107 | failed:  326
batch:     132140 | loss: 4.72825 | failed:  326
batch:     132150 | loss: 4.94229 | failed:  326
batch:     132160 | loss: 5.14947 | failed:  326
batch:     132170 | loss: 5.09366 | failed:  326
batch:     132180 | loss: 5.12825 | failed:  326
batch:     132190 | loss: 5.21419 | failed:  326
batch:     132200 | loss: 5.25726 | failed:  326
batch:     132210 | loss: 5.09900 | failed:  326
batch:     132220 | loss: 5.06340 | failed:  326
batch:     132230 | loss: 5.14355 | failed:  326
batch:     132240 | loss: 5.10209 | failed:  326
batch:     132250 | loss: 5.05341 | failed:  326
batch:     132260 | loss: 5.07142 | failed:  326
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     132270 | loss: 5.14630 | failed:  330
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     132280 | loss: 5.09940 | failed:  331
batch:     132290 | loss: 5.21659 | failed:  331
batch:     132300 | loss: 5.27560 | failed:  331
batch:     132310 | loss: 5.11267 | failed:  331
batch:     132320 | loss: 5.18046 | failed:  331
batch:     132330 | loss: 5.10585 | failed:  331
batch:     132340 | loss: 5.11208 | failed:  331
batch:     132350 | loss: 4.94032 | failed:  331
batch:     132360 | loss: 4.96738 | failed:  331
batch:     132370 | loss: 5.13410 | failed:  331
batch:     132380 | loss: 5.03991 | failed:  331
batch:     132390 | loss: 5.09333 | failed:  331
batch:     132400 | loss: 5.02834 | failed:  331
batch:     132410 | loss: 5.08310 | failed:  331
batch:     132420 | loss: 5.11564 | failed:  331
batch:     132430 | loss: 4.68136 | failed:  331
batch:     132440 | loss: 5.13898 | failed:  331
batch:     132450 | loss: 5.06570 | failed:  331
batch:     132460 | loss: 5.06480 | failed:  331
batch:     132470 | loss: 5.16038 | failed:  331
batch:     132480 | loss: 4.98267 | failed:  331
batch:     132490 | loss: 5.20115 | failed:  331
batch:     132500 | loss: 5.18963 | failed:  331
batch:     132510 | loss: 5.05716 | failed:  331
batch:     132520 | loss: 4.91206 | failed:  331
batch:     132530 | loss: 5.00120 | failed:  331
batch:     132540 | loss: 5.18642 | failed:  331
batch:     132550 | loss: 5.11979 | failed:  331
batch:     132560 | loss: 5.04151 | failed:  331
batch:     132570 | loss: 4.97409 | failed:  331
batch:     132580 | loss: 5.20791 | failed:  331
batch:     132590 | loss: 5.18813 | failed:  331
batch:     132600 | loss: 5.07542 | failed:  331
batch:     132610 | loss: 4.72938 | failed:  331
batch:     132620 | loss: 5.13497 | failed:  331
batch:     132630 | loss: 5.20669 | failed:  331
batch:     132640 | loss: 5.17259 | failed:  331
batch:     132650 | loss: 5.11122 | failed:  331
batch:     132660 | loss: 5.23074 | failed:  331
batch:     132670 | loss: 5.19123 | failed:  331
batch:     132680 | loss: 5.24154 | failed:  331
batch:     132690 | loss: 5.16604 | failed:  331
batch:     132700 | loss: 5.18634 | failed:  331
batch:     132710 | loss: 4.99422 | failed:  331
batch:     132720 | loss: 4.94078 | failed:  331
batch:     132730 | loss: 5.05366 | failed:  331
batch:     132740 | loss: 5.21130 | failed:  331
batch:     132750 | loss: 5.13679 | failed:  331
batch:     132760 | loss: 5.22605 | failed:  331
batch:     132770 | loss: 5.08966 | failed:  331
batch:     132780 | loss: 5.07908 | failed:  331
batch:     132790 | loss: 5.33217 | failed:  331
batch:     132800 | loss: 5.13234 | failed:  331
batch:     132810 | loss: 5.04048 | failed:  331
batch:     132820 | loss: 4.85068 | failed:  331
batch:     132830 | loss: 5.13348 | failed:  331
batch:     132840 | loss: 5.19649 | failed:  331
batch:     132850 | loss: 5.06120 | failed:  331
batch:     132860 | loss: 5.03299 | failed:  331
batch:     132870 | loss: 5.17932 | failed:  331
batch:     132880 | loss: 5.21626 | failed:  331
batch:     132890 | loss: 5.15918 | failed:  331
batch:     132900 | loss: 5.14087 | failed:  331
batch:     132910 | loss: 5.17849 | failed:  331
batch:     132920 | loss: 5.10092 | failed:  331
batch:     132930 | loss: 5.22267 | failed:  331
batch:     132940 | loss: 5.20092 | failed:  331
batch:     132950 | loss: 5.24499 | failed:  331
batch:     132960 | loss: 4.86341 | failed:  331
batch:     132970 | loss: 5.08569 | failed:  331
batch:     132980 | loss: 5.15330 | failed:  331
batch:     132990 | loss: 5.01781 | failed:  331
batch:     133000 | loss: 5.09993 | failed:  331
batch:     133010 | loss: 5.21833 | failed:  331
batch:     133020 | loss: 5.23154 | failed:  331
batch:     133030 | loss: 5.17966 | failed:  331
batch:     133040 | loss: 5.09065 | failed:  331
batch:     133050 | loss: 5.08863 | failed:  331
batch:     133060 | loss: 5.07319 | failed:  331
batch:     133070 | loss: 5.12086 | failed:  331
batch:     133080 | loss: 5.01852 | failed:  331
batch:     133090 | loss: 5.24625 | failed:  331
batch:     133100 | loss: 4.96537 | failed:  331
batch:     133110 | loss: 5.08655 | failed:  331
batch:     133120 | loss: 5.01539 | failed:  331
batch:     133130 | loss: 5.02832 | failed:  331
batch:     133140 | loss: 5.12689 | failed:  331
batch:     133150 | loss: 5.08857 | failed:  331
batch:     133160 | loss: 5.10533 | failed:  331
batch:     133170 | loss: 4.90552 | failed:  331
batch:     133180 | loss: 5.09084 | failed:  331
batch:     133190 | loss: 5.02299 | failed:  331
batch:     133200 | loss: 5.19470 | failed:  331
batch:     133210 | loss: 5.16088 | failed:  331
batch:     133220 | loss: 5.04453 | failed:  331
batch:     133230 | loss: 5.14910 | failed:  331
batch:     133240 | loss: 5.14311 | failed:  331
batch:     133250 | loss: 5.21322 | failed:  331
batch:     133260 | loss: 5.00665 | failed:  331
batch:     133270 | loss: 5.06873 | failed:  331
batch:     133280 | loss: 4.92981 | failed:  331
batch:     133290 | loss: 5.18212 | failed:  331
batch:     133300 | loss: 5.16802 | failed:  331
batch:     133310 | loss: 5.23106 | failed:  331
batch:     133320 | loss: 5.10557 | failed:  331
batch:     133330 | loss: 5.12947 | failed:  331
batch:     133340 | loss: 5.05397 | failed:  331
batch:     133350 | loss: 5.33030 | failed:  331
batch:     133360 | loss: 5.12628 | failed:  331
batch:     133370 | loss: 5.14136 | failed:  331
batch:     133380 | loss: 5.09076 | failed:  331
batch:     133390 | loss: 5.15697 | failed:  331
batch:     133400 | loss: 5.24947 | failed:  331
batch:     133410 | loss: 5.37338 | failed:  331
batch:     133420 | loss: 5.01694 | failed:  331
batch:     133430 | loss: 5.24918 | failed:  331
batch:     133440 | loss: 5.00670 | failed:  331
batch:     133450 | loss: 5.22174 | failed:  331
batch:     133460 | loss: 5.23864 | failed:  331
batch:     133470 | loss: 5.13018 | failed:  331
batch:     133480 | loss: 5.02190 | failed:  331
batch:     133490 | loss: 5.02573 | failed:  331
batch:     133500 | loss: 5.16987 | failed:  331
batch:     133510 | loss: 5.17018 | failed:  331
batch:     133520 | loss: 5.15870 | failed:  331
batch:     133530 | loss: 5.34427 | failed:  331
batch:     133540 | loss: 4.94918 | failed:  331
batch:     133550 | loss: 5.10789 | failed:  331
batch:     133560 | loss: 5.14319 | failed:  331
batch:     133570 | loss: 5.26240 | failed:  331
batch:     133580 | loss: 5.22412 | failed:  331
batch:     133590 | loss: 5.26691 | failed:  331
batch:     133600 | loss: 5.13436 | failed:  331
batch:     133610 | loss: 5.04726 | failed:  331
batch:     133620 | loss: 5.22142 | failed:  331
batch:     133630 | loss: 5.18536 | failed:  331
batch:     133640 | loss: 5.22706 | failed:  331
batch:     133650 | loss: 5.18971 | failed:  331
batch:     133660 | loss: 5.05349 | failed:  331
batch:     133670 | loss: 5.22501 | failed:  331
batch:     133680 | loss: 5.17764 | failed:  331
batch:     133690 | loss: 5.11146 | failed:  331
batch:     133700 | loss: 5.26941 | failed:  331
batch:     133710 | loss: 4.99538 | failed:  331
batch:     133720 | loss: 5.13745 | failed:  331
batch:     133730 | loss: 5.18510 | failed:  331
batch:     133740 | loss: 5.08284 | failed:  331
batch:     133750 | loss: 4.96903 | failed:  331
batch:     133760 | loss: 5.10511 | failed:  331
batch:     133770 | loss: 5.03877 | failed:  331
batch:     133780 | loss: 5.11856 | failed:  331
batch:     133790 | loss: 5.21568 | failed:  331
batch:     133800 | loss: 5.12279 | failed:  331
batch:     133810 | loss: 5.17777 | failed:  331
batch:     133820 | loss: 5.19406 | failed:  331
batch:     133830 | loss: 5.01603 | failed:  331
batch:     133840 | loss: 5.08035 | failed:  331
batch:     133850 | loss: 5.02858 | failed:  331
batch:     133860 | loss: 5.03541 | failed:  331
batch:     133870 | loss: 5.16211 | failed:  331
batch:     133880 | loss: 5.22573 | failed:  331
batch:     133890 | loss: 5.09281 | failed:  331
batch:     133900 | loss: 5.08561 | failed:  331
batch:     133910 | loss: 5.29883 | failed:  331
batch:     133920 | loss: 5.18954 | failed:  331
batch:     133930 | loss: 5.04018 | failed:  331
batch:     133940 | loss: 5.22864 | failed:  331
batch:     133950 | loss: 5.13568 | failed:  331
batch:     133960 | loss: 5.20165 | failed:  331
batch:     133970 | loss: 5.13382 | failed:  331
batch:     133980 | loss: 5.18871 | failed:  331
batch:     133990 | loss: 5.13658 | failed:  331
batch:     134000 | loss: 5.19554 | failed:  331
batch:     134010 | loss: 5.20254 | failed:  331
batch:     134020 | loss: 5.01705 | failed:  331
batch:     134030 | loss: 5.20862 | failed:  331
batch:     134040 | loss: 5.16223 | failed:  331
batch:     134050 | loss: 5.06396 | failed:  331
batch:     134060 | loss: 4.98096 | failed:  331
batch:     134070 | loss: 5.16650 | failed:  331
batch:     134080 | loss: 5.16704 | failed:  331
batch:     134090 | loss: 5.15002 | failed:  331
batch:     134100 | loss: 5.12909 | failed:  331
batch:     134110 | loss: 5.23207 | failed:  331
batch:     134120 | loss: 5.20191 | failed:  331
batch:     134130 | loss: 5.03937 | failed:  331
batch:     134140 | loss: 5.09060 | failed:  331
batch:     134150 | loss: 5.05340 | failed:  331
batch:     134160 | loss: 5.19008 | failed:  331
batch:     134170 | loss: 5.04864 | failed:  331
batch:     134180 | loss: 5.19484 | failed:  331
batch:     134190 | loss: 5.05998 | failed:  331
batch:     134200 | loss: 4.81430 | failed:  331
batch:     134210 | loss: 5.13104 | failed:  331
batch:     134220 | loss: 5.07145 | failed:  331
batch:     134230 | loss: 5.05267 | failed:  331
batch:     134240 | loss: 5.13721 | failed:  331
batch:     134250 | loss: 5.18875 | failed:  331
batch:     134260 | loss: 5.09551 | failed:  331
batch:     134270 | loss: 5.07682 | failed:  331
batch:     134280 | loss: 5.07398 | failed:  331
batch:     134290 | loss: 5.20238 | failed:  331
batch:     134300 | loss: 5.21997 | failed:  331
batch:     134310 | loss: 5.32264 | failed:  331
batch:     134320 | loss: 4.88209 | failed:  331
batch:     134330 | loss: 5.17567 | failed:  331
batch:     134340 | loss: 5.13703 | failed:  331
batch:     134350 | loss: 5.00660 | failed:  331
batch:     134360 | loss: 5.03977 | failed:  331
batch:     134370 | loss: 5.09033 | failed:  331
batch:     134380 | loss: 5.10457 | failed:  331
batch:     134390 | loss: 4.76914 | failed:  331
batch:     134400 | loss: 5.04058 | failed:  331
batch:     134410 | loss: 5.14106 | failed:  331
batch:     134420 | loss: 5.00715 | failed:  331
batch:     134430 | loss: 4.96401 | failed:  331
batch:     134440 | loss: 4.73657 | failed:  331
batch:     134450 | loss: 4.95306 | failed:  331
batch:     134460 | loss: 5.16772 | failed:  331
batch:     134470 | loss: 4.95904 | failed:  331
batch:     134480 | loss: 5.24640 | failed:  331
batch:     134490 | loss: 5.08854 | failed:  331
batch:     134500 | loss: 5.11299 | failed:  331
batch:     134510 | loss: 5.28133 | failed:  331
batch:     134520 | loss: 4.99110 | failed:  331
batch:     134530 | loss: 5.10391 | failed:  331
batch:     134540 | loss: 5.15518 | failed:  331
batch:     134550 | loss: 5.07608 | failed:  331
batch:     134560 | loss: 5.05332 | failed:  331
batch:     134570 | loss: 5.00376 | failed:  331
batch:     134580 | loss: 5.20483 | failed:  331
batch:     134590 | loss: 5.18925 | failed:  331
batch:     134600 | loss: 5.10786 | failed:  331
batch:     134610 | loss: 4.81700 | failed:  331
batch:     134620 | loss: 5.10060 | failed:  331
batch:     134630 | loss: 5.04374 | failed:  331
batch:     134640 | loss: 5.29506 | failed:  331
batch:     134650 | loss: 5.26073 | failed:  331
batch:     134660 | loss: 5.21864 | failed:  331
batch:     134670 | loss: 5.23826 | failed:  331
batch:     134680 | loss: 5.20814 | failed:  331
batch:     134690 | loss: 5.00991 | failed:  331
batch:     134700 | loss: 5.18064 | failed:  331
batch:     134710 | loss: 5.21624 | failed:  331
batch:     134720 | loss: 4.74886 | failed:  331
batch:     134730 | loss: 5.09916 | failed:  331
batch:     134740 | loss: 5.08496 | failed:  331
batch:     134750 | loss: 5.21025 | failed:  331
batch:     134760 | loss: 5.11307 | failed:  331
batch:     134770 | loss: 5.15539 | failed:  331
batch:     134780 | loss: 5.23520 | failed:  331
batch:     134790 | loss: 5.22381 | failed:  331
batch:     134800 | loss: 5.23239 | failed:  331
batch:     134810 | loss: 5.18663 | failed:  331
batch:     134820 | loss: 5.08307 | failed:  331
batch:     134830 | loss: 5.15240 | failed:  331
batch:     134840 | loss: 5.18003 | failed:  331
batch:     134850 | loss: 5.11431 | failed:  331
batch:     134860 | loss: 5.08488 | failed:  331
batch:     134870 | loss: 5.14758 | failed:  331
batch:     134880 | loss: 5.23293 | failed:  331
batch:     134890 | loss: 5.14435 | failed:  331
batch:     134900 | loss: 5.15598 | failed:  331
batch:     134910 | loss: 5.12700 | failed:  331
batch:     134920 | loss: 5.17698 | failed:  331
batch:     134930 | loss: 5.10077 | failed:  331
batch:     134940 | loss: 5.19142 | failed:  331
batch:     134950 | loss: 5.19804 | failed:  331
batch:     134960 | loss: 5.07618 | failed:  331
batch:     134970 | loss: 4.62169 | failed:  331
batch:     134980 | loss: 5.22003 | failed:  331
batch:     134990 | loss: 5.02263 | failed:  331
batch:     135000 | loss: 5.20409 | failed:  331
batch:     135010 | loss: 5.25376 | failed:  331
batch:     135020 | loss: 5.20906 | failed:  331
batch:     135030 | loss: 5.07494 | failed:  331
batch:     135040 | loss: 5.10862 | failed:  331
batch:     135050 | loss: 5.17282 | failed:  331
batch:     135060 | loss: 5.20129 | failed:  331
batch:     135070 | loss: 5.13146 | failed:  331
batch:     135080 | loss: 5.01388 | failed:  331
batch:     135090 | loss: 5.27887 | failed:  331
batch:     135100 | loss: 5.15643 | failed:  331
batch:     135110 | loss: 4.93020 | failed:  331
batch:     135120 | loss: 5.10280 | failed:  331
batch:     135130 | loss: 5.03645 | failed:  331
batch:     135140 | loss: 5.23117 | failed:  331
batch:     135150 | loss: 5.13125 | failed:  331
batch:     135160 | loss: 4.98232 | failed:  331
batch:     135170 | loss: 5.02733 | failed:  331
batch:     135180 | loss: 5.13977 | failed:  331
batch:     135190 | loss: 5.20329 | failed:  331
batch:     135200 | loss: 5.16355 | failed:  331
batch:     135210 | loss: 5.13763 | failed:  331
batch:     135220 | loss: 4.93601 | failed:  331
batch:     135230 | loss: 5.06646 | failed:  331
batch:     135240 | loss: 5.24123 | failed:  331
batch:     135250 | loss: 5.19292 | failed:  331
batch:     135260 | loss: 5.23612 | failed:  331
batch:     135270 | loss: 5.06309 | failed:  331
batch:     135280 | loss: 5.16103 | failed:  331
batch:     135290 | loss: 5.17736 | failed:  331
batch:     135300 | loss: 5.11481 | failed:  331
batch:     135310 | loss: 5.08618 | failed:  331
batch:     135320 | loss: 5.22537 | failed:  331
batch:     135330 | loss: 5.05365 | failed:  331
batch:     135340 | loss: 5.19284 | failed:  331
batch:     135350 | loss: 5.20637 | failed:  331
batch:     135360 | loss: 5.14902 | failed:  331
batch:     135370 | loss: 5.29120 | failed:  331
batch:     135380 | loss: 5.19362 | failed:  331
batch:     135390 | loss: 5.21493 | failed:  331
batch:     135400 | loss: 5.03267 | failed:  331
batch:     135410 | loss: 5.28977 | failed:  331
batch:     135420 | loss: 5.25858 | failed:  331
batch:     135430 | loss: 5.27514 | failed:  331
batch:     135440 | loss: 5.18527 | failed:  331
batch:     135450 | loss: 5.20874 | failed:  331
batch:     135460 | loss: 5.17625 | failed:  331
batch:     135470 | loss: 5.21864 | failed:  331
batch:     135480 | loss: 5.19040 | failed:  331
batch:     135490 | loss: 5.29568 | failed:  331
batch:     135500 | loss: 5.18328 | failed:  331
batch:     135510 | loss: 5.17630 | failed:  331
batch:     135520 | loss: 5.14632 | failed:  331
batch:     135530 | loss: 5.07228 | failed:  331
batch:     135540 | loss: 4.97757 | failed:  331
batch:     135550 | loss: 5.05125 | failed:  331
batch:     135560 | loss: 5.04546 | failed:  331
batch:     135570 | loss: 5.17692 | failed:  331
batch:     135580 | loss: 5.08967 | failed:  331
batch:     135590 | loss: 5.07313 | failed:  331
batch:     135600 | loss: 5.20185 | failed:  331
batch:     135610 | loss: 5.11211 | failed:  331
batch:     135620 | loss: 5.18476 | failed:  331
batch:     135630 | loss: 5.16806 | failed:  331
batch:     135640 | loss: 5.10121 | failed:  331
batch:     135650 | loss: 5.25405 | failed:  331
batch:     135660 | loss: 5.09919 | failed:  331
batch:     135670 | loss: 5.22961 | failed:  331
batch:     135680 | loss: 5.19291 | failed:  331
batch:     135690 | loss: 5.18681 | failed:  331
batch:     135700 | loss: 5.16933 | failed:  331
batch:     135710 | loss: 5.20673 | failed:  331
batch:     135720 | loss: 5.17518 | failed:  331
batch:     135730 | loss: 5.20838 | failed:  331
batch:     135740 | loss: 5.12704 | failed:  331
batch:     135750 | loss: 5.12578 | failed:  331
batch:     135760 | loss: 5.10798 | failed:  331
batch:     135770 | loss: 5.06233 | failed:  331
batch:     135780 | loss: 5.11100 | failed:  331
batch:     135790 | loss: 5.20286 | failed:  331
batch:     135800 | loss: 5.17328 | failed:  331
batch:     135810 | loss: 5.18886 | failed:  331
batch:     135820 | loss: 5.22442 | failed:  331
batch:     135830 | loss: 5.14935 | failed:  331
batch:     135840 | loss: 5.12432 | failed:  331
batch:     135850 | loss: 5.45081 | failed:  331
batch:     135860 | loss: 4.93640 | failed:  331
batch:     135870 | loss: 5.14665 | failed:  331
batch:     135880 | loss: 5.04774 | failed:  331
batch:     135890 | loss: 4.99030 | failed:  331
batch:     135900 | loss: 5.03623 | failed:  331
batch:     135910 | loss: 5.06362 | failed:  331
batch:     135920 | loss: 5.09450 | failed:  331
batch:     135930 | loss: 5.04780 | failed:  331
batch:     135940 | loss: 5.11021 | failed:  331
batch:     135950 | loss: 4.95570 | failed:  331
batch:     135960 | loss: 5.20556 | failed:  331
batch:     135970 | loss: 5.11181 | failed:  331
batch:     135980 | loss: 5.12111 | failed:  331
batch:     135990 | loss: 5.07775 | failed:  331
batch:     136000 | loss: 5.11012 | failed:  331
batch:     136010 | loss: 5.17126 | failed:  331
batch:     136020 | loss: 5.27252 | failed:  331
batch:     136030 | loss: 5.17723 | failed:  331
batch:     136040 | loss: 5.16439 | failed:  331
batch:     136050 | loss: 5.14780 | failed:  331
batch:     136060 | loss: 4.83604 | failed:  331
batch:     136070 | loss: 5.17799 | failed:  331
batch:     136080 | loss: 5.14027 | failed:  331
batch:     136090 | loss: 5.11376 | failed:  331
batch:     136100 | loss: 5.19920 | failed:  331
batch:     136110 | loss: 4.97621 | failed:  331
batch:     136120 | loss: 5.25736 | failed:  331
batch:     136130 | loss: 5.19901 | failed:  331
batch:     136140 | loss: 5.14878 | failed:  331
batch:     136150 | loss: 5.15576 | failed:  331
batch:     136160 | loss: 5.22873 | failed:  331
batch:     136170 | loss: 5.25361 | failed:  331
batch:     136180 | loss: 5.24102 | failed:  331
batch:     136190 | loss: 5.25713 | failed:  331
batch:     136200 | loss: 5.24804 | failed:  331
batch:     136210 | loss: 5.22077 | failed:  331
batch:     136220 | loss: 4.88056 | failed:  331
batch:     136230 | loss: 4.98081 | failed:  331
batch:     136240 | loss: 5.19317 | failed:  331
batch:     136250 | loss: 5.09960 | failed:  331
batch:     136260 | loss: 4.81271 | failed:  331
batch:     136270 | loss: 5.00313 | failed:  331
batch:     136280 | loss: 5.06151 | failed:  331
batch:     136290 | loss: 5.21537 | failed:  331
batch:     136300 | loss: 5.17178 | failed:  331
batch:     136310 | loss: 5.21192 | failed:  331
batch:     136320 | loss: 5.16139 | failed:  331
batch:     136330 | loss: 5.22156 | failed:  331
batch:     136340 | loss: 5.18406 | failed:  331
batch:     136350 | loss: 5.16372 | failed:  331
batch:     136360 | loss: 4.91134 | failed:  331
batch:     136370 | loss: 5.16119 | failed:  331
batch:     136380 | loss: 5.27560 | failed:  331
batch:     136390 | loss: 5.06335 | failed:  331
batch:     136400 | loss: 5.15775 | failed:  331
batch:     136410 | loss: 5.11254 | failed:  331
batch:     136420 | loss: 5.14696 | failed:  331
batch:     136430 | loss: 4.87681 | failed:  331
batch:     136440 | loss: 5.13842 | failed:  331
batch:     136450 | loss: 5.14202 | failed:  331
batch:     136460 | loss: 5.25775 | failed:  331
batch:     136470 | loss: 5.17654 | failed:  331
batch:     136480 | loss: 5.26960 | failed:  331
batch:     136490 | loss: 5.19713 | failed:  331
batch:     136500 | loss: 4.95688 | failed:  331
batch:     136510 | loss: 5.01203 | failed:  331
batch:     136520 | loss: 4.99916 | failed:  331
batch:     136530 | loss: 4.95849 | failed:  331
batch:     136540 | loss: 4.96793 | failed:  331
batch:     136550 | loss: 5.13916 | failed:  331
batch:     136560 | loss: 5.11740 | failed:  331
batch:     136570 | loss: 5.07905 | failed:  331
batch:     136580 | loss: 5.19780 | failed:  331
batch:     136590 | loss: 5.20328 | failed:  331
batch:     136600 | loss: 4.86257 | failed:  331
batch:     136610 | loss: 5.04235 | failed:  331
batch:     136620 | loss: 5.03165 | failed:  331
batch:     136630 | loss: 5.21270 | failed:  331
batch:     136640 | loss: 4.89870 | failed:  331
batch:     136650 | loss: 5.21220 | failed:  331
batch:     136660 | loss: 5.13650 | failed:  331
batch:     136670 | loss: 5.20468 | failed:  331
batch:     136680 | loss: 5.15917 | failed:  331
batch:     136690 | loss: 5.19449 | failed:  331
batch:     136700 | loss: 5.18027 | failed:  331
batch:     136710 | loss: 5.17270 | failed:  331
batch:     136720 | loss: 5.22622 | failed:  331
batch:     136730 | loss: 5.13011 | failed:  331
batch:     136740 | loss: 5.21960 | failed:  331
batch:     136750 | loss: 5.14266 | failed:  331
batch:     136760 | loss: 5.07133 | failed:  331
batch:     136770 | loss: 5.10278 | failed:  331
batch:     136780 | loss: 5.16405 | failed:  331
batch:     136790 | loss: 5.11464 | failed:  331
batch:     136800 | loss: 5.14369 | failed:  331
batch:     136810 | loss: 4.96711 | failed:  331
batch:     136820 | loss: 5.23013 | failed:  331
batch:     136830 | loss: 5.09941 | failed:  331
batch:     136840 | loss: 5.16877 | failed:  331
batch:     136850 | loss: 5.09036 | failed:  331
batch:     136860 | loss: 5.39386 | failed:  331
batch:     136870 | loss: 5.07712 | failed:  331
batch:     136880 | loss: 5.10747 | failed:  331
batch:     136890 | loss: 5.05755 | failed:  331
batch:     136900 | loss: 5.03811 | failed:  331
batch:     136910 | loss: 5.10700 | failed:  331
batch:     136920 | loss: 5.09237 | failed:  331
batch:     136930 | loss: 5.10459 | failed:  331
batch:     136940 | loss: 5.14830 | failed:  331
batch:     136950 | loss: 5.16222 | failed:  331
batch:     136960 | loss: 5.23127 | failed:  331
batch:     136970 | loss: 5.26865 | failed:  331
batch:     136980 | loss: 5.11355 | failed:  331
batch:     136990 | loss: 5.08009 | failed:  331
batch:     137000 | loss: 5.10234 | failed:  331
batch:     137010 | loss: 5.17938 | failed:  331
batch:     137020 | loss: 4.99081 | failed:  331
batch:     137030 | loss: 5.17014 | failed:  331
batch:     137040 | loss: 5.13874 | failed:  331
batch:     137050 | loss: 5.20843 | failed:  331
batch:     137060 | loss: 5.14426 | failed:  331
batch:     137070 | loss: 5.13995 | failed:  331
batch:     137080 | loss: 5.18645 | failed:  331
batch:     137090 | loss: 5.16116 | failed:  331
batch:     137100 | loss: 5.22639 | failed:  331
batch:     137110 | loss: 5.14280 | failed:  331
batch:     137120 | loss: 5.19023 | failed:  331
batch:     137130 | loss: 5.06273 | failed:  331
batch:     137140 | loss: 4.95207 | failed:  331
batch:     137150 | loss: 4.98425 | failed:  331
batch:     137160 | loss: 4.99416 | failed:  331
batch:     137170 | loss: 5.01752 | failed:  331
batch:     137180 | loss: 5.00741 | failed:  331
batch:     137190 | loss: 5.15818 | failed:  331
batch:     137200 | loss: 5.18951 | failed:  331
batch:     137210 | loss: 5.14266 | failed:  331
batch:     137220 | loss: 5.23485 | failed:  331
batch:     137230 | loss: 5.24016 | failed:  331
batch:     137240 | loss: 5.16944 | failed:  331
batch:     137250 | loss: 5.07842 | failed:  331
batch:     137260 | loss: 5.15371 | failed:  331
batch:     137270 | loss: 5.10673 | failed:  331
batch:     137280 | loss: 5.10999 | failed:  331
batch:     137290 | loss: 5.07500 | failed:  331
batch:     137300 | loss: 5.15879 | failed:  331
batch:     137310 | loss: 5.07367 | failed:  331
batch:     137320 | loss: 5.23812 | failed:  331
batch:     137330 | loss: 5.27716 | failed:  331
batch:     137340 | loss: 5.11350 | failed:  331
batch:     137350 | loss: 5.05688 | failed:  331
batch:     137360 | loss: 5.00769 | failed:  331
batch:     137370 | loss: 5.03181 | failed:  331
batch:     137380 | loss: 4.87647 | failed:  331
batch:     137390 | loss: 4.97245 | failed:  331
batch:     137400 | loss: 4.98369 | failed:  331
batch:     137410 | loss: 5.26869 | failed:  331
batch:     137420 | loss: 5.27311 | failed:  331
batch:     137430 | loss: 5.20362 | failed:  331
batch:     137440 | loss: 5.20377 | failed:  331
batch:     137450 | loss: 5.17287 | failed:  331
batch:     137460 | loss: 5.22620 | failed:  331
batch:     137470 | loss: 5.11131 | failed:  331
batch:     137480 | loss: 5.13780 | failed:  331
batch:     137490 | loss: 5.21230 | failed:  331
batch:     137500 | loss: 5.10824 | failed:  331
batch:     137510 | loss: 5.22781 | failed:  331
batch:     137520 | loss: 5.00657 | failed:  331
batch:     137530 | loss: 5.23717 | failed:  331
batch:     137540 | loss: 5.23248 | failed:  331
batch:     137550 | loss: 5.09697 | failed:  331
batch:     137560 | loss: 5.18887 | failed:  331
batch:     137570 | loss: 5.15616 | failed:  331
batch:     137580 | loss: 5.13714 | failed:  331
batch:     137590 | loss: 5.10309 | failed:  331
batch:     137600 | loss: 5.20283 | failed:  331
batch:     137610 | loss: 5.05511 | failed:  331
batch:     137620 | loss: 5.22141 | failed:  331
batch:     137630 | loss: 5.17683 | failed:  331
batch:     137640 | loss: 5.08292 | failed:  331
batch:     137650 | loss: 5.11392 | failed:  331
batch:     137660 | loss: 5.06416 | failed:  331
batch:     137670 | loss: 4.97797 | failed:  331
batch:     137680 | loss: 5.06688 | failed:  331
batch:     137690 | loss: 5.07679 | failed:  331
batch:     137700 | loss: 5.05078 | failed:  331
batch:     137710 | loss: 5.13560 | failed:  331
batch:     137720 | loss: 5.25841 | failed:  331
batch:     137730 | loss: 5.13016 | failed:  331
batch:     137740 | loss: 5.25621 | failed:  331
batch:     137750 | loss: 5.24167 | failed:  331
batch:     137760 | loss: 5.15283 | failed:  331
batch:     137770 | loss: 5.15057 | failed:  331
batch:     137780 | loss: 5.10740 | failed:  331
batch:     137790 | loss: 5.13791 | failed:  331
batch:     137800 | loss: 5.07172 | failed:  331
batch:     137810 | loss: 5.19672 | failed:  331
batch:     137820 | loss: 5.16092 | failed:  331
batch:     137830 | loss: 5.21818 | failed:  331
batch:     137840 | loss: 5.12889 | failed:  331
batch:     137850 | loss: 5.19011 | failed:  331
batch:     137860 | loss: 5.20272 | failed:  331
batch:     137870 | loss: 5.11934 | failed:  331
batch:     137880 | loss: 5.05229 | failed:  331
batch:     137890 | loss: 5.14684 | failed:  331
batch:     137900 | loss: 5.14182 | failed:  331
batch:     137910 | loss: 5.14120 | failed:  331
batch:     137920 | loss: 5.11799 | failed:  331
batch:     137930 | loss: 4.97357 | failed:  331
batch:     137940 | loss: 5.03391 | failed:  331
batch:     137950 | loss: 5.14476 | failed:  331
batch:     137960 | loss: 5.06296 | failed:  331
batch:     137970 | loss: 5.01725 | failed:  331
batch:     137980 | loss: 5.13513 | failed:  331
batch:     137990 | loss: 5.07314 | failed:  331
batch:     138000 | loss: 5.04495 | failed:  331
batch:     138010 | loss: 5.15801 | failed:  331
batch:     138020 | loss: 5.10794 | failed:  331
batch:     138030 | loss: 5.16782 | failed:  331
batch:     138040 | loss: 5.04713 | failed:  331
batch:     138050 | loss: 5.18849 | failed:  331
batch:     138060 | loss: 5.05533 | failed:  331
batch:     138070 | loss: 5.01083 | failed:  331
batch:     138080 | loss: 4.96890 | failed:  331
batch:     138090 | loss: 4.94110 | failed:  331
batch:     138100 | loss: 5.03819 | failed:  331
batch:     138110 | loss: 5.03751 | failed:  331
batch:     138120 | loss: 5.16716 | failed:  331
batch:     138130 | loss: 4.98591 | failed:  331
batch:     138140 | loss: 5.14137 | failed:  331
batch:     138150 | loss: 4.81479 | failed:  331
batch:     138160 | loss: 4.78645 | failed:  331
batch:     138170 | loss: 4.81361 | failed:  331
batch:     138180 | loss: 4.77496 | failed:  331
batch:     138190 | loss: 4.87839 | failed:  331
batch:     138200 | loss: 4.78994 | failed:  331
batch:     138210 | loss: 5.32057 | failed:  331
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     138240 | loss: 5.30051 | failed:  341
batch:     138250 | loss: 5.27192 | failed:  341
batch:     138260 | loss: 5.29292 | failed:  341
batch:     138270 | loss: 5.13661 | failed:  341
batch:     138280 | loss: 4.96236 | failed:  341
batch:     138290 | loss: 5.01240 | failed:  341
batch:     138300 | loss: 5.13050 | failed:  341
batch:     138310 | loss: 5.05625 | failed:  341
batch:     138320 | loss: 5.12065 | failed:  341
batch:     138330 | loss: 5.04481 | failed:  341
batch:     138340 | loss: 5.18344 | failed:  341
batch:     138350 | loss: 5.20502 | failed:  341
batch:     138360 | loss: 5.24198 | failed:  341
batch:     138370 | loss: 5.18112 | failed:  341
batch:     138380 | loss: 5.12631 | failed:  341
batch:     138390 | loss: 5.13690 | failed:  341
batch:     138400 | loss: 5.10427 | failed:  341
batch:     138410 | loss: 5.01506 | failed:  341
batch:     138420 | loss: 4.97423 | failed:  341
batch:     138430 | loss: 5.03399 | failed:  341
batch:     138440 | loss: 5.10568 | failed:  341
batch:     138450 | loss: 5.12368 | failed:  341
batch:     138460 | loss: 5.13473 | failed:  341
batch:     138470 | loss: 5.08968 | failed:  341
batch:     138480 | loss: 5.09620 | failed:  341
batch:     138490 | loss: 5.15365 | failed:  341
batch:     138500 | loss: 5.08449 | failed:  341
batch:     138510 | loss: 5.05659 | failed:  341
batch:     138520 | loss: 5.24981 | failed:  341
batch:     138530 | loss: 5.07602 | failed:  341
batch:     138540 | loss: 4.95709 | failed:  341
batch:     138550 | loss: 5.18421 | failed:  341
batch:     138560 | loss: 5.14137 | failed:  341
batch:     138570 | loss: 5.17615 | failed:  341
batch:     138580 | loss: 5.23670 | failed:  341
batch:     138590 | loss: 5.12242 | failed:  341
batch:     138600 | loss: 5.19701 | failed:  341
batch:     138610 | loss: 5.24696 | failed:  341
batch:     138620 | loss: 5.07719 | failed:  341
batch:     138630 | loss: 5.02912 | failed:  341
batch:     138640 | loss: 5.19830 | failed:  341
batch:     138650 | loss: 5.02576 | failed:  341
batch:     138660 | loss: 5.11137 | failed:  341
batch:     138670 | loss: 5.15952 | failed:  341
batch:     138680 | loss: 5.07690 | failed:  341
batch:     138690 | loss: 5.14496 | failed:  341
batch:     138700 | loss: 5.17067 | failed:  341
batch:     138710 | loss: 5.08225 | failed:  341
batch:     138720 | loss: 5.08332 | failed:  341
batch:     138730 | loss: 5.20792 | failed:  341
batch:     138740 | loss: 5.20056 | failed:  341
batch:     138750 | loss: 5.24927 | failed:  341
batch:     138760 | loss: 5.09260 | failed:  341
batch:     138770 | loss: 5.12931 | failed:  341
batch:     138780 | loss: 5.14862 | failed:  341
batch:     138790 | loss: 4.88694 | failed:  341
batch:     138800 | loss: 5.13598 | failed:  341
batch:     138810 | loss: 5.13884 | failed:  341
batch:     138820 | loss: 5.02702 | failed:  341
batch:     138830 | loss: 5.28058 | failed:  341
batch:     138840 | loss: 5.24509 | failed:  341
batch:     138850 | loss: 5.19429 | failed:  341
batch:     138860 | loss: 5.10863 | failed:  341
batch:     138870 | loss: 5.24863 | failed:  341
batch:     138880 | loss: 5.30505 | failed:  341
batch:     138890 | loss: 5.09044 | failed:  341
batch:     138900 | loss: 5.15138 | failed:  341
batch:     138910 | loss: 5.01167 | failed:  341
batch:     138920 | loss: 5.18645 | failed:  341
batch:     138930 | loss: 5.12887 | failed:  341
batch:     138940 | loss: 5.12294 | failed:  341
batch:     138950 | loss: 5.11130 | failed:  341
batch:     138960 | loss: 4.99023 | failed:  341
batch:     138970 | loss: 5.21146 | failed:  341
batch:     138980 | loss: 5.11892 | failed:  341
batch:     138990 | loss: 5.12144 | failed:  341
batch:     139000 | loss: 5.11080 | failed:  341
batch:     139010 | loss: 5.04362 | failed:  341
batch:     139020 | loss: 5.20223 | failed:  341
batch:     139030 | loss: 5.20465 | failed:  341
batch:     139040 | loss: 5.15269 | failed:  341
batch:     139050 | loss: 5.26530 | failed:  341
batch:     139060 | loss: 5.04919 | failed:  341
batch:     139070 | loss: 5.12304 | failed:  341
batch:     139080 | loss: 5.16089 | failed:  341
batch:     139090 | loss: 5.16665 | failed:  341
batch:     139100 | loss: 5.25588 | failed:  341
batch:     139110 | loss: 5.15682 | failed:  341
batch:     139120 | loss: 5.20395 | failed:  341
batch:     139130 | loss: 5.17810 | failed:  341
batch:     139140 | loss: 5.13670 | failed:  341
batch:     139150 | loss: 5.06278 | failed:  341
batch:     139160 | loss: 5.21816 | failed:  341
batch:     139170 | loss: 5.06304 | failed:  341
batch:     139180 | loss: 5.19107 | failed:  341
batch:     139190 | loss: 5.23931 | failed:  341
batch:     139200 | loss: 4.95236 | failed:  341
batch:     139210 | loss: 5.28480 | failed:  341
batch:     139220 | loss: 5.19914 | failed:  341
batch:     139230 | loss: 5.23511 | failed:  341
batch:     139240 | loss: 5.07679 | failed:  341
batch:     139250 | loss: 5.10758 | failed:  341
batch:     139260 | loss: 5.11817 | failed:  341
batch:     139270 | loss: 5.01299 | failed:  341
batch:     139280 | loss: 5.23352 | failed:  341
batch:     139290 | loss: 5.20612 | failed:  341
batch:     139300 | loss: 5.12492 | failed:  341
batch:     139310 | loss: 5.16748 | failed:  341
batch:     139320 | loss: 5.20002 | failed:  341
batch:     139330 | loss: 5.26286 | failed:  341
batch:     139340 | loss: 5.12364 | failed:  341
batch:     139350 | loss: 4.97589 | failed:  341
batch:     139360 | loss: 5.02877 | failed:  341
batch:     139370 | loss: 4.87204 | failed:  341
batch:     139380 | loss: 4.89505 | failed:  341
batch:     139390 | loss: 4.95277 | failed:  341
batch:     139400 | loss: 5.25077 | failed:  341
batch:     139410 | loss: 5.17068 | failed:  341
batch:     139420 | loss: 5.15026 | failed:  341
batch:     139430 | loss: 5.15363 | failed:  341
batch:     139440 | loss: 5.10185 | failed:  341
batch:     139450 | loss: 5.16552 | failed:  341
batch:     139460 | loss: 5.04659 | failed:  341
batch:     139470 | loss: 5.14463 | failed:  341
batch:     139480 | loss: 5.08958 | failed:  341
batch:     139490 | loss: 4.98961 | failed:  341
batch:     139500 | loss: 5.29687 | failed:  341
batch:     139510 | loss: 5.10744 | failed:  341
batch:     139520 | loss: 5.13869 | failed:  341
batch:     139530 | loss: 4.92545 | failed:  341
batch:     139540 | loss: 5.01836 | failed:  341
batch:     139550 | loss: 5.17233 | failed:  341
batch:     139560 | loss: 4.92064 | failed:  341
batch:     139570 | loss: 4.54316 | failed:  341
batch:     139580 | loss: 5.21657 | failed:  341
batch:     139590 | loss: 5.16811 | failed:  341
batch:     139600 | loss: 5.07710 | failed:  341
batch:     139610 | loss: 4.84769 | failed:  341
batch:     139620 | loss: 5.06348 | failed:  341
batch:     139630 | loss: 5.13202 | failed:  341
batch:     139640 | loss: 5.09307 | failed:  341
batch:     139650 | loss: 5.11063 | failed:  341
batch:     139660 | loss: 5.03327 | failed:  341
batch:     139670 | loss: 4.90576 | failed:  341
batch:     139680 | loss: 4.88854 | failed:  341
batch:     139690 | loss: 4.95770 | failed:  341
batch:     139700 | loss: 4.81659 | failed:  341
batch:     139710 | loss: 5.00731 | failed:  341
batch:     139720 | loss: 5.06524 | failed:  341
batch:     139730 | loss: 5.15057 | failed:  341
batch:     139740 | loss: 5.20301 | failed:  341
batch:     139750 | loss: 5.11468 | failed:  341
batch:     139760 | loss: 5.09701 | failed:  341
batch:     139770 | loss: 4.92806 | failed:  341
batch:     139780 | loss: 5.08208 | failed:  341
batch:     139790 | loss: 5.04990 | failed:  341
batch:     139800 | loss: 5.14900 | failed:  341
batch:     139810 | loss: 5.17303 | failed:  341
batch:     139820 | loss: 5.08875 | failed:  341
batch:     139830 | loss: 5.22755 | failed:  341
batch:     139840 | loss: 5.02113 | failed:  341
batch:     139850 | loss: 5.13311 | failed:  341
batch:     139860 | loss: 5.02265 | failed:  341
batch:     139870 | loss: 5.00136 | failed:  341
batch:     139880 | loss: 5.22552 | failed:  341
batch:     139890 | loss: 5.17016 | failed:  341
batch:     139900 | loss: 5.25686 | failed:  341
batch:     139910 | loss: 4.92962 | failed:  341
batch:     139920 | loss: 5.14208 | failed:  341
batch:     139930 | loss: 5.08310 | failed:  341
batch:     139940 | loss: 5.09083 | failed:  341
batch:     139950 | loss: 5.01942 | failed:  341
batch:     139960 | loss: 5.11486 | failed:  341
batch:     139970 | loss: 5.24928 | failed:  341
batch:     139980 | loss: 5.14805 | failed:  341
batch:     139990 | loss: 5.11601 | failed:  341
batch:     140000 | loss: 5.13805 | failed:  341
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     140010 | loss: 5.13742 | failed:  341
batch:     140020 | loss: 5.12701 | failed:  341
batch:     140030 | loss: 5.18701 | failed:  341
batch:     140040 | loss: 5.18644 | failed:  341
batch:     140050 | loss: 5.15014 | failed:  341
batch:     140060 | loss: 5.16716 | failed:  341
batch:     140070 | loss: 5.30324 | failed:  341
batch:     140080 | loss: 5.23656 | failed:  341
batch:     140090 | loss: 5.08492 | failed:  341
batch:     140100 | loss: 5.07742 | failed:  341
batch:     140110 | loss: 5.21289 | failed:  341
batch:     140120 | loss: 5.13781 | failed:  341
batch:     140130 | loss: 5.09100 | failed:  341
batch:     140140 | loss: 5.22131 | failed:  341
batch:     140150 | loss: 5.17815 | failed:  341
batch:     140160 | loss: 5.16097 | failed:  341
batch:     140170 | loss: 5.00980 | failed:  341
batch:     140180 | loss: 4.90925 | failed:  341
batch:     140190 | loss: 4.92241 | failed:  341
batch:     140200 | loss: 5.05921 | failed:  341
batch:     140210 | loss: 5.04246 | failed:  341
batch:     140220 | loss: 4.88850 | failed:  341
batch:     140230 | loss: 4.95915 | failed:  341
batch:     140240 | loss: 5.20628 | failed:  341
batch:     140250 | loss: 5.15144 | failed:  341
batch:     140260 | loss: 5.05059 | failed:  341
batch:     140270 | loss: 5.05847 | failed:  341
batch:     140280 | loss: 5.05568 | failed:  341
batch:     140290 | loss: 5.05607 | failed:  341
batch:     140300 | loss: 5.18462 | failed:  341
batch:     140310 | loss: 5.17604 | failed:  341
batch:     140320 | loss: 5.13220 | failed:  341
batch:     140330 | loss: 5.11085 | failed:  341
batch:     140340 | loss: 5.07509 | failed:  341
batch:     140350 | loss: 5.11436 | failed:  341
batch:     140360 | loss: 5.15079 | failed:  341
batch:     140370 | loss: 5.18774 | failed:  341
batch:     140380 | loss: 5.20402 | failed:  341
batch:     140390 | loss: 5.11893 | failed:  341
batch:     140400 | loss: 5.16076 | failed:  341
batch:     140410 | loss: 5.26142 | failed:  341
batch:     140420 | loss: 5.20269 | failed:  341
batch:     140430 | loss: 5.14387 | failed:  341
batch:     140440 | loss: 5.17575 | failed:  341
batch:     140450 | loss: 5.12035 | failed:  341
batch:     140460 | loss: 5.01470 | failed:  341
batch:     140470 | loss: 5.23674 | failed:  341
batch:     140480 | loss: 5.24372 | failed:  341
batch:     140490 | loss: 5.12330 | failed:  341
batch:     140500 | loss: 5.00243 | failed:  341
batch:     140510 | loss: 5.27666 | failed:  341
batch:     140520 | loss: 5.23646 | failed:  341
batch:     140530 | loss: 4.99889 | failed:  341
batch:     140540 | loss: 5.09710 | failed:  341
batch:     140550 | loss: 5.17596 | failed:  341
batch:     140560 | loss: 5.11041 | failed:  341
batch:     140570 | loss: 5.02862 | failed:  341
batch:     140580 | loss: 5.14610 | failed:  341
batch:     140590 | loss: 5.02339 | failed:  341
batch:     140600 | loss: 5.11396 | failed:  341
batch:     140610 | loss: 5.02821 | failed:  341
batch:     140620 | loss: 5.05293 | failed:  341
batch:     140630 | loss: 4.97899 | failed:  341
batch:     140640 | loss: 5.06535 | failed:  341
batch:     140650 | loss: 5.05600 | failed:  341
batch:     140660 | loss: 4.81277 | failed:  341
batch:     140670 | loss: 5.26341 | failed:  341
batch:     140680 | loss: 5.02014 | failed:  341
batch:     140690 | loss: 5.13682 | failed:  341
batch:     140700 | loss: 4.97703 | failed:  341
batch:     140710 | loss: 5.16077 | failed:  341
batch:     140720 | loss: 5.13679 | failed:  341
batch:     140730 | loss: 5.28702 | failed:  341
batch:     140740 | loss: 5.08307 | failed:  341
batch:     140750 | loss: 5.18381 | failed:  341
batch:     140760 | loss: 5.20683 | failed:  341
batch:     140770 | loss: 5.00854 | failed:  341
batch:     140780 | loss: 5.11587 | failed:  341
batch:     140790 | loss: 5.16058 | failed:  341
batch:     140800 | loss: 5.10100 | failed:  341
batch:     140810 | loss: 5.28002 | failed:  341
batch:     140820 | loss: 5.23853 | failed:  341
batch:     140830 | loss: 5.20536 | failed:  341
batch:     140840 | loss: 5.18271 | failed:  341
batch:     140850 | loss: 3.95303 | failed:  341
batch:     140860 | loss: 5.10261 | failed:  341
batch:     140870 | loss: 5.09100 | failed:  341
batch:     140880 | loss: 5.00654 | failed:  341
batch:     140890 | loss: 5.22072 | failed:  341
batch:     140900 | loss: 5.12334 | failed:  341
batch:     140910 | loss: 5.12409 | failed:  341
batch:     140920 | loss: 5.11041 | failed:  341
batch:     140930 | loss: 5.14830 | failed:  341
batch:     140940 | loss: 5.07399 | failed:  341
batch:     140950 | loss: 5.02363 | failed:  341
batch:     140960 | loss: 5.02654 | failed:  341
batch:     140970 | loss: 5.24566 | failed:  341
batch:     140980 | loss: 5.21124 | failed:  341
batch:     140990 | loss: 5.12527 | failed:  341
batch:     141000 | loss: 5.22821 | failed:  341
batch:     141010 | loss: 5.26285 | failed:  341
batch:     141020 | loss: 5.16628 | failed:  341
batch:     141030 | loss: 4.94938 | failed:  341
batch:     141040 | loss: 5.08502 | failed:  341
batch:     141050 | loss: 5.16339 | failed:  341
batch:     141060 | loss: 5.13061 | failed:  341
batch:     141070 | loss: 4.90046 | failed:  341
batch:     141080 | loss: 5.03877 | failed:  341
batch:     141090 | loss: 5.04142 | failed:  341
batch:     141100 | loss: 4.83857 | failed:  341
batch:     141110 | loss: 5.07933 | failed:  341
batch:     141120 | loss: 5.10119 | failed:  341
batch:     141130 | loss: 5.06053 | failed:  341
batch:     141140 | loss: 5.01729 | failed:  341
batch:     141150 | loss: 4.85729 | failed:  341
batch:     141160 | loss: 5.04959 | failed:  341
batch:     141170 | loss: 5.07949 | failed:  341
batch:     141180 | loss: 5.12864 | failed:  341
batch:     141190 | loss: 4.99773 | failed:  341
batch:     141200 | loss: 5.03946 | failed:  341
batch:     141210 | loss: 5.17690 | failed:  341
batch:     141220 | loss: 5.12668 | failed:  341
batch:     141230 | loss: 5.18454 | failed:  341
batch:     141240 | loss: 5.20805 | failed:  341
batch:     141250 | loss: 5.23973 | failed:  341
batch:     141260 | loss: 5.26153 | failed:  341
batch:     141270 | loss: 4.97071 | failed:  341
batch:     141280 | loss: 5.13053 | failed:  341
batch:     141290 | loss: 5.16017 | failed:  341
batch:     141300 | loss: 5.17891 | failed:  341
batch:     141310 | loss: 5.19454 | failed:  341
batch:     141320 | loss: 5.15453 | failed:  341
batch:     141330 | loss: 5.04802 | failed:  341
batch:     141340 | loss: 5.17940 | failed:  341
batch:     141350 | loss: 5.12886 | failed:  341
batch:     141360 | loss: 5.21587 | failed:  341
batch:     141370 | loss: 5.28895 | failed:  341
batch:     141380 | loss: 5.11483 | failed:  341
batch:     141390 | loss: 5.13769 | failed:  341
batch:     141400 | loss: 4.96000 | failed:  341
batch:     141410 | loss: 5.09389 | failed:  341
batch:     141420 | loss: 5.08298 | failed:  341
batch:     141430 | loss: 5.15163 | failed:  341
batch:     141440 | loss: 5.23285 | failed:  341
batch:     141450 | loss: 5.18991 | failed:  341
batch:     141460 | loss: 5.18407 | failed:  341
batch:     141470 | loss: 5.10876 | failed:  341
batch:     141480 | loss: 5.06759 | failed:  341
batch:     141490 | loss: 5.01758 | failed:  341
batch:     141500 | loss: 4.97203 | failed:  341
batch:     141510 | loss: 4.93893 | failed:  341
batch:     141520 | loss: 4.93064 | failed:  341
batch:     141530 | loss: 4.59207 | failed:  341
batch:     141540 | loss: 5.16108 | failed:  341
batch:     141550 | loss: 4.51919 | failed:  341
batch:     141560 | loss: 4.83802 | failed:  341
batch:     141570 | loss: 4.49577 | failed:  341
batch:     141580 | loss: 4.87913 | failed:  341
batch:     141590 | loss: 5.09609 | failed:  341
batch:     141600 | loss: 5.25988 | failed:  341
batch:     141610 | loss: 5.25490 | failed:  341
batch:     141620 | loss: 5.06551 | failed:  341
batch:     141630 | loss: 5.13994 | failed:  341
batch:     141640 | loss: 5.11212 | failed:  341
batch:     141650 | loss: 5.15506 | failed:  341
batch:     141660 | loss: 5.14173 | failed:  341
batch:     141670 | loss: 5.12512 | failed:  341
batch:     141680 | loss: 5.17467 | failed:  341
batch:     141690 | loss: 5.18001 | failed:  341
batch:     141700 | loss: 5.21976 | failed:  341
batch:     141710 | loss: 5.23857 | failed:  341
batch:     141720 | loss: 5.09299 | failed:  341
batch:     141730 | loss: 4.94464 | failed:  341
batch:     141740 | loss: 5.08018 | failed:  341
batch:     141750 | loss: 4.99836 | failed:  341
batch:     141760 | loss: 5.04183 | failed:  341
batch:     141770 | loss: 5.14562 | failed:  341
batch:     141780 | loss: 4.99837 | failed:  341
batch:     141790 | loss: 5.18090 | failed:  341
batch:     141800 | loss: 5.13161 | failed:  341
batch:     141810 | loss: 5.17601 | failed:  341
batch:     141820 | loss: 5.16001 | failed:  341
batch:     141830 | loss: 5.06750 | failed:  341
batch:     141840 | loss: 5.06145 | failed:  341
batch:     141850 | loss: 5.11406 | failed:  341
batch:     141860 | loss: 5.00823 | failed:  341
batch:     141870 | loss: 5.13484 | failed:  341
batch:     141880 | loss: 4.93119 | failed:  341
batch:     141890 | loss: 5.17203 | failed:  341
batch:     141900 | loss: 5.13466 | failed:  341
batch:     141910 | loss: 5.12253 | failed:  341
batch:     141920 | loss: 5.22866 | failed:  341
batch:     141930 | loss: 5.03337 | failed:  341
batch:     141940 | loss: 5.14913 | failed:  341
batch:     141950 | loss: 5.14058 | failed:  341
batch:     141960 | loss: 5.13519 | failed:  341
batch:     141970 | loss: 5.05574 | failed:  341
batch:     141980 | loss: 5.14181 | failed:  341
batch:     141990 | loss: 5.16196 | failed:  341
batch:     142000 | loss: 5.14465 | failed:  341
batch:     142010 | loss: 5.11750 | failed:  341
batch:     142020 | loss: 5.00269 | failed:  341
batch:     142030 | loss: 5.14666 | failed:  341
batch:     142040 | loss: 5.06701 | failed:  341
batch:     142050 | loss: 5.09246 | failed:  341
batch:     142060 | loss: 5.22006 | failed:  341
batch:     142070 | loss: 5.13353 | failed:  341
batch:     142080 | loss: 4.94092 | failed:  341
batch:     142090 | loss: 5.15069 | failed:  341
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     142100 | loss: 5.32444 | failed:  342
batch:     142110 | loss: 5.20049 | failed:  342
batch:     142120 | loss: 5.06540 | failed:  342
batch:     142130 | loss: 5.28432 | failed:  342
batch:     142140 | loss: 5.28481 | failed:  342
batch:     142150 | loss: 4.86649 | failed:  342
batch:     142160 | loss: 4.79673 | failed:  342
batch:     142170 | loss: 5.20930 | failed:  342
batch:     142180 | loss: 5.17393 | failed:  342
batch:     142190 | loss: 4.98242 | failed:  342
batch:     142200 | loss: 5.21631 | failed:  342
batch:     142210 | loss: 5.13652 | failed:  342
batch:     142220 | loss: 5.21997 | failed:  342
batch:     142230 | loss: 5.13626 | failed:  342
batch:     142240 | loss: 5.12742 | failed:  342
batch:     142250 | loss: 5.12977 | failed:  342
batch:     142260 | loss: 5.20323 | failed:  342
batch:     142270 | loss: 5.13873 | failed:  342
batch:     142280 | loss: 5.08791 | failed:  342
batch:     142290 | loss: 5.19887 | failed:  342
batch:     142300 | loss: 5.16626 | failed:  342
batch:     142310 | loss: 5.17389 | failed:  342
batch:     142320 | loss: 5.08334 | failed:  342
batch:     142330 | loss: 5.03868 | failed:  342
batch:     142340 | loss: 5.21513 | failed:  342
batch:     142350 | loss: 5.10166 | failed:  342
batch:     142360 | loss: 5.15187 | failed:  342
batch:     142370 | loss: 5.17741 | failed:  342
batch:     142380 | loss: 5.12343 | failed:  342
batch:     142390 | loss: 4.85290 | failed:  342
batch:     142400 | loss: 5.22348 | failed:  342
batch:     142410 | loss: 5.23019 | failed:  342
batch:     142420 | loss: 5.07106 | failed:  342
batch:     142430 | loss: 5.11201 | failed:  342
batch:     142440 | loss: 5.14841 | failed:  342
batch:     142450 | loss: 5.16577 | failed:  342
batch:     142460 | loss: 4.94054 | failed:  342
batch:     142470 | loss: 5.02106 | failed:  342
batch:     142480 | loss: 4.73233 | failed:  342
batch:     142490 | loss: 5.03707 | failed:  342
batch:     142500 | loss: 5.12418 | failed:  342
batch:     142510 | loss: 4.85715 | failed:  342
batch:     142520 | loss: 5.10182 | failed:  342
batch:     142530 | loss: 5.21194 | failed:  342
batch:     142540 | loss: 5.09949 | failed:  342
batch:     142550 | loss: 5.16817 | failed:  342
batch:     142560 | loss: 5.07299 | failed:  342
batch:     142570 | loss: 5.16360 | failed:  342
batch:     142580 | loss: 4.60880 | failed:  342
batch:     142590 | loss: 5.25369 | failed:  342
batch:     142600 | loss: 5.22886 | failed:  342
batch:     142610 | loss: 5.25292 | failed:  342
batch:     142620 | loss: 4.64819 | failed:  342
batch:     142630 | loss: 5.23855 | failed:  342
batch:     142640 | loss: 5.13895 | failed:  342
batch:     142650 | loss: 5.17294 | failed:  342
batch:     142660 | loss: 5.14234 | failed:  342
batch:     142670 | loss: 5.01735 | failed:  342
batch:     142680 | loss: 5.09101 | failed:  342
batch:     142690 | loss: 5.13958 | failed:  342
batch:     142700 | loss: 5.08635 | failed:  342
batch:     142710 | loss: 5.01938 | failed:  342
batch:     142720 | loss: 5.15412 | failed:  342
batch:     142730 | loss: 5.01146 | failed:  342
batch:     142740 | loss: 5.08703 | failed:  342
batch:     142750 | loss: 5.27578 | failed:  342
batch:     142760 | loss: 5.09944 | failed:  342
batch:     142770 | loss: 5.18839 | failed:  342
batch:     142780 | loss: 5.27360 | failed:  342
batch:     142790 | loss: 5.25890 | failed:  342
batch:     142800 | loss: 5.24616 | failed:  342
batch:     142810 | loss: 5.25232 | failed:  342
batch:     142820 | loss: 5.10271 | failed:  342
batch:     142830 | loss: 5.15362 | failed:  342
batch:     142840 | loss: 5.25351 | failed:  342
batch:     142850 | loss: 5.23231 | failed:  342
batch:     142860 | loss: 5.07130 | failed:  342
batch:     142870 | loss: 5.05398 | failed:  342
batch:     142880 | loss: 5.27473 | failed:  342
batch:     142890 | loss: 5.11754 | failed:  342
batch:     142900 | loss: 5.18031 | failed:  342
batch:     142910 | loss: 5.12073 | failed:  342
batch:     142920 | loss: 5.23120 | failed:  342
batch:     142930 | loss: 5.22389 | failed:  342
batch:     142940 | loss: 5.08231 | failed:  342
batch:     142950 | loss: 4.89188 | failed:  342
batch:     142960 | loss: 5.09397 | failed:  342
batch:     142970 | loss: 5.13534 | failed:  342
batch:     142980 | loss: 5.13107 | failed:  342
batch:     142990 | loss: 5.01690 | failed:  342
batch:     143000 | loss: 5.17572 | failed:  342
batch:     143010 | loss: 5.18159 | failed:  342
batch:     143020 | loss: 5.14653 | failed:  342
batch:     143030 | loss: 5.19259 | failed:  342
batch:     143040 | loss: 5.13815 | failed:  342
batch:     143050 | loss: 5.01524 | failed:  342
batch:     143060 | loss: 5.09211 | failed:  342
batch:     143070 | loss: 5.19910 | failed:  342
batch:     143080 | loss: 5.14845 | failed:  342
batch:     143090 | loss: 5.15656 | failed:  342
batch:     143100 | loss: 5.08876 | failed:  342
batch:     143110 | loss: 5.07646 | failed:  342
batch:     143120 | loss: 5.14120 | failed:  342
batch:     143130 | loss: 5.19444 | failed:  342
batch:     143140 | loss: 5.18157 | failed:  342
batch:     143150 | loss: 5.05973 | failed:  342
batch:     143160 | loss: 5.13646 | failed:  342
batch:     143170 | loss: 5.19284 | failed:  342
batch:     143180 | loss: 5.06474 | failed:  342
batch:     143190 | loss: 5.13937 | failed:  342
batch:     143200 | loss: 5.09149 | failed:  342
batch:     143210 | loss: 5.10595 | failed:  342
batch:     143220 | loss: 4.85942 | failed:  342
batch:     143230 | loss: 5.20273 | failed:  342
batch:     143240 | loss: 5.12945 | failed:  342
batch:     143250 | loss: 5.27269 | failed:  342
batch:     143260 | loss: 4.97879 | failed:  342
batch:     143270 | loss: 5.14750 | failed:  342
batch:     143280 | loss: 5.13345 | failed:  342
batch:     143290 | loss: 5.13909 | failed:  342
batch:     143300 | loss: 5.17477 | failed:  342
batch:     143310 | loss: 5.13044 | failed:  342
batch:     143320 | loss: 4.65018 | failed:  342
batch:     143330 | loss: 4.70684 | failed:  342
batch:     143340 | loss: 5.19028 | failed:  342
batch:     143350 | loss: 5.15172 | failed:  342
batch:     143360 | loss: 5.12499 | failed:  342
batch:     143370 | loss: 5.06736 | failed:  342
batch:     143380 | loss: 5.01704 | failed:  342
batch:     143390 | loss: 5.15188 | failed:  342
batch:     143400 | loss: 5.22485 | failed:  342
batch:     143410 | loss: 5.16039 | failed:  342
batch:     143420 | loss: 5.07971 | failed:  342
batch:     143430 | loss: 5.16630 | failed:  342
batch:     143440 | loss: 5.00175 | failed:  342
batch:     143450 | loss: 5.09380 | failed:  342
batch:     143460 | loss: 4.88245 | failed:  342
batch:     143470 | loss: 5.22793 | failed:  342
batch:     143480 | loss: 5.14615 | failed:  342
batch:     143490 | loss: 5.13363 | failed:  342
batch:     143500 | loss: 5.10436 | failed:  342
batch:     143510 | loss: 5.14119 | failed:  342
batch:     143520 | loss: 4.94742 | failed:  342
batch:     143530 | loss: 5.03323 | failed:  342
batch:     143540 | loss: 5.11753 | failed:  342
batch:     143550 | loss: 5.05553 | failed:  342
batch:     143560 | loss: 5.15213 | failed:  342
batch:     143570 | loss: 5.20200 | failed:  342
batch:     143580 | loss: 5.25035 | failed:  342
batch:     143590 | loss: 5.20539 | failed:  342
batch:     143600 | loss: 5.25133 | failed:  342
batch:     143610 | loss: 5.24534 | failed:  342
batch:     143620 | loss: 5.22129 | failed:  342
batch:     143630 | loss: 5.20377 | failed:  342
batch:     143640 | loss: 5.22288 | failed:  342
batch:     143650 | loss: 5.16818 | failed:  342
batch:     143660 | loss: 5.09965 | failed:  342
batch:     143670 | loss: 5.12508 | failed:  342
batch:     143680 | loss: 5.16263 | failed:  342
batch:     143690 | loss: 4.63251 | failed:  342
batch:     143700 | loss: 4.60927 | failed:  342
batch:     143710 | loss: 4.62581 | failed:  342
batch:     143720 | loss: 4.76660 | failed:  342
batch:     143730 | loss: 5.16829 | failed:  342
batch:     143740 | loss: 5.22240 | failed:  342
batch:     143750 | loss: 5.05022 | failed:  342
batch:     143760 | loss: 5.08353 | failed:  342
batch:     143770 | loss: 5.28433 | failed:  342
batch:     143780 | loss: 5.23591 | failed:  342
batch:     143790 | loss: 5.22301 | failed:  342
batch:     143800 | loss: 5.11160 | failed:  342
batch:     143810 | loss: 5.20547 | failed:  342
batch:     143820 | loss: 5.00976 | failed:  342
batch:     143830 | loss: 5.10318 | failed:  342
batch:     143840 | loss: 5.15188 | failed:  342
batch:     143850 | loss: 5.08877 | failed:  342
batch:     143860 | loss: 5.10020 | failed:  342
batch:     143870 | loss: 5.19126 | failed:  342
batch:     143880 | loss: 5.05631 | failed:  342
batch:     143890 | loss: 5.13843 | failed:  342
batch:     143900 | loss: 5.08800 | failed:  342
batch:     143910 | loss: 5.11463 | failed:  342
batch:     143920 | loss: 5.20827 | failed:  342
batch:     143930 | loss: 5.26985 | failed:  342
batch:     143940 | loss: 5.14140 | failed:  342
batch:     143950 | loss: 5.07686 | failed:  342
batch:     143960 | loss: 5.15358 | failed:  342
batch:     143970 | loss: 5.07410 | failed:  342
batch:     143980 | loss: 5.22988 | failed:  342
batch:     143990 | loss: 5.11808 | failed:  342
batch:     144000 | loss: 5.19210 | failed:  342
batch:     144010 | loss: 5.17793 | failed:  342
batch:     144020 | loss: 4.96526 | failed:  342
batch:     144030 | loss: 5.16029 | failed:  342
batch:     144040 | loss: 5.16264 | failed:  342
batch:     144050 | loss: 5.19756 | failed:  342
batch:     144060 | loss: 5.17783 | failed:  342
batch:     144070 | loss: 5.20306 | failed:  342
batch:     144080 | loss: 5.21649 | failed:  342
batch:     144090 | loss: 5.22414 | failed:  342
batch:     144100 | loss: 5.17744 | failed:  342
batch:     144110 | loss: 5.13483 | failed:  342
batch:     144120 | loss: 5.15148 | failed:  342
batch:     144130 | loss: 5.16773 | failed:  342
batch:     144140 | loss: 5.18714 | failed:  342
batch:     144150 | loss: 5.16011 | failed:  342
batch:     144160 | loss: 5.18335 | failed:  342
batch:     144170 | loss: 5.14539 | failed:  342
batch:     144180 | loss: 4.98447 | failed:  342
batch:     144190 | loss: 5.03568 | failed:  342
batch:     144200 | loss: 5.10600 | failed:  342
batch:     144210 | loss: 5.08569 | failed:  342
batch:     144220 | loss: 4.53188 | failed:  342
batch:     144230 | loss: 5.08781 | failed:  342
batch:     144240 | loss: 5.13522 | failed:  342
batch:     144250 | loss: 4.74114 | failed:  342
batch:     144260 | loss: 5.16852 | failed:  342
batch:     144270 | loss: 4.48242 | failed:  342
batch:     144280 | loss: 5.07434 | failed:  342
batch:     144290 | loss: 5.11491 | failed:  342
batch:     144300 | loss: 5.12599 | failed:  342
batch:     144310 | loss: 5.04382 | failed:  342
batch:     144320 | loss: 5.24846 | failed:  342
batch:     144330 | loss: 5.19534 | failed:  342
batch:     144340 | loss: 5.21574 | failed:  342
batch:     144350 | loss: 5.13274 | failed:  342
batch:     144360 | loss: 5.12790 | failed:  342
batch:     144370 | loss: 5.16694 | failed:  342
batch:     144380 | loss: 4.77993 | failed:  342
batch:     144390 | loss: 5.41118 | failed:  342
batch:     144400 | loss: 5.14797 | failed:  342
batch:     144410 | loss: 5.05288 | failed:  342
batch:     144420 | loss: 5.22132 | failed:  342
batch:     144430 | loss: 5.09439 | failed:  342
batch:     144440 | loss: 5.11074 | failed:  342
batch:     144450 | loss: 5.12892 | failed:  342
batch:     144460 | loss: 5.07228 | failed:  342
batch:     144470 | loss: 5.25293 | failed:  342
batch:     144480 | loss: 5.18812 | failed:  342
batch:     144490 | loss: 5.15146 | failed:  342
batch:     144500 | loss: 5.16410 | failed:  342
batch:     144510 | loss: 5.07809 | failed:  342
batch:     144520 | loss: 5.08447 | failed:  342
batch:     144530 | loss: 5.18996 | failed:  342
batch:     144540 | loss: 5.07527 | failed:  342
batch:     144550 | loss: 5.01432 | failed:  342
batch:     144560 | loss: 5.27071 | failed:  342
batch:     144570 | loss: 5.22363 | failed:  342
batch:     144580 | loss: 5.19361 | failed:  342
batch:     144590 | loss: 4.25495 | failed:  342
batch:     144600 | loss: 5.14615 | failed:  342
batch:     144610 | loss: 5.13203 | failed:  342
batch:     144620 | loss: 5.20526 | failed:  342
batch:     144630 | loss: 4.80575 | failed:  342
batch:     144640 | loss: 5.23211 | failed:  342
batch:     144650 | loss: 4.71072 | failed:  342
batch:     144660 | loss: 4.80178 | failed:  342
batch:     144670 | loss: 5.17211 | failed:  342
batch:     144680 | loss: 5.17048 | failed:  342
batch:     144690 | loss: 5.00388 | failed:  342
batch:     144700 | loss: 5.11913 | failed:  342
batch:     144710 | loss: 5.09653 | failed:  342
batch:     144720 | loss: 5.20453 | failed:  342
batch:     144730 | loss: 5.17658 | failed:  342
batch:     144740 | loss: 5.10892 | failed:  342
batch:     144750 | loss: 5.01844 | failed:  342
batch:     144760 | loss: 5.23163 | failed:  342
batch:     144770 | loss: 5.12452 | failed:  342
batch:     144780 | loss: 5.08633 | failed:  342
batch:     144790 | loss: 5.04994 | failed:  342
batch:     144800 | loss: 5.35555 | failed:  342
batch:     144810 | loss: 5.10658 | failed:  342
batch:     144820 | loss: 5.26717 | failed:  342
batch:     144830 | loss: 5.25441 | failed:  342
batch:     144840 | loss: 5.23141 | failed:  342
batch:     144850 | loss: 5.15304 | failed:  342
batch:     144860 | loss: 5.20516 | failed:  342
batch:     144870 | loss: 5.11962 | failed:  342
batch:     144880 | loss: 4.74396 | failed:  342
batch:     144890 | loss: 5.11490 | failed:  342
batch:     144900 | loss: 5.09729 | failed:  342
batch:     144910 | loss: 4.93055 | failed:  342
batch:     144920 | loss: 5.18747 | failed:  342
batch:     144930 | loss: 5.05146 | failed:  342
batch:     144940 | loss: 5.05755 | failed:  342
batch:     144950 | loss: 4.91361 | failed:  342
batch:     144960 | loss: 4.91167 | failed:  342
batch:     144970 | loss: 5.22018 | failed:  342
batch:     144980 | loss: 5.11418 | failed:  342
batch:     144990 | loss: 5.12062 | failed:  342
batch:     145000 | loss: 5.17685 | failed:  342
batch:     145010 | loss: 5.14238 | failed:  342
batch:     145020 | loss: 5.15465 | failed:  342
batch:     145030 | loss: 5.02734 | failed:  342
batch:     145040 | loss: 4.94104 | failed:  342
batch:     145050 | loss: 5.16066 | failed:  342
batch:     145060 | loss: 5.14433 | failed:  342
batch:     145070 | loss: 5.14898 | failed:  342
batch:     145080 | loss: 5.03420 | failed:  342
batch:     145090 | loss: 5.06730 | failed:  342
batch:     145100 | loss: 5.31608 | failed:  342
batch:     145110 | loss: 5.25646 | failed:  342
batch:     145120 | loss: 5.15169 | failed:  342
batch:     145130 | loss: 5.23248 | failed:  342
batch:     145140 | loss: 4.98504 | failed:  342
batch:     145150 | loss: 5.10526 | failed:  342
batch:     145160 | loss: 4.78677 | failed:  342
batch:     145170 | loss: 4.98775 | failed:  342
batch:     145180 | loss: 4.99063 | failed:  342
batch:     145190 | loss: 5.02571 | failed:  342
batch:     145200 | loss: 5.20245 | failed:  342
batch:     145210 | loss: 5.19195 | failed:  342
batch:     145220 | loss: 4.98919 | failed:  342
batch:     145230 | loss: 4.99618 | failed:  342
batch:     145240 | loss: 5.23087 | failed:  342
batch:     145250 | loss: 5.02928 | failed:  342
batch:     145260 | loss: 5.22512 | failed:  342
batch:     145270 | loss: 5.17482 | failed:  342
batch:     145280 | loss: 5.00449 | failed:  342
batch:     145290 | loss: 5.01495 | failed:  342
batch:     145300 | loss: 5.17787 | failed:  342
batch:     145310 | loss: 5.20150 | failed:  342
batch:     145320 | loss: 4.98120 | failed:  342
batch:     145330 | loss: 5.10212 | failed:  342
batch:     145340 | loss: 5.13243 | failed:  342
batch:     145350 | loss: 5.06337 | failed:  342
batch:     145360 | loss: 5.19476 | failed:  342
batch:     145370 | loss: 5.09149 | failed:  342
batch:     145380 | loss: 5.11327 | failed:  342
batch:     145390 | loss: 5.04582 | failed:  342
batch:     145400 | loss: 5.16572 | failed:  342
batch:     145410 | loss: 5.19269 | failed:  342
batch:     145420 | loss: 5.10299 | failed:  342
batch:     145430 | loss: 5.10147 | failed:  342
batch:     145440 | loss: 5.03153 | failed:  342
batch:     145450 | loss: 4.87686 | failed:  342
batch:     145460 | loss: 4.98397 | failed:  342
batch:     145470 | loss: 5.03116 | failed:  342
batch:     145480 | loss: 5.13484 | failed:  342
batch:     145490 | loss: 5.01772 | failed:  342
batch:     145500 | loss: 4.96506 | failed:  342
batch:     145510 | loss: 5.28757 | failed:  342
batch:     145520 | loss: 5.02422 | failed:  342
batch:     145530 | loss: 5.17983 | failed:  342
batch:     145540 | loss: 5.14798 | failed:  342
batch:     145550 | loss: 5.07192 | failed:  342
batch:     145560 | loss: 5.29748 | failed:  342
batch:     145570 | loss: 5.13154 | failed:  342
batch:     145580 | loss: 5.26152 | failed:  342
batch:     145590 | loss: 5.18123 | failed:  342
batch:     145600 | loss: 5.14923 | failed:  342
batch:     145610 | loss: 5.12499 | failed:  342
batch:     145620 | loss: 5.05878 | failed:  342
batch:     145630 | loss: 5.05187 | failed:  342
batch:     145640 | loss: 4.88829 | failed:  342
batch:     145650 | loss: 5.24477 | failed:  342
batch:     145660 | loss: 4.98984 | failed:  342
batch:     145670 | loss: 5.08720 | failed:  342
batch:     145680 | loss: 5.09183 | failed:  342
batch:     145690 | loss: 5.16841 | failed:  342
batch:     145700 | loss: 5.19030 | failed:  342
batch:     145710 | loss: 5.25863 | failed:  342
batch:     145720 | loss: 4.97600 | failed:  342
batch:     145730 | loss: 4.97124 | failed:  342
batch:     145740 | loss: 5.10376 | failed:  342
batch:     145750 | loss: 5.11108 | failed:  342
batch:     145760 | loss: 4.93420 | failed:  342
batch:     145770 | loss: 5.05668 | failed:  342
batch:     145780 | loss: 5.15030 | failed:  342
batch:     145790 | loss: 5.14445 | failed:  342
batch:     145800 | loss: 4.45994 | failed:  342
batch:     145810 | loss: 4.30121 | failed:  342
batch:     145820 | loss: 5.16121 | failed:  342
batch:     145830 | loss: 5.13443 | failed:  342
batch:     145840 | loss: 4.92235 | failed:  342
batch:     145850 | loss: 5.10249 | failed:  342
batch:     145860 | loss: 4.95671 | failed:  342
batch:     145870 | loss: 4.74538 | failed:  342
batch:     145880 | loss: 4.31796 | failed:  342
batch:     145890 | loss: 5.18014 | failed:  342
batch:     145900 | loss: 5.11129 | failed:  342
batch:     145910 | loss: 5.18178 | failed:  342
batch:     145920 | loss: 5.14076 | failed:  342
batch:     145930 | loss: 5.07962 | failed:  342
batch:     145940 | loss: 5.08569 | failed:  342
batch:     145950 | loss: 5.10996 | failed:  342
batch:     145960 | loss: 5.08671 | failed:  342
batch:     145970 | loss: 4.98277 | failed:  342
batch:     145980 | loss: 5.27643 | failed:  342
batch:     145990 | loss: 5.24984 | failed:  342
batch:     146000 | loss: 5.27885 | failed:  342
batch:     146010 | loss: 5.23538 | failed:  342
batch:     146020 | loss: 5.11126 | failed:  342
batch:     146030 | loss: 4.69737 | failed:  342
batch:     146040 | loss: 5.20249 | failed:  342
batch:     146050 | loss: 4.70024 | failed:  342
batch:     146060 | loss: 5.11500 | failed:  342
batch:     146070 | loss: 5.07179 | failed:  342
batch:     146080 | loss: 5.15092 | failed:  342
batch:     146090 | loss: 5.07659 | failed:  342
batch:     146100 | loss: 5.08969 | failed:  342
batch:     146110 | loss: 5.03156 | failed:  342
batch:     146120 | loss: 5.25505 | failed:  342
batch:     146130 | loss: 5.16761 | failed:  342
batch:     146140 | loss: 5.09961 | failed:  342
batch:     146150 | loss: 5.05154 | failed:  342
batch:     146160 | loss: 5.04327 | failed:  342
batch:     146170 | loss: 5.13818 | failed:  342
batch:     146180 | loss: 4.96213 | failed:  342
batch:     146190 | loss: 5.10388 | failed:  342
batch:     146200 | loss: 5.08375 | failed:  342
batch:     146210 | loss: 5.05575 | failed:  342
batch:     146220 | loss: 5.10159 | failed:  342
batch:     146230 | loss: 5.08728 | failed:  342
batch:     146240 | loss: 5.07424 | failed:  342
batch:     146250 | loss: 5.10641 | failed:  342
batch:     146260 | loss: 5.06314 | failed:  342
batch:     146270 | loss: 5.12892 | failed:  342
batch:     146280 | loss: 5.01086 | failed:  342
batch:     146290 | loss: 5.23219 | failed:  342
batch:     146300 | loss: 5.10125 | failed:  342
batch:     146310 | loss: 5.26006 | failed:  342
batch:     146320 | loss: 5.28652 | failed:  342
batch:     146330 | loss: 5.19273 | failed:  342
batch:     146340 | loss: 5.11594 | failed:  342
batch:     146350 | loss: 5.18159 | failed:  342
batch:     146360 | loss: 5.09026 | failed:  342
batch:     146370 | loss: 4.85899 | failed:  342
batch:     146380 | loss: 5.17455 | failed:  342
batch:     146390 | loss: 4.91708 | failed:  342
batch:     146400 | loss: 5.05679 | failed:  342
batch:     146410 | loss: 4.85497 | failed:  342
batch:     146420 | loss: 5.11536 | failed:  342
batch:     146430 | loss: 5.05470 | failed:  342
batch:     146440 | loss: 5.02359 | failed:  342
batch:     146450 | loss: 5.12950 | failed:  342
batch:     146460 | loss: 5.23555 | failed:  342
batch:     146470 | loss: 5.19781 | failed:  342
batch:     146480 | loss: 5.14083 | failed:  342
batch:     146490 | loss: 4.84112 | failed:  342
batch:     146500 | loss: 5.03110 | failed:  342
batch:     146510 | loss: 5.14966 | failed:  342
batch:     146520 | loss: 5.09607 | failed:  342
batch:     146530 | loss: 5.17084 | failed:  342
batch:     146540 | loss: 5.16778 | failed:  342
batch:     146550 | loss: 5.16602 | failed:  342
batch:     146560 | loss: 4.98710 | failed:  342
batch:     146570 | loss: 5.11067 | failed:  342
batch:     146580 | loss: 5.21857 | failed:  342
batch:     146590 | loss: 5.18803 | failed:  342
batch:     146600 | loss: 5.19986 | failed:  342
batch:     146610 | loss: 5.20189 | failed:  342
batch:     146620 | loss: 5.20471 | failed:  342
batch:     146630 | loss: 5.10602 | failed:  342
batch:     146640 | loss: 5.19145 | failed:  342
batch:     146650 | loss: 5.18960 | failed:  342
batch:     146660 | loss: 5.11641 | failed:  342
batch:     146670 | loss: 5.12871 | failed:  342
batch:     146680 | loss: 4.81988 | failed:  342
batch:     146690 | loss: 5.06860 | failed:  342
batch:     146700 | loss: 5.22569 | failed:  342
batch:     146710 | loss: 5.12980 | failed:  342
batch:     146720 | loss: 5.01389 | failed:  342
batch:     146730 | loss: 5.14487 | failed:  342
batch:     146740 | loss: 5.13475 | failed:  342
batch:     146750 | loss: 4.96873 | failed:  342
batch:     146760 | loss: 5.09281 | failed:  342
batch:     146770 | loss: 5.06786 | failed:  342
batch:     146780 | loss: 5.05879 | failed:  342
batch:     146790 | loss: 5.16027 | failed:  342
batch:     146800 | loss: 5.01738 | failed:  342
batch:     146810 | loss: 5.01363 | failed:  342
batch:     146820 | loss: 5.10376 | failed:  342
batch:     146830 | loss: 5.06147 | failed:  342
batch:     146840 | loss: 5.01860 | failed:  342
batch:     146850 | loss: 5.04811 | failed:  342
batch:     146860 | loss: 4.86754 | failed:  342
batch:     146870 | loss: 5.02837 | failed:  342
batch:     146880 | loss: 5.03408 | failed:  342
batch:     146890 | loss: 5.02225 | failed:  342
batch:     146900 | loss: 5.04542 | failed:  342
batch:     146910 | loss: 5.10128 | failed:  342
batch:     146920 | loss: 5.01455 | failed:  342
batch:     146930 | loss: 5.02214 | failed:  342
batch:     146940 | loss: 5.11299 | failed:  342
batch:     146950 | loss: 5.16880 | failed:  342
batch:     146960 | loss: 5.16053 | failed:  342
batch:     146970 | loss: 5.10763 | failed:  342
batch:     146980 | loss: 5.18843 | failed:  342
batch:     146990 | loss: 5.14613 | failed:  342
batch:     147000 | loss: 5.01628 | failed:  342
batch:     147010 | loss: 5.19244 | failed:  342
batch:     147020 | loss: 5.14577 | failed:  342
batch:     147030 | loss: 4.98711 | failed:  342
batch:     147040 | loss: 4.78374 | failed:  342
batch:     147050 | loss: 5.18987 | failed:  342
batch:     147060 | loss: 5.11215 | failed:  342
batch:     147070 | loss: 4.89396 | failed:  342
batch:     147080 | loss: 4.95116 | failed:  342
batch:     147090 | loss: 5.25378 | failed:  342
batch:     147100 | loss: 5.17806 | failed:  342
batch:     147110 | loss: 5.05513 | failed:  342
batch:     147120 | loss: 4.98640 | failed:  342
batch:     147130 | loss: 4.93998 | failed:  342
batch:     147140 | loss: 5.19088 | failed:  342
batch:     147150 | loss: 5.16515 | failed:  342
batch:     147160 | loss: 5.04784 | failed:  342
batch:     147170 | loss: 5.13549 | failed:  342
batch:     147180 | loss: 5.08983 | failed:  342
batch:     147190 | loss: 5.26965 | failed:  342
batch:     147200 | loss: 5.21172 | failed:  342
batch:     147210 | loss: 4.88046 | failed:  342
batch:     147220 | loss: 5.18103 | failed:  342
batch:     147230 | loss: 5.14012 | failed:  342
batch:     147240 | loss: 5.07271 | failed:  342
batch:     147250 | loss: 5.10893 | failed:  342
batch:     147260 | loss: 5.06358 | failed:  342
batch:     147270 | loss: 5.25338 | failed:  342
batch:     147280 | loss: 5.26346 | failed:  342
batch:     147290 | loss: 5.24640 | failed:  342
batch:     147300 | loss: 5.01489 | failed:  342
batch:     147310 | loss: 5.22191 | failed:  342
batch:     147320 | loss: 5.03928 | failed:  342
batch:     147330 | loss: 5.16364 | failed:  342
batch:     147340 | loss: 5.07176 | failed:  342
batch:     147350 | loss: 5.25785 | failed:  342
batch:     147360 | loss: 5.14053 | failed:  342
batch:     147370 | loss: 5.10213 | failed:  342
batch:     147380 | loss: 5.13686 | failed:  342
batch:     147390 | loss: 5.11807 | failed:  342
batch:     147400 | loss: 4.93071 | failed:  342
batch:     147410 | loss: 5.10925 | failed:  342
batch:     147420 | loss: 5.12767 | failed:  342
batch:     147430 | loss: 5.00472 | failed:  342
batch:     147440 | loss: 5.13595 | failed:  342
batch:     147450 | loss: 4.45247 | failed:  342
batch:     147460 | loss: 5.13222 | failed:  342
batch:     147470 | loss: 5.13130 | failed:  342
batch:     147480 | loss: 4.97288 | failed:  342
batch:     147490 | loss: 5.06810 | failed:  342
batch:     147500 | loss: 5.25004 | failed:  342
batch:     147510 | loss: 5.11576 | failed:  342
batch:     147520 | loss: 5.21859 | failed:  342
batch:     147530 | loss: 5.24962 | failed:  342
batch:     147540 | loss: 5.10564 | failed:  342
batch:     147550 | loss: 4.97449 | failed:  342
batch:     147560 | loss: 5.20291 | failed:  342
batch:     147570 | loss: 5.37668 | failed:  342
batch:     147580 | loss: 5.02644 | failed:  342
batch:     147590 | loss: 5.10008 | failed:  342
batch:     147600 | loss: 5.19809 | failed:  342
batch:     147610 | loss: 4.92059 | failed:  342
batch:     147620 | loss: 5.08797 | failed:  342
batch:     147630 | loss: 5.07455 | failed:  342
batch:     147640 | loss: 5.08783 | failed:  342
batch:     147650 | loss: 4.94903 | failed:  342
batch:     147660 | loss: 5.00398 | failed:  342
batch:     147670 | loss: 4.80848 | failed:  342
batch:     147680 | loss: 5.26320 | failed:  342
batch:     147690 | loss: 5.19992 | failed:  342
batch:     147700 | loss: 5.07026 | failed:  342
batch:     147710 | loss: 5.18380 | failed:  342
batch:     147720 | loss: 5.07104 | failed:  342
batch:     147730 | loss: 4.96545 | failed:  342
batch:     147740 | loss: 5.22520 | failed:  342
batch:     147750 | loss: 5.09996 | failed:  342
batch:     147760 | loss: 5.20704 | failed:  342
batch:     147770 | loss: 5.15992 | failed:  342
batch:     147780 | loss: 5.08828 | failed:  342
batch:     147790 | loss: 5.12405 | failed:  342
batch:     147800 | loss: 5.14090 | failed:  342
batch:     147810 | loss: 5.11481 | failed:  342
batch:     147820 | loss: 5.22958 | failed:  342
batch:     147830 | loss: 5.18770 | failed:  342
batch:     147840 | loss: 5.12849 | failed:  342
batch:     147850 | loss: 5.10807 | failed:  342
batch:     147860 | loss: 5.04881 | failed:  342
batch:     147870 | loss: 5.14499 | failed:  342
batch:     147880 | loss: 4.99674 | failed:  342
batch:     147890 | loss: 5.23495 | failed:  342
batch:     147900 | loss: 5.09051 | failed:  342
batch:     147910 | loss: 5.16532 | failed:  342
batch:     147920 | loss: 5.12612 | failed:  342
batch:     147930 | loss: 5.23932 | failed:  342
batch:     147940 | loss: 5.18297 | failed:  342
batch:     147950 | loss: 5.16166 | failed:  342
batch:     147960 | loss: 5.21406 | failed:  342
batch:     147970 | loss: 5.18292 | failed:  342
batch:     147980 | loss: 4.99047 | failed:  342
batch:     147990 | loss: 5.22005 | failed:  342
batch:     148000 | loss: 5.20178 | failed:  342
batch:     148010 | loss: 5.25171 | failed:  342
batch:     148020 | loss: 5.07085 | failed:  342
batch:     148030 | loss: 4.97701 | failed:  342
batch:     148040 | loss: 5.04939 | failed:  342
batch:     148050 | loss: 5.18177 | failed:  342
batch:     148060 | loss: 4.99520 | failed:  342
batch:     148070 | loss: 5.20036 | failed:  342
batch:     148080 | loss: 5.16848 | failed:  342
batch:     148090 | loss: 5.17300 | failed:  342
batch:     148100 | loss: 5.23293 | failed:  342
batch:     148110 | loss: 5.23269 | failed:  342
batch:     148120 | loss: 5.07103 | failed:  342
batch:     148130 | loss: 5.11422 | failed:  342
batch:     148140 | loss: 5.14398 | failed:  342
batch:     148150 | loss: 5.10668 | failed:  342
batch:     148160 | loss: 4.56370 | failed:  342
batch:     148170 | loss: 4.37844 | failed:  342
batch:     148180 | loss: 5.11502 | failed:  342
batch:     148190 | loss: 5.15786 | failed:  342
batch:     148200 | loss: 5.22948 | failed:  342
batch:     148210 | loss: 5.21590 | failed:  342
batch:     148220 | loss: 5.29016 | failed:  342
batch:     148230 | loss: 5.16522 | failed:  342
batch:     148240 | loss: 5.19289 | failed:  342
batch:     148250 | loss: 5.20474 | failed:  342
batch:     148260 | loss: 5.10300 | failed:  342
batch:     148270 | loss: 5.11363 | failed:  342
batch:     148280 | loss: 5.03319 | failed:  342
batch:     148290 | loss: 5.26037 | failed:  342
batch:     148300 | loss: 4.28936 | failed:  342
batch:     148310 | loss: 5.12786 | failed:  342
batch:     148320 | loss: 5.15594 | failed:  342
batch:     148330 | loss: 5.24631 | failed:  342
batch:     148340 | loss: 5.15680 | failed:  342
batch:     148350 | loss: 5.10155 | failed:  342
batch:     148360 | loss: 4.84482 | failed:  342
batch:     148370 | loss: 5.21187 | failed:  342
batch:     148380 | loss: 5.10905 | failed:  342
batch:     148390 | loss: 5.15079 | failed:  342
batch:     148400 | loss: 5.26686 | failed:  342
batch:     148410 | loss: 5.06216 | failed:  342
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     148420 | loss: 5.25839 | failed:  348
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     148430 | loss: 5.04955 | failed:  350
batch:     148440 | loss: 5.10956 | failed:  350
batch:     148450 | loss: 5.01697 | failed:  350
batch:     148460 | loss: 5.09220 | failed:  350
batch:     148470 | loss: 5.13406 | failed:  350
batch:     148480 | loss: 5.26732 | failed:  350
batch:     148490 | loss: 5.19728 | failed:  350
batch:     148500 | loss: 5.13140 | failed:  350
batch:     148510 | loss: 5.03740 | failed:  350
batch:     148520 | loss: 5.02825 | failed:  350
batch:     148530 | loss: 5.29170 | failed:  350
batch:     148540 | loss: 5.09114 | failed:  350
batch:     148550 | loss: 5.22318 | failed:  350
batch:     148560 | loss: 5.06186 | failed:  350
batch:     148570 | loss: 4.96280 | failed:  350
batch:     148580 | loss: 5.11527 | failed:  350
batch:     148590 | loss: 5.01219 | failed:  350
batch:     148600 | loss: 4.96180 | failed:  350
batch:     148610 | loss: 5.15284 | failed:  350
batch:     148620 | loss: 5.03774 | failed:  350
batch:     148630 | loss: 4.98505 | failed:  350
batch:     148640 | loss: 5.03101 | failed:  350
batch:     148650 | loss: 5.21470 | failed:  350
batch:     148660 | loss: 5.16263 | failed:  350
batch:     148670 | loss: 5.16351 | failed:  350
batch:     148680 | loss: 5.20659 | failed:  350
batch:     148690 | loss: 4.86274 | failed:  350
batch:     148700 | loss: 4.62752 | failed:  350
batch:     148710 | loss: 4.63958 | failed:  350
batch:     148720 | loss: 5.14062 | failed:  350
batch:     148730 | loss: 5.30716 | failed:  350
batch:     148740 | loss: 5.21092 | failed:  350
batch:     148750 | loss: 4.72088 | failed:  350
batch:     148760 | loss: 3.88848 | failed:  350
batch:     148770 | loss: 5.15080 | failed:  350
batch:     148780 | loss: 4.90487 | failed:  350
batch:     148790 | loss: 5.03507 | failed:  350
batch:     148800 | loss: 5.05170 | failed:  350
batch:     148810 | loss: 5.08951 | failed:  350
batch:     148820 | loss: 5.13591 | failed:  350
batch:     148830 | loss: 5.21233 | failed:  350
batch:     148840 | loss: 5.15575 | failed:  350
batch:     148850 | loss: 5.16446 | failed:  350
batch:     148860 | loss: 5.13522 | failed:  350
batch:     148870 | loss: 5.19303 | failed:  350
batch:     148880 | loss: 4.91843 | failed:  350
batch:     148890 | loss: 5.08852 | failed:  350
batch:     148900 | loss: 5.09229 | failed:  350
batch:     148910 | loss: 5.18755 | failed:  350
batch:     148920 | loss: 5.17970 | failed:  350
batch:     148930 | loss: 4.77712 | failed:  350
batch:     148940 | loss: 5.08317 | failed:  350
batch:     148950 | loss: 5.19939 | failed:  350
batch:     148960 | loss: 5.17248 | failed:  350
batch:     148970 | loss: 5.19148 | failed:  350
batch:     148980 | loss: 5.15707 | failed:  350
batch:     148990 | loss: 5.20643 | failed:  350
batch:     149000 | loss: 5.21829 | failed:  350
batch:     149010 | loss: 5.15651 | failed:  350
batch:     149020 | loss: 5.15179 | failed:  350
batch:     149030 | loss: 4.87579 | failed:  350
batch:     149040 | loss: 5.27734 | failed:  350
batch:     149050 | loss: 5.18633 | failed:  350
batch:     149060 | loss: 5.14384 | failed:  350
batch:     149070 | loss: 4.99791 | failed:  350
batch:     149080 | loss: 5.17435 | failed:  350
batch:     149090 | loss: 5.22554 | failed:  350
batch:     149100 | loss: 5.05092 | failed:  350
batch:     149110 | loss: 5.14889 | failed:  350
batch:     149120 | loss: 4.98201 | failed:  350
batch:     149130 | loss: 5.23358 | failed:  350
batch:     149140 | loss: 5.21298 | failed:  350
batch:     149150 | loss: 5.11051 | failed:  350
batch:     149160 | loss: 5.11353 | failed:  350
batch:     149170 | loss: 5.20549 | failed:  350
batch:     149180 | loss: 5.20840 | failed:  350
batch:     149190 | loss: 5.15888 | failed:  350
batch:     149200 | loss: 5.12849 | failed:  350
batch:     149210 | loss: 5.16863 | failed:  350
batch:     149220 | loss: 5.17650 | failed:  350
batch:     149230 | loss: 5.16193 | failed:  350
batch:     149240 | loss: 5.17000 | failed:  350
batch:     149250 | loss: 5.17583 | failed:  350
batch:     149260 | loss: 5.16796 | failed:  350
batch:     149270 | loss: 5.01476 | failed:  350
batch:     149280 | loss: 5.10613 | failed:  350
batch:     149290 | loss: 4.98820 | failed:  350
batch:     149300 | loss: 5.17648 | failed:  350
batch:     149310 | loss: 5.14810 | failed:  350
batch:     149320 | loss: 5.16165 | failed:  350
batch:     149330 | loss: 5.11830 | failed:  350
batch:     149340 | loss: 5.22156 | failed:  350
batch:     149350 | loss: 5.27443 | failed:  350
batch:     149360 | loss: 5.15076 | failed:  350
batch:     149370 | loss: 5.24729 | failed:  350
batch:     149380 | loss: 5.22004 | failed:  350
batch:     149390 | loss: 5.18343 | failed:  350
batch:     149400 | loss: 5.14947 | failed:  350
batch:     149410 | loss: 5.10512 | failed:  350
batch:     149420 | loss: 5.12413 | failed:  350
batch:     149430 | loss: 4.91399 | failed:  350
batch:     149440 | loss: 5.14111 | failed:  350
batch:     149450 | loss: 5.07651 | failed:  350
batch:     149460 | loss: 5.14761 | failed:  350
batch:     149470 | loss: 5.14142 | failed:  350
batch:     149480 | loss: 5.02053 | failed:  350
batch:     149490 | loss: 5.18523 | failed:  350
batch:     149500 | loss: 5.15128 | failed:  350
batch:     149510 | loss: 4.83071 | failed:  350
batch:     149520 | loss: 4.97870 | failed:  350
batch:     149530 | loss: 4.92094 | failed:  350
batch:     149540 | loss: 5.21337 | failed:  350
batch:     149550 | loss: 5.04382 | failed:  350
batch:     149560 | loss: 5.00319 | failed:  350
batch:     149570 | loss: 5.12860 | failed:  350
batch:     149580 | loss: 4.98604 | failed:  350
batch:     149590 | loss: 5.04178 | failed:  350
batch:     149600 | loss: 5.13123 | failed:  350
batch:     149610 | loss: 5.22961 | failed:  350
batch:     149620 | loss: 5.02807 | failed:  350
batch:     149630 | loss: 4.85885 | failed:  350
batch:     149640 | loss: 5.05908 | failed:  350
batch:     149650 | loss: 5.07595 | failed:  350
batch:     149660 | loss: 4.92642 | failed:  350
batch:     149670 | loss: 5.32858 | failed:  350
batch:     149680 | loss: 5.07154 | failed:  350
batch:     149690 | loss: 5.25058 | failed:  350
batch:     149700 | loss: 5.13835 | failed:  350
batch:     149710 | loss: 5.10573 | failed:  350
batch:     149720 | loss: 5.21900 | failed:  350
batch:     149730 | loss: 5.16719 | failed:  350
batch:     149740 | loss: 5.14138 | failed:  350
batch:     149750 | loss: 4.92496 | failed:  350
batch:     149760 | loss: 5.18065 | failed:  350
batch:     149770 | loss: 5.22000 | failed:  350
batch:     149780 | loss: 4.97094 | failed:  350
batch:     149790 | loss: 5.21653 | failed:  350
batch:     149800 | loss: 5.11618 | failed:  350
batch:     149810 | loss: 5.20099 | failed:  350
batch:     149820 | loss: 5.10784 | failed:  350
batch:     149830 | loss: 5.22430 | failed:  350
batch:     149840 | loss: 5.13013 | failed:  350
batch:     149850 | loss: 5.19425 | failed:  350
batch:     149860 | loss: 5.06415 | failed:  350
batch:     149870 | loss: 5.20167 | failed:  350
batch:     149880 | loss: 5.12114 | failed:  350
batch:     149890 | loss: 5.16812 | failed:  350
batch:     149900 | loss: 5.01004 | failed:  350
batch:     149910 | loss: 5.04859 | failed:  350
batch:     149920 | loss: 5.05641 | failed:  350
batch:     149930 | loss: 5.14235 | failed:  350
batch:     149940 | loss: 5.08286 | failed:  350
batch:     149950 | loss: 5.09456 | failed:  350
batch:     149960 | loss: 5.14690 | failed:  350
batch:     149970 | loss: 5.26292 | failed:  350
batch:     149980 | loss: 5.25287 | failed:  350
batch:     149990 | loss: 5.23851 | failed:  350
batch:     150000 | loss: 5.21206 | failed:  350
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     150010 | loss: 5.21891 | failed:  350
batch:     150020 | loss: 5.22477 | failed:  350
batch:     150030 | loss: 5.07932 | failed:  350
batch:     150040 | loss: 5.16284 | failed:  350
batch:     150050 | loss: 5.11114 | failed:  350
batch:     150060 | loss: 5.06002 | failed:  350
batch:     150070 | loss: 4.87353 | failed:  350
batch:     150080 | loss: 5.07067 | failed:  350
batch:     150090 | loss: 5.01789 | failed:  350
batch:     150100 | loss: 5.14183 | failed:  350
batch:     150110 | loss: 5.12202 | failed:  350
batch:     150120 | loss: 5.16585 | failed:  350
batch:     150130 | loss: 5.00001 | failed:  350
batch:     150140 | loss: 5.27381 | failed:  350
batch:     150150 | loss: 5.08233 | failed:  350
batch:     150160 | loss: 5.06817 | failed:  350
batch:     150170 | loss: 5.20732 | failed:  350
batch:     150180 | loss: 5.18830 | failed:  350
batch:     150190 | loss: 5.22224 | failed:  350
batch:     150200 | loss: 5.09987 | failed:  350
batch:     150210 | loss: 5.13144 | failed:  350
batch:     150220 | loss: 5.08037 | failed:  350
batch:     150230 | loss: 5.19175 | failed:  350
batch:     150240 | loss: 5.20542 | failed:  350
batch:     150250 | loss: 4.99207 | failed:  350
batch:     150260 | loss: 5.02534 | failed:  350
batch:     150270 | loss: 5.13212 | failed:  350
batch:     150280 | loss: 4.97518 | failed:  350
batch:     150290 | loss: 5.11066 | failed:  350
batch:     150300 | loss: 4.98649 | failed:  350
batch:     150310 | loss: 4.96364 | failed:  350
batch:     150320 | loss: 4.90167 | failed:  350
batch:     150330 | loss: 5.19141 | failed:  350
batch:     150340 | loss: 5.03655 | failed:  350
batch:     150350 | loss: 5.11021 | failed:  350
batch:     150360 | loss: 5.17308 | failed:  350
batch:     150370 | loss: 5.05737 | failed:  350
batch:     150380 | loss: 5.13769 | failed:  350
batch:     150390 | loss: 5.14814 | failed:  350
batch:     150400 | loss: 5.13239 | failed:  350
batch:     150410 | loss: 5.12957 | failed:  350
batch:     150420 | loss: 5.16652 | failed:  350
batch:     150430 | loss: 5.18852 | failed:  350
batch:     150440 | loss: 5.06584 | failed:  350
batch:     150450 | loss: 5.07799 | failed:  350
batch:     150460 | loss: 5.09406 | failed:  350
batch:     150470 | loss: 5.19353 | failed:  350
batch:     150480 | loss: 4.96516 | failed:  350
batch:     150490 | loss: 5.19705 | failed:  350
batch:     150500 | loss: 5.19585 | failed:  350
batch:     150510 | loss: 4.86756 | failed:  350
batch:     150520 | loss: 4.70659 | failed:  350
batch:     150530 | loss: 5.02908 | failed:  350
batch:     150540 | loss: 5.07781 | failed:  350
batch:     150550 | loss: 5.05271 | failed:  350
batch:     150560 | loss: 5.22906 | failed:  350
batch:     150570 | loss: 5.13105 | failed:  350
batch:     150580 | loss: 4.89093 | failed:  350
batch:     150590 | loss: 5.23502 | failed:  350
batch:     150600 | loss: 5.01165 | failed:  350
batch:     150610 | loss: 5.11594 | failed:  350
batch:     150620 | loss: 4.75870 | failed:  350
batch:     150630 | loss: 5.21963 | failed:  350
batch:     150640 | loss: 5.20578 | failed:  350
batch:     150650 | loss: 5.07611 | failed:  350
batch:     150660 | loss: 5.20969 | failed:  350
batch:     150670 | loss: 5.07845 | failed:  350
batch:     150680 | loss: 5.21890 | failed:  350
batch:     150690 | loss: 5.20565 | failed:  350
batch:     150700 | loss: 4.98465 | failed:  350
batch:     150710 | loss: 4.97046 | failed:  350
batch:     150720 | loss: 5.04965 | failed:  350
batch:     150730 | loss: 5.06686 | failed:  350
batch:     150740 | loss: 5.05262 | failed:  350
batch:     150750 | loss: 5.10209 | failed:  350
batch:     150760 | loss: 5.16408 | failed:  350
batch:     150770 | loss: 5.18895 | failed:  350
batch:     150780 | loss: 5.08359 | failed:  350
batch:     150790 | loss: 5.16357 | failed:  350
batch:     150800 | loss: 5.13162 | failed:  350
batch:     150810 | loss: 5.00162 | failed:  350
batch:     150820 | loss: 5.03810 | failed:  350
batch:     150830 | loss: 5.20808 | failed:  350
batch:     150840 | loss: 5.21760 | failed:  350
batch:     150850 | loss: 5.15676 | failed:  350
batch:     150860 | loss: 5.02588 | failed:  350
batch:     150870 | loss: 5.05950 | failed:  350
batch:     150880 | loss: 5.17749 | failed:  350
batch:     150890 | loss: 5.17146 | failed:  350
batch:     150900 | loss: 5.10388 | failed:  350
batch:     150910 | loss: 5.08623 | failed:  350
batch:     150920 | loss: 5.09523 | failed:  350
batch:     150930 | loss: 5.07220 | failed:  350
batch:     150940 | loss: 5.10510 | failed:  350
batch:     150950 | loss: 5.17580 | failed:  350
batch:     150960 | loss: 5.14371 | failed:  350
batch:     150970 | loss: 5.06046 | failed:  350
batch:     150980 | loss: 5.06968 | failed:  350
batch:     150990 | loss: 5.14270 | failed:  350
batch:     151000 | loss: 5.08825 | failed:  350
batch:     151010 | loss: 5.05999 | failed:  350
batch:     151020 | loss: 5.29964 | failed:  350
batch:     151030 | loss: 5.15078 | failed:  350
batch:     151040 | loss: 4.97903 | failed:  350
batch:     151050 | loss: 5.14662 | failed:  350
batch:     151060 | loss: 5.16872 | failed:  350
batch:     151070 | loss: 5.10311 | failed:  350
batch:     151080 | loss: 5.14899 | failed:  350
batch:     151090 | loss: 5.14671 | failed:  350
batch:     151100 | loss: 5.14970 | failed:  350
batch:     151110 | loss: 5.21173 | failed:  350
batch:     151120 | loss: 5.16283 | failed:  350
batch:     151130 | loss: 5.13298 | failed:  350
batch:     151140 | loss: 5.13287 | failed:  350
batch:     151150 | loss: 5.08851 | failed:  350
batch:     151160 | loss: 5.08870 | failed:  350
batch:     151170 | loss: 5.16295 | failed:  350
batch:     151180 | loss: 5.07640 | failed:  350
batch:     151190 | loss: 5.16279 | failed:  350
batch:     151200 | loss: 5.08307 | failed:  350
batch:     151210 | loss: 5.03723 | failed:  350
batch:     151220 | loss: 5.08507 | failed:  350
batch:     151230 | loss: 4.91895 | failed:  350
batch:     151240 | loss: 5.17920 | failed:  350
batch:     151250 | loss: 4.99204 | failed:  350
batch:     151260 | loss: 5.15053 | failed:  350
batch:     151270 | loss: 4.93205 | failed:  350
batch:     151280 | loss: 5.16038 | failed:  350
batch:     151290 | loss: 5.10481 | failed:  350
batch:     151300 | loss: 5.09282 | failed:  350
batch:     151310 | loss: 5.03969 | failed:  350
batch:     151320 | loss: 4.26834 | failed:  350
batch:     151330 | loss: 5.20555 | failed:  350
batch:     151340 | loss: 5.10669 | failed:  350
batch:     151350 | loss: 5.23063 | failed:  350
batch:     151360 | loss: 5.21612 | failed:  350
batch:     151370 | loss: 5.18834 | failed:  350
batch:     151380 | loss: 5.19228 | failed:  350
batch:     151390 | loss: 5.15259 | failed:  350
batch:     151400 | loss: 5.14402 | failed:  350
batch:     151410 | loss: 5.07854 | failed:  350
batch:     151420 | loss: 5.07804 | failed:  350
batch:     151430 | loss: 5.06087 | failed:  350
batch:     151440 | loss: 5.07110 | failed:  350
batch:     151450 | loss: 5.07564 | failed:  350
batch:     151460 | loss: 5.20975 | failed:  350
batch:     151470 | loss: 5.14305 | failed:  350
batch:     151480 | loss: 5.02184 | failed:  350
batch:     151490 | loss: 5.05059 | failed:  350
batch:     151500 | loss: 5.07463 | failed:  350
batch:     151510 | loss: 5.04819 | failed:  350
batch:     151520 | loss: 5.20463 | failed:  350
batch:     151530 | loss: 5.07919 | failed:  350
batch:     151540 | loss: 5.16430 | failed:  350
batch:     151550 | loss: 5.22425 | failed:  350
batch:     151560 | loss: 5.18619 | failed:  350
batch:     151570 | loss: 4.88049 | failed:  350
batch:     151580 | loss: 5.17044 | failed:  350
batch:     151590 | loss: 5.11196 | failed:  350
batch:     151600 | loss: 5.25276 | failed:  350
batch:     151610 | loss: 5.07441 | failed:  350
batch:     151620 | loss: 4.93005 | failed:  350
batch:     151630 | loss: 5.05702 | failed:  350
batch:     151640 | loss: 5.14878 | failed:  350
batch:     151650 | loss: 5.08270 | failed:  350
batch:     151660 | loss: 5.10117 | failed:  350
batch:     151670 | loss: 5.04725 | failed:  350
batch:     151680 | loss: 5.04178 | failed:  350
batch:     151690 | loss: 5.17251 | failed:  350
batch:     151700 | loss: 4.84139 | failed:  350
batch:     151710 | loss: 4.98621 | failed:  350
batch:     151720 | loss: 5.11217 | failed:  350
batch:     151730 | loss: 4.87548 | failed:  350
batch:     151740 | loss: 5.05803 | failed:  350
batch:     151750 | loss: 5.07057 | failed:  350
batch:     151760 | loss: 5.11172 | failed:  350
batch:     151770 | loss: 5.19269 | failed:  350
batch:     151780 | loss: 5.13629 | failed:  350
batch:     151790 | loss: 5.03834 | failed:  350
batch:     151800 | loss: 4.91160 | failed:  350
batch:     151810 | loss: 5.10874 | failed:  350
batch:     151820 | loss: 5.18581 | failed:  350
batch:     151830 | loss: 5.12266 | failed:  350
batch:     151840 | loss: 5.06098 | failed:  350
batch:     151850 | loss: 5.06213 | failed:  350
batch:     151860 | loss: 4.96127 | failed:  350
batch:     151870 | loss: 4.98784 | failed:  350
batch:     151880 | loss: 4.80593 | failed:  350
batch:     151890 | loss: 5.05190 | failed:  350
batch:     151900 | loss: 5.23219 | failed:  350
batch:     151910 | loss: 5.24802 | failed:  350
batch:     151920 | loss: 5.13213 | failed:  350
batch:     151930 | loss: 5.10673 | failed:  350
batch:     151940 | loss: 5.14083 | failed:  350
batch:     151950 | loss: 5.12069 | failed:  350
batch:     151960 | loss: 5.18290 | failed:  350
batch:     151970 | loss: 5.20128 | failed:  350
batch:     151980 | loss: 4.84749 | failed:  350
batch:     151990 | loss: 5.06493 | failed:  350
batch:     152000 | loss: 5.05441 | failed:  350
batch:     152010 | loss: 4.78059 | failed:  350
batch:     152020 | loss: 5.07516 | failed:  350
batch:     152030 | loss: 5.08330 | failed:  350
batch:     152040 | loss: 5.10474 | failed:  350
batch:     152050 | loss: 5.18880 | failed:  350
batch:     152060 | loss: 5.24537 | failed:  350
batch:     152070 | loss: 5.04658 | failed:  350
batch:     152080 | loss: 5.10043 | failed:  350
batch:     152090 | loss: 5.10253 | failed:  350
batch:     152100 | loss: 5.16127 | failed:  350
batch:     152110 | loss: 5.18094 | failed:  350
batch:     152120 | loss: 5.37564 | failed:  350
batch:     152130 | loss: 5.22266 | failed:  350
batch:     152140 | loss: 5.08382 | failed:  350
batch:     152150 | loss: 5.09054 | failed:  350
batch:     152160 | loss: 5.08319 | failed:  350
batch:     152170 | loss: 5.20049 | failed:  350
batch:     152180 | loss: 5.03302 | failed:  350
batch:     152190 | loss: 5.12373 | failed:  350
batch:     152200 | loss: 5.15059 | failed:  350
batch:     152210 | loss: 5.07694 | failed:  350
batch:     152220 | loss: 5.07218 | failed:  350
batch:     152230 | loss: 5.21962 | failed:  350
batch:     152240 | loss: 5.11902 | failed:  350
batch:     152250 | loss: 4.91619 | failed:  350
batch:     152260 | loss: 5.06391 | failed:  350
batch:     152270 | loss: 5.18980 | failed:  350
batch:     152280 | loss: 5.06275 | failed:  350
batch:     152290 | loss: 5.11294 | failed:  350
batch:     152300 | loss: 4.98112 | failed:  350
batch:     152310 | loss: 5.38023 | failed:  350
batch:     152320 | loss: 5.27183 | failed:  350
batch:     152330 | loss: 5.19484 | failed:  350
batch:     152340 | loss: 4.78944 | failed:  350
batch:     152350 | loss: 5.18549 | failed:  350
batch:     152360 | loss: 5.17490 | failed:  350
batch:     152370 | loss: 5.21528 | failed:  350
batch:     152380 | loss: 5.06513 | failed:  350
batch:     152390 | loss: 4.99029 | failed:  350
batch:     152400 | loss: 5.04238 | failed:  350
batch:     152410 | loss: 5.13510 | failed:  350
batch:     152420 | loss: 5.06499 | failed:  350
batch:     152430 | loss: 5.16051 | failed:  350
batch:     152440 | loss: 5.03677 | failed:  350
batch:     152450 | loss: 5.02948 | failed:  350
batch:     152460 | loss: 5.09151 | failed:  350
batch:     152470 | loss: 4.89218 | failed:  350
batch:     152480 | loss: 4.91212 | failed:  350
batch:     152490 | loss: 5.00520 | failed:  350
batch:     152500 | loss: 5.12433 | failed:  350
batch:     152510 | loss: 5.01803 | failed:  350
batch:     152520 | loss: 5.13692 | failed:  350
batch:     152530 | loss: 5.20598 | failed:  350
batch:     152540 | loss: 5.16774 | failed:  350
batch:     152550 | loss: 4.99592 | failed:  350
batch:     152560 | loss: 5.11323 | failed:  350
batch:     152570 | loss: 5.12853 | failed:  350
batch:     152580 | loss: 5.18013 | failed:  350
batch:     152590 | loss: 5.05961 | failed:  350
batch:     152600 | loss: 5.05164 | failed:  350
batch:     152610 | loss: 5.17344 | failed:  350
batch:     152620 | loss: 5.01313 | failed:  350
batch:     152630 | loss: 5.16560 | failed:  350
batch:     152640 | loss: 5.17706 | failed:  350
batch:     152650 | loss: 5.21862 | failed:  350
batch:     152660 | loss: 5.22248 | failed:  350
batch:     152670 | loss: 5.13570 | failed:  350
batch:     152680 | loss: 5.13365 | failed:  350
batch:     152690 | loss: 4.84972 | failed:  350
batch:     152700 | loss: 5.11169 | failed:  350
batch:     152710 | loss: 5.15543 | failed:  350
batch:     152720 | loss: 5.16150 | failed:  350
batch:     152730 | loss: 5.13472 | failed:  350
batch:     152740 | loss: 5.19366 | failed:  350
batch:     152750 | loss: 5.06101 | failed:  350
batch:     152760 | loss: 4.94502 | failed:  350
batch:     152770 | loss: 5.07614 | failed:  350
batch:     152780 | loss: 5.15380 | failed:  350
batch:     152790 | loss: 5.24475 | failed:  350
batch:     152800 | loss: 5.08277 | failed:  350
batch:     152810 | loss: 5.12515 | failed:  350
batch:     152820 | loss: 5.16310 | failed:  350
batch:     152830 | loss: 5.12804 | failed:  350
batch:     152840 | loss: 5.12977 | failed:  350
batch:     152850 | loss: 5.01608 | failed:  350
batch:     152860 | loss: 5.21188 | failed:  350
batch:     152870 | loss: 5.18943 | failed:  350
batch:     152880 | loss: 5.18633 | failed:  350
batch:     152890 | loss: 5.02214 | failed:  350
batch:     152900 | loss: 5.18360 | failed:  350
batch:     152910 | loss: 5.37634 | failed:  350
batch:     152920 | loss: 5.30919 | failed:  350
batch:     152930 | loss: 5.26318 | failed:  350
batch:     152940 | loss: 5.18742 | failed:  350
batch:     152950 | loss: 5.12471 | failed:  350
batch:     152960 | loss: 5.13546 | failed:  350
batch:     152970 | loss: 5.09709 | failed:  350
batch:     152980 | loss: 5.03518 | failed:  350
batch:     152990 | loss: 5.12310 | failed:  350
batch:     153000 | loss: 5.08252 | failed:  350
batch:     153010 | loss: 5.01218 | failed:  350
batch:     153020 | loss: 5.04797 | failed:  350
batch:     153030 | loss: 5.18483 | failed:  350
batch:     153040 | loss: 5.13892 | failed:  350
batch:     153050 | loss: 5.03781 | failed:  350
batch:     153060 | loss: 5.15107 | failed:  350
batch:     153070 | loss: 5.29354 | failed:  350
batch:     153080 | loss: 5.14905 | failed:  350
batch:     153090 | loss: 5.13471 | failed:  350
batch:     153100 | loss: 5.05917 | failed:  350
batch:     153110 | loss: 5.12713 | failed:  350
batch:     153120 | loss: 5.17785 | failed:  350
batch:     153130 | loss: 5.08736 | failed:  350
batch:     153140 | loss: 5.07099 | failed:  350
batch:     153150 | loss: 5.11512 | failed:  350
batch:     153160 | loss: 5.16237 | failed:  350
batch:     153170 | loss: 5.03385 | failed:  350
batch:     153180 | loss: 5.13177 | failed:  350
batch:     153190 | loss: 4.68154 | failed:  350
batch:     153200 | loss: 5.00464 | failed:  350
batch:     153210 | loss: 5.18824 | failed:  350
batch:     153220 | loss: 5.05118 | failed:  350
batch:     153230 | loss: 5.01631 | failed:  350
batch:     153240 | loss: 5.09114 | failed:  350
batch:     153250 | loss: 5.10304 | failed:  350
batch:     153260 | loss: 5.86436 | failed:  350
batch:     153270 | loss: 5.08544 | failed:  350
batch:     153280 | loss: 5.14051 | failed:  350
batch:     153290 | loss: 5.14782 | failed:  350
batch:     153300 | loss: 5.03841 | failed:  350
batch:     153310 | loss: 5.08220 | failed:  350
batch:     153320 | loss: 5.12056 | failed:  350
batch:     153330 | loss: 5.14158 | failed:  350
batch:     153340 | loss: 5.04882 | failed:  350
batch:     153350 | loss: 5.14210 | failed:  350
batch:     153360 | loss: 5.06737 | failed:  350
batch:     153370 | loss: 5.19782 | failed:  350
batch:     153380 | loss: 5.05236 | failed:  350
batch:     153390 | loss: 5.07258 | failed:  350
batch:     153400 | loss: 5.00022 | failed:  350
batch:     153410 | loss: 5.01253 | failed:  350
batch:     153420 | loss: 5.10685 | failed:  350
batch:     153430 | loss: 5.07685 | failed:  350
batch:     153440 | loss: 5.15814 | failed:  350
batch:     153450 | loss: 5.12307 | failed:  350
batch:     153460 | loss: 5.15153 | failed:  350
batch:     153470 | loss: 5.01585 | failed:  350
batch:     153480 | loss: 5.15506 | failed:  350
batch:     153490 | loss: 5.08889 | failed:  350
batch:     153500 | loss: 5.15388 | failed:  350
batch:     153510 | loss: 5.05628 | failed:  350
batch:     153520 | loss: 5.24516 | failed:  350
batch:     153530 | loss: 5.00245 | failed:  350
batch:     153540 | loss: 5.22977 | failed:  350
batch:     153550 | loss: 5.32724 | failed:  350
batch:     153560 | loss: 5.07792 | failed:  350
batch:     153570 | loss: 5.17402 | failed:  350
batch:     153580 | loss: 4.91511 | failed:  350
batch:     153590 | loss: 5.16397 | failed:  350
batch:     153600 | loss: 4.97837 | failed:  350
batch:     153610 | loss: 5.22454 | failed:  350
batch:     153620 | loss: 4.41197 | failed:  350
batch:     153630 | loss: 4.73217 | failed:  350
batch:     153640 | loss: 4.98594 | failed:  350
batch:     153650 | loss: 5.18413 | failed:  350
batch:     153660 | loss: 5.24857 | failed:  350
batch:     153670 | loss: 5.05454 | failed:  350
batch:     153680 | loss: 5.09874 | failed:  350
batch:     153690 | loss: 5.11614 | failed:  350
batch:     153700 | loss: 5.11117 | failed:  350
batch:     153710 | loss: 5.09854 | failed:  350
batch:     153720 | loss: 5.15816 | failed:  350
batch:     153730 | loss: 5.12615 | failed:  350
batch:     153740 | loss: 5.20815 | failed:  350
batch:     153750 | loss: 5.24674 | failed:  350
batch:     153760 | loss: 5.15887 | failed:  350
batch:     153770 | loss: 5.10857 | failed:  350
batch:     153780 | loss: 5.15367 | failed:  350
batch:     153790 | loss: 5.15957 | failed:  350
batch:     153800 | loss: 4.81344 | failed:  350
batch:     153810 | loss: 5.12090 | failed:  350
batch:     153820 | loss: 5.10254 | failed:  350
batch:     153830 | loss: 5.02640 | failed:  350
batch:     153840 | loss: 5.11506 | failed:  350
batch:     153850 | loss: 5.22192 | failed:  350
batch:     153860 | loss: 5.18272 | failed:  350
batch:     153870 | loss: 5.20746 | failed:  350
batch:     153880 | loss: 4.23274 | failed:  350
batch:     153890 | loss: 5.13403 | failed:  350
batch:     153900 | loss: 5.10373 | failed:  350
batch:     153910 | loss: 5.19645 | failed:  350
batch:     153920 | loss: 5.14322 | failed:  350
batch:     153930 | loss: 5.08899 | failed:  350
batch:     153940 | loss: 5.04791 | failed:  350
batch:     153950 | loss: 4.98771 | failed:  350
batch:     153960 | loss: 5.10189 | failed:  350
batch:     153970 | loss: 5.08157 | failed:  350
batch:     153980 | loss: 5.29696 | failed:  350
batch:     153990 | loss: 5.13719 | failed:  350
batch:     154000 | loss: 4.99098 | failed:  350
batch:     154010 | loss: 5.03610 | failed:  350
batch:     154020 | loss: 4.86402 | failed:  350
batch:     154030 | loss: 4.87557 | failed:  350
batch:     154040 | loss: 5.18525 | failed:  350
batch:     154050 | loss: 5.14743 | failed:  350
batch:     154060 | loss: 5.15464 | failed:  350
batch:     154070 | loss: 5.20364 | failed:  350
batch:     154080 | loss: 5.15712 | failed:  350
batch:     154090 | loss: 5.10095 | failed:  350
batch:     154100 | loss: 4.98893 | failed:  350
batch:     154110 | loss: 5.03346 | failed:  350
batch:     154120 | loss: 5.09269 | failed:  350
batch:     154130 | loss: 5.12654 | failed:  350
batch:     154140 | loss: 4.96524 | failed:  350
batch:     154150 | loss: 5.15012 | failed:  350
batch:     154160 | loss: 4.98026 | failed:  350
batch:     154170 | loss: 5.15134 | failed:  350
batch:     154180 | loss: 5.08688 | failed:  350
batch:     154190 | loss: 5.09365 | failed:  350
batch:     154200 | loss: 5.25072 | failed:  350
batch:     154210 | loss: 5.17746 | failed:  350
batch:     154220 | loss: 5.09154 | failed:  350
batch:     154230 | loss: 5.01531 | failed:  350
batch:     154240 | loss: 5.00559 | failed:  350
batch:     154250 | loss: 5.19569 | failed:  350
batch:     154260 | loss: 5.22186 | failed:  350
batch:     154270 | loss: 5.20257 | failed:  350
batch:     154280 | loss: 5.17205 | failed:  350
batch:     154290 | loss: 4.63060 | failed:  350
batch:     154300 | loss: 5.11170 | failed:  350
batch:     154310 | loss: 5.07139 | failed:  350
batch:     154320 | loss: 5.18740 | failed:  350
batch:     154330 | loss: 5.10206 | failed:  350
batch:     154340 | loss: 5.08139 | failed:  350
batch:     154350 | loss: 5.24496 | failed:  350
batch:     154360 | loss: 5.23317 | failed:  350
batch:     154370 | loss: 4.99267 | failed:  350
batch:     154380 | loss: 5.01095 | failed:  350
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     154390 | loss: 5.14496 | failed:  355
batch:     154400 | loss: 5.11507 | failed:  355
batch:     154410 | loss: 5.05296 | failed:  355
batch:     154420 | loss: 4.96172 | failed:  355
batch:     154430 | loss: 5.04017 | failed:  355
batch:     154440 | loss: 5.17603 | failed:  355
batch:     154450 | loss: 5.10530 | failed:  355
batch:     154460 | loss: 5.15685 | failed:  355
batch:     154470 | loss: 5.14520 | failed:  355
batch:     154480 | loss: 5.10635 | failed:  355
batch:     154490 | loss: 5.15054 | failed:  355
batch:     154500 | loss: 5.16920 | failed:  355
batch:     154510 | loss: 5.17676 | failed:  355
batch:     154520 | loss: 5.21823 | failed:  355
batch:     154530 | loss: 5.17792 | failed:  355
batch:     154540 | loss: 5.11948 | failed:  355
batch:     154550 | loss: 5.14472 | failed:  355
batch:     154560 | loss: 5.08520 | failed:  355
batch:     154570 | loss: 5.09500 | failed:  355
batch:     154580 | loss: 5.08624 | failed:  355
batch:     154590 | loss: 5.15134 | failed:  355
batch:     154600 | loss: 5.14048 | failed:  355
batch:     154610 | loss: 5.13513 | failed:  355
batch:     154620 | loss: 5.08611 | failed:  355
batch:     154630 | loss: 5.03042 | failed:  355
batch:     154640 | loss: 5.12225 | failed:  355
batch:     154650 | loss: 5.06423 | failed:  355
batch:     154660 | loss: 5.23134 | failed:  355
batch:     154670 | loss: 5.09777 | failed:  355
batch:     154680 | loss: 5.15565 | failed:  355
batch:     154690 | loss: 5.06694 | failed:  355
batch:     154700 | loss: 5.03177 | failed:  355
batch:     154710 | loss: 5.16323 | failed:  355
batch:     154720 | loss: 5.01180 | failed:  355
batch:     154730 | loss: 4.79916 | failed:  355
batch:     154740 | loss: 5.12245 | failed:  355
batch:     154750 | loss: 5.09964 | failed:  355
batch:     154760 | loss: 5.15884 | failed:  355
batch:     154770 | loss: 5.07849 | failed:  355
batch:     154780 | loss: 5.08057 | failed:  355
batch:     154790 | loss: 5.19513 | failed:  355
batch:     154800 | loss: 5.08480 | failed:  355
batch:     154810 | loss: 5.09194 | failed:  355
batch:     154820 | loss: 5.05998 | failed:  355
batch:     154830 | loss: 5.14526 | failed:  355
batch:     154840 | loss: 4.84649 | failed:  355
batch:     154850 | loss: 4.99269 | failed:  355
batch:     154860 | loss: 5.10102 | failed:  355
batch:     154870 | loss: 5.07679 | failed:  355
batch:     154880 | loss: 5.09978 | failed:  355
batch:     154890 | loss: 5.03826 | failed:  355
batch:     154900 | loss: 5.01437 | failed:  355
batch:     154910 | loss: 5.24464 | failed:  355
batch:     154920 | loss: 5.18888 | failed:  355
batch:     154930 | loss: 5.12909 | failed:  355
batch:     154940 | loss: 5.15626 | failed:  355
batch:     154950 | loss: 5.13073 | failed:  355
batch:     154960 | loss: 5.04807 | failed:  355
batch:     154970 | loss: 5.04104 | failed:  355
batch:     154980 | loss: 5.20505 | failed:  355
batch:     154990 | loss: 4.94776 | failed:  355
batch:     155000 | loss: 5.08469 | failed:  355
batch:     155010 | loss: 4.94449 | failed:  355
batch:     155020 | loss: 5.21578 | failed:  355
batch:     155030 | loss: 5.41347 | failed:  355
batch:     155040 | loss: 5.07457 | failed:  355
batch:     155050 | loss: 5.15706 | failed:  355
batch:     155060 | loss: 5.10519 | failed:  355
batch:     155070 | loss: 5.17893 | failed:  355
batch:     155080 | loss: 5.22243 | failed:  355
batch:     155090 | loss: 5.27982 | failed:  355
batch:     155100 | loss: 4.78652 | failed:  355
batch:     155110 | loss: 5.16175 | failed:  355
batch:     155120 | loss: 5.17620 | failed:  355
batch:     155130 | loss: 4.30196 | failed:  355
batch:     155140 | loss: 5.17658 | failed:  355
batch:     155150 | loss: 5.09276 | failed:  355
batch:     155160 | loss: 5.14131 | failed:  355
batch:     155170 | loss: 5.11255 | failed:  355
batch:     155180 | loss: 5.18108 | failed:  355
batch:     155190 | loss: 5.06173 | failed:  355
batch:     155200 | loss: 5.05743 | failed:  355
batch:     155210 | loss: 5.02442 | failed:  355
batch:     155220 | loss: 5.16004 | failed:  355
batch:     155230 | loss: 5.17543 | failed:  355
batch:     155240 | loss: 5.11578 | failed:  355
batch:     155250 | loss: 5.01711 | failed:  355
batch:     155260 | loss: 5.14496 | failed:  355
batch:     155270 | loss: 5.05291 | failed:  355
batch:     155280 | loss: 5.10885 | failed:  355
batch:     155290 | loss: 4.90565 | failed:  355
batch:     155300 | loss: 4.78650 | failed:  355
batch:     155310 | loss: 5.12234 | failed:  355
batch:     155320 | loss: 5.04027 | failed:  355
batch:     155330 | loss: 5.19919 | failed:  355
batch:     155340 | loss: 5.03629 | failed:  355
batch:     155350 | loss: 5.19006 | failed:  355
batch:     155360 | loss: 5.12857 | failed:  355
batch:     155370 | loss: 5.23435 | failed:  355
batch:     155380 | loss: 5.17586 | failed:  355
batch:     155390 | loss: 5.01538 | failed:  355
batch:     155400 | loss: 4.43217 | failed:  355
batch:     155410 | loss: 5.21881 | failed:  355
batch:     155420 | loss: 5.21473 | failed:  355
batch:     155430 | loss: 5.14815 | failed:  355
batch:     155440 | loss: 5.28875 | failed:  355
batch:     155450 | loss: 5.10160 | failed:  355
batch:     155460 | loss: 5.16679 | failed:  355
batch:     155470 | loss: 5.18861 | failed:  355
batch:     155480 | loss: 5.07425 | failed:  355
batch:     155490 | loss: 5.15488 | failed:  355
batch:     155500 | loss: 5.15920 | failed:  355
batch:     155510 | loss: 5.14964 | failed:  355
batch:     155520 | loss: 5.18930 | failed:  355
batch:     155530 | loss: 5.08149 | failed:  355
batch:     155540 | loss: 5.13505 | failed:  355
batch:     155550 | loss: 4.96718 | failed:  355
batch:     155560 | loss: 4.86637 | failed:  355
batch:     155570 | loss: 5.08792 | failed:  355
batch:     155580 | loss: 5.16422 | failed:  355
batch:     155590 | loss: 5.15801 | failed:  355
batch:     155600 | loss: 5.21773 | failed:  355
batch:     155610 | loss: 5.22396 | failed:  355
batch:     155620 | loss: 5.15981 | failed:  355
batch:     155630 | loss: 5.19316 | failed:  355
batch:     155640 | loss: 5.20709 | failed:  355
batch:     155650 | loss: 5.18655 | failed:  355
batch:     155660 | loss: 5.04977 | failed:  355
batch:     155670 | loss: 5.17628 | failed:  355
batch:     155680 | loss: 5.07892 | failed:  355
batch:     155690 | loss: 5.17640 | failed:  355
batch:     155700 | loss: 4.95512 | failed:  355
batch:     155710 | loss: 5.03908 | failed:  355
batch:     155720 | loss: 5.15613 | failed:  355
batch:     155730 | loss: 4.78241 | failed:  355
batch:     155740 | loss: 5.20195 | failed:  355
batch:     155750 | loss: 5.01278 | failed:  355
batch:     155760 | loss: 5.11535 | failed:  355
batch:     155770 | loss: 4.86185 | failed:  355
batch:     155780 | loss: 5.15491 | failed:  355
batch:     155790 | loss: 5.21534 | failed:  355
batch:     155800 | loss: 5.13835 | failed:  355
batch:     155810 | loss: 5.14222 | failed:  355
batch:     155820 | loss: 5.14244 | failed:  355
batch:     155830 | loss: 5.20903 | failed:  355
batch:     155840 | loss: 5.10009 | failed:  355
batch:     155850 | loss: 5.27966 | failed:  355
batch:     155860 | loss: 5.25908 | failed:  355
batch:     155870 | loss: 5.18807 | failed:  355
batch:     155880 | loss: 5.02769 | failed:  355
batch:     155890 | loss: 5.06052 | failed:  355
batch:     155900 | loss: 5.12815 | failed:  355
batch:     155910 | loss: 5.04698 | failed:  355
batch:     155920 | loss: 5.07399 | failed:  355
batch:     155930 | loss: 5.16930 | failed:  355
batch:     155940 | loss: 5.08499 | failed:  355
batch:     155950 | loss: 5.02888 | failed:  355
batch:     155960 | loss: 5.21515 | failed:  355
batch:     155970 | loss: 5.16479 | failed:  355
batch:     155980 | loss: 5.14569 | failed:  355
batch:     155990 | loss: 4.94526 | failed:  355
batch:     156000 | loss: 5.09296 | failed:  355
batch:     156010 | loss: 5.10358 | failed:  355
batch:     156020 | loss: 5.08737 | failed:  355
batch:     156030 | loss: 5.06332 | failed:  355
batch:     156040 | loss: 4.98379 | failed:  355
batch:     156050 | loss: 5.25839 | failed:  355
batch:     156060 | loss: 5.22169 | failed:  355
batch:     156070 | loss: 5.01088 | failed:  355
batch:     156080 | loss: 5.21929 | failed:  355
batch:     156090 | loss: 5.00417 | failed:  355
batch:     156100 | loss: 5.10037 | failed:  355
batch:     156110 | loss: 5.12048 | failed:  355
batch:     156120 | loss: 4.99900 | failed:  355
batch:     156130 | loss: 4.77367 | failed:  355
batch:     156140 | loss: 5.29110 | failed:  355
batch:     156150 | loss: 5.20293 | failed:  355
batch:     156160 | loss: 5.00826 | failed:  355
batch:     156170 | loss: 5.16288 | failed:  355
batch:     156180 | loss: 5.06530 | failed:  355
batch:     156190 | loss: 5.18034 | failed:  355
batch:     156200 | loss: 5.12903 | failed:  355
batch:     156210 | loss: 5.15239 | failed:  355
batch:     156220 | loss: 5.05104 | failed:  355
batch:     156230 | loss: 5.10713 | failed:  355
batch:     156240 | loss: 5.18790 | failed:  355
batch:     156250 | loss: 5.13057 | failed:  355
batch:     156260 | loss: 5.13531 | failed:  355
batch:     156270 | loss: 4.91876 | failed:  355
batch:     156280 | loss: 5.07636 | failed:  355
batch:     156290 | loss: 5.13396 | failed:  355
batch:     156300 | loss: 5.14285 | failed:  355
batch:     156310 | loss: 4.86695 | failed:  355
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     156320 | loss: 5.01892 | failed:  361
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     156330 | loss: 5.36840 | failed:  366
batch:     156340 | loss: 5.06287 | failed:  366
batch:     156350 | loss: 5.17167 | failed:  366
batch:     156360 | loss: 5.09104 | failed:  366
batch:     156370 | loss: 5.15745 | failed:  366
batch:     156380 | loss: 5.12491 | failed:  366
batch:     156390 | loss: 5.19351 | failed:  366
batch:     156400 | loss: 5.17232 | failed:  366
batch:     156410 | loss: 5.09687 | failed:  366
batch:     156420 | loss: 5.09721 | failed:  366
batch:     156430 | loss: 4.99700 | failed:  366
batch:     156440 | loss: 5.05102 | failed:  366
batch:     156450 | loss: 4.97507 | failed:  366
batch:     156460 | loss: 5.08365 | failed:  366
batch:     156470 | loss: 4.86299 | failed:  366
batch:     156480 | loss: 5.17846 | failed:  366
batch:     156490 | loss: 5.17888 | failed:  366
batch:     156500 | loss: 5.11171 | failed:  366
batch:     156510 | loss: 5.09839 | failed:  366
batch:     156520 | loss: 4.90060 | failed:  366
batch:     156530 | loss: 5.21167 | failed:  366
batch:     156540 | loss: 5.13439 | failed:  366
batch:     156550 | loss: 5.23426 | failed:  366
batch:     156560 | loss: 5.14255 | failed:  366
batch:     156570 | loss: 5.07192 | failed:  366
batch:     156580 | loss: 3.95147 | failed:  366
batch:     156590 | loss: 5.13993 | failed:  366
batch:     156600 | loss: 5.22420 | failed:  366
batch:     156610 | loss: 5.01720 | failed:  366
batch:     156620 | loss: 5.16705 | failed:  366
batch:     156630 | loss: 4.85528 | failed:  366
batch:     156640 | loss: 4.98330 | failed:  366
batch:     156650 | loss: 5.01277 | failed:  366
batch:     156660 | loss: 5.14701 | failed:  366
batch:     156670 | loss: 5.10668 | failed:  366
batch:     156680 | loss: 5.13223 | failed:  366
batch:     156690 | loss: 5.12772 | failed:  366
batch:     156700 | loss: 5.08964 | failed:  366
batch:     156710 | loss: 5.16423 | failed:  366
batch:     156720 | loss: 5.08554 | failed:  366
batch:     156730 | loss: 5.16724 | failed:  366
batch:     156740 | loss: 5.05341 | failed:  366
batch:     156750 | loss: 5.29209 | failed:  366
batch:     156760 | loss: 5.16398 | failed:  366
batch:     156770 | loss: 5.04819 | failed:  366
batch:     156780 | loss: 5.17407 | failed:  366
batch:     156790 | loss: 5.29969 | failed:  366
batch:     156800 | loss: 5.08863 | failed:  366
batch:     156810 | loss: 5.28962 | failed:  366
batch:     156820 | loss: 5.15856 | failed:  366
batch:     156830 | loss: 4.93351 | failed:  366
batch:     156840 | loss: 5.15977 | failed:  366
batch:     156850 | loss: 4.95802 | failed:  366
batch:     156860 | loss: 5.07888 | failed:  366
batch:     156870 | loss: 4.96674 | failed:  366
batch:     156880 | loss: 5.05907 | failed:  366
batch:     156890 | loss: 5.21638 | failed:  366
batch:     156900 | loss: 5.08421 | failed:  366
batch:     156910 | loss: 4.93030 | failed:  366
batch:     156920 | loss: 5.00188 | failed:  366
batch:     156930 | loss: 5.13621 | failed:  366
batch:     156940 | loss: 5.19716 | failed:  366
batch:     156950 | loss: 5.13329 | failed:  366
batch:     156960 | loss: 5.32100 | failed:  366
batch:     156970 | loss: 5.23596 | failed:  366
batch:     156980 | loss: 5.19208 | failed:  366
batch:     156990 | loss: 5.07759 | failed:  366
batch:     157000 | loss: 4.98997 | failed:  366
batch:     157010 | loss: 5.04837 | failed:  366
batch:     157020 | loss: 5.02923 | failed:  366
batch:     157030 | loss: 4.95626 | failed:  366
batch:     157040 | loss: 5.08351 | failed:  366
batch:     157050 | loss: 5.23069 | failed:  366
batch:     157060 | loss: 5.13427 | failed:  366
batch:     157070 | loss: 4.80785 | failed:  366
batch:     157080 | loss: 5.23684 | failed:  366
batch:     157090 | loss: 5.21354 | failed:  366
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     157100 | loss: 5.02107 | failed:  371
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     157120 | loss: 5.07223 | failed:  381
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     157130 | loss: 4.99599 | failed:  383
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     157140 | loss: 5.11899 | failed:  384
batch:     157150 | loss: 5.16516 | failed:  384
batch:     157160 | loss: 5.20721 | failed:  384
batch:     157170 | loss: 4.90416 | failed:  384
batch:     157180 | loss: 5.27632 | failed:  384
batch:     157190 | loss: 4.91830 | failed:  384
batch:     157200 | loss: 5.09971 | failed:  384
batch:     157210 | loss: 5.07207 | failed:  384
batch:     157220 | loss: 5.09675 | failed:  384
batch:     157230 | loss: 5.04225 | failed:  384
batch:     157240 | loss: 5.19206 | failed:  384
batch:     157250 | loss: 4.99880 | failed:  384
batch:     157260 | loss: 5.25407 | failed:  384
batch:     157270 | loss: 4.88416 | failed:  384
batch:     157280 | loss: 5.09085 | failed:  384
batch:     157290 | loss: 5.15974 | failed:  384
batch:     157300 | loss: 5.04672 | failed:  384
batch:     157310 | loss: 5.14120 | failed:  384
batch:     157320 | loss: 4.91696 | failed:  384
batch:     157330 | loss: 5.16118 | failed:  384
batch:     157340 | loss: 5.06627 | failed:  384
batch:     157350 | loss: 5.10974 | failed:  384
batch:     157360 | loss: 5.21941 | failed:  384
batch:     157370 | loss: 5.12335 | failed:  384
batch:     157380 | loss: 5.01205 | failed:  384
batch:     157390 | loss: 5.01247 | failed:  384
batch:     157400 | loss: 5.15742 | failed:  384
batch:     157410 | loss: 5.16294 | failed:  384
batch:     157420 | loss: 5.19289 | failed:  384
batch:     157430 | loss: 4.80778 | failed:  384
batch:     157440 | loss: 5.20452 | failed:  384
batch:     157450 | loss: 5.87215 | failed:  384
batch:     157460 | loss: 5.09929 | failed:  384
batch:     157470 | loss: 4.98518 | failed:  384
batch:     157480 | loss: 4.97394 | failed:  384
batch:     157490 | loss: 5.13580 | failed:  384
batch:     157500 | loss: 5.08145 | failed:  384
batch:     157510 | loss: 5.16101 | failed:  384
batch:     157520 | loss: 4.64547 | failed:  384
batch:     157530 | loss: 4.97674 | failed:  384
batch:     157540 | loss: 5.16206 | failed:  384
batch:     157550 | loss: 5.12511 | failed:  384
batch:     157560 | loss: 4.99547 | failed:  384
batch:     157570 | loss: 5.05930 | failed:  384
batch:     157580 | loss: 4.22622 | failed:  384
batch:     157590 | loss: 5.08863 | failed:  384
batch:     157600 | loss: 3.34779 | failed:  384
batch:     157610 | loss: 3.71114 | failed:  384
batch:     157620 | loss: 4.95827 | failed:  384
batch:     157630 | loss: 5.04695 | failed:  384
batch:     157640 | loss: 5.26479 | failed:  384
batch:     157650 | loss: 4.90662 | failed:  384
batch:     157660 | loss: 5.18512 | failed:  384
batch:     157670 | loss: 5.03562 | failed:  384
batch:     157680 | loss: 5.12177 | failed:  384
batch:     157690 | loss: 5.15653 | failed:  384
batch:     157700 | loss: 5.11447 | failed:  384
batch:     157710 | loss: 5.26479 | failed:  384
batch:     157720 | loss: 4.79634 | failed:  384
batch:     157730 | loss: 5.01709 | failed:  384
batch:     157740 | loss: 5.15075 | failed:  384
batch:     157750 | loss: 5.02971 | failed:  384
batch:     157760 | loss: 5.11771 | failed:  384
batch:     157770 | loss: 5.19173 | failed:  384
batch:     157780 | loss: 5.14274 | failed:  384
batch:     157790 | loss: 5.14452 | failed:  384
batch:     157800 | loss: 4.15583 | failed:  384
batch:     157810 | loss: 3.15888 | failed:  384
batch:     157820 | loss: 4.10443 | failed:  384
batch:     157830 | loss: 5.10815 | failed:  384
batch:     157840 | loss: 5.01677 | failed:  384
batch:     157850 | loss: 5.14264 | failed:  384
batch:     157860 | loss: 5.07836 | failed:  384
batch:     157870 | loss: 5.10977 | failed:  384
batch:     157880 | loss: 5.09708 | failed:  384
batch:     157890 | loss: 5.06592 | failed:  384
batch:     157900 | loss: 5.03885 | failed:  384
batch:     157910 | loss: 5.14822 | failed:  384
batch:     157920 | loss: 5.20263 | failed:  384
batch:     157930 | loss: 5.11191 | failed:  384
batch:     157940 | loss: 5.22758 | failed:  384
batch:     157950 | loss: 5.16071 | failed:  384
batch:     157960 | loss: 5.13408 | failed:  384
batch:     157970 | loss: 4.16246 | failed:  384
batch:     157980 | loss: 4.85844 | failed:  384
batch:     157990 | loss: 5.11975 | failed:  384
batch:     158000 | loss: 5.16178 | failed:  384
batch:     158010 | loss: 5.03617 | failed:  384
batch:     158020 | loss: 5.27726 | failed:  384
batch:     158030 | loss: 5.01313 | failed:  384
batch:     158040 | loss: 5.05714 | failed:  384
batch:     158050 | loss: 4.99573 | failed:  384
batch:     158060 | loss: 4.92844 | failed:  384
batch:     158070 | loss: 5.33705 | failed:  384
batch:     158080 | loss: 5.26475 | failed:  384
batch:     158090 | loss: 5.08276 | failed:  384
batch:     158100 | loss: 5.19540 | failed:  384
batch:     158110 | loss: 5.23418 | failed:  384
batch:     158120 | loss: 5.21262 | failed:  384
batch:     158130 | loss: 5.09991 | failed:  384
batch:     158140 | loss: 4.99161 | failed:  384
batch:     158150 | loss: 5.03310 | failed:  384
batch:     158160 | loss: 4.53447 | failed:  384
batch:     158170 | loss: 5.07297 | failed:  384
batch:     158180 | loss: 5.09341 | failed:  384
batch:     158190 | loss: 5.11573 | failed:  384
batch:     158200 | loss: 4.99591 | failed:  384
batch:     158210 | loss: 5.11981 | failed:  384
batch:     158220 | loss: 5.08979 | failed:  384
batch:     158230 | loss: 5.02803 | failed:  384
batch:     158240 | loss: 5.21701 | failed:  384
batch:     158250 | loss: 5.29851 | failed:  384
batch:     158260 | loss: 5.23704 | failed:  384
batch:     158270 | loss: 5.19949 | failed:  384
batch:     158280 | loss: 5.13316 | failed:  384
batch:     158290 | loss: 5.03112 | failed:  384
batch:     158300 | loss: 5.14049 | failed:  384
batch:     158310 | loss: 5.15522 | failed:  384
batch:     158320 | loss: 5.12168 | failed:  384
batch:     158330 | loss: 5.22394 | failed:  384
batch:     158340 | loss: 5.13875 | failed:  384
batch:     158350 | loss: 5.12712 | failed:  384
batch:     158360 | loss: 5.13946 | failed:  384
batch:     158370 | loss: 4.97018 | failed:  384
batch:     158380 | loss: 5.16784 | failed:  384
batch:     158390 | loss: 5.12440 | failed:  384
batch:     158400 | loss: 5.11040 | failed:  384
batch:     158410 | loss: 5.22127 | failed:  384
batch:     158420 | loss: 5.13901 | failed:  384
batch:     158430 | loss: 5.10732 | failed:  384
batch:     158440 | loss: 5.16198 | failed:  384
batch:     158450 | loss: 5.00373 | failed:  384
batch:     158460 | loss: 5.17484 | failed:  384
batch:     158470 | loss: 5.19728 | failed:  384
batch:     158480 | loss: 5.20108 | failed:  384
batch:     158490 | loss: 5.15550 | failed:  384
batch:     158500 | loss: 5.17686 | failed:  384
batch:     158510 | loss: 5.18713 | failed:  384
batch:     158520 | loss: 5.09234 | failed:  384
batch:     158530 | loss: 4.77703 | failed:  384
batch:     158540 | loss: 5.01974 | failed:  384
batch:     158550 | loss: 4.98461 | failed:  384
batch:     158560 | loss: 5.18943 | failed:  384
batch:     158570 | loss: 5.12565 | failed:  384
batch:     158580 | loss: 5.13687 | failed:  384
batch:     158590 | loss: 5.15610 | failed:  384
batch:     158600 | loss: 5.15450 | failed:  384
batch:     158610 | loss: 5.17503 | failed:  384
batch:     158620 | loss: 5.17152 | failed:  384
batch:     158630 | loss: 5.20361 | failed:  384
batch:     158640 | loss: 5.17935 | failed:  384
batch:     158650 | loss: 5.15322 | failed:  384
batch:     158660 | loss: 5.15601 | failed:  384
batch:     158670 | loss: 5.17382 | failed:  384
batch:     158680 | loss: 5.09651 | failed:  384
batch:     158690 | loss: 5.12224 | failed:  384
batch:     158700 | loss: 5.23454 | failed:  384
batch:     158710 | loss: 5.17660 | failed:  384
batch:     158720 | loss: 5.22656 | failed:  384
batch:     158730 | loss: 5.01836 | failed:  384
batch:     158740 | loss: 5.06431 | failed:  384
batch:     158750 | loss: 5.15376 | failed:  384
batch:     158760 | loss: 5.18878 | failed:  384
batch:     158770 | loss: 5.18394 | failed:  384
batch:     158780 | loss: 4.97938 | failed:  384
batch:     158790 | loss: 5.00560 | failed:  384
batch:     158800 | loss: 5.27061 | failed:  384
batch:     158810 | loss: 5.27579 | failed:  384
batch:     158820 | loss: 5.23727 | failed:  384
batch:     158830 | loss: 5.17142 | failed:  384
batch:     158840 | loss: 5.10659 | failed:  384
batch:     158850 | loss: 5.15257 | failed:  384
batch:     158860 | loss: 5.13291 | failed:  384
batch:     158870 | loss: 5.01573 | failed:  384
batch:     158880 | loss: 5.18430 | failed:  384
batch:     158890 | loss: 5.08238 | failed:  384
batch:     158900 | loss: 5.11457 | failed:  384
batch:     158910 | loss: 5.11101 | failed:  384
batch:     158920 | loss: 5.30937 | failed:  384
batch:     158930 | loss: 4.93580 | failed:  384
batch:     158940 | loss: 5.09185 | failed:  384
batch:     158950 | loss: 5.07511 | failed:  384
batch:     158960 | loss: 4.96286 | failed:  384
batch:     158970 | loss: 5.00484 | failed:  384
batch:     158980 | loss: 3.48217 | failed:  384
batch:     158990 | loss: 5.19501 | failed:  384
batch:     159000 | loss: 5.24413 | failed:  384
batch:     159010 | loss: 5.28103 | failed:  384
batch:     159020 | loss: 5.16448 | failed:  384
batch:     159030 | loss: 5.11804 | failed:  384
batch:     159040 | loss: 5.12285 | failed:  384
batch:     159050 | loss: 5.08250 | failed:  384
batch:     159060 | loss: 5.18398 | failed:  384
batch:     159070 | loss: 5.16485 | failed:  384
batch:     159080 | loss: 5.11593 | failed:  384
batch:     159090 | loss: 5.16185 | failed:  384
batch:     159100 | loss: 5.15488 | failed:  384
batch:     159110 | loss: 5.16715 | failed:  384
batch:     159120 | loss: 5.17143 | failed:  384
batch:     159130 | loss: 5.14101 | failed:  384
batch:     159140 | loss: 5.21180 | failed:  384
batch:     159150 | loss: 5.13045 | failed:  384
batch:     159160 | loss: 5.19541 | failed:  384
batch:     159170 | loss: 5.19271 | failed:  384
batch:     159180 | loss: 4.99931 | failed:  384
batch:     159190 | loss: 5.04616 | failed:  384
batch:     159200 | loss: 5.02812 | failed:  384
batch:     159210 | loss: 5.02641 | failed:  384
batch:     159220 | loss: 5.10269 | failed:  384
batch:     159230 | loss: 5.20414 | failed:  384
batch:     159240 | loss: 5.24353 | failed:  384
batch:     159250 | loss: 5.10409 | failed:  384
batch:     159260 | loss: 5.05594 | failed:  384
batch:     159270 | loss: 5.11867 | failed:  384
batch:     159280 | loss: 5.22571 | failed:  384
batch:     159290 | loss: 5.24160 | failed:  384
batch:     159300 | loss: 4.56812 | failed:  384
batch:     159310 | loss: 4.13588 | failed:  384
batch:     159320 | loss: 5.16659 | failed:  384
batch:     159330 | loss: 4.64374 | failed:  384
batch:     159340 | loss: 5.20548 | failed:  384
batch:     159350 | loss: 5.11430 | failed:  384
batch:     159360 | loss: 5.09389 | failed:  384
batch:     159370 | loss: 5.17560 | failed:  384
batch:     159380 | loss: 5.08893 | failed:  384
batch:     159390 | loss: 4.34615 | failed:  384
batch:     159400 | loss: 5.09729 | failed:  384
batch:     159410 | loss: 4.95782 | failed:  384
batch:     159420 | loss: 5.23088 | failed:  384
batch:     159430 | loss: 5.19044 | failed:  384
batch:     159440 | loss: 5.20822 | failed:  384
batch:     159450 | loss: 5.18808 | failed:  384
batch:     159460 | loss: 3.02610 | failed:  384
batch:     159470 | loss: 5.18466 | failed:  384
batch:     159480 | loss: 5.01409 | failed:  384
batch:     159490 | loss: 5.05596 | failed:  384
batch:     159500 | loss: 5.10883 | failed:  384
batch:     159510 | loss: 4.96028 | failed:  384
batch:     159520 | loss: 5.04510 | failed:  384
batch:     159530 | loss: 5.12912 | failed:  384
batch:     159540 | loss: 5.08411 | failed:  384
batch:     159550 | loss: 4.87196 | failed:  384
batch:     159560 | loss: 2.36678 | failed:  384
batch:     159570 | loss: 3.81900 | failed:  384
batch:     159580 | loss: 5.15856 | failed:  384
batch:     159590 | loss: 5.15792 | failed:  384
batch:     159600 | loss: 5.25746 | failed:  384
batch:     159610 | loss: 5.06301 | failed:  384
batch:     159620 | loss: 5.15196 | failed:  384
batch:     159630 | loss: 5.19175 | failed:  384
batch:     159640 | loss: 5.21820 | failed:  384
batch:     159650 | loss: 5.14279 | failed:  384
batch:     159660 | loss: 5.15506 | failed:  384
batch:     159670 | loss: 5.02206 | failed:  384
batch:     159680 | loss: 3.67254 | failed:  384
batch:     159690 | loss: 5.23072 | failed:  384
batch:     159700 | loss: 5.16527 | failed:  384
batch:     159710 | loss: 5.19021 | failed:  384
batch:     159720 | loss: 5.21951 | failed:  384
batch:     159730 | loss: 4.26265 | failed:  384
batch:     159740 | loss: 5.21815 | failed:  384
batch:     159750 | loss: 4.10950 | failed:  384
batch:     159760 | loss: 5.18348 | failed:  384
batch:     159770 | loss: 5.26575 | failed:  384
batch:     159780 | loss: 5.07673 | failed:  384
batch:     159790 | loss: 5.21760 | failed:  384
batch:     159800 | loss: 5.16385 | failed:  384
batch:     159810 | loss: 5.20180 | failed:  384
batch:     159820 | loss: 5.02625 | failed:  384
batch:     159830 | loss: 5.14182 | failed:  384
batch:     159840 | loss: 5.09309 | failed:  384
batch:     159850 | loss: 4.99778 | failed:  384
batch:     159860 | loss: 5.03106 | failed:  384
batch:     159870 | loss: 5.13577 | failed:  384
batch:     159880 | loss: 5.13021 | failed:  384
batch:     159890 | loss: 5.22400 | failed:  384
batch:     159900 | loss: 5.17931 | failed:  384
batch:     159910 | loss: 5.28096 | failed:  384
batch:     159920 | loss: 5.26313 | failed:  384
batch:     159930 | loss: 5.23196 | failed:  384
batch:     159940 | loss: 5.29070 | failed:  384
batch:     159950 | loss: 5.16423 | failed:  384
batch:     159960 | loss: 5.16014 | failed:  384
batch:     159970 | loss: 4.67407 | failed:  384
batch:     159980 | loss: 5.13810 | failed:  384
batch:     159990 | loss: 5.17430 | failed:  384
batch:     160000 | loss: 5.14616 | failed:  384
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/checkpoint_latest.pth
batch:     160010 | loss: 4.45196 | failed:  384
batch:     160020 | loss: 5.15751 | failed:  384
batch:     160030 | loss: 2.21427 | failed:  384
batch:     160040 | loss: 4.91591 | failed:  384
batch:     160050 | loss: 5.24688 | failed:  384
batch:     160060 | loss: 5.03377 | failed:  384
batch:     160070 | loss: 4.91198 | failed:  384
batch:     160080 | loss: 5.09358 | failed:  384
batch:     160090 | loss: 5.11150 | failed:  384
batch:     160100 | loss: 4.81279 | failed:  384
batch:     160110 | loss: 5.20754 | failed:  384
batch:     160120 | loss: 5.19131 | failed:  384
batch:     160130 | loss: 5.03229 | failed:  384
batch:     160140 | loss: 5.11646 | failed:  384
batch:     160150 | loss: 5.28124 | failed:  384
batch:     160160 | loss: 5.25337 | failed:  384
batch:     160170 | loss: 5.10835 | failed:  384
batch:     160180 | loss: 5.22763 | failed:  384
batch:     160190 | loss: 5.22741 | failed:  384
batch:     160200 | loss: 5.16033 | failed:  384
batch:     160210 | loss: 5.18696 | failed:  384
batch:     160220 | loss: 5.19728 | failed:  384
batch:     160230 | loss: 5.04312 | failed:  384
batch:     160240 | loss: 5.06880 | failed:  384
batch:     160250 | loss: 5.08255 | failed:  384
batch:     160260 | loss: 3.11886 | failed:  384
batch:     160270 | loss: 5.12473 | failed:  384
batch:     160280 | loss: 4.43723 | failed:  384
batch:     160290 | loss: 4.97796 | failed:  384
batch:     160300 | loss: 4.11904 | failed:  384
batch:     160310 | loss: 3.54188 | failed:  384
batch:     160320 | loss: 4.99053 | failed:  384
batch:     160330 | loss: 5.04973 | failed:  384
batch:     160340 | loss: 5.11102 | failed:  384
batch:     160350 | loss: 5.12791 | failed:  384
batch:     160360 | loss: 5.22128 | failed:  384
batch:     160370 | loss: 5.16428 | failed:  384
batch:     160380 | loss: 5.18526 | failed:  384
batch:     160390 | loss: 5.12971 | failed:  384
batch:     160400 | loss: 5.14907 | failed:  384
batch:     160410 | loss: 5.15932 | failed:  384
batch:     160420 | loss: 5.17139 | failed:  384
batch:     160430 | loss: 5.04113 | failed:  384
batch:     160440 | loss: 5.17601 | failed:  384
batch:     160450 | loss: 5.16214 | failed:  384
batch:     160460 | loss: 4.99906 | failed:  384
batch:     160470 | loss: 4.94803 | failed:  384
batch:     160480 | loss: 5.21289 | failed:  384
batch:     160490 | loss: 5.25078 | failed:  384
batch:     160500 | loss: 5.13109 | failed:  384
batch:     160510 | loss: 5.15454 | failed:  384
batch:     160520 | loss: 5.16202 | failed:  384
batch:     160530 | loss: 5.05863 | failed:  384
batch:     160540 | loss: 5.01372 | failed:  384
batch:     160550 | loss: 5.06963 | failed:  384
batch:     160560 | loss: 4.89356 | failed:  384
batch:     160570 | loss: 4.84186 | failed:  384
batch:     160580 | loss: 4.87128 | failed:  384
batch:     160590 | loss: 5.17419 | failed:  384
batch:     160600 | loss: 5.08142 | failed:  384
batch:     160610 | loss: 5.01887 | failed:  384
batch:     160620 | loss: 5.14692 | failed:  384
batch:     160630 | loss: 5.13275 | failed:  384
batch:     160640 | loss: 5.22185 | failed:  384
batch:     160650 | loss: 4.91521 | failed:  384
batch:     160660 | loss: 4.85846 | failed:  384
batch:     160670 | loss: 5.13596 | failed:  384
batch:     160680 | loss: 5.14101 | failed:  384
batch:     160690 | loss: 5.06497 | failed:  384
batch:     160700 | loss: 5.10865 | failed:  384
batch:     160710 | loss: 5.16138 | failed:  384
batch:     160720 | loss: 5.03335 | failed:  384
batch:     160730 | loss: 5.15945 | failed:  384
batch:     160740 | loss: 5.10669 | failed:  384
batch:     160750 | loss: 5.13989 | failed:  384
batch:     160760 | loss: 5.06254 | failed:  384
batch:     160770 | loss: 4.99556 | failed:  384
batch:     160780 | loss: 5.05066 | failed:  384
batch:     160790 | loss: 5.11308 | failed:  384
batch:     160800 | loss: 4.86046 | failed:  384
batch:     160810 | loss: 5.14407 | failed:  384
batch:     160820 | loss: 4.94708 | failed:  384
batch:     160830 | loss: 5.26274 | failed:  384
batch:     160840 | loss: 5.18841 | failed:  384
batch:     160850 | loss: 5.22785 | failed:  384
batch:     160860 | loss: 5.18577 | failed:  384
batch:     160870 | loss: 5.19097 | failed:  384
batch:     160880 | loss: 5.13176 | failed:  384
batch:     160890 | loss: 5.17008 | failed:  384
batch:     160900 | loss: 5.02862 | failed:  384
batch:     160910 | loss: 4.77613 | failed:  384
batch:     160920 | loss: 5.16277 | failed:  384
batch:     160930 | loss: 5.19448 | failed:  384
batch:     160940 | loss: 5.22001 | failed:  384
batch:     160950 | loss: 4.81402 | failed:  384
batch:     160960 | loss: 5.17571 | failed:  384
batch:     160970 | loss: 5.09852 | failed:  384
batch:     160980 | loss: 5.15700 | failed:  384
batch:     160990 | loss: 5.22542 | failed:  384
batch:     161000 | loss: 5.08941 | failed:  384
batch:     161010 | loss: 5.18071 | failed:  384
batch:     161020 | loss: 5.20136 | failed:  384
batch:     161030 | loss: 5.14131 | failed:  384
batch:     161040 | loss: 5.19600 | failed:  384
batch:     161050 | loss: 4.88005 | failed:  384
batch:     161060 | loss: 5.05872 | failed:  384
batch:     161070 | loss: 5.10736 | failed:  384
batch:     161080 | loss: 5.18074 | failed:  384
batch:     161090 | loss: 5.01025 | failed:  384
batch:     161100 | loss: 5.13300 | failed:  384
batch:     161110 | loss: 5.21222 | failed:  384
batch:     161120 | loss: 5.17890 | failed:  384
batch:     161130 | loss: 5.20522 | failed:  384
batch:     161140 | loss: 5.15371 | failed:  384
batch:     161150 | loss: 5.19870 | failed:  384
batch:     161160 | loss: 5.10026 | failed:  384
batch:     161170 | loss: 5.08974 | failed:  384
batch:     161180 | loss: 5.09150 | failed:  384
batch:     161190 | loss: 5.12340 | failed:  384
batch:     161200 | loss: 5.12450 | failed:  384
batch:     161210 | loss: 5.11949 | failed:  384
batch:     161220 | loss: 5.17674 | failed:  384
batch:     161230 | loss: 5.13484 | failed:  384
batch:     161240 | loss: 5.26854 | failed:  384
batch:     161250 | loss: 5.10991 | failed:  384
batch:     161260 | loss: 5.03831 | failed:  384
batch:     161270 | loss: 5.07536 | failed:  384
batch:     161280 | loss: 5.05076 | failed:  384
batch:     161290 | loss: 5.06203 | failed:  384
batch:     161300 | loss: 5.12462 | failed:  384
batch:     161310 | loss: 5.08472 | failed:  384
batch:     161320 | loss: 5.09176 | failed:  384
batch:     161330 | loss: 4.99768 | failed:  384
batch:     161340 | loss: 5.27015 | failed:  384
batch:     161350 | loss: 5.01959 | failed:  384
batch:     161360 | loss: 4.96903 | failed:  384
batch:     161370 | loss: 5.17856 | failed:  384
batch:     161380 | loss: 5.18834 | failed:  384
batch:     161390 | loss: 5.07257 | failed:  384
batch:     161400 | loss: 4.86758 | failed:  384
batch:     161410 | loss: 4.63907 | failed:  384
batch:     161420 | loss: 5.13680 | failed:  384
batch:     161430 | loss: 4.97954 | failed:  384
batch:     161440 | loss: 5.20216 | failed:  384
batch:     161450 | loss: 5.10963 | failed:  384
batch:     161460 | loss: 5.11153 | failed:  384
batch:     161470 | loss: 5.18158 | failed:  384
batch:     161480 | loss: 5.12406 | failed:  384
batch:     161490 | loss: 5.10060 | failed:  384
batch:     161500 | loss: 5.11257 | failed:  384
batch:     161510 | loss: 5.11020 | failed:  384
batch:     161520 | loss: 5.07606 | failed:  384
batch:     161530 | loss: 5.01751 | failed:  384
batch:     161540 | loss: 5.08180 | failed:  384
batch:     161550 | loss: 4.83448 | failed:  384
batch:     161560 | loss: 5.11319 | failed:  384
batch:     161570 | loss: 5.21229 | failed:  384
batch:     161580 | loss: 5.09194 | failed:  384
batch:     161590 | loss: 4.74556 | failed:  384
batch:     161600 | loss: 5.35854 | failed:  384
batch:     161610 | loss: 5.22389 | failed:  384
batch:     161620 | loss: 5.22747 | failed:  384
batch:     161630 | loss: 5.19360 | failed:  384
batch:     161640 | loss: 5.08074 | failed:  384
batch:     161650 | loss: 5.21088 | failed:  384
batch:     161660 | loss: 5.02235 | failed:  384
batch:     161670 | loss: 5.25905 | failed:  384
batch:     161680 | loss: 4.80293 | failed:  384
batch:     161690 | loss: 5.16968 | failed:  384
batch:     161700 | loss: 5.03416 | failed:  384
batch:     161710 | loss: 5.29189 | failed:  384
batch:     161720 | loss: 3.95049 | failed:  384
batch:     161730 | loss: 5.14746 | failed:  384
batch:     161740 | loss: 5.13676 | failed:  384
batch:     161750 | loss: 5.00435 | failed:  384
batch:     161760 | loss: 5.27051 | failed:  384
batch:     161770 | loss: 5.06360 | failed:  384
batch:     161780 | loss: 5.23453 | failed:  384
batch:     161790 | loss: 5.12482 | failed:  384
batch:     161800 | loss: 5.09976 | failed:  384
batch:     161810 | loss: 5.20222 | failed:  384
batch:     161820 | loss: 5.08189 | failed:  384
batch:     161830 | loss: 5.08410 | failed:  384
batch:     161840 | loss: 4.86678 | failed:  384
batch:     161850 | loss: 5.09680 | failed:  384
batch:     161860 | loss: 5.26900 | failed:  384
batch:     161870 | loss: 5.13966 | failed:  384
batch:     161880 | loss: 5.09754 | failed:  384
batch:     161890 | loss: 5.13437 | failed:  384
batch:     161900 | loss: 5.23567 | failed:  384
batch:     161910 | loss: 5.16607 | failed:  384
batch:     161920 | loss: 5.08578 | failed:  384
batch:     161930 | loss: 5.17728 | failed:  384
batch:     161940 | loss: 5.77139 | failed:  384
batch:     161950 | loss: 5.02876 | failed:  384
batch:     161960 | loss: 4.89368 | failed:  384
batch:     161970 | loss: 4.92253 | failed:  384
batch:     161980 | loss: 5.19197 | failed:  384
batch:     161990 | loss: 4.98027 | failed:  384
batch:     162000 | loss: 5.14881 | failed:  384
batch:     162010 | loss: 5.12503 | failed:  384
batch:     162020 | loss: 5.12761 | failed:  384
batch:     162030 | loss: 5.05046 | failed:  384
batch:     162040 | loss: 5.06080 | failed:  384
batch:     162050 | loss: 5.03728 | failed:  384
batch:     162060 | loss: 5.10656 | failed:  384
batch:     162070 | loss: 4.94919 | failed:  384
batch:     162080 | loss: 4.99172 | failed:  384
batch:     162090 | loss: 5.02674 | failed:  384
batch:     162100 | loss: 5.06743 | failed:  384
batch:     162110 | loss: 5.19333 | failed:  384
batch:     162120 | loss: 5.11932 | failed:  384
batch:     162130 | loss: 5.19612 | failed:  384
batch:     162140 | loss: 4.94335 | failed:  384
batch:     162150 | loss: 5.03319 | failed:  384
batch:     162160 | loss: 5.15585 | failed:  384
batch:     162170 | loss: 5.12172 | failed:  384
batch:     162180 | loss: 4.80417 | failed:  384
batch:     162190 | loss: 4.83324 | failed:  384
batch:     162200 | loss: 5.16486 | failed:  384
batch:     162210 | loss: 5.07219 | failed:  384
batch:     162220 | loss: 5.09796 | failed:  384
batch:     162230 | loss: 5.12765 | failed:  384
batch:     162240 | loss: 5.23616 | failed:  384
batch:     162250 | loss: 5.08578 | failed:  384
batch:     162260 | loss: 5.03012 | failed:  384
batch:     162270 | loss: 5.09450 | failed:  384
batch:     162280 | loss: 4.98042 | failed:  384
batch:     162290 | loss: 5.10658 | failed:  384
batch:     162300 | loss: 4.98326 | failed:  384
batch:     162310 | loss: 5.09404 | failed:  384
batch:     162320 | loss: 5.12925 | failed:  384
batch:     162330 | loss: 5.35330 | failed:  384
batch:     162340 | loss: 5.05670 | failed:  384
batch:     162350 | loss: 5.12012 | failed:  384
batch:     162360 | loss: 5.12822 | failed:  384
batch:     162370 | loss: 5.03916 | failed:  384
batch:     162380 | loss: 4.95044 | failed:  384
batch:     162390 | loss: 5.15406 | failed:  384
batch:     162400 | loss: 5.20695 | failed:  384
batch:     162410 | loss: 4.16088 | failed:  384
batch:     162420 | loss: 5.21170 | failed:  384
batch:     162430 | loss: 5.19589 | failed:  384
batch:     162440 | loss: 5.15356 | failed:  384
batch:     162450 | loss: 4.88042 | failed:  384
batch:     162460 | loss: 4.96297 | failed:  384
batch:     162470 | loss: 4.97817 | failed:  384
batch:     162480 | loss: 4.81309 | failed:  384
batch:     162490 | loss: 4.86975 | failed:  384
batch:     162500 | loss: 5.05011 | failed:  384
batch:     162510 | loss: 5.25093 | failed:  384
batch:     162520 | loss: 5.23879 | failed:  384
batch:     162530 | loss: 5.04924 | failed:  384
batch:     162540 | loss: 5.15978 | failed:  384
batch:     162550 | loss: 5.02396 | failed:  384
batch:     162560 | loss: 4.92256 | failed:  384
batch:     162570 | loss: 4.51892 | failed:  384
batch:     162580 | loss: 4.56808 | failed:  384
batch:     162590 | loss: 4.99581 | failed:  384
batch:     162600 | loss: 5.12482 | failed:  384
batch:     162610 | loss: 5.08827 | failed:  384
batch:     162620 | loss: 5.17226 | failed:  384
batch:     162630 | loss: 5.00500 | failed:  384
batch:     162640 | loss: 5.14928 | failed:  384
batch:     162650 | loss: 5.10484 | failed:  384
batch:     162660 | loss: 5.29921 | failed:  384
batch:     162670 | loss: 5.04449 | failed:  384
batch:     162680 | loss: 5.08804 | failed:  384
batch:     162690 | loss: 4.98778 | failed:  384
batch:     162700 | loss: 5.19318 | failed:  384
batch:     162710 | loss: 5.18136 | failed:  384
batch:     162720 | loss: 5.08203 | failed:  384
batch:     162730 | loss: 5.04559 | failed:  384
batch:     162740 | loss: 5.45833 | failed:  384
batch:     162750 | loss: 5.22793 | failed:  384
batch:     162760 | loss: 5.05088 | failed:  384
batch:     162770 | loss: 4.92004 | failed:  384
batch:     162780 | loss: 5.02069 | failed:  384
batch:     162790 | loss: 5.11789 | failed:  384
batch:     162800 | loss: 5.07498 | failed:  384
batch:     162810 | loss: 5.11433 | failed:  384
batch:     162820 | loss: 5.08562 | failed:  384
batch:     162830 | loss: 5.05859 | failed:  384
batch:     162840 | loss: 5.12710 | failed:  384
batch:     162850 | loss: 5.09449 | failed:  384
batch:     162860 | loss: 5.27875 | failed:  384
batch:     162870 | loss: 5.14797 | failed:  384
batch:     162880 | loss: 5.10139 | failed:  384
batch:     162890 | loss: 4.96967 | failed:  384
batch:     162900 | loss: 5.18885 | failed:  384
batch:     162910 | loss: 5.00618 | failed:  384
batch:     162920 | loss: 4.92181 | failed:  384
batch:     162930 | loss: 5.10977 | failed:  384
batch:     162940 | loss: 5.07800 | failed:  384
batch:     162950 | loss: 5.12534 | failed:  384
batch:     162960 | loss: 5.14684 | failed:  384
batch:     162970 | loss: 5.04837 | failed:  384
batch:     162980 | loss: 5.00197 | failed:  384
batch:     162990 | loss: 5.18115 | failed:  384
batch:     163000 | loss: 5.06279 | failed:  384
batch:     163010 | loss: 5.16631 | failed:  384
batch:     163020 | loss: 5.08611 | failed:  384
batch:     163030 | loss: 5.12199 | failed:  384
batch:     163040 | loss: 5.14202 | failed:  384
batch:     163050 | loss: 5.13678 | failed:  384
batch:     163060 | loss: 5.07452 | failed:  384
batch:     163070 | loss: 5.06534 | failed:  384
batch:     163080 | loss: 5.12872 | failed:  384
batch:     163090 | loss: 5.12630 | failed:  384
batch:     163100 | loss: 5.04931 | failed:  384
batch:     163110 | loss: 5.01295 | failed:  384
batch:     163120 | loss: 5.08579 | failed:  384
batch:     163130 | loss: 5.14659 | failed:  384
batch:     163140 | loss: 4.93450 | failed:  384
batch:     163150 | loss: 5.10654 | failed:  384
batch:     163160 | loss: 5.05294 | failed:  384
batch:     163170 | loss: 4.79746 | failed:  384
batch:     163180 | loss: 5.28734 | failed:  384
batch:     163190 | loss: 5.20337 | failed:  384
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     163200 | loss: 5.23410 | failed:  386
batch:     163210 | loss: 5.42383 | failed:  386
batch:     163220 | loss: 5.03580 | failed:  386
batch:     163230 | loss: 5.25853 | failed:  386
batch:     163240 | loss: 5.21854 | failed:  386
batch:     163250 | loss: 5.19036 | failed:  386
batch:     163260 | loss: 5.22922 | failed:  386
batch:     163270 | loss: 5.06359 | failed:  386
batch:     163280 | loss: 5.21496 | failed:  386
batch:     163290 | loss: 5.23537 | failed:  386
batch:     163300 | loss: 5.21960 | failed:  386
batch:     163310 | loss: 5.10008 | failed:  386
batch:     163320 | loss: 5.09553 | failed:  386
batch:     163330 | loss: 5.04619 | failed:  386
batch:     163340 | loss: 5.18985 | failed:  386
batch:     163350 | loss: 5.18032 | failed:  386
batch:     163360 | loss: 5.17144 | failed:  386
batch:     163370 | loss: 5.22219 | failed:  386
batch:     163380 | loss: 5.05180 | failed:  386
batch:     163390 | loss: 5.09241 | failed:  386
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     163400 | loss: 5.15690 | failed:  387
batch:     163410 | loss: 5.13449 | failed:  387
batch:     163420 | loss: 5.19207 | failed:  387
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     163430 | loss: 5.00369 | failed:  388
batch:     163440 | loss: 4.72084 | failed:  388
batch:     163450 | loss: 4.89077 | failed:  388
batch:     163460 | loss: 5.12314 | failed:  388
batch:     163470 | loss: 5.06852 | failed:  388
batch:     163480 | loss: 4.93570 | failed:  388
batch:     163490 | loss: 5.19026 | failed:  388
batch:     163500 | loss: 5.21217 | failed:  388
batch:     163510 | loss: 5.07837 | failed:  388
batch:     163520 | loss: 5.23524 | failed:  388
batch:     163530 | loss: 5.29545 | failed:  388
batch:     163540 | loss: 5.25458 | failed:  388
batch:     163550 | loss: 5.16766 | failed:  388
batch:     163560 | loss: 5.19981 | failed:  388
batch:     163570 | loss: 5.15575 | failed:  388
batch:     163580 | loss: 5.03236 | failed:  388
batch:     163590 | loss: 5.19614 | failed:  388
batch:     163600 | loss: 5.21747 | failed:  388
batch:     163610 | loss: 5.22138 | failed:  388
batch:     163620 | loss: 5.22162 | failed:  388
batch:     163630 | loss: 5.08526 | failed:  388
batch:     163640 | loss: 5.22869 | failed:  388
batch:     163650 | loss: 5.04038 | failed:  388
batch:     163660 | loss: 5.06163 | failed:  388
batch:     163670 | loss: 5.18678 | failed:  388
batch:     163680 | loss: 5.07658 | failed:  388
batch:     163690 | loss: 5.09222 | failed:  388
batch:     163700 | loss: 4.69385 | failed:  388
batch:     163710 | loss: 5.13834 | failed:  388
batch:     163720 | loss: 5.14840 | failed:  388
batch:     163730 | loss: 5.12169 | failed:  388
batch:     163740 | loss: 5.10566 | failed:  388
batch:     163750 | loss: 5.04389 | failed:  388
batch:     163760 | loss: 5.21347 | failed:  388
batch:     163770 | loss: 5.12297 | failed:  388
batch:     163780 | loss: 5.20232 | failed:  388
batch:     163790 | loss: 5.16151 | failed:  388
batch:     163800 | loss: 5.16153 | failed:  388
batch:     163810 | loss: 5.09370 | failed:  388
batch:     163820 | loss: 5.16733 | failed:  388
batch:     163830 | loss: 5.07301 | failed:  388
batch:     163840 | loss: 5.13597 | failed:  388
batch:     163850 | loss: 5.10735 | failed:  388
batch:     163860 | loss: 5.13512 | failed:  388
batch:     163870 | loss: 5.09042 | failed:  388
batch:     163880 | loss: 5.24259 | failed:  388
batch:     163890 | loss: 4.93322 | failed:  388
batch:     163900 | loss: 5.16956 | failed:  388
batch:     163910 | loss: 5.21797 | failed:  388
batch:     163920 | loss: 5.03501 | failed:  388
batch:     163930 | loss: 5.18815 | failed:  388
batch:     163940 | loss: 5.20150 | failed:  388
batch:     163950 | loss: 5.19509 | failed:  388
batch:     163960 | loss: 5.15142 | failed:  388
batch:     163970 | loss: 5.15792 | failed:  388
batch:     163980 | loss: 5.21723 | failed:  388
batch:     163990 | loss: 4.75489 | failed:  388
batch:     164000 | loss: 5.08674 | failed:  388
batch:     164010 | loss: 5.10120 | failed:  388
batch:     164020 | loss: 5.08015 | failed:  388
batch:     164030 | loss: 5.01738 | failed:  388
batch:     164040 | loss: 5.22361 | failed:  388
batch:     164050 | loss: 5.21343 | failed:  388
batch:     164060 | loss: 5.17554 | failed:  388
batch:     164070 | loss: 5.19865 | failed:  388
batch:     164080 | loss: 5.19102 | failed:  388
batch:     164090 | loss: 5.28204 | failed:  388
batch:     164100 | loss: 3.99004 | failed:  388
batch:     164110 | loss: 5.10769 | failed:  388
batch:     164120 | loss: 5.11262 | failed:  388
batch:     164130 | loss: 5.13902 | failed:  388
batch:     164140 | loss: 5.08275 | failed:  388
batch:     164150 | loss: 5.06044 | failed:  388
batch:     164160 | loss: 5.20926 | failed:  388
batch:     164170 | loss: 5.05759 | failed:  388
batch:     164180 | loss: 4.97997 | failed:  388
batch:     164190 | loss: 5.29238 | failed:  388
batch:     164200 | loss: 5.08131 | failed:  388
batch:     164210 | loss: 5.22890 | failed:  388
batch:     164220 | loss: 5.08961 | failed:  388
batch:     164230 | loss: 4.96335 | failed:  388
batch:     164240 | loss: 4.98143 | failed:  388
batch:     164250 | loss: 5.15728 | failed:  388
batch:     164260 | loss: 5.18489 | failed:  388
batch:     164270 | loss: 5.19641 | failed:  388
batch:     164280 | loss: 5.08187 | failed:  388
batch:     164290 | loss: 5.15647 | failed:  388
batch:     164300 | loss: 5.19146 | failed:  388
batch:     164310 | loss: 5.15198 | failed:  388
batch:     164320 | loss: 5.09079 | failed:  388
batch:     164330 | loss: 5.08295 | failed:  388
batch:     164340 | loss: 5.02344 | failed:  388
batch:     164350 | loss: 5.29417 | failed:  388
batch:     164360 | loss: 5.05248 | failed:  388
batch:     164370 | loss: 5.10545 | failed:  388
batch:     164380 | loss: 5.07450 | failed:  388
batch:     164390 | loss: 5.09561 | failed:  388
batch:     164400 | loss: 4.88807 | failed:  388
batch:     164410 | loss: 5.10647 | failed:  388
batch:     164420 | loss: 5.20404 | failed:  388
batch:     164430 | loss: 5.09887 | failed:  388
batch:     164440 | loss: 5.19837 | failed:  388
batch:     164450 | loss: 5.10107 | failed:  388
batch:     164460 | loss: 5.08015 | failed:  388
batch:     164470 | loss: 5.04161 | failed:  388
batch:     164480 | loss: 5.16591 | failed:  388
batch:     164490 | loss: 5.19199 | failed:  388
batch:     164500 | loss: 5.19248 | failed:  388
batch:     164510 | loss: 5.16680 | failed:  388
batch:     164520 | loss: 5.16696 | failed:  388
batch:     164530 | loss: 5.22730 | failed:  388
batch:     164540 | loss: 5.10251 | failed:  388
batch:     164550 | loss: 5.03634 | failed:  388
batch:     164560 | loss: 5.15467 | failed:  388
batch:     164570 | loss: 4.93910 | failed:  388
batch:     164580 | loss: 5.04045 | failed:  388
batch:     164590 | loss: 5.21108 | failed:  388
batch:     164600 | loss: 5.22298 | failed:  388
batch:     164610 | loss: 5.02770 | failed:  388
batch:     164620 | loss: 5.19237 | failed:  388
batch:     164630 | loss: 5.00809 | failed:  388
batch:     164640 | loss: 5.15669 | failed:  388
batch:     164650 | loss: 5.21263 | failed:  388
batch:     164660 | loss: 5.20915 | failed:  388
batch:     164670 | loss: 5.15912 | failed:  388
batch:     164680 | loss: 5.06410 | failed:  388
batch:     164690 | loss: 5.18151 | failed:  388
batch:     164700 | loss: 5.25337 | failed:  388
batch:     164710 | loss: 5.14184 | failed:  388
batch:     164720 | loss: 5.17901 | failed:  388
batch:     164730 | loss: 5.18984 | failed:  388
batch:     164740 | loss: 5.37493 | failed:  388
batch:     164750 | loss: 5.18223 | failed:  388
batch:     164760 | loss: 5.13113 | failed:  388
batch:     164770 | loss: 5.12011 | failed:  388
batch:     164780 | loss: 5.18813 | failed:  388
batch:     164790 | loss: 5.09002 | failed:  388
batch:     164800 | loss: 5.07614 | failed:  388
batch:     164810 | loss: 5.14674 | failed:  388
batch:     164820 | loss: 5.11961 | failed:  388
batch:     164830 | loss: 5.20588 | failed:  388
batch:     164840 | loss: 5.20456 | failed:  388
batch:     164850 | loss: 5.12692 | failed:  388
batch:     164860 | loss: 5.04515 | failed:  388
batch:     164870 | loss: 5.20678 | failed:  388
batch:     164880 | loss: 5.12267 | failed:  388
batch:     164890 | loss: 5.13165 | failed:  388
batch:     164900 | loss: 4.82002 | failed:  388
batch:     164910 | loss: 5.07781 | failed:  388
batch:     164920 | loss: 5.13304 | failed:  388
batch:     164930 | loss: 5.07515 | failed:  388
batch:     164940 | loss: 4.84738 | failed:  388
batch:     164950 | loss: 5.18464 | failed:  388
batch:     164960 | loss: 5.05732 | failed:  388
batch:     164970 | loss: 5.28960 | failed:  388
batch:     164980 | loss: 5.25991 | failed:  388
batch:     164990 | loss: 5.00651 | failed:  388
batch:     165000 | loss: 5.11781 | failed:  388
batch:     165010 | loss: 5.04316 | failed:  388
batch:     165020 | loss: 5.11289 | failed:  388
batch:     165030 | loss: 5.09406 | failed:  388
batch:     165040 | loss: 5.17628 | failed:  388
batch:     165050 | loss: 5.12826 | failed:  388
batch:     165060 | loss: 5.00377 | failed:  388
batch:     165070 | loss: 5.20683 | failed:  388
batch:     165080 | loss: 5.01544 | failed:  388
batch:     165090 | loss: 5.15012 | failed:  388
batch:     165100 | loss: 5.15383 | failed:  388
batch:     165110 | loss: 5.02472 | failed:  388
batch:     165120 | loss: 4.99774 | failed:  388
batch:     165130 | loss: 5.14774 | failed:  388
batch:     165140 | loss: 5.21616 | failed:  388
batch:     165150 | loss: 5.15401 | failed:  388
batch:     165160 | loss: 5.15200 | failed:  388
batch:     165170 | loss: 5.04106 | failed:  388
batch:     165180 | loss: 5.07470 | failed:  388
batch:     165190 | loss: 5.05362 | failed:  388
batch:     165200 | loss: 5.04488 | failed:  388
batch:     165210 | loss: 5.20387 | failed:  388
batch:     165220 | loss: 5.19976 | failed:  388
batch:     165230 | loss: 5.21288 | failed:  388
batch:     165240 | loss: 4.94442 | failed:  388
batch:     165250 | loss: 5.06598 | failed:  388
batch:     165260 | loss: 4.80281 | failed:  388
batch:     165270 | loss: 4.93173 | failed:  388
batch:     165280 | loss: 5.15838 | failed:  388
batch:     165290 | loss: 4.05883 | failed:  388
batch:     165300 | loss: 5.13201 | failed:  388
batch:     165310 | loss: 5.12185 | failed:  388
batch:     165320 | loss: 5.04094 | failed:  388
batch:     165330 | loss: 5.05640 | failed:  388
batch:     165340 | loss: 5.27871 | failed:  388
batch:     165350 | loss: 5.11721 | failed:  388
batch:     165360 | loss: 4.96517 | failed:  388
batch:     165370 | loss: 5.20607 | failed:  388
batch:     165380 | loss: 5.00590 | failed:  388
batch:     165390 | loss: 4.98758 | failed:  388
batch:     165400 | loss: 5.20341 | failed:  388
batch:     165410 | loss: 4.95834 | failed:  388
batch:     165420 | loss: 5.06317 | failed:  388
batch:     165430 | loss: 5.12304 | failed:  388
batch:     165440 | loss: 5.09636 | failed:  388
batch:     165450 | loss: 5.04618 | failed:  388
batch:     165460 | loss: 4.76587 | failed:  388
batch:     165470 | loss: 5.02443 | failed:  388
batch:     165480 | loss: 4.96871 | failed:  388
batch:     165490 | loss: 5.01928 | failed:  388
batch:     165500 | loss: 4.94778 | failed:  388
batch:     165510 | loss: 5.09897 | failed:  388
batch:     165520 | loss: 5.23172 | failed:  388
batch:     165530 | loss: 5.24161 | failed:  388
batch:     165540 | loss: 5.16067 | failed:  388
batch:     165550 | loss: 5.16210 | failed:  388
batch:     165560 | loss: 5.25570 | failed:  388
batch:     165570 | loss: 5.20987 | failed:  388
batch:     165580 | loss: 5.21433 | failed:  388
batch:     165590 | loss: 4.94651 | failed:  388
batch:     165600 | loss: 5.07539 | failed:  388
batch:     165610 | loss: 5.14760 | failed:  388
batch:     165620 | loss: 5.11087 | failed:  388
batch:     165630 | loss: 5.16710 | failed:  388
batch:     165640 | loss: 5.09756 | failed:  388
batch:     165650 | loss: 5.25160 | failed:  388
batch:     165660 | loss: 5.07955 | failed:  388
batch:     165670 | loss: 5.07797 | failed:  388
batch:     165680 | loss: 5.11146 | failed:  388
batch:     165690 | loss: 4.99389 | failed:  388
batch:     165700 | loss: 5.04082 | failed:  388
batch:     165710 | loss: 5.12670 | failed:  388
batch:     165720 | loss: 4.36238 | failed:  388
batch:     165730 | loss: 4.79674 | failed:  388
batch:     165740 | loss: 5.15224 | failed:  388
batch:     165750 | loss: 4.99330 | failed:  388
batch:     165760 | loss: 4.97138 | failed:  388
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     165770 | loss: 5.06679 | failed:  391
batch:     165780 | loss: 5.18014 | failed:  391
batch:     165790 | loss: 4.91046 | failed:  391
batch:     165800 | loss: 4.84295 | failed:  391
batch:     165810 | loss: 5.18813 | failed:  391
batch:     165820 | loss: 5.18276 | failed:  391
batch:     165830 | loss: 5.16805 | failed:  391
batch:     165840 | loss: 5.15975 | failed:  391
batch:     165850 | loss: 5.17716 | failed:  391
batch:     165860 | loss: 5.04551 | failed:  391
batch:     165870 | loss: 4.93066 | failed:  391
batch:     165880 | loss: 4.90155 | failed:  391
batch:     165890 | loss: 5.11485 | failed:  391
batch:     165900 | loss: 5.11419 | failed:  391
batch:     165910 | loss: 5.16388 | failed:  391
batch:     165920 | loss: 5.03802 | failed:  391
batch:     165930 | loss: 5.26661 | failed:  391
batch:     165940 | loss: 5.06987 | failed:  391
batch:     165950 | loss: 5.07149 | failed:  391
batch:     165960 | loss: 5.19979 | failed:  391
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     165980 | loss: 5.36079 | failed:  404
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
Traceback (most recent call last):
  File "train.py", line 27, in safe_pack_sequence
    return pack_sequence(x, enforce_sorted=False)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 412, in pack_sequence
    return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)
  File "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py", line 268, in pack_padded_sequence
    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0
batch:     166000 | loss: 5.08007 | failed:  414
batch:     166010 | loss: 5.08360 | failed:  414
batch:     166020 | loss: 5.22501 | failed:  414
batch:     166030 | loss: 5.26479 | failed:  414
batch:     166040 | loss: 4.93266 | failed:  414
batch:     166050 | loss: 5.15483 | failed:  414
batch:     166060 | loss: 5.08218 | failed:  414
batch:     166070 | loss: 5.16772 | failed:  414
batch:     166080 | loss: 5.17414 | failed:  414
batch:     166090 | loss: 4.95547 | failed:  414
batch:     166100 | loss: 5.22265 | failed:  414
batch:     166110 | loss: 4.88507 | failed:  414
batch:     166120 | loss: 5.17852 | failed:  414
batch:     166130 | loss: 5.24997 | failed:  414
batch:     166140 | loss: 5.14898 | failed:  414
batch:     166150 | loss: 5.08723 | failed:  414
batch:     166160 | loss: 5.14607 | failed:  414
batch:     166170 | loss: 4.97199 | failed:  414
batch:     166180 | loss: 5.10221 | failed:  414
batch:     166190 | loss: 5.14416 | failed:  414
batch:     166200 | loss: 5.20041 | failed:  414
batch:     166210 | loss: 5.21254 | failed:  414
batch:     166220 | loss: 5.26546 | failed:  414
batch:     166230 | loss: 5.04564 | failed:  414
batch:     166240 | loss: 5.08833 | failed:  414
batch:     166250 | loss: 5.22824 | failed:  414
batch:     166260 | loss: 5.12943 | failed:  414
batch:     166270 | loss: 4.99831 | failed:  414
batch:     166280 | loss: 5.04222 | failed:  414
batch:     166290 | loss: 5.01194 | failed:  414
batch:     166300 | loss: 5.11959 | failed:  414
batch:     166310 | loss: 4.99614 | failed:  414
batch:     166320 | loss: 4.88275 | failed:  414
batch:     166330 | loss: 4.93629 | failed:  414
batch:     166340 | loss: 4.20271 | failed:  414
batch:     166350 | loss: 5.13707 | failed:  414
batch:     166360 | loss: 5.17631 | failed:  414
batch:     166370 | loss: 5.18599 | failed:  414
batch:     166380 | loss: 5.07745 | failed:  414
batch:     166390 | loss: 5.09391 | failed:  414
batch:     166400 | loss: 5.16776 | failed:  414
batch:     166410 | loss: 5.14920 | failed:  414
batch:     166420 | loss: 5.10355 | failed:  414
batch:     166430 | loss: 5.15701 | failed:  414
batch:     166440 | loss: 5.12876 | failed:  414
batch:     166450 | loss: 5.06364 | failed:  414
batch:     166460 | loss: 5.17399 | failed:  414
batch:     166470 | loss: 5.10155 | failed:  414
batch:     166480 | loss: 5.16078 | failed:  414
batch:     166490 | loss: 5.16690 | failed:  414
batch:     166500 | loss: 5.16877 | failed:  414
batch:     166510 | loss: 5.18676 | failed:  414
batch:     166520 | loss: 5.22096 | failed:  414
batch:     166530 | loss: 5.11471 | failed:  414
batch:     166540 | loss: 5.06491 | failed:  414
batch:     166550 | loss: 5.10490 | failed:  414
batch:     166560 | loss: 4.95833 | failed:  414
batch:     166570 | loss: 5.24104 | failed:  414
batch:     166580 | loss: 5.08255 | failed:  414
batch:     166590 | loss: 5.03488 | failed:  414
batch:     166600 | loss: 5.11680 | failed:  414
batch:     166610 | loss: 4.91345 | failed:  414
batch:     166620 | loss: 5.16043 | failed:  414
batch:     166630 | loss: 5.18905 | failed:  414
batch:     166640 | loss: 5.28167 | failed:  414
batch:     166650 | loss: 5.13210 | failed:  414
batch:     166660 | loss: 5.24650 | failed:  414
batch:     166670 | loss: 5.17809 | failed:  414
batch:     166680 | loss: 5.13489 | failed:  414
batch:     166690 | loss: 5.21253 | failed:  414
batch:     166700 | loss: 5.01209 | failed:  414
batch:     166710 | loss: 5.07453 | failed:  414
batch:     166720 | loss: 5.18677 | failed:  414
batch:     166730 | loss: 5.12924 | failed:  414
batch:     166740 | loss: 4.90026 | failed:  414
batch:     166750 | loss: 5.24224 | failed:  414
batch:     166760 | loss: 5.06487 | failed:  414
batch:     166770 | loss: 5.21675 | failed:  414
batch:     166780 | loss: 5.23470 | failed:  414
batch:     166790 | loss: 5.07999 | failed:  414
batch:     166800 | loss: 5.17946 | failed:  414
batch:     166810 | loss: 5.18261 | failed:  414
batch:     166820 | loss: 5.00406 | failed:  414
batch:     166830 | loss: 5.15604 | failed:  414
batch:     166840 | loss: 5.17968 | failed:  414
batch:     166850 | loss: 5.11161 | failed:  414
batch:     166860 | loss: 4.99364 | failed:  414
batch:     166870 | loss: 5.09941 | failed:  414
batch:     166880 | loss: 5.21909 | failed:  414
batch:     166890 | loss: 5.31002 | failed:  414
batch:     166900 | loss: 5.09960 | failed:  414
batch:     166910 | loss: 5.25164 | failed:  414
batch:     166920 | loss: 5.23751 | failed:  414
batch:     166930 | loss: 5.29947 | failed:  414
batch:     166940 | loss: 5.25609 | failed:  414
batch:     166950 | loss: 5.19899 | failed:  414
batch:     166960 | loss: 5.20535 | failed:  414
batch:     166970 | loss: 5.13573 | failed:  414
batch:     166980 | loss: 5.16170 | failed:  414
batch:     166990 | loss: 5.13454 | failed:  414
batch:     167000 | loss: 5.20125 | failed:  414
batch:     167010 | loss: 5.11197 | failed:  414
batch:     167020 | loss: 5.14685 | failed:  414
batch:     167030 | loss: 5.11603 | failed:  414
batch:     167040 | loss: 5.19262 | failed:  414
batch:     167050 | loss: 5.11179 | failed:  414
batch:     167060 | loss: 5.10175 | failed:  414
batch:     167070 | loss: 5.07207 | failed:  414
batch:     167080 | loss: 4.76664 | failed:  414
batch:     167090 | loss: 4.92403 | failed:  414
batch:     167100 | loss: 5.14247 | failed:  414
batch:     167110 | loss: 5.10786 | failed:  414
batch:     167120 | loss: 5.21096 | failed:  414
batch:     167130 | loss: 5.10632 | failed:  414
batch:     167140 | loss: 5.01990 | failed:  414
batch:     167150 | loss: 5.17153 | failed:  414
batch:     167160 | loss: 5.15583 | failed:  414
batch:     167170 | loss: 5.09646 | failed:  414
batch:     167180 | loss: 5.17184 | failed:  414
batch:     167190 | loss: 5.20216 | failed:  414
batch:     167200 | loss: 5.18808 | failed:  414
batch:     167210 | loss: 5.12121 | failed:  414
batch:     167220 | loss: 5.17591 | failed:  414
batch:     167230 | loss: 5.07700 | failed:  414
batch:     167240 | loss: 5.15406 | failed:  414
batch:     167250 | loss: 5.06015 | failed:  414
batch:     167260 | loss: 5.06988 | failed:  414
batch:     167270 | loss: 5.05993 | failed:  414
batch:     167280 | loss: 5.13054 | failed:  414
batch:     167290 | loss: 5.11766 | failed:  414
batch:     167300 | loss: 5.03862 | failed:  414
batch:     167310 | loss: 5.16103 | failed:  414
batch:     167320 | loss: 5.23765 | failed:  414
batch:     167330 | loss: 5.04596 | failed:  414
batch:     167340 | loss: 3.29269 | failed:  414
batch:     167350 | loss: 4.94751 | failed:  414
batch:     167360 | loss: 5.00897 | failed:  414
batch:     167370 | loss: 5.01520 | failed:  414
batch:     167380 | loss: 5.01681 | failed:  414
batch:     167390 | loss: 4.99225 | failed:  414
batch:     167400 | loss: 4.85021 | failed:  414
batch:     167410 | loss: 5.03200 | failed:  414
batch:     167420 | loss: 5.02807 | failed:  414
batch:     167430 | loss: 5.09751 | failed:  414
batch:     167440 | loss: 5.12090 | failed:  414
batch:     167450 | loss: 5.02227 | failed:  414
batch:     167460 | loss: 5.08361 | failed:  414
batch:     167470 | loss: 5.16396 | failed:  414
batch:     167480 | loss: 4.86785 | failed:  414
batch:     167490 | loss: 5.10449 | failed:  414
batch:     167500 | loss: 5.18834 | failed:  414
batch:     167510 | loss: 5.20886 | failed:  414
batch:     167520 | loss: 5.05809 | failed:  414
batch:     167530 | loss: 4.92993 | failed:  414
batch:     167540 | loss: 5.23120 | failed:  414
batch:     167550 | loss: 5.21093 | failed:  414
batch:     167560 | loss: 5.14487 | failed:  414
batch:     167570 | loss: 5.13710 | failed:  414
batch:     167580 | loss: 5.02874 | failed:  414
batch:     167590 | loss: 4.67406 | failed:  414
batch:     167600 | loss: 5.16564 | failed:  414
batch:     167610 | loss: 5.20521 | failed:  414
batch:     167620 | loss: 5.10421 | failed:  414
batch:     167630 | loss: 5.16393 | failed:  414
batch:     167640 | loss: 5.11177 | failed:  414
batch:     167650 | loss: 5.08605 | failed:  414
batch:     167660 | loss: 4.91140 | failed:  414
batch:     167670 | loss: 5.10715 | failed:  414
batch:     167680 | loss: 5.16275 | failed:  414
batch:     167690 | loss: 5.16166 | failed:  414
batch:     167700 | loss: 5.19459 | failed:  414
batch:     167710 | loss: 5.10430 | failed:  414
batch:     167720 | loss: 5.12047 | failed:  414
batch:     167730 | loss: 5.14157 | failed:  414
batch:     167740 | loss: 5.07034 | failed:  414
batch:     167750 | loss: 5.20571 | failed:  414
batch:     167760 | loss: 5.18959 | failed:  414
batch:     167770 | loss: 5.21158 | failed:  414
batch:     167780 | loss: 5.13284 | failed:  414
batch:     167790 | loss: 5.09571 | failed:  414
batch:     167800 | loss: 5.23812 | failed:  414
batch:     167810 | loss: 5.09953 | failed:  414
batch:     167820 | loss: 5.10244 | failed:  414
batch:     167830 | loss: 5.21783 | failed:  414
batch:     167840 | loss: 5.14183 | failed:  414
batch:     167850 | loss: 5.16097 | failed:  414
batch:     167860 | loss: 5.10300 | failed:  414
batch:     167870 | loss: 5.08132 | failed:  414
batch:     167880 | loss: 5.06431 | failed:  414
batch:     167890 | loss: 4.94709 | failed:  414
batch:     167900 | loss: 5.03304 | failed:  414
batch:     167910 | loss: 5.02489 | failed:  414
batch:     167920 | loss: 4.96386 | failed:  414
batch:     167930 | loss: 5.17742 | failed:  414
batch:     167940 | loss: 5.22441 | failed:  414
batch:     167950 | loss: 5.15836 | failed:  414
batch:     167960 | loss: 5.14805 | failed:  414
batch:     167970 | loss: 5.15729 | failed:  414
batch:     167980 | loss: 5.10709 | failed:  414
batch:     167990 | loss: 5.15297 | failed:  414
batch:     168000 | loss: 5.08502 | failed:  414
batch:     168010 | loss: 5.09280 | failed:  414
batch:     168020 | loss: 5.08290 | failed:  414
batch:     168030 | loss: 5.08879 | failed:  414
batch:     168040 | loss: 4.98181 | failed:  414
batch:     168050 | loss: 5.03306 | failed:  414
batch:     168060 | loss: 5.11604 | failed:  414
batch:     168070 | loss: 5.11384 | failed:  414
batch:     168080 | loss: 4.98581 | failed:  414
batch:     168090 | loss: 5.03963 | failed:  414
batch:     168100 | loss: 5.10238 | failed:  414
batch:     168110 | loss: 5.12703 | failed:  414
batch:     168120 | loss: 5.23955 | failed:  414
batch:     168130 | loss: 5.03514 | failed:  414
batch:     168140 | loss: 5.14701 | failed:  414
batch:     168150 | loss: 5.18020 | failed:  414
batch:     168160 | loss: 5.07568 | failed:  414
batch:     168170 | loss: 5.09845 | failed:  414
batch:     168180 | loss: 5.03402 | failed:  414
batch:     168190 | loss: 5.15253 | failed:  414
batch:     168200 | loss: 4.92144 | failed:  414
batch:     168210 | loss: 5.14048 | failed:  414
batch:     168220 | loss: 5.11542 | failed:  414
batch:     168230 | loss: 5.05717 | failed:  414
batch:     168240 | loss: 5.10186 | failed:  414
batch:     168250 | loss: 5.15347 | failed:  414
batch:     168260 | loss: 5.10731 | failed:  414
batch:     168270 | loss: 5.16263 | failed:  414
batch:     168280 | loss: 5.03868 | failed:  414
batch:     168290 | loss: 5.01601 | failed:  414
batch:     168300 | loss: 4.90420 | failed:  414
batch:     168310 | loss: 5.06612 | failed:  414
batch:     168320 | loss: 4.99118 | failed:  414
batch:     168330 | loss: 5.18643 | failed:  414
batch:     168340 | loss: 4.99387 | failed:  414
batch:     168350 | loss: 4.94126 | failed:  414
batch:     168360 | loss: 5.20040 | failed:  414
batch:     168370 | loss: 5.22804 | failed:  414
batch:     168380 | loss: 5.09241 | failed:  414
batch:     168390 | loss: 5.12662 | failed:  414
batch:     168400 | loss: 5.16410 | failed:  414
batch:     168410 | loss: 5.14612 | failed:  414
batch:     168420 | loss: 5.20151 | failed:  414
batch:     168430 | loss: 5.26761 | failed:  414
batch:     168440 | loss: 5.14322 | failed:  414
batch:     168450 | loss: 4.90250 | failed:  414
batch:     168460 | loss: 5.17092 | failed:  414
batch:     168470 | loss: 5.05939 | failed:  414
batch:     168480 | loss: 5.19695 | failed:  414
batch:     168490 | loss: 5.19704 | failed:  414
batch:     168500 | loss: 5.08932 | failed:  414
batch:     168510 | loss: 4.94829 | failed:  414
batch:     168520 | loss: 5.14318 | failed:  414
batch:     168530 | loss: 5.12794 | failed:  414
batch:     168540 | loss: 5.01658 | failed:  414
batch:     168550 | loss: 4.95698 | failed:  414
batch:     168560 | loss: 5.14327 | failed:  414
batch:     168570 | loss: 5.18079 | failed:  414
batch:     168580 | loss: 5.15533 | failed:  414
batch:     168590 | loss: 5.16910 | failed:  414
batch:     168600 | loss: 5.12303 | failed:  414
batch:     168610 | loss: 5.13294 | failed:  414
batch:     168620 | loss: 5.14984 | failed:  414
batch:     168630 | loss: 5.02957 | failed:  414
batch:     168640 | loss: 5.14266 | failed:  414
batch:     168650 | loss: 5.18222 | failed:  414
batch:     168660 | loss: 5.03390 | failed:  414
batch:     168670 | loss: 5.17251 | failed:  414
batch:     168680 | loss: 5.12552 | failed:  414
batch:     168690 | loss: 5.19103 | failed:  414
batch:     168700 | loss: 5.10448 | failed:  414
batch:     168710 | loss: 5.19192 | failed:  414
batch:     168720 | loss: 5.10881 | failed:  414
batch:     168730 | loss: 5.23385 | failed:  414
batch:     168740 | loss: 5.03137 | failed:  414
batch:     168750 | loss: 5.20219 | failed:  414
batch:     168760 | loss: 4.88741 | failed:  414
batch:     168770 | loss: 5.14080 | failed:  414
batch:     168780 | loss: 5.14944 | failed:  414
batch:     168790 | loss: 5.09153 | failed:  414
batch:     168800 | loss: 5.19810 | failed:  414
batch:     168810 | loss: 5.07408 | failed:  414
batch:     168820 | loss: 5.17112 | failed:  414
batch:     168830 | loss: 5.16876 | failed:  414
batch:     168840 | loss: 5.14473 | failed:  414
batch:     168850 | loss: 5.08752 | failed:  414
batch:     168860 | loss: 5.08925 | failed:  414
batch:     168870 | loss: 5.13973 | failed:  414
batch:     168880 | loss: 5.05351 | failed:  414
batch:     168890 | loss: 5.14310 | failed:  414
batch:     168900 | loss: 5.11080 | failed:  414
batch:     168910 | loss: 4.92394 | failed:  414
batch:     168920 | loss: 4.94877 | failed:  414
batch:     168930 | loss: 5.28396 | failed:  414
batch:     168940 | loss: 5.17425 | failed:  414
batch:     168950 | loss: 5.11110 | failed:  414
batch:     168960 | loss: 4.98224 | failed:  414
batch:     168970 | loss: 5.14351 | failed:  414
batch:     168980 | loss: 5.14868 | failed:  414
batch:     168990 | loss: 5.15949 | failed:  414
batch:     169000 | loss: 4.95943 | failed:  414
batch:     169010 | loss: 4.83173 | failed:  414
batch:     169020 | loss: 5.18604 | failed:  414
batch:     169030 | loss: 4.98405 | failed:  414
batch:     169040 | loss: 4.99716 | failed:  414
batch:     169050 | loss: 5.11525 | failed:  414
batch:     169060 | loss: 4.82403 | failed:  414
batch:     169070 | loss: 5.09979 | failed:  414
batch:     169080 | loss: 5.00641 | failed:  414
batch:     169090 | loss: 5.23900 | failed:  414
batch:     169100 | loss: 5.09460 | failed:  414
batch:     169110 | loss: 5.13594 | failed:  414
batch:     169120 | loss: 5.09983 | failed:  414
batch:     169130 | loss: 5.21425 | failed:  414
batch:     169140 | loss: 5.15175 | failed:  414
batch:     169150 | loss: 5.07110 | failed:  414
batch:     169160 | loss: 5.13798 | failed:  414
batch:     169170 | loss: 5.14548 | failed:  414
batch:     169180 | loss: 5.15728 | failed:  414
batch:     169190 | loss: 5.08747 | failed:  414
batch:     169200 | loss: 5.12729 | failed:  414
batch:     169210 | loss: 5.15904 | failed:  414
batch:     169220 | loss: 5.08805 | failed:  414
batch:     169230 | loss: 5.16552 | failed:  414
batch:     169240 | loss: 5.13443 | failed:  414
batch:     169250 | loss: 5.05432 | failed:  414
batch:     169260 | loss: 5.08466 | failed:  414
batch:     169270 | loss: 5.22341 | failed:  414
batch:     169280 | loss: 5.12332 | failed:  414
batch:     169290 | loss: 5.29430 | failed:  414
batch:     169300 | loss: 5.11177 | failed:  414
batch:     169310 | loss: 5.05387 | failed:  414
batch:     169320 | loss: 5.06371 | failed:  414
batch:     169330 | loss: 5.11326 | failed:  414
batch:     169340 | loss: 4.98263 | failed:  414
batch:     169350 | loss: 4.92817 | failed:  414
batch:     169360 | loss: 4.93799 | failed:  414
batch:     169370 | loss: 5.16467 | failed:  414
batch:     169380 | loss: 5.21539 | failed:  414
batch:     169390 | loss: 5.14800 | failed:  414
batch:     169400 | loss: 5.07149 | failed:  414
batch:     169410 | loss: 5.11123 | failed:  414
batch:     169420 | loss: 5.10336 | failed:  414
batch:     169430 | loss: 5.16052 | failed:  414
batch:     169440 | loss: 5.40925 | failed:  414
batch:     169450 | loss: 5.02415 | failed:  414
batch:     169460 | loss: 5.17770 | failed:  414
batch:     169470 | loss: 5.20973 | failed:  414
batch:     169480 | loss: 5.24041 | failed:  414
batch:     169490 | loss: 5.21752 | failed:  414
batch:     169500 | loss: 5.13630 | failed:  414
batch:     169510 | loss: 5.17972 | failed:  414
Saving file at location : /home/jcjessecai/quickthoughts/checkpoints/FINAL_MODEL.pth
Finished Training!!!
